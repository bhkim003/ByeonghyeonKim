{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16958/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        smallest_now_T = 99999\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "                now_T = inputs.shape[1]\n",
    "                if epoch == 0 and now_T < smallest_now_T:\n",
    "                    smallest_now_T = now_T\n",
    "                    print(f'smallest_now_T updated: {smallest_now_T}')\n",
    "                now_time_steps = temporal_filter*TIME\n",
    "                if now_T < now_time_steps:\n",
    "                    # Î∂ÄÏ°±Ìïú timestep Í∞úÏàò\n",
    "                    diff = now_time_steps - now_T\n",
    "\n",
    "                    # ÎßàÏßÄÎßâ timestep Î≥µÏÇ¨ (shape: [B, 1, C, H, W])\n",
    "                    last_frame = inputs[:, -1:, :, :, :]\n",
    "\n",
    "                    # diffÎßåÌÅº repeatÌïòÏó¨ Ìå®Îî© Íµ¨ÏÑ±\n",
    "                    pad_frames = last_frame.repeat(1, diff, 1, 1, 1)\n",
    "\n",
    "                    # ÏõêÎ≥∏ + Ìå®Îî© Í≤∞Ìï©\n",
    "                    inputs = torch.cat([inputs, pad_frames], dim=1)\n",
    "                else:\n",
    "                    # start_idx = random.randint(0, now_T - now_time_steps)\n",
    "                    start_idx = random.choice(range(0, now_T - now_time_steps + 1, now_time_steps))\n",
    "                    # start_idx = random.choice([i for i in range(0, now_T - now_time_steps + 1, now_time_steps)])\n",
    "                    inputs = inputs[:, start_idx : start_idx + now_time_steps]\n",
    "                if dvs_clipping != 0:\n",
    "                    inputs[inputs<dvs_clipping] = 0.0\n",
    "                    inputs[inputs>=dvs_clipping] = 1.0\n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            smallest_now_T_val = 99999\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            now_T = inputs_val.shape[1]\n",
    "                            if epoch == 0 and now_T < smallest_now_T_val:\n",
    "                                smallest_now_T_val = now_T\n",
    "                                print(f'smallest_now_T_val updated: {smallest_now_T_val}')\n",
    "                            now_time_steps = temporal_filter*TIME\n",
    "\n",
    "                            if now_T < now_time_steps:\n",
    "                                # Î∂ÄÏ°±Ìïú timestep Í∞úÏàò\n",
    "                                diff = now_time_steps - now_T\n",
    "\n",
    "                                # ÎßàÏßÄÎßâ timestep Î≥µÏÇ¨ (shape: [B, 1, C, H, W])\n",
    "                                last_frame = inputs_val[:, -1:, :, :, :]\n",
    "\n",
    "                                # diffÎßåÌÅº repeatÌïòÏó¨ Ìå®Îî© Íµ¨ÏÑ±\n",
    "                                pad_frames = last_frame.repeat(1, diff, 1, 1, 1)\n",
    "\n",
    "                                # ÏõêÎ≥∏ + Ìå®Îî© Í≤∞Ìï©\n",
    "                                inputs_val = torch.cat([inputs_val, pad_frames], dim=1)\n",
    "                            else:\n",
    "                                pass\n",
    "                            \n",
    "                            start_idx = 0\n",
    "                            inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if dvs_clipping != 0:\n",
    "                                inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "                \n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    assert val_acc_best > 0.2\n",
    "                elif epoch > 10:\n",
    "                    assert val_acc_best > 0.4\n",
    "                elif epoch > 30:\n",
    "                    assert val_acc_best > 0.5\n",
    "                elif epoch > 100:\n",
    "                    assert val_acc_best > 0.8\n",
    "                elif epoch > 150:\n",
    "                    assert val_acc_best > 0.88\n",
    "                    \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"1\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 2871,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.25,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 1/512, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 200,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = -1, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[-9,-9],[-9,-9],[-8,-8]], \n",
    "# # 1w -11~-9\n",
    "# # 1b -11~ -7\n",
    "# # 2w -10~-8\n",
    "# # 2b -10~-8\n",
    "# # 3w -10\n",
    "# # 3b -10\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: pyz704uj\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 07rpr5tk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_204428-07rpr5tk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/07rpr5tk' target=\"_blank\">sandy-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/07rpr5tk' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/07rpr5tk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251117_204437_376', 'my_seed': 42, 'TIME': 5, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = df820968b21bc2412e6634ebdd407347\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 67\n",
      "fc layer 1 self.abs_max_out: 388.0\n",
      "lif layer 1 self.abs_max_v: 388.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 445.0\n",
      "lif layer 2 self.abs_max_v: 445.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 197.0\n",
      "lif layer 1 self.abs_max_v: 399.5\n",
      "fc layer 2 self.abs_max_out: 519.0\n",
      "lif layer 2 self.abs_max_v: 678.5\n",
      "lif layer 1 self.abs_max_v: 440.5\n",
      "lif layer 2 self.abs_max_v: 743.0\n",
      "fc layer 2 self.abs_max_out: 522.0\n",
      "lif layer 2 self.abs_max_v: 744.5\n",
      "fc layer 3 self.abs_max_out: 206.0\n",
      "lif layer 1 self.abs_max_v: 477.0\n",
      "lif layer 2 self.abs_max_v: 806.5\n",
      "smallest_now_T updated: 60\n",
      "fc layer 2 self.abs_max_out: 562.0\n",
      "fc layer 1 self.abs_max_out: 409.0\n",
      "lif layer 1 self.abs_max_v: 577.5\n",
      "fc layer 2 self.abs_max_out: 563.0\n",
      "lif layer 2 self.abs_max_v: 820.0\n",
      "lif layer 2 self.abs_max_v: 956.0\n",
      "lif layer 1 self.abs_max_v: 649.5\n",
      "fc layer 3 self.abs_max_out: 256.0\n",
      "smallest_now_T updated: 45\n",
      "fc layer 1 self.abs_max_out: 482.0\n",
      "fc layer 2 self.abs_max_out: 591.0\n",
      "lif layer 1 self.abs_max_v: 675.0\n",
      "fc layer 1 self.abs_max_out: 561.0\n",
      "fc layer 2 self.abs_max_out: 656.0\n",
      "fc layer 1 self.abs_max_out: 620.0\n",
      "lif layer 1 self.abs_max_v: 875.5\n",
      "lif layer 1 self.abs_max_v: 890.0\n",
      "lif layer 1 self.abs_max_v: 896.5\n",
      "lif layer 2 self.abs_max_v: 968.0\n",
      "fc layer 2 self.abs_max_out: 681.0\n",
      "lif layer 2 self.abs_max_v: 1022.5\n",
      "smallest_now_T updated: 37\n",
      "lif layer 1 self.abs_max_v: 934.0\n",
      "smallest_now_T updated: 34\n",
      "fc layer 3 self.abs_max_out: 266.0\n",
      "fc layer 3 self.abs_max_out: 323.0\n",
      "fc layer 3 self.abs_max_out: 333.0\n",
      "smallest_now_T updated: 30\n",
      "lif layer 2 self.abs_max_v: 1097.0\n",
      "fc layer 2 self.abs_max_out: 736.0\n",
      "fc layer 1 self.abs_max_out: 655.0\n",
      "lif layer 1 self.abs_max_v: 977.5\n",
      "fc layer 1 self.abs_max_out: 665.0\n",
      "lif layer 1 self.abs_max_v: 1019.0\n",
      "lif layer 1 self.abs_max_v: 1054.5\n",
      "smallest_now_T updated: 26\n",
      "smallest_now_T updated: 25\n",
      "fc layer 1 self.abs_max_out: 748.0\n",
      "lif layer 1 self.abs_max_v: 1196.0\n",
      "fc layer 3 self.abs_max_out: 341.0\n",
      "smallest_now_T_val updated: 62\n",
      "smallest_now_T_val updated: 51\n",
      "smallest_now_T_val updated: 50\n",
      "smallest_now_T_val updated: 49\n",
      "smallest_now_T_val updated: 40\n",
      "smallest_now_T_val updated: 25\n",
      "lif layer 2 self.abs_max_v: 1102.5\n",
      "fc layer 2 self.abs_max_out: 747.0\n",
      "fc layer 3 self.abs_max_out: 366.0\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  2.341877/  2.351346, val:  14.17%, val_best:  14.17%, tr:  15.32%, tr_best:  15.32%, epoch time: 46.23 seconds, 0.77 minutes\n",
      "layer   1  Sparsity: 82.3988%\n",
      "layer   2  Sparsity: 69.2824%\n",
      "layer   3  Sparsity: 71.5666%\n",
      "total_backward_count 4895 real_backward_count 4271  87.252%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 1153.0\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  2.339894/  2.351346, val:  14.17%, val_best:  14.17%, tr:  14.50%, tr_best:  15.32%, epoch time: 45.21 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 82.4084%\n",
      "layer   2  Sparsity: 69.2840%\n",
      "layer   3  Sparsity: 71.6031%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48798dfa0d344af3a79c1784ea6181d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>tr_acc</td><td>‚ñà‚ñÅ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>iter_acc</td><td>0.0</td></tr><tr><td>tr_acc</td><td>0.14505</td></tr><tr><td>tr_epoch_loss</td><td>2.33989</td></tr><tr><td>val_acc_best</td><td>0.14167</td></tr><tr><td>val_acc_now</td><td>0.14167</td></tr><tr><td>val_loss</td><td>2.35135</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sandy-sweep-1</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/07rpr5tk' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/07rpr5tk</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_204428-07rpr5tk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 07rpr5tk errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_16958/1036422853.py\", line 111, in hyper_iter\n",
      "    my_snn_system(\n",
      "  File \"/tmp/ipykernel_16958/2288660666.py\", line 949, in my_snn_system\n",
      "    assert val_acc_best > 0.2\n",
      "AssertionError\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 07rpr5tk errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_16958/1036422853.py\", line 111, in hyper_iter\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     my_snn_system(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_16958/2288660666.py\", line 949, in my_snn_system\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     assert val_acc_best > 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m AssertionError\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: csts8ug3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 75000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_204640-csts8ug3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/csts8ug3' target=\"_blank\">graceful-sweep-2</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/csts8ug3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/csts8ug3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251117_204649_136', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 25, 'dvs_duration': 75000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = d42a941a5ec61eeb71eeb0784ba56a39\n",
      "cache path doesn't exist\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 91\n",
      "fc layer 1 self.abs_max_out: 1057.0\n",
      "lif layer 1 self.abs_max_v: 1057.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 1065.0\n",
      "lif layer 1 self.abs_max_v: 1430.0\n",
      "fc layer 2 self.abs_max_out: 632.0\n",
      "lif layer 2 self.abs_max_v: 632.0\n",
      "lif layer 1 self.abs_max_v: 1497.0\n",
      "lif layer 2 self.abs_max_v: 664.5\n",
      "fc layer 1 self.abs_max_out: 1931.0\n",
      "lif layer 1 self.abs_max_v: 2155.0\n",
      "fc layer 1 self.abs_max_out: 2379.0\n",
      "lif layer 1 self.abs_max_v: 2379.0\n",
      "fc layer 2 self.abs_max_out: 816.0\n",
      "lif layer 2 self.abs_max_v: 1015.5\n",
      "fc layer 1 self.abs_max_out: 3392.0\n",
      "lif layer 1 self.abs_max_v: 3392.0\n",
      "fc layer 2 self.abs_max_out: 860.0\n",
      "lif layer 2 self.abs_max_v: 1270.0\n",
      "fc layer 3 self.abs_max_out: 128.0\n",
      "lif layer 2 self.abs_max_v: 1329.0\n",
      "fc layer 1 self.abs_max_out: 3489.0\n",
      "lif layer 1 self.abs_max_v: 3489.0\n",
      "fc layer 2 self.abs_max_out: 882.0\n",
      "lif layer 2 self.abs_max_v: 1363.5\n",
      "fc layer 2 self.abs_max_out: 913.0\n",
      "lif layer 2 self.abs_max_v: 1512.0\n",
      "smallest_now_T updated: 82\n",
      "fc layer 2 self.abs_max_out: 1129.0\n",
      "fc layer 3 self.abs_max_out: 233.0\n",
      "fc layer 1 self.abs_max_out: 5132.0\n",
      "lif layer 1 self.abs_max_v: 5132.0\n",
      "fc layer 2 self.abs_max_out: 1532.0\n",
      "lif layer 2 self.abs_max_v: 1577.0\n",
      "lif layer 2 self.abs_max_v: 1881.5\n",
      "fc layer 2 self.abs_max_out: 1629.0\n",
      "lif layer 2 self.abs_max_v: 2219.0\n",
      "fc layer 3 self.abs_max_out: 249.0\n",
      "fc layer 2 self.abs_max_out: 1728.0\n",
      "smallest_now_T updated: 61\n",
      "fc layer 3 self.abs_max_out: 340.0\n",
      "fc layer 2 self.abs_max_out: 1745.0\n",
      "lif layer 2 self.abs_max_v: 2397.0\n",
      "fc layer 3 self.abs_max_out: 432.0\n",
      "lif layer 2 self.abs_max_v: 2447.5\n",
      "fc layer 2 self.abs_max_out: 1786.0\n",
      "lif layer 2 self.abs_max_v: 2518.5\n",
      "lif layer 2 self.abs_max_v: 2524.5\n",
      "lif layer 2 self.abs_max_v: 2742.0\n",
      "lif layer 2 self.abs_max_v: 2775.0\n",
      "fc layer 2 self.abs_max_out: 1820.0\n",
      "lif layer 2 self.abs_max_v: 2843.5\n",
      "fc layer 1 self.abs_max_out: 5288.0\n",
      "lif layer 1 self.abs_max_v: 5288.0\n",
      "lif layer 2 self.abs_max_v: 2863.0\n",
      "fc layer 1 self.abs_max_out: 5821.0\n",
      "lif layer 1 self.abs_max_v: 5821.0\n",
      "fc layer 2 self.abs_max_out: 1867.0\n",
      "lif layer 2 self.abs_max_v: 3156.5\n",
      "lif layer 2 self.abs_max_v: 3239.5\n",
      "fc layer 2 self.abs_max_out: 1928.0\n",
      "fc layer 3 self.abs_max_out: 459.0\n",
      "fc layer 2 self.abs_max_out: 2065.0\n",
      "fc layer 2 self.abs_max_out: 2106.0\n",
      "fc layer 2 self.abs_max_out: 2110.0\n",
      "lif layer 2 self.abs_max_v: 3273.5\n",
      "lif layer 2 self.abs_max_v: 3319.0\n",
      "fc layer 3 self.abs_max_out: 476.0\n",
      "fc layer 2 self.abs_max_out: 2146.0\n",
      "fc layer 1 self.abs_max_out: 5840.0\n",
      "lif layer 1 self.abs_max_v: 5840.0\n",
      "lif layer 1 self.abs_max_v: 7136.0\n",
      "lif layer 1 self.abs_max_v: 7491.5\n",
      "fc layer 2 self.abs_max_out: 2161.0\n",
      "fc layer 2 self.abs_max_out: 2236.0\n",
      "fc layer 3 self.abs_max_out: 514.0\n",
      "fc layer 2 self.abs_max_out: 2289.0\n",
      "fc layer 2 self.abs_max_out: 2378.0\n",
      "fc layer 2 self.abs_max_out: 2595.0\n",
      "fc layer 3 self.abs_max_out: 631.0\n",
      "fc layer 3 self.abs_max_out: 896.0\n",
      "smallest_now_T updated: 51\n",
      "fc layer 1 self.abs_max_out: 6036.0\n",
      "lif layer 1 self.abs_max_v: 8310.5\n",
      "lif layer 1 self.abs_max_v: 8629.0\n",
      "fc layer 1 self.abs_max_out: 6987.0\n",
      "fc layer 1 self.abs_max_out: 7935.0\n",
      "fc layer 1 self.abs_max_out: 8321.0\n",
      "lif layer 1 self.abs_max_v: 9046.5\n",
      "fc layer 1 self.abs_max_out: 8830.0\n",
      "lif layer 1 self.abs_max_v: 9376.0\n",
      "fc layer 2 self.abs_max_out: 2610.0\n",
      "lif layer 2 self.abs_max_v: 3426.5\n",
      "smallest_now_T updated: 50\n",
      "lif layer 1 self.abs_max_v: 9968.5\n",
      "lif layer 1 self.abs_max_v: 10638.5\n",
      "lif layer 1 self.abs_max_v: 12042.5\n",
      "lif layer 2 self.abs_max_v: 3511.5\n",
      "lif layer 2 self.abs_max_v: 3564.0\n",
      "fc layer 2 self.abs_max_out: 2651.0\n",
      "lif layer 2 self.abs_max_v: 3631.5\n",
      "lif layer 2 self.abs_max_v: 3740.0\n",
      "lif layer 2 self.abs_max_v: 3851.0\n",
      "lif layer 2 self.abs_max_v: 3982.0\n",
      "fc layer 1 self.abs_max_out: 8914.0\n",
      "fc layer 1 self.abs_max_out: 9086.0\n",
      "lif layer 2 self.abs_max_v: 4010.5\n",
      "fc layer 2 self.abs_max_out: 2660.0\n",
      "fc layer 2 self.abs_max_out: 2888.0\n",
      "lif layer 2 self.abs_max_v: 4241.0\n",
      "lif layer 2 self.abs_max_v: 4358.5\n",
      "lif layer 2 self.abs_max_v: 4492.5\n",
      "lif layer 2 self.abs_max_v: 4559.5\n",
      "fc layer 2 self.abs_max_out: 3040.0\n",
      "lif layer 2 self.abs_max_v: 4725.5\n",
      "fc layer 2 self.abs_max_out: 3127.0\n",
      "lif layer 1 self.abs_max_v: 12820.5\n",
      "lif layer 1 self.abs_max_v: 13699.5\n",
      "fc layer 1 self.abs_max_out: 9272.0\n",
      "lif layer 1 self.abs_max_v: 14045.0\n",
      "fc layer 2 self.abs_max_out: 3175.0\n",
      "fc layer 1 self.abs_max_out: 9285.0\n",
      "lif layer 1 self.abs_max_v: 14823.5\n",
      "lif layer 1 self.abs_max_v: 15212.0\n",
      "fc layer 1 self.abs_max_out: 9492.0\n",
      "lif layer 1 self.abs_max_v: 15679.5\n",
      "fc layer 2 self.abs_max_out: 3273.0\n",
      "fc layer 1 self.abs_max_out: 9514.0\n",
      "lif layer 1 self.abs_max_v: 16055.0\n",
      "lif layer 1 self.abs_max_v: 16062.0\n",
      "lif layer 1 self.abs_max_v: 16735.5\n",
      "fc layer 2 self.abs_max_out: 3329.0\n",
      "fc layer 2 self.abs_max_out: 3349.0\n",
      "fc layer 1 self.abs_max_out: 9901.0\n",
      "lif layer 1 self.abs_max_v: 16761.0\n",
      "lif layer 1 self.abs_max_v: 16886.5\n",
      "lif layer 1 self.abs_max_v: 17294.5\n",
      "fc layer 1 self.abs_max_out: 10011.0\n",
      "fc layer 2 self.abs_max_out: 3376.0\n",
      "fc layer 1 self.abs_max_out: 10046.0\n",
      "fc layer 1 self.abs_max_out: 10573.0\n",
      "fc layer 2 self.abs_max_out: 3421.0\n",
      "fc layer 1 self.abs_max_out: 10960.0\n",
      "fc layer 1 self.abs_max_out: 11665.0\n",
      "lif layer 1 self.abs_max_v: 17515.5\n",
      "lif layer 1 self.abs_max_v: 17541.0\n",
      "lif layer 1 self.abs_max_v: 17691.5\n",
      "lif layer 1 self.abs_max_v: 18571.0\n",
      "lif layer 1 self.abs_max_v: 19602.0\n",
      "lif layer 1 self.abs_max_v: 19805.5\n",
      "lif layer 2 self.abs_max_v: 4788.5\n",
      "lif layer 1 self.abs_max_v: 20192.0\n",
      "lif layer 1 self.abs_max_v: 20457.0\n",
      "lif layer 2 self.abs_max_v: 4882.0\n",
      "lif layer 2 self.abs_max_v: 4948.0\n",
      "smallest_now_T_val updated: 84\n",
      "smallest_now_T_val updated: 69\n",
      "lif layer 1 self.abs_max_v: 20798.0\n",
      "smallest_now_T_val updated: 68\n",
      "smallest_now_T_val updated: 67\n",
      "smallest_now_T_val updated: 55\n",
      "smallest_now_T_val updated: 50\n",
      "fc layer 1 self.abs_max_out: 12364.0\n",
      "lif layer 1 self.abs_max_v: 21081.0\n",
      "fc layer 1 self.abs_max_out: 12613.0\n",
      "lif layer 1 self.abs_max_v: 21380.5\n",
      "fc layer 1 self.abs_max_out: 12669.0\n",
      "lif layer 1 self.abs_max_v: 22507.5\n",
      "lif layer 1 self.abs_max_v: 22985.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  2.120291/  2.198744, val:  38.33%, val_best:  38.33%, tr:  92.85%, tr_best:  92.85%, epoch time: 90.21 seconds, 1.50 minutes\n",
      "layer   1  Sparsity: 87.4745%\n",
      "layer   2  Sparsity: 85.8279%\n",
      "layer   3  Sparsity: 95.2102%\n",
      "total_backward_count 9790 real_backward_count 2530  25.843%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 12761.0\n",
      "lif layer 2 self.abs_max_v: 4960.5\n",
      "lif layer 2 self.abs_max_v: 5312.5\n",
      "fc layer 1 self.abs_max_out: 13086.0\n",
      "fc layer 3 self.abs_max_out: 939.0\n",
      "fc layer 1 self.abs_max_out: 13232.0\n",
      "lif layer 1 self.abs_max_v: 24083.0\n",
      "fc layer 1 self.abs_max_out: 13773.0\n",
      "fc layer 1 self.abs_max_out: 13815.0\n",
      "lif layer 1 self.abs_max_v: 25830.0\n",
      "fc layer 2 self.abs_max_out: 3500.0\n",
      "fc layer 1 self.abs_max_out: 15701.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  2.111198/  2.183777, val:  42.08%, val_best:  42.08%, tr:  92.34%, tr_best:  92.85%, epoch time: 85.98 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4754%\n",
      "layer   2  Sparsity: 87.9765%\n",
      "layer   3  Sparsity: 95.2772%\n",
      "total_backward_count 19580 real_backward_count 5091  26.001%\n",
      "fc layer 2 self.abs_max_out: 3548.0\n",
      "fc layer 2 self.abs_max_out: 3861.0\n",
      "fc layer 1 self.abs_max_out: 16454.0\n",
      "fc layer 2 self.abs_max_out: 4042.0\n",
      "fc layer 3 self.abs_max_out: 952.0\n",
      "lif layer 1 self.abs_max_v: 26383.5\n",
      "fc layer 3 self.abs_max_out: 995.0\n",
      "fc layer 3 self.abs_max_out: 1027.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  2.068735/  2.121819, val:  43.33%, val_best:  43.33%, tr:  93.05%, tr_best:  93.05%, epoch time: 86.61 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.4839%\n",
      "layer   2  Sparsity: 88.8850%\n",
      "layer   3  Sparsity: 94.0146%\n",
      "total_backward_count 29370 real_backward_count 7565  25.758%\n",
      "fc layer 3 self.abs_max_out: 1039.0\n",
      "fc layer 3 self.abs_max_out: 1094.0\n",
      "fc layer 3 self.abs_max_out: 1180.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  2.027097/  2.116114, val:  42.50%, val_best:  43.33%, tr:  92.34%, tr_best:  93.05%, epoch time: 86.87 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.4901%\n",
      "layer   2  Sparsity: 88.8682%\n",
      "layer   3  Sparsity: 92.6524%\n",
      "total_backward_count 39160 real_backward_count 9871  25.207%\n",
      "lif layer 2 self.abs_max_v: 5463.0\n",
      "lif layer 1 self.abs_max_v: 27243.0\n",
      "lif layer 1 self.abs_max_v: 27918.5\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.993567/  2.120122, val:  39.17%, val_best:  43.33%, tr:  92.34%, tr_best:  93.05%, epoch time: 86.68 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.4906%\n",
      "layer   2  Sparsity: 88.9050%\n",
      "layer   3  Sparsity: 91.7954%\n",
      "total_backward_count 48950 real_backward_count 12254  25.034%\n",
      "fc layer 3 self.abs_max_out: 1188.0\n",
      "fc layer 3 self.abs_max_out: 1201.0\n",
      "fc layer 3 self.abs_max_out: 1265.0\n",
      "fc layer 3 self.abs_max_out: 1275.0\n",
      "fc layer 3 self.abs_max_out: 1333.0\n",
      "fc layer 3 self.abs_max_out: 1339.0\n",
      "fc layer 3 self.abs_max_out: 1343.0\n",
      "fc layer 3 self.abs_max_out: 1351.0\n",
      "fc layer 3 self.abs_max_out: 1357.0\n",
      "lif layer 1 self.abs_max_v: 28594.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.945627/  2.041732, val:  46.67%, val_best:  46.67%, tr:  92.34%, tr_best:  93.05%, epoch time: 86.54 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.4774%\n",
      "layer   2  Sparsity: 88.9393%\n",
      "layer   3  Sparsity: 89.9219%\n",
      "total_backward_count 58740 real_backward_count 14538  24.750%\n",
      "lif layer 2 self.abs_max_v: 5579.5\n",
      "lif layer 2 self.abs_max_v: 5582.0\n",
      "lif layer 2 self.abs_max_v: 5620.5\n",
      "lif layer 2 self.abs_max_v: 5658.5\n",
      "lif layer 2 self.abs_max_v: 5707.5\n",
      "lif layer 2 self.abs_max_v: 5761.0\n",
      "fc layer 2 self.abs_max_out: 4238.0\n",
      "fc layer 1 self.abs_max_out: 17955.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.918882/  1.992556, val:  50.00%, val_best:  50.00%, tr:  94.89%, tr_best:  94.89%, epoch time: 85.82 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4799%\n",
      "layer   2  Sparsity: 88.7394%\n",
      "layer   3  Sparsity: 89.5068%\n",
      "total_backward_count 68530 real_backward_count 16639  24.280%\n",
      "lif layer 2 self.abs_max_v: 5799.5\n",
      "lif layer 2 self.abs_max_v: 5863.0\n",
      "fc layer 2 self.abs_max_out: 4303.0\n",
      "lif layer 1 self.abs_max_v: 28632.5\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.886855/  2.082665, val:  47.08%, val_best:  50.00%, tr:  96.63%, tr_best:  96.63%, epoch time: 85.50 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4921%\n",
      "layer   2  Sparsity: 88.5925%\n",
      "layer   3  Sparsity: 89.9167%\n",
      "total_backward_count 78320 real_backward_count 18570  23.710%\n",
      "lif layer 1 self.abs_max_v: 28801.5\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.928162/  2.032781, val:  57.50%, val_best:  57.50%, tr:  93.97%, tr_best:  96.63%, epoch time: 85.78 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4977%\n",
      "layer   2  Sparsity: 88.7393%\n",
      "layer   3  Sparsity: 90.2655%\n",
      "total_backward_count 88110 real_backward_count 20684  23.475%\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.890563/  2.072262, val:  42.08%, val_best:  57.50%, tr:  95.61%, tr_best:  96.63%, epoch time: 86.01 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4801%\n",
      "layer   2  Sparsity: 88.6109%\n",
      "layer   3  Sparsity: 89.6991%\n",
      "total_backward_count 97900 real_backward_count 22609  23.094%\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.914498/  2.054312, val:  40.83%, val_best:  57.50%, tr:  95.10%, tr_best:  96.63%, epoch time: 85.91 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4823%\n",
      "layer   2  Sparsity: 88.3256%\n",
      "layer   3  Sparsity: 89.8981%\n",
      "total_backward_count 107690 real_backward_count 24590  22.834%\n",
      "fc layer 3 self.abs_max_out: 1410.0\n",
      "lif layer 2 self.abs_max_v: 6051.5\n",
      "lif layer 2 self.abs_max_v: 6100.0\n",
      "fc layer 3 self.abs_max_out: 1455.0\n",
      "fc layer 3 self.abs_max_out: 1460.0\n",
      "fc layer 3 self.abs_max_out: 1464.0\n",
      "fc layer 3 self.abs_max_out: 1568.0\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.860248/  1.996859, val:  47.92%, val_best:  57.50%, tr:  96.83%, tr_best:  96.83%, epoch time: 86.11 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.4859%\n",
      "layer   2  Sparsity: 87.5485%\n",
      "layer   3  Sparsity: 87.7595%\n",
      "total_backward_count 117480 real_backward_count 26375  22.451%\n",
      "lif layer 2 self.abs_max_v: 6209.0\n",
      "lif layer 2 self.abs_max_v: 6249.0\n",
      "lif layer 2 self.abs_max_v: 6273.0\n",
      "lif layer 2 self.abs_max_v: 6552.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.881644/  2.095606, val:  49.17%, val_best:  57.50%, tr:  96.12%, tr_best:  96.83%, epoch time: 86.12 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.4931%\n",
      "layer   2  Sparsity: 88.4722%\n",
      "layer   3  Sparsity: 89.6083%\n",
      "total_backward_count 127270 real_backward_count 28228  22.180%\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.896914/  2.043310, val:  45.00%, val_best:  57.50%, tr:  96.53%, tr_best:  96.83%, epoch time: 86.01 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4936%\n",
      "layer   2  Sparsity: 88.0137%\n",
      "layer   3  Sparsity: 89.2761%\n",
      "total_backward_count 137060 real_backward_count 30054  21.928%\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.880190/  2.046736, val:  45.42%, val_best:  57.50%, tr:  97.24%, tr_best:  97.24%, epoch time: 86.24 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.4563%\n",
      "layer   2  Sparsity: 87.4526%\n",
      "layer   3  Sparsity: 89.5420%\n",
      "total_backward_count 146850 real_backward_count 31703  21.589%\n",
      "lif layer 2 self.abs_max_v: 6784.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.925974/  2.055835, val:  57.08%, val_best:  57.50%, tr:  96.32%, tr_best:  97.24%, epoch time: 86.31 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.4783%\n",
      "layer   2  Sparsity: 88.0411%\n",
      "layer   3  Sparsity: 90.6159%\n",
      "total_backward_count 156640 real_backward_count 33570  21.431%\n",
      "lif layer 1 self.abs_max_v: 29250.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.890272/  2.041440, val:  51.67%, val_best:  57.50%, tr:  97.85%, tr_best:  97.85%, epoch time: 86.10 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4813%\n",
      "layer   2  Sparsity: 87.1678%\n",
      "layer   3  Sparsity: 89.3531%\n",
      "total_backward_count 166430 real_backward_count 35077  21.076%\n",
      "lif layer 2 self.abs_max_v: 6790.5\n",
      "lif layer 2 self.abs_max_v: 6951.0\n",
      "lif layer 2 self.abs_max_v: 7090.5\n",
      "lif layer 2 self.abs_max_v: 7225.0\n",
      "lif layer 2 self.abs_max_v: 7249.5\n",
      "fc layer 1 self.abs_max_out: 18123.0\n",
      "lif layer 1 self.abs_max_v: 29951.5\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.955232/  2.098052, val:  52.08%, val_best:  57.50%, tr:  98.06%, tr_best:  98.06%, epoch time: 85.87 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4877%\n",
      "layer   2  Sparsity: 87.5692%\n",
      "layer   3  Sparsity: 90.4491%\n",
      "total_backward_count 176220 real_backward_count 36805  20.886%\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.948446/  2.044631, val:  54.58%, val_best:  57.50%, tr:  96.32%, tr_best:  98.06%, epoch time: 85.79 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4884%\n",
      "layer   2  Sparsity: 87.5080%\n",
      "layer   3  Sparsity: 90.5732%\n",
      "total_backward_count 186010 real_backward_count 38577  20.739%\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.920472/  2.090010, val:  32.08%, val_best:  57.50%, tr:  97.85%, tr_best:  98.06%, epoch time: 85.72 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4694%\n",
      "layer   2  Sparsity: 87.4078%\n",
      "layer   3  Sparsity: 89.7752%\n",
      "total_backward_count 195800 real_backward_count 40140  20.501%\n",
      "fc layer 3 self.abs_max_out: 1609.0\n",
      "lif layer 2 self.abs_max_v: 7477.0\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.900372/  2.025727, val:  47.08%, val_best:  57.50%, tr:  98.37%, tr_best:  98.37%, epoch time: 86.19 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.4846%\n",
      "layer   2  Sparsity: 86.9713%\n",
      "layer   3  Sparsity: 89.0463%\n",
      "total_backward_count 205590 real_backward_count 41664  20.266%\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.933511/  2.126506, val:  41.25%, val_best:  57.50%, tr:  97.24%, tr_best:  98.37%, epoch time: 86.65 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.4868%\n",
      "layer   2  Sparsity: 87.7452%\n",
      "layer   3  Sparsity: 90.8300%\n",
      "total_backward_count 215380 real_backward_count 43412  20.156%\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.961129/  2.078113, val:  52.50%, val_best:  57.50%, tr:  97.65%, tr_best:  98.37%, epoch time: 86.02 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.5048%\n",
      "layer   2  Sparsity: 87.4945%\n",
      "layer   3  Sparsity: 90.7599%\n",
      "total_backward_count 225170 real_backward_count 45121  20.039%\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.915246/  2.076054, val:  49.58%, val_best:  57.50%, tr:  96.42%, tr_best:  98.37%, epoch time: 85.36 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4940%\n",
      "layer   2  Sparsity: 88.3999%\n",
      "layer   3  Sparsity: 89.8376%\n",
      "total_backward_count 234960 real_backward_count 46838  19.934%\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.913010/  2.048861, val:  56.25%, val_best:  57.50%, tr:  97.55%, tr_best:  98.37%, epoch time: 85.22 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4770%\n",
      "layer   2  Sparsity: 87.9328%\n",
      "layer   3  Sparsity: 89.9056%\n",
      "total_backward_count 244750 real_backward_count 48460  19.800%\n",
      "lif layer 2 self.abs_max_v: 7623.5\n",
      "lif layer 2 self.abs_max_v: 7727.5\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.857047/  2.038572, val:  53.33%, val_best:  57.50%, tr:  97.96%, tr_best:  98.37%, epoch time: 86.42 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.4893%\n",
      "layer   2  Sparsity: 87.2652%\n",
      "layer   3  Sparsity: 88.5425%\n",
      "total_backward_count 254540 real_backward_count 49961  19.628%\n",
      "lif layer 2 self.abs_max_v: 8002.0\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.844575/  2.049030, val:  53.33%, val_best:  57.50%, tr:  98.57%, tr_best:  98.57%, epoch time: 86.71 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.4739%\n",
      "layer   2  Sparsity: 87.2169%\n",
      "layer   3  Sparsity: 88.4429%\n",
      "total_backward_count 264330 real_backward_count 51388  19.441%\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.834445/  2.008389, val:  58.75%, val_best:  58.75%, tr:  98.88%, tr_best:  98.88%, epoch time: 85.52 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4855%\n",
      "layer   2  Sparsity: 86.7150%\n",
      "layer   3  Sparsity: 88.7675%\n",
      "total_backward_count 274120 real_backward_count 52852  19.281%\n",
      "fc layer 2 self.abs_max_out: 4487.0\n",
      "fc layer 2 self.abs_max_out: 4791.0\n",
      "lif layer 2 self.abs_max_v: 8653.5\n",
      "lif layer 2 self.abs_max_v: 8875.0\n",
      "lif layer 2 self.abs_max_v: 9011.5\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  1.856165/  2.019631, val:  47.92%, val_best:  58.75%, tr:  98.67%, tr_best:  98.88%, epoch time: 85.96 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4795%\n",
      "layer   2  Sparsity: 86.9739%\n",
      "layer   3  Sparsity: 89.1897%\n",
      "total_backward_count 283910 real_backward_count 54342  19.141%\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.871999/  2.026380, val:  49.58%, val_best:  58.75%, tr:  98.57%, tr_best:  98.88%, epoch time: 84.99 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4565%\n",
      "layer   2  Sparsity: 87.1096%\n",
      "layer   3  Sparsity: 89.1096%\n",
      "total_backward_count 293700 real_backward_count 55860  19.019%\n",
      "lif layer 1 self.abs_max_v: 30244.0\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.882728/  2.065021, val:  57.92%, val_best:  58.75%, tr:  97.65%, tr_best:  98.88%, epoch time: 85.58 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4846%\n",
      "layer   2  Sparsity: 87.2058%\n",
      "layer   3  Sparsity: 89.3876%\n",
      "total_backward_count 303490 real_backward_count 57394  18.911%\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.880953/  2.060819, val:  43.75%, val_best:  58.75%, tr:  97.96%, tr_best:  98.88%, epoch time: 85.19 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4813%\n",
      "layer   2  Sparsity: 86.8407%\n",
      "layer   3  Sparsity: 89.6370%\n",
      "total_backward_count 313280 real_backward_count 58920  18.807%\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  1.889066/  2.068682, val:  56.25%, val_best:  58.75%, tr:  97.85%, tr_best:  98.88%, epoch time: 85.08 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4758%\n",
      "layer   2  Sparsity: 86.6740%\n",
      "layer   3  Sparsity: 90.1324%\n",
      "total_backward_count 323070 real_backward_count 60400  18.696%\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  1.915944/  2.035930, val:  57.92%, val_best:  58.75%, tr:  97.34%, tr_best:  98.88%, epoch time: 85.18 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4886%\n",
      "layer   2  Sparsity: 87.2458%\n",
      "layer   3  Sparsity: 90.5110%\n",
      "total_backward_count 332860 real_backward_count 62029  18.635%\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  1.883109/  2.042131, val:  56.67%, val_best:  58.75%, tr:  98.37%, tr_best:  98.88%, epoch time: 84.89 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 87.4837%\n",
      "layer   2  Sparsity: 87.4991%\n",
      "layer   3  Sparsity: 89.8968%\n",
      "total_backward_count 342650 real_backward_count 63604  18.562%\n",
      "fc layer 1 self.abs_max_out: 18461.0\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  1.852724/  2.009385, val:  50.42%, val_best:  58.75%, tr:  98.88%, tr_best:  98.88%, epoch time: 85.34 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4995%\n",
      "layer   2  Sparsity: 86.7591%\n",
      "layer   3  Sparsity: 89.8932%\n",
      "total_backward_count 352440 real_backward_count 65069  18.462%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  1.871186/  2.022820, val:  60.00%, val_best:  60.00%, tr:  98.88%, tr_best:  98.88%, epoch time: 85.67 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4816%\n",
      "layer   2  Sparsity: 87.1019%\n",
      "layer   3  Sparsity: 89.9937%\n",
      "total_backward_count 362230 real_backward_count 66507  18.360%\n",
      "fc layer 3 self.abs_max_out: 1648.0\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  1.904358/  2.060214, val:  57.08%, val_best:  60.00%, tr:  97.96%, tr_best:  98.88%, epoch time: 86.17 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.4871%\n",
      "layer   2  Sparsity: 87.4093%\n",
      "layer   3  Sparsity: 90.1352%\n",
      "total_backward_count 372020 real_backward_count 67958  18.267%\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  1.891830/  2.063887, val:  54.17%, val_best:  60.00%, tr:  98.37%, tr_best:  98.88%, epoch time: 85.72 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4831%\n",
      "layer   2  Sparsity: 87.3636%\n",
      "layer   3  Sparsity: 89.8969%\n",
      "total_backward_count 381810 real_backward_count 69545  18.215%\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  1.902838/  2.044201, val:  54.58%, val_best:  60.00%, tr:  97.55%, tr_best:  98.88%, epoch time: 85.45 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4961%\n",
      "layer   2  Sparsity: 87.4331%\n",
      "layer   3  Sparsity: 90.0495%\n",
      "total_backward_count 391600 real_backward_count 71098  18.156%\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  1.905560/  2.067433, val:  53.75%, val_best:  60.00%, tr:  98.37%, tr_best:  98.88%, epoch time: 85.31 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4830%\n",
      "layer   2  Sparsity: 87.4410%\n",
      "layer   3  Sparsity: 90.3179%\n",
      "total_backward_count 401390 real_backward_count 72639  18.097%\n",
      "lif layer 1 self.abs_max_v: 30997.0\n",
      "lif layer 1 self.abs_max_v: 32269.5\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  1.879307/  2.038808, val:  53.33%, val_best:  60.00%, tr:  97.85%, tr_best:  98.88%, epoch time: 86.53 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.5031%\n",
      "layer   2  Sparsity: 87.8759%\n",
      "layer   3  Sparsity: 89.7072%\n",
      "total_backward_count 411180 real_backward_count 74163  18.037%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  1.871759/  2.019803, val:  49.58%, val_best:  60.00%, tr:  98.06%, tr_best:  98.88%, epoch time: 86.25 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.4726%\n",
      "layer   2  Sparsity: 87.3680%\n",
      "layer   3  Sparsity: 89.3075%\n",
      "total_backward_count 420970 real_backward_count 75704  17.983%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  1.894526/  2.030484, val:  57.08%, val_best:  60.00%, tr:  97.55%, tr_best:  98.88%, epoch time: 85.70 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4917%\n",
      "layer   2  Sparsity: 87.9503%\n",
      "layer   3  Sparsity: 89.7307%\n",
      "total_backward_count 430760 real_backward_count 77320  17.950%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  1.906504/  2.047378, val:  49.17%, val_best:  60.00%, tr:  98.26%, tr_best:  98.88%, epoch time: 85.36 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4796%\n",
      "layer   2  Sparsity: 88.0230%\n",
      "layer   3  Sparsity: 89.7249%\n",
      "total_backward_count 440550 real_backward_count 78809  17.889%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  1.937138/  2.080940, val:  46.67%, val_best:  60.00%, tr:  96.83%, tr_best:  98.88%, epoch time: 85.35 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4655%\n",
      "layer   2  Sparsity: 88.2547%\n",
      "layer   3  Sparsity: 90.6162%\n",
      "total_backward_count 450340 real_backward_count 80533  17.883%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  1.920262/  2.019079, val:  55.00%, val_best:  60.00%, tr:  97.24%, tr_best:  98.88%, epoch time: 86.30 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.4824%\n",
      "layer   2  Sparsity: 87.7764%\n",
      "layer   3  Sparsity: 90.1445%\n",
      "total_backward_count 460130 real_backward_count 82104  17.844%\n",
      "fc layer 3 self.abs_max_out: 1651.0\n",
      "lif layer 2 self.abs_max_v: 9080.5\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  1.843240/  2.037935, val:  50.83%, val_best:  60.00%, tr:  99.08%, tr_best:  99.08%, epoch time: 85.74 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4817%\n",
      "layer   2  Sparsity: 87.3071%\n",
      "layer   3  Sparsity: 88.6293%\n",
      "total_backward_count 469920 real_backward_count 83541  17.778%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  1.859367/  2.062595, val:  55.42%, val_best:  60.00%, tr:  97.96%, tr_best:  99.08%, epoch time: 86.08 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4825%\n",
      "layer   2  Sparsity: 87.6240%\n",
      "layer   3  Sparsity: 89.4805%\n",
      "total_backward_count 479710 real_backward_count 84971  17.713%\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  1.868170/  2.025104, val:  55.00%, val_best:  60.00%, tr:  98.98%, tr_best:  99.08%, epoch time: 85.86 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.5023%\n",
      "layer   2  Sparsity: 87.5439%\n",
      "layer   3  Sparsity: 89.4438%\n",
      "total_backward_count 489500 real_backward_count 86506  17.672%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  1.895058/  2.035985, val:  51.67%, val_best:  60.00%, tr:  98.37%, tr_best:  99.08%, epoch time: 85.92 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4809%\n",
      "layer   2  Sparsity: 87.3998%\n",
      "layer   3  Sparsity: 89.4887%\n",
      "total_backward_count 499290 real_backward_count 88077  17.640%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  1.887973/  2.014240, val:  58.75%, val_best:  60.00%, tr:  98.47%, tr_best:  99.08%, epoch time: 85.50 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4935%\n",
      "layer   2  Sparsity: 87.2972%\n",
      "layer   3  Sparsity: 89.3582%\n",
      "total_backward_count 509080 real_backward_count 89519  17.584%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  1.907700/  2.070398, val:  54.17%, val_best:  60.00%, tr:  98.06%, tr_best:  99.08%, epoch time: 85.94 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4952%\n",
      "layer   2  Sparsity: 86.9296%\n",
      "layer   3  Sparsity: 89.9203%\n",
      "total_backward_count 518870 real_backward_count 91005  17.539%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  1.914367/  2.057451, val:  57.50%, val_best:  60.00%, tr:  97.34%, tr_best:  99.08%, epoch time: 85.72 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4667%\n",
      "layer   2  Sparsity: 87.6645%\n",
      "layer   3  Sparsity: 90.5390%\n",
      "total_backward_count 528660 real_backward_count 92507  17.498%\n",
      "fc layer 3 self.abs_max_out: 1664.0\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  1.917784/  2.020699, val:  60.42%, val_best:  60.42%, tr:  96.94%, tr_best:  99.08%, epoch time: 85.89 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4968%\n",
      "layer   2  Sparsity: 87.5998%\n",
      "layer   3  Sparsity: 90.1235%\n",
      "total_backward_count 538450 real_backward_count 94115  17.479%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  1.917825/  2.036172, val:  62.50%, val_best:  62.50%, tr:  98.47%, tr_best:  99.08%, epoch time: 84.99 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4882%\n",
      "layer   2  Sparsity: 87.4368%\n",
      "layer   3  Sparsity: 90.0811%\n",
      "total_backward_count 548240 real_backward_count 95629  17.443%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  1.878446/  2.010761, val:  50.42%, val_best:  62.50%, tr:  98.47%, tr_best:  99.08%, epoch time: 85.22 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4672%\n",
      "layer   2  Sparsity: 86.9758%\n",
      "layer   3  Sparsity: 89.2505%\n",
      "total_backward_count 558030 real_backward_count 97052  17.392%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  1.880991/  2.026272, val:  60.83%, val_best:  62.50%, tr:  98.06%, tr_best:  99.08%, epoch time: 86.01 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4815%\n",
      "layer   2  Sparsity: 87.5157%\n",
      "layer   3  Sparsity: 89.3386%\n",
      "total_backward_count 567820 real_backward_count 98541  17.354%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  1.900781/  2.037870, val:  54.17%, val_best:  62.50%, tr:  97.24%, tr_best:  99.08%, epoch time: 86.18 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.4780%\n",
      "layer   2  Sparsity: 87.4188%\n",
      "layer   3  Sparsity: 90.3675%\n",
      "total_backward_count 577610 real_backward_count 100132  17.336%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  1.879587/  2.032340, val:  54.58%, val_best:  62.50%, tr:  97.75%, tr_best:  99.08%, epoch time: 85.44 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.5099%\n",
      "layer   2  Sparsity: 87.2354%\n",
      "layer   3  Sparsity: 89.5751%\n",
      "total_backward_count 587400 real_backward_count 101593  17.295%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  1.878397/  2.044285, val:  52.92%, val_best:  62.50%, tr:  98.88%, tr_best:  99.08%, epoch time: 85.33 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4835%\n",
      "layer   2  Sparsity: 87.3631%\n",
      "layer   3  Sparsity: 89.4197%\n",
      "total_backward_count 597190 real_backward_count 102977  17.244%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  1.881536/  2.032748, val:  50.00%, val_best:  62.50%, tr:  98.47%, tr_best:  99.08%, epoch time: 85.13 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4688%\n",
      "layer   2  Sparsity: 87.3411%\n",
      "layer   3  Sparsity: 89.8113%\n",
      "total_backward_count 606980 real_backward_count 104418  17.203%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  1.903298/  2.039833, val:  59.58%, val_best:  62.50%, tr:  97.85%, tr_best:  99.08%, epoch time: 85.52 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4984%\n",
      "layer   2  Sparsity: 87.8961%\n",
      "layer   3  Sparsity: 90.6692%\n",
      "total_backward_count 616770 real_backward_count 105927  17.174%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  1.871415/  2.049562, val:  62.92%, val_best:  62.92%, tr:  98.06%, tr_best:  99.08%, epoch time: 85.70 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4763%\n",
      "layer   2  Sparsity: 88.0507%\n",
      "layer   3  Sparsity: 90.2923%\n",
      "total_backward_count 626560 real_backward_count 107409  17.143%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  1.897990/  2.080786, val:  50.00%, val_best:  62.92%, tr:  97.45%, tr_best:  99.08%, epoch time: 85.93 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4787%\n",
      "layer   2  Sparsity: 87.6817%\n",
      "layer   3  Sparsity: 90.7925%\n",
      "total_backward_count 636350 real_backward_count 108925  17.117%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  1.886848/  2.045819, val:  52.08%, val_best:  62.92%, tr:  97.85%, tr_best:  99.08%, epoch time: 85.62 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4808%\n",
      "layer   2  Sparsity: 87.4866%\n",
      "layer   3  Sparsity: 89.6088%\n",
      "total_backward_count 646140 real_backward_count 110321  17.074%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  1.894791/  2.060907, val:  46.67%, val_best:  62.92%, tr:  98.26%, tr_best:  99.08%, epoch time: 85.61 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4683%\n",
      "layer   2  Sparsity: 87.2873%\n",
      "layer   3  Sparsity: 89.9000%\n",
      "total_backward_count 655930 real_backward_count 111801  17.045%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  1.899511/  2.058826, val:  51.25%, val_best:  62.92%, tr:  98.16%, tr_best:  99.08%, epoch time: 85.73 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4755%\n",
      "layer   2  Sparsity: 87.9989%\n",
      "layer   3  Sparsity: 90.3413%\n",
      "total_backward_count 665720 real_backward_count 113276  17.016%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  1.909477/  2.039402, val:  54.17%, val_best:  62.92%, tr:  98.26%, tr_best:  99.08%, epoch time: 86.21 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.4848%\n",
      "layer   2  Sparsity: 87.7240%\n",
      "layer   3  Sparsity: 90.6243%\n",
      "total_backward_count 675510 real_backward_count 114793  16.994%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  1.896509/  2.082330, val:  52.92%, val_best:  62.92%, tr:  97.45%, tr_best:  99.08%, epoch time: 86.52 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.4826%\n",
      "layer   2  Sparsity: 87.8046%\n",
      "layer   3  Sparsity: 90.4894%\n",
      "total_backward_count 685300 real_backward_count 116258  16.965%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  1.901385/  2.037486, val:  57.08%, val_best:  62.92%, tr:  95.30%, tr_best:  99.08%, epoch time: 85.55 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4933%\n",
      "layer   2  Sparsity: 87.8371%\n",
      "layer   3  Sparsity: 90.3854%\n",
      "total_backward_count 695090 real_backward_count 117964  16.971%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  1.890659/  2.097595, val:  55.00%, val_best:  62.92%, tr:  97.45%, tr_best:  99.08%, epoch time: 85.59 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4892%\n",
      "layer   2  Sparsity: 88.3022%\n",
      "layer   3  Sparsity: 90.5253%\n",
      "total_backward_count 704880 real_backward_count 119464  16.948%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  1.879507/  2.067780, val:  57.08%, val_best:  62.92%, tr:  98.47%, tr_best:  99.08%, epoch time: 86.11 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.4808%\n",
      "layer   2  Sparsity: 88.2038%\n",
      "layer   3  Sparsity: 89.5288%\n",
      "total_backward_count 714670 real_backward_count 120934  16.922%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  1.879375/  2.074514, val:  50.00%, val_best:  62.92%, tr:  98.26%, tr_best:  99.08%, epoch time: 85.30 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4896%\n",
      "layer   2  Sparsity: 88.1960%\n",
      "layer   3  Sparsity: 89.6589%\n",
      "total_backward_count 724460 real_backward_count 122424  16.899%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  1.894622/  2.039878, val:  47.08%, val_best:  62.92%, tr:  97.55%, tr_best:  99.08%, epoch time: 85.64 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4885%\n",
      "layer   2  Sparsity: 88.0686%\n",
      "layer   3  Sparsity: 89.7372%\n",
      "total_backward_count 734250 real_backward_count 123962  16.883%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  1.891201/  2.018396, val:  61.67%, val_best:  62.92%, tr:  97.65%, tr_best:  99.08%, epoch time: 85.23 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4794%\n",
      "layer   2  Sparsity: 87.9501%\n",
      "layer   3  Sparsity: 89.6436%\n",
      "total_backward_count 744040 real_backward_count 125397  16.854%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  1.880965/  2.033928, val:  65.83%, val_best:  65.83%, tr:  97.85%, tr_best:  99.08%, epoch time: 85.72 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4803%\n",
      "layer   2  Sparsity: 88.0359%\n",
      "layer   3  Sparsity: 89.6521%\n",
      "total_backward_count 753830 real_backward_count 126943  16.840%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  1.909998/  2.093946, val:  42.50%, val_best:  65.83%, tr:  97.85%, tr_best:  99.08%, epoch time: 85.39 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4787%\n",
      "layer   2  Sparsity: 87.7119%\n",
      "layer   3  Sparsity: 90.0160%\n",
      "total_backward_count 763620 real_backward_count 128445  16.821%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  1.923226/  2.043322, val:  41.67%, val_best:  65.83%, tr:  97.24%, tr_best:  99.08%, epoch time: 84.41 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 87.4914%\n",
      "layer   2  Sparsity: 87.6215%\n",
      "layer   3  Sparsity: 90.0180%\n",
      "total_backward_count 773410 real_backward_count 130032  16.813%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  1.897692/  2.035681, val:  51.67%, val_best:  65.83%, tr:  97.65%, tr_best:  99.08%, epoch time: 84.67 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 87.4750%\n",
      "layer   2  Sparsity: 87.2878%\n",
      "layer   3  Sparsity: 89.6755%\n",
      "total_backward_count 783200 real_backward_count 131529  16.794%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  1.895488/  2.004862, val:  57.08%, val_best:  65.83%, tr:  98.37%, tr_best:  99.08%, epoch time: 85.05 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4727%\n",
      "layer   2  Sparsity: 87.6003%\n",
      "layer   3  Sparsity: 89.5769%\n",
      "total_backward_count 792990 real_backward_count 132956  16.766%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  1.894078/  2.058863, val:  47.08%, val_best:  65.83%, tr:  97.45%, tr_best:  99.08%, epoch time: 82.85 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 87.4892%\n",
      "layer   2  Sparsity: 87.6057%\n",
      "layer   3  Sparsity: 89.8398%\n",
      "total_backward_count 802780 real_backward_count 134513  16.756%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  1.873398/  2.055851, val:  54.17%, val_best:  65.83%, tr:  98.37%, tr_best:  99.08%, epoch time: 86.14 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.4799%\n",
      "layer   2  Sparsity: 87.1568%\n",
      "layer   3  Sparsity: 89.2067%\n",
      "total_backward_count 812570 real_backward_count 135882  16.722%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  1.907465/  2.116315, val:  47.08%, val_best:  65.83%, tr:  98.57%, tr_best:  99.08%, epoch time: 85.99 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4954%\n",
      "layer   2  Sparsity: 86.9375%\n",
      "layer   3  Sparsity: 89.9164%\n",
      "total_backward_count 822360 real_backward_count 137362  16.703%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  1.899781/  2.065484, val:  58.33%, val_best:  65.83%, tr:  98.37%, tr_best:  99.08%, epoch time: 85.82 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4828%\n",
      "layer   2  Sparsity: 86.8941%\n",
      "layer   3  Sparsity: 90.1308%\n",
      "total_backward_count 832150 real_backward_count 138852  16.686%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  1.874371/  2.064824, val:  51.67%, val_best:  65.83%, tr:  98.67%, tr_best:  99.08%, epoch time: 85.77 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4769%\n",
      "layer   2  Sparsity: 87.4465%\n",
      "layer   3  Sparsity: 89.5238%\n",
      "total_backward_count 841940 real_backward_count 140233  16.656%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  1.868109/  2.046775, val:  51.67%, val_best:  65.83%, tr:  98.88%, tr_best:  99.08%, epoch time: 85.39 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4780%\n",
      "layer   2  Sparsity: 87.8589%\n",
      "layer   3  Sparsity: 89.7694%\n",
      "total_backward_count 851730 real_backward_count 141596  16.625%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  1.877256/  2.045450, val:  50.00%, val_best:  65.83%, tr:  98.57%, tr_best:  99.08%, epoch time: 85.82 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4911%\n",
      "layer   2  Sparsity: 87.9464%\n",
      "layer   3  Sparsity: 89.9356%\n",
      "total_backward_count 861520 real_backward_count 143072  16.607%\n",
      "fc layer 3 self.abs_max_out: 1727.0\n",
      "fc layer 3 self.abs_max_out: 1820.0\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  1.863298/  2.077923, val:  52.50%, val_best:  65.83%, tr:  97.75%, tr_best:  99.08%, epoch time: 84.85 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 87.4863%\n",
      "layer   2  Sparsity: 87.7918%\n",
      "layer   3  Sparsity: 89.5178%\n",
      "total_backward_count 871310 real_backward_count 144499  16.584%\n",
      "fc layer 3 self.abs_max_out: 1892.0\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  1.905039/  2.065061, val:  59.17%, val_best:  65.83%, tr:  97.96%, tr_best:  99.08%, epoch time: 85.03 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4996%\n",
      "layer   2  Sparsity: 88.1663%\n",
      "layer   3  Sparsity: 89.9533%\n",
      "total_backward_count 881100 real_backward_count 146097  16.581%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  1.885171/  1.998809, val:  63.33%, val_best:  65.83%, tr:  98.57%, tr_best:  99.08%, epoch time: 85.53 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4911%\n",
      "layer   2  Sparsity: 88.2240%\n",
      "layer   3  Sparsity: 89.4830%\n",
      "total_backward_count 890890 real_backward_count 147616  16.569%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  1.875222/  1.980365, val:  65.00%, val_best:  65.83%, tr:  97.96%, tr_best:  99.08%, epoch time: 85.87 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4861%\n",
      "layer   2  Sparsity: 88.4578%\n",
      "layer   3  Sparsity: 89.3108%\n",
      "total_backward_count 900680 real_backward_count 149104  16.555%\n",
      "fc layer 3 self.abs_max_out: 2005.0\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  1.837034/  2.041358, val:  43.75%, val_best:  65.83%, tr:  98.88%, tr_best:  99.08%, epoch time: 86.46 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.4860%\n",
      "layer   2  Sparsity: 88.0007%\n",
      "layer   3  Sparsity: 88.9612%\n",
      "total_backward_count 910470 real_backward_count 150517  16.532%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  1.882413/  2.005262, val:  61.67%, val_best:  65.83%, tr:  98.37%, tr_best:  99.08%, epoch time: 85.87 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4855%\n",
      "layer   2  Sparsity: 88.0902%\n",
      "layer   3  Sparsity: 89.4765%\n",
      "total_backward_count 920260 real_backward_count 152095  16.527%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  1.850930/  2.013117, val:  50.00%, val_best:  65.83%, tr:  98.88%, tr_best:  99.08%, epoch time: 85.66 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4961%\n",
      "layer   2  Sparsity: 87.9385%\n",
      "layer   3  Sparsity: 89.1363%\n",
      "total_backward_count 930050 real_backward_count 153582  16.513%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  1.848032/  2.015222, val:  62.08%, val_best:  65.83%, tr:  99.18%, tr_best:  99.18%, epoch time: 86.05 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4828%\n",
      "layer   2  Sparsity: 88.3542%\n",
      "layer   3  Sparsity: 89.0738%\n",
      "total_backward_count 939840 real_backward_count 155041  16.497%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  1.848580/  1.995274, val:  50.42%, val_best:  65.83%, tr:  97.96%, tr_best:  99.18%, epoch time: 86.41 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.4998%\n",
      "layer   2  Sparsity: 88.6087%\n",
      "layer   3  Sparsity: 88.8523%\n",
      "total_backward_count 949630 real_backward_count 156554  16.486%\n",
      "fc layer 3 self.abs_max_out: 2063.0\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  1.823340/  1.962981, val:  52.08%, val_best:  65.83%, tr:  98.37%, tr_best:  99.18%, epoch time: 86.02 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4807%\n",
      "layer   2  Sparsity: 88.8023%\n",
      "layer   3  Sparsity: 88.2481%\n",
      "total_backward_count 959420 real_backward_count 158067  16.475%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  1.842717/  2.032623, val:  50.42%, val_best:  65.83%, tr:  98.06%, tr_best:  99.18%, epoch time: 85.76 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4770%\n",
      "layer   2  Sparsity: 88.4271%\n",
      "layer   3  Sparsity: 88.8393%\n",
      "total_backward_count 969210 real_backward_count 159601  16.467%\n",
      "fc layer 3 self.abs_max_out: 2133.0\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  1.841606/  2.026305, val:  41.67%, val_best:  65.83%, tr:  98.06%, tr_best:  99.18%, epoch time: 85.66 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4901%\n",
      "layer   2  Sparsity: 88.2845%\n",
      "layer   3  Sparsity: 88.8603%\n",
      "total_backward_count 979000 real_backward_count 161071  16.453%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  1.861385/  2.019408, val:  63.75%, val_best:  65.83%, tr:  97.85%, tr_best:  99.18%, epoch time: 86.15 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.4754%\n",
      "layer   2  Sparsity: 88.5688%\n",
      "layer   3  Sparsity: 89.8863%\n",
      "total_backward_count 988790 real_backward_count 162641  16.448%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  1.899835/  1.993355, val:  66.25%, val_best:  66.25%, tr:  96.94%, tr_best:  99.18%, epoch time: 85.32 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4790%\n",
      "layer   2  Sparsity: 88.9501%\n",
      "layer   3  Sparsity: 90.3659%\n",
      "total_backward_count 998580 real_backward_count 164208  16.444%\n",
      "fc layer 3 self.abs_max_out: 2157.0\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  1.868474/  2.022320, val:  56.67%, val_best:  66.25%, tr:  98.16%, tr_best:  99.18%, epoch time: 86.01 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4888%\n",
      "layer   2  Sparsity: 88.5389%\n",
      "layer   3  Sparsity: 89.3090%\n",
      "total_backward_count 1008370 real_backward_count 165638  16.426%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  1.887990/  2.051661, val:  49.58%, val_best:  66.25%, tr:  97.96%, tr_best:  99.18%, epoch time: 85.59 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4896%\n",
      "layer   2  Sparsity: 88.6777%\n",
      "layer   3  Sparsity: 89.7499%\n",
      "total_backward_count 1018160 real_backward_count 167108  16.413%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  1.847609/  2.010159, val:  55.42%, val_best:  66.25%, tr:  97.75%, tr_best:  99.18%, epoch time: 84.37 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 87.4842%\n",
      "layer   2  Sparsity: 88.2929%\n",
      "layer   3  Sparsity: 88.9836%\n",
      "total_backward_count 1027950 real_backward_count 168638  16.405%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  1.853334/  2.032652, val:  62.08%, val_best:  66.25%, tr:  98.06%, tr_best:  99.18%, epoch time: 85.08 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4823%\n",
      "layer   2  Sparsity: 88.6660%\n",
      "layer   3  Sparsity: 89.6279%\n",
      "total_backward_count 1037740 real_backward_count 170138  16.395%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  1.844929/  2.050365, val:  48.75%, val_best:  66.25%, tr:  98.57%, tr_best:  99.18%, epoch time: 85.38 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4682%\n",
      "layer   2  Sparsity: 88.5494%\n",
      "layer   3  Sparsity: 89.2903%\n",
      "total_backward_count 1047530 real_backward_count 171559  16.377%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  1.860495/  2.034524, val:  44.58%, val_best:  66.25%, tr:  98.16%, tr_best:  99.18%, epoch time: 85.93 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4822%\n",
      "layer   2  Sparsity: 87.9998%\n",
      "layer   3  Sparsity: 89.4719%\n",
      "total_backward_count 1057320 real_backward_count 173092  16.371%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  1.874940/  2.003811, val:  51.25%, val_best:  66.25%, tr:  98.06%, tr_best:  99.18%, epoch time: 86.19 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.4989%\n",
      "layer   2  Sparsity: 88.3698%\n",
      "layer   3  Sparsity: 89.8705%\n",
      "total_backward_count 1067110 real_backward_count 174636  16.365%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  1.864518/  2.016385, val:  59.17%, val_best:  66.25%, tr:  97.65%, tr_best:  99.18%, epoch time: 85.73 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4858%\n",
      "layer   2  Sparsity: 88.4646%\n",
      "layer   3  Sparsity: 89.8224%\n",
      "total_backward_count 1076900 real_backward_count 176125  16.355%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  1.858132/  1.975360, val:  60.00%, val_best:  66.25%, tr:  97.85%, tr_best:  99.18%, epoch time: 86.11 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.4992%\n",
      "layer   2  Sparsity: 88.5217%\n",
      "layer   3  Sparsity: 89.0256%\n",
      "total_backward_count 1086690 real_backward_count 177618  16.345%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  1.881661/  2.047978, val:  57.08%, val_best:  66.25%, tr:  97.75%, tr_best:  99.18%, epoch time: 84.78 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 87.4832%\n",
      "layer   2  Sparsity: 88.6821%\n",
      "layer   3  Sparsity: 89.3966%\n",
      "total_backward_count 1096480 real_backward_count 179101  16.334%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  1.870041/  2.025712, val:  58.33%, val_best:  66.25%, tr:  97.85%, tr_best:  99.18%, epoch time: 83.86 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 87.4727%\n",
      "layer   2  Sparsity: 88.6334%\n",
      "layer   3  Sparsity: 88.9592%\n",
      "total_backward_count 1106270 real_backward_count 180594  16.325%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  1.858161/  2.033896, val:  54.58%, val_best:  66.25%, tr:  98.47%, tr_best:  99.18%, epoch time: 85.63 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4963%\n",
      "layer   2  Sparsity: 88.5330%\n",
      "layer   3  Sparsity: 88.6527%\n",
      "total_backward_count 1116060 real_backward_count 182077  16.314%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  1.855239/  2.041075, val:  54.17%, val_best:  66.25%, tr:  98.16%, tr_best:  99.18%, epoch time: 85.90 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4836%\n",
      "layer   2  Sparsity: 88.4577%\n",
      "layer   3  Sparsity: 88.2891%\n",
      "total_backward_count 1125850 real_backward_count 183586  16.306%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  1.885151/  2.059882, val:  55.00%, val_best:  66.25%, tr:  98.47%, tr_best:  99.18%, epoch time: 85.36 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4721%\n",
      "layer   2  Sparsity: 88.6945%\n",
      "layer   3  Sparsity: 88.9488%\n",
      "total_backward_count 1135640 real_backward_count 185179  16.306%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  1.899791/  2.041929, val:  57.92%, val_best:  66.25%, tr:  97.65%, tr_best:  99.18%, epoch time: 85.10 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4799%\n",
      "layer   2  Sparsity: 89.0012%\n",
      "layer   3  Sparsity: 89.4828%\n",
      "total_backward_count 1145430 real_backward_count 186821  16.310%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  1.852550/  2.045828, val:  57.50%, val_best:  66.25%, tr:  98.37%, tr_best:  99.18%, epoch time: 85.73 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4745%\n",
      "layer   2  Sparsity: 88.3879%\n",
      "layer   3  Sparsity: 89.1166%\n",
      "total_backward_count 1155220 real_backward_count 188317  16.301%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  1.879916/  2.016966, val:  48.33%, val_best:  66.25%, tr:  98.16%, tr_best:  99.18%, epoch time: 86.16 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.4846%\n",
      "layer   2  Sparsity: 88.0465%\n",
      "layer   3  Sparsity: 88.6612%\n",
      "total_backward_count 1165010 real_backward_count 189828  16.294%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  1.846793/  2.034853, val:  68.75%, val_best:  68.75%, tr:  98.26%, tr_best:  99.18%, epoch time: 86.15 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.4934%\n",
      "layer   2  Sparsity: 88.2721%\n",
      "layer   3  Sparsity: 88.8823%\n",
      "total_backward_count 1174800 real_backward_count 191305  16.284%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  1.868439/  2.010820, val:  64.17%, val_best:  68.75%, tr:  98.37%, tr_best:  99.18%, epoch time: 85.66 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4856%\n",
      "layer   2  Sparsity: 88.0795%\n",
      "layer   3  Sparsity: 89.5230%\n",
      "total_backward_count 1184590 real_backward_count 192798  16.276%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  1.875902/  2.032317, val:  47.08%, val_best:  68.75%, tr:  98.26%, tr_best:  99.18%, epoch time: 85.48 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4995%\n",
      "layer   2  Sparsity: 88.0784%\n",
      "layer   3  Sparsity: 88.9870%\n",
      "total_backward_count 1194380 real_backward_count 194263  16.265%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  1.871225/  2.066161, val:  44.17%, val_best:  68.75%, tr:  97.85%, tr_best:  99.18%, epoch time: 86.13 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.4942%\n",
      "layer   2  Sparsity: 88.0745%\n",
      "layer   3  Sparsity: 89.1478%\n",
      "total_backward_count 1204170 real_backward_count 195739  16.255%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  1.849604/  2.002913, val:  47.92%, val_best:  68.75%, tr:  98.67%, tr_best:  99.18%, epoch time: 85.22 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4898%\n",
      "layer   2  Sparsity: 88.0058%\n",
      "layer   3  Sparsity: 88.5739%\n",
      "total_backward_count 1213960 real_backward_count 197146  16.240%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  1.853392/  1.982061, val:  70.00%, val_best:  70.00%, tr:  97.85%, tr_best:  99.18%, epoch time: 85.76 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.5017%\n",
      "layer   2  Sparsity: 87.7795%\n",
      "layer   3  Sparsity: 89.3192%\n",
      "total_backward_count 1223750 real_backward_count 198657  16.233%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  1.844136/  1.984539, val:  62.08%, val_best:  70.00%, tr:  98.47%, tr_best:  99.18%, epoch time: 85.58 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4910%\n",
      "layer   2  Sparsity: 88.3660%\n",
      "layer   3  Sparsity: 89.2609%\n",
      "total_backward_count 1233540 real_backward_count 200075  16.220%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  1.839674/  2.032328, val:  60.42%, val_best:  70.00%, tr:  98.47%, tr_best:  99.18%, epoch time: 85.50 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4875%\n",
      "layer   2  Sparsity: 88.1359%\n",
      "layer   3  Sparsity: 88.8724%\n",
      "total_backward_count 1243330 real_backward_count 201472  16.204%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  1.865144/  2.068535, val:  62.50%, val_best:  70.00%, tr:  98.26%, tr_best:  99.18%, epoch time: 84.65 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 87.4792%\n",
      "layer   2  Sparsity: 88.4502%\n",
      "layer   3  Sparsity: 89.5743%\n",
      "total_backward_count 1253120 real_backward_count 202845  16.187%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  1.841145/  1.963292, val:  59.17%, val_best:  70.00%, tr:  97.85%, tr_best:  99.18%, epoch time: 84.69 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 87.4874%\n",
      "layer   2  Sparsity: 88.2164%\n",
      "layer   3  Sparsity: 88.2863%\n",
      "total_backward_count 1262910 real_backward_count 204297  16.177%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  1.843897/  1.966598, val:  64.17%, val_best:  70.00%, tr:  97.96%, tr_best:  99.18%, epoch time: 85.03 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4827%\n",
      "layer   2  Sparsity: 88.0266%\n",
      "layer   3  Sparsity: 88.7106%\n",
      "total_backward_count 1272700 real_backward_count 205732  16.165%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  1.831619/  2.008991, val:  51.25%, val_best:  70.00%, tr:  98.57%, tr_best:  99.18%, epoch time: 85.43 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4796%\n",
      "layer   2  Sparsity: 88.2582%\n",
      "layer   3  Sparsity: 88.9008%\n",
      "total_backward_count 1282490 real_backward_count 207103  16.149%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  1.850910/  2.069746, val:  53.33%, val_best:  70.00%, tr:  98.37%, tr_best:  99.18%, epoch time: 85.30 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4961%\n",
      "layer   2  Sparsity: 88.3705%\n",
      "layer   3  Sparsity: 89.2303%\n",
      "total_backward_count 1292280 real_backward_count 208610  16.143%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  1.855240/  2.027363, val:  45.83%, val_best:  70.00%, tr:  98.16%, tr_best:  99.18%, epoch time: 85.41 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4640%\n",
      "layer   2  Sparsity: 88.2775%\n",
      "layer   3  Sparsity: 88.5067%\n",
      "total_backward_count 1302070 real_backward_count 210063  16.133%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  1.856261/  2.038010, val:  54.17%, val_best:  70.00%, tr:  98.26%, tr_best:  99.18%, epoch time: 84.86 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 87.4612%\n",
      "layer   2  Sparsity: 88.2329%\n",
      "layer   3  Sparsity: 89.1543%\n",
      "total_backward_count 1311860 real_backward_count 211567  16.127%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  1.842835/  2.016978, val:  55.42%, val_best:  70.00%, tr:  98.67%, tr_best:  99.18%, epoch time: 85.37 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4879%\n",
      "layer   2  Sparsity: 88.2273%\n",
      "layer   3  Sparsity: 88.8708%\n",
      "total_backward_count 1321650 real_backward_count 212953  16.113%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  1.870807/  2.027745, val:  63.75%, val_best:  70.00%, tr:  97.85%, tr_best:  99.18%, epoch time: 85.44 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4896%\n",
      "layer   2  Sparsity: 88.3007%\n",
      "layer   3  Sparsity: 89.7495%\n",
      "total_backward_count 1331440 real_backward_count 214475  16.108%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  1.842087/  2.020142, val:  58.33%, val_best:  70.00%, tr:  97.96%, tr_best:  99.18%, epoch time: 85.67 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4803%\n",
      "layer   2  Sparsity: 88.3339%\n",
      "layer   3  Sparsity: 89.4403%\n",
      "total_backward_count 1341230 real_backward_count 215895  16.097%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  1.874106/  2.045264, val:  45.00%, val_best:  70.00%, tr:  97.55%, tr_best:  99.18%, epoch time: 85.88 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4765%\n",
      "layer   2  Sparsity: 88.3439%\n",
      "layer   3  Sparsity: 89.7301%\n",
      "total_backward_count 1351020 real_backward_count 217431  16.094%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  1.863023/  2.018249, val:  56.25%, val_best:  70.00%, tr:  98.57%, tr_best:  99.18%, epoch time: 85.18 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4780%\n",
      "layer   2  Sparsity: 88.7352%\n",
      "layer   3  Sparsity: 89.5084%\n",
      "total_backward_count 1360810 real_backward_count 218882  16.085%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  1.872949/  2.032124, val:  55.00%, val_best:  70.00%, tr:  97.45%, tr_best:  99.18%, epoch time: 85.39 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4866%\n",
      "layer   2  Sparsity: 88.2011%\n",
      "layer   3  Sparsity: 89.7345%\n",
      "total_backward_count 1370600 real_backward_count 220436  16.083%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  1.868115/  1.999636, val:  57.50%, val_best:  70.00%, tr:  98.47%, tr_best:  99.18%, epoch time: 85.05 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4856%\n",
      "layer   2  Sparsity: 88.2699%\n",
      "layer   3  Sparsity: 89.2963%\n",
      "total_backward_count 1380390 real_backward_count 221862  16.072%\n",
      "fc layer 3 self.abs_max_out: 2201.0\n",
      "fc layer 3 self.abs_max_out: 2424.0\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  1.868379/  1.977913, val:  57.08%, val_best:  70.00%, tr:  98.26%, tr_best:  99.18%, epoch time: 84.97 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4816%\n",
      "layer   2  Sparsity: 88.4733%\n",
      "layer   3  Sparsity: 88.8238%\n",
      "total_backward_count 1390180 real_backward_count 223383  16.069%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  1.855052/  1.967873, val:  60.83%, val_best:  70.00%, tr:  98.67%, tr_best:  99.18%, epoch time: 85.01 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4863%\n",
      "layer   2  Sparsity: 88.5025%\n",
      "layer   3  Sparsity: 88.2899%\n",
      "total_backward_count 1399970 real_backward_count 224887  16.064%\n",
      "fc layer 3 self.abs_max_out: 2497.0\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  1.838994/  2.003253, val:  60.42%, val_best:  70.00%, tr:  97.65%, tr_best:  99.18%, epoch time: 84.75 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 87.4980%\n",
      "layer   2  Sparsity: 88.8600%\n",
      "layer   3  Sparsity: 88.5389%\n",
      "total_backward_count 1409760 real_backward_count 226380  16.058%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  1.832007/  2.022427, val:  54.17%, val_best:  70.00%, tr:  98.37%, tr_best:  99.18%, epoch time: 85.37 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4860%\n",
      "layer   2  Sparsity: 88.5506%\n",
      "layer   3  Sparsity: 88.3301%\n",
      "total_backward_count 1419550 real_backward_count 227887  16.053%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  1.826625/  2.021757, val:  48.33%, val_best:  70.00%, tr:  98.37%, tr_best:  99.18%, epoch time: 85.84 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4879%\n",
      "layer   2  Sparsity: 88.7961%\n",
      "layer   3  Sparsity: 88.0004%\n",
      "total_backward_count 1429340 real_backward_count 229381  16.048%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  1.867349/  2.031129, val:  48.75%, val_best:  70.00%, tr:  96.32%, tr_best:  99.18%, epoch time: 86.07 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4841%\n",
      "layer   2  Sparsity: 89.3521%\n",
      "layer   3  Sparsity: 89.3178%\n",
      "total_backward_count 1439130 real_backward_count 231069  16.056%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  1.844270/  2.007211, val:  51.67%, val_best:  70.00%, tr:  98.37%, tr_best:  99.18%, epoch time: 85.23 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4763%\n",
      "layer   2  Sparsity: 88.9285%\n",
      "layer   3  Sparsity: 88.4925%\n",
      "total_backward_count 1448920 real_backward_count 232551  16.050%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  1.832153/  2.023038, val:  50.83%, val_best:  70.00%, tr:  98.47%, tr_best:  99.18%, epoch time: 85.24 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4884%\n",
      "layer   2  Sparsity: 88.8308%\n",
      "layer   3  Sparsity: 88.4296%\n",
      "total_backward_count 1458710 real_backward_count 234051  16.045%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  1.858758/  1.999433, val:  63.33%, val_best:  70.00%, tr:  98.26%, tr_best:  99.18%, epoch time: 85.21 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4811%\n",
      "layer   2  Sparsity: 89.0564%\n",
      "layer   3  Sparsity: 89.0842%\n",
      "total_backward_count 1468500 real_backward_count 235624  16.045%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  1.851507/  2.028113, val:  55.83%, val_best:  70.00%, tr:  97.85%, tr_best:  99.18%, epoch time: 85.83 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4856%\n",
      "layer   2  Sparsity: 88.9780%\n",
      "layer   3  Sparsity: 88.9527%\n",
      "total_backward_count 1478290 real_backward_count 237168  16.043%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  1.850836/  2.007547, val:  60.00%, val_best:  70.00%, tr:  96.83%, tr_best:  99.18%, epoch time: 84.63 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 87.4743%\n",
      "layer   2  Sparsity: 88.6173%\n",
      "layer   3  Sparsity: 89.1189%\n",
      "total_backward_count 1488080 real_backward_count 238813  16.048%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  1.825591/  2.019779, val:  58.75%, val_best:  70.00%, tr:  98.57%, tr_best:  99.18%, epoch time: 85.31 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4845%\n",
      "layer   2  Sparsity: 88.3676%\n",
      "layer   3  Sparsity: 88.1352%\n",
      "total_backward_count 1497870 real_backward_count 240204  16.036%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  1.814023/  1.943086, val:  65.00%, val_best:  70.00%, tr:  98.06%, tr_best:  99.18%, epoch time: 85.36 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4933%\n",
      "layer   2  Sparsity: 88.3508%\n",
      "layer   3  Sparsity: 88.0393%\n",
      "total_backward_count 1507660 real_backward_count 241662  16.029%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  1.802106/  1.992601, val:  56.67%, val_best:  70.00%, tr:  98.98%, tr_best:  99.18%, epoch time: 84.90 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4942%\n",
      "layer   2  Sparsity: 88.2468%\n",
      "layer   3  Sparsity: 88.3475%\n",
      "total_backward_count 1517450 real_backward_count 243022  16.015%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  1.837532/  2.027745, val:  55.42%, val_best:  70.00%, tr:  97.85%, tr_best:  99.18%, epoch time: 85.29 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4753%\n",
      "layer   2  Sparsity: 88.0884%\n",
      "layer   3  Sparsity: 88.8125%\n",
      "total_backward_count 1527240 real_backward_count 244491  16.009%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  1.832543/  2.026398, val:  54.58%, val_best:  70.00%, tr:  98.37%, tr_best:  99.18%, epoch time: 84.90 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4686%\n",
      "layer   2  Sparsity: 87.9801%\n",
      "layer   3  Sparsity: 89.0642%\n",
      "total_backward_count 1537030 real_backward_count 245876  15.997%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  1.858356/  2.054802, val:  59.58%, val_best:  70.00%, tr:  97.85%, tr_best:  99.18%, epoch time: 85.79 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4834%\n",
      "layer   2  Sparsity: 87.9248%\n",
      "layer   3  Sparsity: 89.4489%\n",
      "total_backward_count 1546820 real_backward_count 247300  15.988%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  1.867990/  2.028184, val:  61.25%, val_best:  70.00%, tr:  98.77%, tr_best:  99.18%, epoch time: 85.85 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4698%\n",
      "layer   2  Sparsity: 88.2373%\n",
      "layer   3  Sparsity: 89.8075%\n",
      "total_backward_count 1556610 real_backward_count 248775  15.982%\n",
      "fc layer 3 self.abs_max_out: 2552.0\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  1.835998/  2.028498, val:  55.42%, val_best:  70.00%, tr:  98.37%, tr_best:  99.18%, epoch time: 85.04 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4801%\n",
      "layer   2  Sparsity: 88.0918%\n",
      "layer   3  Sparsity: 89.6523%\n",
      "total_backward_count 1566400 real_backward_count 250191  15.972%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  1.895771/  2.024493, val:  64.58%, val_best:  70.00%, tr:  97.75%, tr_best:  99.18%, epoch time: 85.00 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4933%\n",
      "layer   2  Sparsity: 88.1291%\n",
      "layer   3  Sparsity: 90.3569%\n",
      "total_backward_count 1576190 real_backward_count 251689  15.968%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  1.899714/  2.025848, val:  57.08%, val_best:  70.00%, tr:  98.06%, tr_best:  99.18%, epoch time: 85.00 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4915%\n",
      "layer   2  Sparsity: 88.3980%\n",
      "layer   3  Sparsity: 90.3815%\n",
      "total_backward_count 1585980 real_backward_count 253236  15.967%\n",
      "fc layer 3 self.abs_max_out: 2631.0\n",
      "fc layer 3 self.abs_max_out: 2848.0\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  1.881682/  2.073449, val:  48.75%, val_best:  70.00%, tr:  97.34%, tr_best:  99.18%, epoch time: 85.28 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4896%\n",
      "layer   2  Sparsity: 88.2817%\n",
      "layer   3  Sparsity: 90.0586%\n",
      "total_backward_count 1595770 real_backward_count 254877  15.972%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  1.863056/  2.028411, val:  53.33%, val_best:  70.00%, tr:  97.04%, tr_best:  99.18%, epoch time: 85.42 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4797%\n",
      "layer   2  Sparsity: 88.1475%\n",
      "layer   3  Sparsity: 89.5727%\n",
      "total_backward_count 1605560 real_backward_count 256425  15.971%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  1.868450/  2.017081, val:  52.92%, val_best:  70.00%, tr:  97.45%, tr_best:  99.18%, epoch time: 85.56 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4798%\n",
      "layer   2  Sparsity: 88.4804%\n",
      "layer   3  Sparsity: 89.6305%\n",
      "total_backward_count 1615350 real_backward_count 257979  15.970%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  1.880128/  2.038255, val:  62.08%, val_best:  70.00%, tr:  97.34%, tr_best:  99.18%, epoch time: 85.39 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4826%\n",
      "layer   2  Sparsity: 88.0703%\n",
      "layer   3  Sparsity: 89.4386%\n",
      "total_backward_count 1625140 real_backward_count 259561  15.972%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  1.871612/  2.002596, val:  54.17%, val_best:  70.00%, tr:  97.75%, tr_best:  99.18%, epoch time: 86.17 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.4892%\n",
      "layer   2  Sparsity: 88.2966%\n",
      "layer   3  Sparsity: 89.7240%\n",
      "total_backward_count 1634930 real_backward_count 261010  15.965%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  1.853912/  2.060184, val:  44.58%, val_best:  70.00%, tr:  97.85%, tr_best:  99.18%, epoch time: 85.08 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4914%\n",
      "layer   2  Sparsity: 88.5135%\n",
      "layer   3  Sparsity: 89.3407%\n",
      "total_backward_count 1644720 real_backward_count 262509  15.961%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  1.879311/  2.016448, val:  45.00%, val_best:  70.00%, tr:  98.47%, tr_best:  99.18%, epoch time: 84.98 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4790%\n",
      "layer   2  Sparsity: 88.1845%\n",
      "layer   3  Sparsity: 88.8446%\n",
      "total_backward_count 1654510 real_backward_count 264008  15.957%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  1.847748/  1.979178, val:  51.67%, val_best:  70.00%, tr:  98.57%, tr_best:  99.18%, epoch time: 85.17 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4812%\n",
      "layer   2  Sparsity: 88.4378%\n",
      "layer   3  Sparsity: 89.0327%\n",
      "total_backward_count 1664300 real_backward_count 265496  15.952%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  1.845907/  2.035616, val:  46.67%, val_best:  70.00%, tr:  97.96%, tr_best:  99.18%, epoch time: 84.79 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 87.4876%\n",
      "layer   2  Sparsity: 88.2419%\n",
      "layer   3  Sparsity: 89.1635%\n",
      "total_backward_count 1674090 real_backward_count 266921  15.944%\n",
      "fc layer 3 self.abs_max_out: 2954.0\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  1.862413/  2.017413, val:  52.92%, val_best:  70.00%, tr:  97.75%, tr_best:  99.18%, epoch time: 84.28 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 87.4928%\n",
      "layer   2  Sparsity: 88.0961%\n",
      "layer   3  Sparsity: 89.0103%\n",
      "total_backward_count 1683880 real_backward_count 268458  15.943%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  1.864819/  2.046438, val:  53.75%, val_best:  70.00%, tr:  97.75%, tr_best:  99.18%, epoch time: 85.03 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4948%\n",
      "layer   2  Sparsity: 88.0526%\n",
      "layer   3  Sparsity: 89.9772%\n",
      "total_backward_count 1693670 real_backward_count 269990  15.941%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  1.842470/  2.031477, val:  49.17%, val_best:  70.00%, tr:  98.37%, tr_best:  99.18%, epoch time: 85.18 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4767%\n",
      "layer   2  Sparsity: 87.8532%\n",
      "layer   3  Sparsity: 88.7216%\n",
      "total_backward_count 1703460 real_backward_count 271499  15.938%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  1.885998/  2.006810, val:  67.08%, val_best:  70.00%, tr:  97.24%, tr_best:  99.18%, epoch time: 85.20 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4826%\n",
      "layer   2  Sparsity: 88.0936%\n",
      "layer   3  Sparsity: 89.7665%\n",
      "total_backward_count 1713250 real_backward_count 273119  15.942%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  1.872217/  1.991967, val:  62.92%, val_best:  70.00%, tr:  97.45%, tr_best:  99.18%, epoch time: 84.60 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 87.4931%\n",
      "layer   2  Sparsity: 88.4786%\n",
      "layer   3  Sparsity: 89.3205%\n",
      "total_backward_count 1723040 real_backward_count 274684  15.942%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  1.842509/  2.011905, val:  48.33%, val_best:  70.00%, tr:  98.16%, tr_best:  99.18%, epoch time: 84.37 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 87.4622%\n",
      "layer   2  Sparsity: 88.4955%\n",
      "layer   3  Sparsity: 89.2301%\n",
      "total_backward_count 1732830 real_backward_count 276200  15.939%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  1.847936/  1.996659, val:  55.00%, val_best:  70.00%, tr:  98.26%, tr_best:  99.18%, epoch time: 85.19 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4886%\n",
      "layer   2  Sparsity: 88.1048%\n",
      "layer   3  Sparsity: 89.1511%\n",
      "total_backward_count 1742620 real_backward_count 277661  15.934%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  1.792369/  1.936303, val:  61.67%, val_best:  70.00%, tr:  98.57%, tr_best:  99.18%, epoch time: 85.32 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4908%\n",
      "layer   2  Sparsity: 88.2106%\n",
      "layer   3  Sparsity: 87.8226%\n",
      "total_backward_count 1752410 real_backward_count 279037  15.923%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  1.796779/  2.026776, val:  58.75%, val_best:  70.00%, tr:  97.65%, tr_best:  99.18%, epoch time: 85.41 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4753%\n",
      "layer   2  Sparsity: 88.0566%\n",
      "layer   3  Sparsity: 88.8464%\n",
      "total_backward_count 1762200 real_backward_count 280506  15.918%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  1.843774/  1.987269, val:  60.00%, val_best:  70.00%, tr:  98.16%, tr_best:  99.18%, epoch time: 85.47 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4780%\n",
      "layer   2  Sparsity: 87.9980%\n",
      "layer   3  Sparsity: 89.4818%\n",
      "total_backward_count 1771990 real_backward_count 281920  15.910%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  1.838005/  2.025165, val:  60.83%, val_best:  70.00%, tr:  97.24%, tr_best:  99.18%, epoch time: 85.35 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4881%\n",
      "layer   2  Sparsity: 88.4013%\n",
      "layer   3  Sparsity: 89.8170%\n",
      "total_backward_count 1781780 real_backward_count 283478  15.910%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  1.864454/  2.000111, val:  60.83%, val_best:  70.00%, tr:  96.94%, tr_best:  99.18%, epoch time: 85.17 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4758%\n",
      "layer   2  Sparsity: 88.2979%\n",
      "layer   3  Sparsity: 89.5779%\n",
      "total_backward_count 1791570 real_backward_count 285064  15.911%\n",
      "fc layer 1 self.abs_max_out: 18625.0\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  1.849326/  2.041746, val:  43.75%, val_best:  70.00%, tr:  97.65%, tr_best:  99.18%, epoch time: 85.74 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4890%\n",
      "layer   2  Sparsity: 88.1969%\n",
      "layer   3  Sparsity: 89.5446%\n",
      "total_backward_count 1801360 real_backward_count 286597  15.910%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  1.840546/  2.032016, val:  46.25%, val_best:  70.00%, tr:  97.85%, tr_best:  99.18%, epoch time: 85.32 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4875%\n",
      "layer   2  Sparsity: 87.6648%\n",
      "layer   3  Sparsity: 89.1656%\n",
      "total_backward_count 1811150 real_backward_count 287919  15.897%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  1.848130/  2.003523, val:  57.92%, val_best:  70.00%, tr:  97.45%, tr_best:  99.18%, epoch time: 85.67 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4793%\n",
      "layer   2  Sparsity: 87.7475%\n",
      "layer   3  Sparsity: 89.4200%\n",
      "total_backward_count 1820940 real_backward_count 289426  15.894%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  1.841453/  1.965115, val:  53.33%, val_best:  70.00%, tr:  97.45%, tr_best:  99.18%, epoch time: 82.38 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 87.4766%\n",
      "layer   2  Sparsity: 87.6689%\n",
      "layer   3  Sparsity: 88.6987%\n",
      "total_backward_count 1830730 real_backward_count 290908  15.890%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  1.813354/  2.006352, val:  52.50%, val_best:  70.00%, tr:  98.37%, tr_best:  99.18%, epoch time: 84.65 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 87.4973%\n",
      "layer   2  Sparsity: 87.3926%\n",
      "layer   3  Sparsity: 87.9940%\n",
      "total_backward_count 1840520 real_backward_count 292292  15.881%\n",
      "fc layer 1 self.abs_max_out: 19516.0\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  1.835087/  1.996060, val:  58.33%, val_best:  70.00%, tr:  98.57%, tr_best:  99.18%, epoch time: 84.48 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 87.4980%\n",
      "layer   2  Sparsity: 88.1715%\n",
      "layer   3  Sparsity: 89.2470%\n",
      "total_backward_count 1850310 real_backward_count 293750  15.876%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  1.814363/  1.966596, val:  61.25%, val_best:  70.00%, tr:  98.77%, tr_best:  99.18%, epoch time: 84.96 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4724%\n",
      "layer   2  Sparsity: 87.8108%\n",
      "layer   3  Sparsity: 89.0298%\n",
      "total_backward_count 1860100 real_backward_count 295091  15.864%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  1.850853/  1.978288, val:  59.58%, val_best:  70.00%, tr:  97.75%, tr_best:  99.18%, epoch time: 85.25 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.5118%\n",
      "layer   2  Sparsity: 87.7915%\n",
      "layer   3  Sparsity: 89.4945%\n",
      "total_backward_count 1869890 real_backward_count 296540  15.859%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  1.836840/  2.031832, val:  47.08%, val_best:  70.00%, tr:  97.75%, tr_best:  99.18%, epoch time: 85.51 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4885%\n",
      "layer   2  Sparsity: 87.5740%\n",
      "layer   3  Sparsity: 89.4658%\n",
      "total_backward_count 1879680 real_backward_count 298003  15.854%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  1.820199/  1.996218, val:  54.17%, val_best:  70.00%, tr:  97.96%, tr_best:  99.18%, epoch time: 85.83 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 87.4751%\n",
      "layer   2  Sparsity: 87.6227%\n",
      "layer   3  Sparsity: 88.9600%\n",
      "total_backward_count 1889470 real_backward_count 299484  15.850%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  1.826146/  1.961109, val:  59.58%, val_best:  70.00%, tr:  97.65%, tr_best:  99.18%, epoch time: 85.23 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4690%\n",
      "layer   2  Sparsity: 88.1699%\n",
      "layer   3  Sparsity: 89.2277%\n",
      "total_backward_count 1899260 real_backward_count 301008  15.849%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  1.838827/  2.028441, val:  45.00%, val_best:  70.00%, tr:  97.65%, tr_best:  99.18%, epoch time: 85.25 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4912%\n",
      "layer   2  Sparsity: 88.1930%\n",
      "layer   3  Sparsity: 89.6153%\n",
      "total_backward_count 1909050 real_backward_count 302605  15.851%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  1.838790/  1.990747, val:  60.83%, val_best:  70.00%, tr:  98.37%, tr_best:  99.18%, epoch time: 84.60 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 87.4961%\n",
      "layer   2  Sparsity: 87.7043%\n",
      "layer   3  Sparsity: 89.4507%\n",
      "total_backward_count 1918840 real_backward_count 304148  15.851%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  1.838735/  1.988737, val:  52.50%, val_best:  70.00%, tr:  98.16%, tr_best:  99.18%, epoch time: 85.46 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4880%\n",
      "layer   2  Sparsity: 88.0341%\n",
      "layer   3  Sparsity: 89.3454%\n",
      "total_backward_count 1928630 real_backward_count 305624  15.847%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  1.804552/  1.955555, val:  57.08%, val_best:  70.00%, tr:  98.26%, tr_best:  99.18%, epoch time: 85.40 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.4784%\n",
      "layer   2  Sparsity: 88.0191%\n",
      "layer   3  Sparsity: 88.6303%\n",
      "total_backward_count 1938420 real_backward_count 307077  15.842%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  1.813579/  2.004875, val:  52.92%, val_best:  70.00%, tr:  97.85%, tr_best:  99.18%, epoch time: 84.72 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 87.4946%\n",
      "layer   2  Sparsity: 88.1216%\n",
      "layer   3  Sparsity: 88.6429%\n",
      "total_backward_count 1948210 real_backward_count 308414  15.831%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  1.795675/  1.959045, val:  66.25%, val_best:  70.00%, tr:  97.65%, tr_best:  99.18%, epoch time: 84.56 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 87.4977%\n",
      "layer   2  Sparsity: 88.1808%\n",
      "layer   3  Sparsity: 88.5987%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b693718a4a49ce9995c72adddd2579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÅ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñá‚ñÉ‚ñÑ‚ñÖ‚ñÅ‚ñÉ‚ñÉ‚ñà‚ñÑ‚ñà‚ñÇ‚ñÖ‚ñÜ‚ñÉ‚ñá‚ñÖ‚ñÇ‚ñÜ‚ñÑ‚ñà‚ñÜ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÜ‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÅ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÅ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñá‚ñÉ‚ñÑ‚ñÖ‚ñÅ‚ñÉ‚ñÉ‚ñà‚ñÑ‚ñà‚ñÇ‚ñÖ‚ñÜ‚ñÉ‚ñá‚ñÖ‚ñÇ‚ñÜ‚ñÑ‚ñà‚ñÜ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÜ‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.97651</td></tr><tr><td>tr_epoch_loss</td><td>1.79568</td></tr><tr><td>val_acc_best</td><td>0.7</td></tr><tr><td>val_acc_now</td><td>0.6625</td></tr><tr><td>val_loss</td><td>1.95905</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">graceful-sweep-2</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/csts8ug3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/csts8ug3</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_204640-csts8ug3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7kiuuo0a with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 50000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_013225-7kiuuo0a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7kiuuo0a' target=\"_blank\">fast-sweep-6</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7kiuuo0a' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7kiuuo0a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251118_013234_097', 'my_seed': 42, 'TIME': 5, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 15, 'dvs_duration': 50000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = dac77cc348b2d880ae59906e26f08f17\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 139\n",
      "fc layer 1 self.abs_max_out: 286.0\n",
      "lif layer 1 self.abs_max_v: 286.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 504.0\n",
      "lif layer 2 self.abs_max_v: 504.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 210.0\n",
      "fc layer 1 self.abs_max_out: 315.0\n",
      "lif layer 1 self.abs_max_v: 356.0\n",
      "lif layer 2 self.abs_max_v: 665.0\n",
      "fc layer 1 self.abs_max_out: 361.0\n",
      "lif layer 1 self.abs_max_v: 422.0\n",
      "fc layer 2 self.abs_max_out: 541.0\n",
      "lif layer 2 self.abs_max_v: 789.0\n",
      "fc layer 3 self.abs_max_out: 252.0\n",
      "fc layer 1 self.abs_max_out: 403.0\n",
      "lif layer 1 self.abs_max_v: 533.0\n",
      "fc layer 2 self.abs_max_out: 609.0\n",
      "lif layer 2 self.abs_max_v: 873.0\n",
      "fc layer 3 self.abs_max_out: 378.0\n",
      "fc layer 1 self.abs_max_out: 636.0\n",
      "lif layer 1 self.abs_max_v: 664.0\n",
      "lif layer 2 self.abs_max_v: 953.0\n",
      "smallest_now_T updated: 125\n",
      "fc layer 1 self.abs_max_out: 917.0\n",
      "lif layer 1 self.abs_max_v: 917.0\n",
      "fc layer 2 self.abs_max_out: 814.0\n",
      "fc layer 3 self.abs_max_out: 397.0\n",
      "fc layer 1 self.abs_max_out: 1126.0\n",
      "lif layer 1 self.abs_max_v: 1126.0\n",
      "fc layer 1 self.abs_max_out: 1155.0\n",
      "lif layer 1 self.abs_max_v: 1155.0\n",
      "lif layer 2 self.abs_max_v: 980.5\n",
      "smallest_now_T updated: 94\n",
      "fc layer 1 self.abs_max_out: 1195.0\n",
      "lif layer 1 self.abs_max_v: 1195.0\n",
      "fc layer 1 self.abs_max_out: 1273.0\n",
      "lif layer 1 self.abs_max_v: 1273.0\n",
      "fc layer 1 self.abs_max_out: 1319.0\n",
      "lif layer 1 self.abs_max_v: 1319.0\n",
      "fc layer 1 self.abs_max_out: 1357.0\n",
      "lif layer 1 self.abs_max_v: 1357.0\n",
      "fc layer 1 self.abs_max_out: 1503.0\n",
      "lif layer 1 self.abs_max_v: 1503.0\n",
      "fc layer 2 self.abs_max_out: 845.0\n",
      "lif layer 2 self.abs_max_v: 1094.5\n",
      "fc layer 1 self.abs_max_out: 1514.0\n",
      "lif layer 1 self.abs_max_v: 1514.0\n",
      "lif layer 2 self.abs_max_v: 1186.5\n",
      "fc layer 3 self.abs_max_out: 403.0\n",
      "fc layer 2 self.abs_max_out: 861.0\n",
      "fc layer 2 self.abs_max_out: 872.0\n",
      "lif layer 2 self.abs_max_v: 1202.0\n",
      "fc layer 2 self.abs_max_out: 930.0\n",
      "fc layer 2 self.abs_max_out: 1072.0\n",
      "lif layer 2 self.abs_max_v: 1224.5\n",
      "lif layer 2 self.abs_max_v: 1260.5\n",
      "lif layer 2 self.abs_max_v: 1344.5\n",
      "smallest_now_T updated: 79\n",
      "lif layer 2 self.abs_max_v: 1434.5\n",
      "fc layer 2 self.abs_max_out: 1077.0\n",
      "lif layer 2 self.abs_max_v: 1447.0\n",
      "lif layer 2 self.abs_max_v: 1627.0\n",
      "lif layer 2 self.abs_max_v: 1765.5\n",
      "lif layer 2 self.abs_max_v: 1792.0\n",
      "fc layer 2 self.abs_max_out: 1087.0\n",
      "fc layer 2 self.abs_max_out: 1115.0\n",
      "lif layer 1 self.abs_max_v: 1984.0\n",
      "fc layer 1 self.abs_max_out: 1570.0\n",
      "lif layer 1 self.abs_max_v: 2428.0\n",
      "lif layer 1 self.abs_max_v: 2436.0\n",
      "fc layer 2 self.abs_max_out: 1128.0\n",
      "lif layer 1 self.abs_max_v: 2556.0\n",
      "lif layer 2 self.abs_max_v: 1871.0\n",
      "fc layer 2 self.abs_max_out: 1150.0\n",
      "smallest_now_T updated: 73\n",
      "fc layer 1 self.abs_max_out: 1611.0\n",
      "fc layer 2 self.abs_max_out: 1154.0\n",
      "fc layer 3 self.abs_max_out: 408.0\n",
      "fc layer 2 self.abs_max_out: 1199.0\n",
      "lif layer 1 self.abs_max_v: 2604.0\n",
      "lif layer 2 self.abs_max_v: 1872.5\n",
      "lif layer 2 self.abs_max_v: 1953.0\n",
      "smallest_now_T updated: 65\n",
      "fc layer 2 self.abs_max_out: 1223.0\n",
      "fc layer 2 self.abs_max_out: 1241.0\n",
      "lif layer 2 self.abs_max_v: 1958.5\n",
      "lif layer 2 self.abs_max_v: 1992.5\n",
      "fc layer 3 self.abs_max_out: 416.0\n",
      "fc layer 2 self.abs_max_out: 1295.0\n",
      "fc layer 1 self.abs_max_out: 1615.0\n",
      "lif layer 2 self.abs_max_v: 2061.5\n",
      "lif layer 1 self.abs_max_v: 2627.0\n",
      "lif layer 2 self.abs_max_v: 2141.0\n",
      "fc layer 3 self.abs_max_out: 444.0\n",
      "lif layer 2 self.abs_max_v: 2248.5\n",
      "lif layer 2 self.abs_max_v: 2331.5\n",
      "fc layer 1 self.abs_max_out: 1758.0\n",
      "fc layer 1 self.abs_max_out: 1857.0\n",
      "lif layer 1 self.abs_max_v: 2750.0\n",
      "lif layer 1 self.abs_max_v: 2874.0\n",
      "fc layer 3 self.abs_max_out: 463.0\n",
      "lif layer 1 self.abs_max_v: 2960.0\n",
      "fc layer 2 self.abs_max_out: 1336.0\n",
      "fc layer 3 self.abs_max_out: 470.0\n",
      "fc layer 1 self.abs_max_out: 1901.0\n",
      "fc layer 3 self.abs_max_out: 496.0\n",
      "smallest_now_T updated: 56\n",
      "smallest_now_T updated: 43\n",
      "lif layer 2 self.abs_max_v: 2354.0\n",
      "fc layer 2 self.abs_max_out: 1469.0\n",
      "lif layer 2 self.abs_max_v: 2544.0\n",
      "lif layer 1 self.abs_max_v: 3079.0\n",
      "lif layer 1 self.abs_max_v: 3255.5\n",
      "lif layer 2 self.abs_max_v: 2695.0\n",
      "fc layer 3 self.abs_max_out: 551.0\n",
      "fc layer 1 self.abs_max_out: 1925.0\n",
      "fc layer 1 self.abs_max_out: 2205.0\n",
      "lif layer 1 self.abs_max_v: 3458.5\n",
      "lif layer 1 self.abs_max_v: 3677.5\n",
      "lif layer 1 self.abs_max_v: 3845.0\n",
      "fc layer 2 self.abs_max_out: 1532.0\n",
      "fc layer 3 self.abs_max_out: 565.0\n",
      "fc layer 3 self.abs_max_out: 578.0\n",
      "fc layer 1 self.abs_max_out: 2622.0\n",
      "lif layer 1 self.abs_max_v: 4013.5\n",
      "lif layer 1 self.abs_max_v: 4123.5\n",
      "lif layer 1 self.abs_max_v: 4395.0\n",
      "lif layer 1 self.abs_max_v: 4748.5\n",
      "fc layer 1 self.abs_max_out: 2736.0\n",
      "fc layer 3 self.abs_max_out: 585.0\n",
      "lif layer 1 self.abs_max_v: 4900.0\n",
      "lif layer 1 self.abs_max_v: 5117.0\n",
      "fc layer 1 self.abs_max_out: 2762.0\n",
      "fc layer 3 self.abs_max_out: 597.0\n",
      "fc layer 3 self.abs_max_out: 616.0\n",
      "fc layer 1 self.abs_max_out: 3065.0\n",
      "fc layer 1 self.abs_max_out: 3135.0\n",
      "fc layer 1 self.abs_max_out: 3328.0\n",
      "lif layer 1 self.abs_max_v: 5662.0\n",
      "fc layer 1 self.abs_max_out: 3420.0\n",
      "lif layer 1 self.abs_max_v: 5859.0\n",
      "fc layer 2 self.abs_max_out: 1593.0\n",
      "lif layer 1 self.abs_max_v: 6052.0\n",
      "fc layer 3 self.abs_max_out: 628.0\n",
      "fc layer 3 self.abs_max_out: 631.0\n",
      "fc layer 3 self.abs_max_out: 662.0\n",
      "fc layer 1 self.abs_max_out: 3473.0\n",
      "lif layer 1 self.abs_max_v: 6098.0\n",
      "fc layer 3 self.abs_max_out: 665.0\n",
      "fc layer 3 self.abs_max_out: 797.0\n",
      "lif layer 2 self.abs_max_v: 2724.5\n",
      "lif layer 2 self.abs_max_v: 2843.5\n",
      "fc layer 2 self.abs_max_out: 1595.0\n",
      "fc layer 2 self.abs_max_out: 1616.0\n",
      "lif layer 2 self.abs_max_v: 2883.5\n",
      "lif layer 2 self.abs_max_v: 2998.0\n",
      "fc layer 1 self.abs_max_out: 3795.0\n",
      "fc layer 2 self.abs_max_out: 1759.0\n",
      "fc layer 2 self.abs_max_out: 1809.0\n",
      "lif layer 1 self.abs_max_v: 6261.0\n",
      "smallest_now_T_val updated: 129\n",
      "smallest_now_T_val updated: 106\n",
      "smallest_now_T_val updated: 104\n",
      "smallest_now_T_val updated: 102\n",
      "smallest_now_T_val updated: 85\n",
      "smallest_now_T_val updated: 29\n",
      "fc layer 1 self.abs_max_out: 3797.0\n",
      "lif layer 1 self.abs_max_v: 6493.0\n",
      "fc layer 1 self.abs_max_out: 3845.0\n",
      "fc layer 1 self.abs_max_out: 3933.0\n",
      "lif layer 1 self.abs_max_v: 6708.0\n",
      "lif layer 1 self.abs_max_v: 6970.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.401135/  1.764699, val:  30.42%, val_best:  30.42%, tr:  93.56%, tr_best:  93.56%, epoch time: 44.85 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3296%\n",
      "layer   2  Sparsity: 68.6394%\n",
      "layer   3  Sparsity: 52.5549%\n",
      "total_backward_count 4895 real_backward_count 1062  21.696%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 3990.0\n",
      "lif layer 1 self.abs_max_v: 7065.0\n",
      "fc layer 1 self.abs_max_out: 4019.0\n",
      "lif layer 1 self.abs_max_v: 7366.5\n",
      "fc layer 1 self.abs_max_out: 4175.0\n",
      "fc layer 1 self.abs_max_out: 4285.0\n",
      "lif layer 1 self.abs_max_v: 7701.0\n",
      "lif layer 1 self.abs_max_v: 7763.0\n",
      "fc layer 3 self.abs_max_out: 844.0\n",
      "lif layer 2 self.abs_max_v: 3116.5\n",
      "fc layer 3 self.abs_max_out: 848.0\n",
      "fc layer 3 self.abs_max_out: 877.0\n",
      "fc layer 3 self.abs_max_out: 880.0\n",
      "fc layer 3 self.abs_max_out: 881.0\n",
      "fc layer 3 self.abs_max_out: 882.0\n",
      "fc layer 1 self.abs_max_out: 4287.0\n",
      "lif layer 1 self.abs_max_v: 8071.0\n",
      "lif layer 2 self.abs_max_v: 3211.5\n",
      "fc layer 1 self.abs_max_out: 4390.0\n",
      "fc layer 2 self.abs_max_out: 1872.0\n",
      "lif layer 2 self.abs_max_v: 3293.0\n",
      "lif layer 2 self.abs_max_v: 3308.5\n",
      "fc layer 1 self.abs_max_out: 4412.0\n",
      "fc layer 3 self.abs_max_out: 900.0\n",
      "fc layer 3 self.abs_max_out: 948.0\n",
      "lif layer 2 self.abs_max_v: 3336.0\n",
      "fc layer 1 self.abs_max_out: 4726.0\n",
      "fc layer 2 self.abs_max_out: 2008.0\n",
      "lif layer 2 self.abs_max_v: 3344.5\n",
      "lif layer 2 self.abs_max_v: 3653.5\n",
      "lif layer 2 self.abs_max_v: 3742.0\n",
      "lif layer 1 self.abs_max_v: 8226.5\n",
      "fc layer 1 self.abs_max_out: 4776.0\n",
      "fc layer 1 self.abs_max_out: 4828.0\n",
      "lif layer 1 self.abs_max_v: 8894.5\n",
      "lif layer 1 self.abs_max_v: 8903.5\n",
      "fc layer 1 self.abs_max_out: 4840.0\n",
      "fc layer 2 self.abs_max_out: 2045.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.197934/  1.578075, val:  50.00%, val_best:  50.00%, tr:  92.85%, tr_best:  93.56%, epoch time: 44.65 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3200%\n",
      "layer   2  Sparsity: 74.7996%\n",
      "layer   3  Sparsity: 58.7627%\n",
      "total_backward_count 9790 real_backward_count 2021  20.644%\n",
      "fc layer 2 self.abs_max_out: 2105.0\n",
      "lif layer 2 self.abs_max_v: 3787.5\n",
      "fc layer 3 self.abs_max_out: 949.0\n",
      "lif layer 1 self.abs_max_v: 8913.5\n",
      "fc layer 1 self.abs_max_out: 4868.0\n",
      "fc layer 1 self.abs_max_out: 5167.0\n",
      "fc layer 1 self.abs_max_out: 5179.0\n",
      "lif layer 1 self.abs_max_v: 9345.0\n",
      "lif layer 1 self.abs_max_v: 9364.5\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.173844/  1.677986, val:  40.83%, val_best:  50.00%, tr:  93.77%, tr_best:  93.77%, epoch time: 44.98 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.4090%\n",
      "layer   2  Sparsity: 75.8381%\n",
      "layer   3  Sparsity: 58.2616%\n",
      "total_backward_count 14685 real_backward_count 2948  20.075%\n",
      "fc layer 2 self.abs_max_out: 2225.0\n",
      "fc layer 2 self.abs_max_out: 2292.0\n",
      "lif layer 2 self.abs_max_v: 3837.0\n",
      "fc layer 3 self.abs_max_out: 969.0\n",
      "lif layer 2 self.abs_max_v: 4005.0\n",
      "lif layer 2 self.abs_max_v: 4127.5\n",
      "lif layer 2 self.abs_max_v: 4195.0\n",
      "fc layer 1 self.abs_max_out: 5245.0\n",
      "fc layer 1 self.abs_max_out: 5603.0\n",
      "lif layer 1 self.abs_max_v: 9552.5\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.135326/  1.555967, val:  40.83%, val_best:  50.00%, tr:  93.87%, tr_best:  93.87%, epoch time: 44.86 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3806%\n",
      "layer   2  Sparsity: 76.2452%\n",
      "layer   3  Sparsity: 59.4067%\n",
      "total_backward_count 19580 real_backward_count 3841  19.617%\n",
      "fc layer 1 self.abs_max_out: 5822.0\n",
      "lif layer 1 self.abs_max_v: 9777.5\n",
      "fc layer 2 self.abs_max_out: 2306.0\n",
      "fc layer 3 self.abs_max_out: 990.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.120777/  1.599098, val:  41.67%, val_best:  50.00%, tr:  94.18%, tr_best:  94.18%, epoch time: 44.81 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.2990%\n",
      "layer   2  Sparsity: 76.8435%\n",
      "layer   3  Sparsity: 59.9243%\n",
      "total_backward_count 24475 real_backward_count 4702  19.211%\n",
      "fc layer 2 self.abs_max_out: 2332.0\n",
      "lif layer 2 self.abs_max_v: 4211.0\n",
      "fc layer 1 self.abs_max_out: 6303.0\n",
      "lif layer 1 self.abs_max_v: 10865.5\n",
      "lif layer 1 self.abs_max_v: 10951.0\n",
      "fc layer 2 self.abs_max_out: 2436.0\n",
      "fc layer 1 self.abs_max_out: 6410.0\n",
      "lif layer 2 self.abs_max_v: 4268.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.116930/  1.473972, val:  54.58%, val_best:  54.58%, tr:  94.79%, tr_best:  94.79%, epoch time: 44.68 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3842%\n",
      "layer   2  Sparsity: 77.3033%\n",
      "layer   3  Sparsity: 60.7211%\n",
      "total_backward_count 29370 real_backward_count 5528  18.822%\n",
      "fc layer 2 self.abs_max_out: 2495.0\n",
      "lif layer 1 self.abs_max_v: 11109.0\n",
      "lif layer 1 self.abs_max_v: 11183.5\n",
      "fc layer 1 self.abs_max_out: 6609.0\n",
      "lif layer 1 self.abs_max_v: 11200.5\n",
      "fc layer 3 self.abs_max_out: 1004.0\n",
      "fc layer 3 self.abs_max_out: 1103.0\n",
      "lif layer 2 self.abs_max_v: 4289.0\n",
      "lif layer 2 self.abs_max_v: 4350.5\n",
      "fc layer 2 self.abs_max_out: 2561.0\n",
      "fc layer 2 self.abs_max_out: 2700.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.096957/  1.564763, val:  44.17%, val_best:  54.58%, tr:  95.61%, tr_best:  95.61%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3459%\n",
      "layer   2  Sparsity: 77.2508%\n",
      "layer   3  Sparsity: 62.1391%\n",
      "total_backward_count 34265 real_backward_count 6335  18.488%\n",
      "lif layer 1 self.abs_max_v: 11302.0\n",
      "lif layer 1 self.abs_max_v: 11383.5\n",
      "lif layer 2 self.abs_max_v: 4363.0\n",
      "lif layer 2 self.abs_max_v: 4495.5\n",
      "fc layer 2 self.abs_max_out: 2723.0\n",
      "lif layer 2 self.abs_max_v: 4784.5\n",
      "fc layer 2 self.abs_max_out: 2988.0\n",
      "lif layer 2 self.abs_max_v: 5380.5\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.088493/  1.424658, val:  60.42%, val_best:  60.42%, tr:  94.48%, tr_best:  95.61%, epoch time: 44.43 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3662%\n",
      "layer   2  Sparsity: 75.5546%\n",
      "layer   3  Sparsity: 61.7335%\n",
      "total_backward_count 39160 real_backward_count 7103  18.138%\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.053758/  1.490695, val:  52.08%, val_best:  60.42%, tr:  96.42%, tr_best:  96.42%, epoch time: 44.96 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3223%\n",
      "layer   2  Sparsity: 75.3549%\n",
      "layer   3  Sparsity: 62.6961%\n",
      "total_backward_count 44055 real_backward_count 7872  17.869%\n",
      "fc layer 2 self.abs_max_out: 3144.0\n",
      "fc layer 2 self.abs_max_out: 3156.0\n",
      "lif layer 2 self.abs_max_v: 5454.0\n",
      "lif layer 2 self.abs_max_v: 5831.0\n",
      "lif layer 2 self.abs_max_v: 5895.5\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.025700/  1.430368, val:  57.08%, val_best:  60.42%, tr:  96.12%, tr_best:  96.42%, epoch time: 44.93 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3962%\n",
      "layer   2  Sparsity: 74.5578%\n",
      "layer   3  Sparsity: 63.5476%\n",
      "total_backward_count 48950 real_backward_count 8610  17.589%\n",
      "fc layer 1 self.abs_max_out: 6718.0\n",
      "lif layer 1 self.abs_max_v: 12096.0\n",
      "fc layer 1 self.abs_max_out: 6905.0\n",
      "fc layer 1 self.abs_max_out: 6972.0\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.026758/  1.554863, val:  43.75%, val_best:  60.42%, tr:  96.94%, tr_best:  96.94%, epoch time: 44.55 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3060%\n",
      "layer   2  Sparsity: 74.8297%\n",
      "layer   3  Sparsity: 64.7765%\n",
      "total_backward_count 53845 real_backward_count 9339  17.344%\n",
      "fc layer 2 self.abs_max_out: 3293.0\n",
      "lif layer 1 self.abs_max_v: 12162.5\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.005383/  1.382167, val:  56.67%, val_best:  60.42%, tr:  96.42%, tr_best:  96.94%, epoch time: 44.07 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.3160%\n",
      "layer   2  Sparsity: 75.1080%\n",
      "layer   3  Sparsity: 64.2372%\n",
      "total_backward_count 58740 real_backward_count 10077  17.155%\n",
      "fc layer 1 self.abs_max_out: 6998.0\n",
      "lif layer 1 self.abs_max_v: 12464.0\n",
      "fc layer 2 self.abs_max_out: 3295.0\n",
      "lif layer 1 self.abs_max_v: 12492.0\n",
      "fc layer 1 self.abs_max_out: 7078.0\n",
      "fc layer 1 self.abs_max_out: 7150.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  0.993896/  1.380701, val:  61.67%, val_best:  61.67%, tr:  96.83%, tr_best:  96.94%, epoch time: 45.11 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3024%\n",
      "layer   2  Sparsity: 74.8100%\n",
      "layer   3  Sparsity: 64.4538%\n",
      "total_backward_count 63635 real_backward_count 10765  16.917%\n",
      "fc layer 2 self.abs_max_out: 3321.0\n",
      "fc layer 3 self.abs_max_out: 1108.0\n",
      "fc layer 1 self.abs_max_out: 7203.0\n",
      "fc layer 3 self.abs_max_out: 1139.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  0.993483/  1.423712, val:  52.08%, val_best:  61.67%, tr:  97.24%, tr_best:  97.24%, epoch time: 44.20 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3233%\n",
      "layer   2  Sparsity: 74.7796%\n",
      "layer   3  Sparsity: 64.8366%\n",
      "total_backward_count 68530 real_backward_count 11469  16.736%\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  0.949916/  1.405585, val:  59.58%, val_best:  61.67%, tr:  97.24%, tr_best:  97.24%, epoch time: 44.60 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3873%\n",
      "layer   2  Sparsity: 75.0815%\n",
      "layer   3  Sparsity: 63.8723%\n",
      "total_backward_count 73425 real_backward_count 12149  16.546%\n",
      "fc layer 2 self.abs_max_out: 3385.0\n",
      "fc layer 1 self.abs_max_out: 7213.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  0.977650/  1.313126, val:  64.58%, val_best:  64.58%, tr:  96.63%, tr_best:  97.24%, epoch time: 44.44 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3460%\n",
      "layer   2  Sparsity: 74.8209%\n",
      "layer   3  Sparsity: 65.1691%\n",
      "total_backward_count 78320 real_backward_count 12800  16.343%\n",
      "fc layer 1 self.abs_max_out: 7471.0\n",
      "lif layer 1 self.abs_max_v: 13331.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  0.938243/  1.436279, val:  52.50%, val_best:  64.58%, tr:  96.63%, tr_best:  97.24%, epoch time: 44.39 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3259%\n",
      "layer   2  Sparsity: 75.1692%\n",
      "layer   3  Sparsity: 65.4984%\n",
      "total_backward_count 83215 real_backward_count 13466  16.182%\n",
      "fc layer 3 self.abs_max_out: 1196.0\n",
      "fc layer 1 self.abs_max_out: 8243.0\n",
      "lif layer 1 self.abs_max_v: 14134.0\n",
      "fc layer 2 self.abs_max_out: 3410.0\n",
      "lif layer 2 self.abs_max_v: 5897.5\n",
      "fc layer 2 self.abs_max_out: 3853.0\n",
      "lif layer 2 self.abs_max_v: 6126.0\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  0.935986/  1.327593, val:  61.67%, val_best:  64.58%, tr:  97.24%, tr_best:  97.24%, epoch time: 44.55 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3325%\n",
      "layer   2  Sparsity: 75.1074%\n",
      "layer   3  Sparsity: 65.2817%\n",
      "total_backward_count 88110 real_backward_count 14094  15.996%\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  0.923784/  1.292869, val:  66.67%, val_best:  66.67%, tr:  97.65%, tr_best:  97.65%, epoch time: 44.57 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3770%\n",
      "layer   2  Sparsity: 74.9786%\n",
      "layer   3  Sparsity: 65.7874%\n",
      "total_backward_count 93005 real_backward_count 14733  15.841%\n",
      "lif layer 2 self.abs_max_v: 6201.5\n",
      "lif layer 2 self.abs_max_v: 6337.0\n",
      "fc layer 1 self.abs_max_out: 8304.0\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  0.895412/  1.321164, val:  59.17%, val_best:  66.67%, tr:  96.73%, tr_best:  97.65%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3906%\n",
      "layer   2  Sparsity: 74.8926%\n",
      "layer   3  Sparsity: 66.2445%\n",
      "total_backward_count 97900 real_backward_count 15335  15.664%\n",
      "lif layer 2 self.abs_max_v: 6582.5\n",
      "fc layer 1 self.abs_max_out: 8767.0\n",
      "lif layer 1 self.abs_max_v: 15130.5\n",
      "fc layer 3 self.abs_max_out: 1225.0\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  0.862396/  1.305543, val:  62.08%, val_best:  66.67%, tr:  97.65%, tr_best:  97.65%, epoch time: 44.88 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3287%\n",
      "layer   2  Sparsity: 74.7505%\n",
      "layer   3  Sparsity: 65.3623%\n",
      "total_backward_count 102795 real_backward_count 15927  15.494%\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  0.866644/  1.267657, val:  55.83%, val_best:  66.67%, tr:  96.94%, tr_best:  97.65%, epoch time: 44.76 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3510%\n",
      "layer   2  Sparsity: 74.7516%\n",
      "layer   3  Sparsity: 65.5600%\n",
      "total_backward_count 107690 real_backward_count 16591  15.406%\n",
      "lif layer 1 self.abs_max_v: 15188.0\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  0.867988/  1.233830, val:  64.58%, val_best:  66.67%, tr:  98.37%, tr_best:  98.37%, epoch time: 44.81 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3514%\n",
      "layer   2  Sparsity: 74.3125%\n",
      "layer   3  Sparsity: 65.6112%\n",
      "total_backward_count 112585 real_backward_count 17207  15.284%\n",
      "fc layer 1 self.abs_max_out: 9112.0\n",
      "lif layer 1 self.abs_max_v: 15740.5\n",
      "lif layer 2 self.abs_max_v: 6707.5\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  0.865017/  1.252676, val:  74.58%, val_best:  74.58%, tr:  97.45%, tr_best:  98.37%, epoch time: 44.29 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3118%\n",
      "layer   2  Sparsity: 74.5970%\n",
      "layer   3  Sparsity: 66.3926%\n",
      "total_backward_count 117480 real_backward_count 17782  15.136%\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  0.871688/  1.316113, val:  62.50%, val_best:  74.58%, tr:  97.45%, tr_best:  98.37%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3145%\n",
      "layer   2  Sparsity: 73.7258%\n",
      "layer   3  Sparsity: 65.6651%\n",
      "total_backward_count 122375 real_backward_count 18395  15.032%\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  0.865609/  1.277881, val:  65.00%, val_best:  74.58%, tr:  97.85%, tr_best:  98.37%, epoch time: 44.32 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3707%\n",
      "layer   2  Sparsity: 73.8095%\n",
      "layer   3  Sparsity: 65.6168%\n",
      "total_backward_count 127270 real_backward_count 18977  14.911%\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  0.850103/  1.257667, val:  58.33%, val_best:  74.58%, tr:  97.96%, tr_best:  98.37%, epoch time: 44.42 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3777%\n",
      "layer   2  Sparsity: 74.6207%\n",
      "layer   3  Sparsity: 65.6723%\n",
      "total_backward_count 132165 real_backward_count 19529  14.776%\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  0.830396/  1.149548, val:  72.92%, val_best:  74.58%, tr:  98.26%, tr_best:  98.37%, epoch time: 44.27 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3402%\n",
      "layer   2  Sparsity: 74.2479%\n",
      "layer   3  Sparsity: 67.3409%\n",
      "total_backward_count 137060 real_backward_count 20059  14.635%\n",
      "lif layer 1 self.abs_max_v: 15883.0\n",
      "fc layer 1 self.abs_max_out: 9192.0\n",
      "fc layer 1 self.abs_max_out: 9815.0\n",
      "lif layer 1 self.abs_max_v: 16688.0\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  0.810329/  1.275248, val:  61.25%, val_best:  74.58%, tr:  98.77%, tr_best:  98.77%, epoch time: 44.70 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3683%\n",
      "layer   2  Sparsity: 73.9988%\n",
      "layer   3  Sparsity: 66.4683%\n",
      "total_backward_count 141955 real_backward_count 20566  14.488%\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  0.779094/  1.267260, val:  67.50%, val_best:  74.58%, tr:  98.47%, tr_best:  98.77%, epoch time: 44.47 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3838%\n",
      "layer   2  Sparsity: 74.0214%\n",
      "layer   3  Sparsity: 64.9416%\n",
      "total_backward_count 146850 real_backward_count 21085  14.358%\n",
      "lif layer 1 self.abs_max_v: 16862.5\n",
      "lif layer 1 self.abs_max_v: 16967.5\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  0.763950/  1.124460, val:  76.67%, val_best:  76.67%, tr:  98.77%, tr_best:  98.77%, epoch time: 45.00 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3053%\n",
      "layer   2  Sparsity: 73.3912%\n",
      "layer   3  Sparsity: 64.7865%\n",
      "total_backward_count 151745 real_backward_count 21578  14.220%\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  0.738057/  1.144711, val:  71.25%, val_best:  76.67%, tr:  98.16%, tr_best:  98.77%, epoch time: 44.46 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3869%\n",
      "layer   2  Sparsity: 74.1343%\n",
      "layer   3  Sparsity: 64.9924%\n",
      "total_backward_count 156640 real_backward_count 22056  14.081%\n",
      "lif layer 1 self.abs_max_v: 16970.5\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  0.739457/  1.146295, val:  70.83%, val_best:  76.67%, tr:  98.57%, tr_best:  98.77%, epoch time: 44.49 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3151%\n",
      "layer   2  Sparsity: 73.6610%\n",
      "layer   3  Sparsity: 66.1622%\n",
      "total_backward_count 161535 real_backward_count 22524  13.944%\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  0.747670/  1.109548, val:  72.50%, val_best:  76.67%, tr:  98.57%, tr_best:  98.77%, epoch time: 44.53 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3231%\n",
      "layer   2  Sparsity: 73.5415%\n",
      "layer   3  Sparsity: 66.7474%\n",
      "total_backward_count 166430 real_backward_count 23001  13.820%\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  0.772776/  1.138774, val:  71.25%, val_best:  76.67%, tr:  98.57%, tr_best:  98.77%, epoch time: 44.98 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.4198%\n",
      "layer   2  Sparsity: 73.9680%\n",
      "layer   3  Sparsity: 66.7956%\n",
      "total_backward_count 171325 real_backward_count 23483  13.707%\n",
      "fc layer 1 self.abs_max_out: 10174.0\n",
      "lif layer 1 self.abs_max_v: 17754.5\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  0.736637/  1.111660, val:  79.17%, val_best:  79.17%, tr:  98.98%, tr_best:  98.98%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3338%\n",
      "layer   2  Sparsity: 74.1922%\n",
      "layer   3  Sparsity: 65.8760%\n",
      "total_backward_count 176220 real_backward_count 23973  13.604%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  0.736290/  1.138582, val:  66.25%, val_best:  79.17%, tr:  98.26%, tr_best:  98.98%, epoch time: 44.68 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.4101%\n",
      "layer   2  Sparsity: 74.3897%\n",
      "layer   3  Sparsity: 66.5573%\n",
      "total_backward_count 181115 real_backward_count 24424  13.485%\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  0.717385/  1.119279, val:  73.33%, val_best:  79.17%, tr:  98.77%, tr_best:  98.98%, epoch time: 45.19 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3914%\n",
      "layer   2  Sparsity: 74.1765%\n",
      "layer   3  Sparsity: 66.8513%\n",
      "total_backward_count 186010 real_backward_count 24840  13.354%\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  0.711369/  1.102698, val:  78.33%, val_best:  79.17%, tr:  98.98%, tr_best:  98.98%, epoch time: 44.91 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3520%\n",
      "layer   2  Sparsity: 74.2934%\n",
      "layer   3  Sparsity: 67.9633%\n",
      "total_backward_count 190905 real_backward_count 25251  13.227%\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  0.684634/  1.222290, val:  64.17%, val_best:  79.17%, tr:  98.37%, tr_best:  98.98%, epoch time: 44.34 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3603%\n",
      "layer   2  Sparsity: 74.0936%\n",
      "layer   3  Sparsity: 67.1780%\n",
      "total_backward_count 195800 real_backward_count 25673  13.112%\n",
      "fc layer 1 self.abs_max_out: 10364.0\n",
      "lif layer 1 self.abs_max_v: 18105.0\n",
      "fc layer 1 self.abs_max_out: 10415.0\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  0.681487/  1.139686, val:  64.58%, val_best:  79.17%, tr:  98.57%, tr_best:  98.98%, epoch time: 44.40 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3984%\n",
      "layer   2  Sparsity: 74.2287%\n",
      "layer   3  Sparsity: 66.2281%\n",
      "total_backward_count 200695 real_backward_count 26113  13.011%\n",
      "lif layer 1 self.abs_max_v: 18238.0\n",
      "fc layer 1 self.abs_max_out: 10874.0\n",
      "lif layer 1 self.abs_max_v: 19001.0\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  0.691604/  1.077111, val:  73.33%, val_best:  79.17%, tr:  98.88%, tr_best:  98.98%, epoch time: 44.82 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3697%\n",
      "layer   2  Sparsity: 73.9921%\n",
      "layer   3  Sparsity: 65.9287%\n",
      "total_backward_count 205590 real_backward_count 26557  12.917%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  0.671237/  1.027522, val:  70.83%, val_best:  79.17%, tr:  99.08%, tr_best:  99.08%, epoch time: 44.82 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3529%\n",
      "layer   2  Sparsity: 74.2571%\n",
      "layer   3  Sparsity: 65.9121%\n",
      "total_backward_count 210485 real_backward_count 26971  12.814%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  0.682891/  1.061600, val:  77.92%, val_best:  79.17%, tr:  98.77%, tr_best:  99.08%, epoch time: 44.50 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3311%\n",
      "layer   2  Sparsity: 74.1111%\n",
      "layer   3  Sparsity: 66.3427%\n",
      "total_backward_count 215380 real_backward_count 27398  12.721%\n",
      "fc layer 1 self.abs_max_out: 10956.0\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  0.650197/  1.072991, val:  67.92%, val_best:  79.17%, tr:  98.88%, tr_best:  99.08%, epoch time: 44.38 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.4252%\n",
      "layer   2  Sparsity: 73.5486%\n",
      "layer   3  Sparsity: 66.3811%\n",
      "total_backward_count 220275 real_backward_count 27766  12.605%\n",
      "fc layer 3 self.abs_max_out: 1233.0\n",
      "lif layer 1 self.abs_max_v: 19279.0\n",
      "fc layer 3 self.abs_max_out: 1257.0\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  0.671238/  1.028227, val:  78.75%, val_best:  79.17%, tr:  98.98%, tr_best:  99.08%, epoch time: 44.21 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3024%\n",
      "layer   2  Sparsity: 73.8392%\n",
      "layer   3  Sparsity: 65.9107%\n",
      "total_backward_count 225170 real_backward_count 28203  12.525%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  0.652557/  1.040000, val:  75.83%, val_best:  79.17%, tr:  98.67%, tr_best:  99.08%, epoch time: 43.75 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.3305%\n",
      "layer   2  Sparsity: 74.1620%\n",
      "layer   3  Sparsity: 66.1365%\n",
      "total_backward_count 230065 real_backward_count 28600  12.431%\n",
      "fc layer 3 self.abs_max_out: 1262.0\n",
      "fc layer 3 self.abs_max_out: 1275.0\n",
      "fc layer 2 self.abs_max_out: 3978.0\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  0.638481/  1.064436, val:  72.92%, val_best:  79.17%, tr:  99.18%, tr_best:  99.18%, epoch time: 44.59 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3535%\n",
      "layer   2  Sparsity: 74.0627%\n",
      "layer   3  Sparsity: 66.0559%\n",
      "total_backward_count 234960 real_backward_count 28991  12.339%\n",
      "fc layer 3 self.abs_max_out: 1283.0\n",
      "fc layer 3 self.abs_max_out: 1301.0\n",
      "fc layer 3 self.abs_max_out: 1323.0\n",
      "fc layer 3 self.abs_max_out: 1388.0\n",
      "fc layer 3 self.abs_max_out: 1389.0\n",
      "fc layer 3 self.abs_max_out: 1404.0\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  0.625598/  1.062525, val:  72.92%, val_best:  79.17%, tr:  99.49%, tr_best:  99.49%, epoch time: 44.36 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3605%\n",
      "layer   2  Sparsity: 74.3578%\n",
      "layer   3  Sparsity: 66.4783%\n",
      "total_backward_count 239855 real_backward_count 29368  12.244%\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  0.641097/  1.049964, val:  78.33%, val_best:  79.17%, tr:  99.59%, tr_best:  99.59%, epoch time: 44.50 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3323%\n",
      "layer   2  Sparsity: 73.6823%\n",
      "layer   3  Sparsity: 66.0559%\n",
      "total_backward_count 244750 real_backward_count 29766  12.162%\n",
      "lif layer 2 self.abs_max_v: 6753.5\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  0.620917/  1.109369, val:  67.92%, val_best:  79.17%, tr:  99.18%, tr_best:  99.59%, epoch time: 44.27 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.4147%\n",
      "layer   2  Sparsity: 73.9489%\n",
      "layer   3  Sparsity: 66.2817%\n",
      "total_backward_count 249645 real_backward_count 30125  12.067%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  0.610623/  1.077831, val:  76.67%, val_best:  79.17%, tr:  98.88%, tr_best:  99.59%, epoch time: 45.19 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3514%\n",
      "layer   2  Sparsity: 73.8373%\n",
      "layer   3  Sparsity: 65.9780%\n",
      "total_backward_count 254540 real_backward_count 30492  11.979%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  0.619501/  1.051253, val:  75.83%, val_best:  79.17%, tr:  99.28%, tr_best:  99.59%, epoch time: 44.46 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3152%\n",
      "layer   2  Sparsity: 74.0337%\n",
      "layer   3  Sparsity: 65.7463%\n",
      "total_backward_count 259435 real_backward_count 30849  11.891%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  0.604937/  1.084481, val:  73.33%, val_best:  79.17%, tr:  99.49%, tr_best:  99.59%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3441%\n",
      "layer   2  Sparsity: 73.6007%\n",
      "layer   3  Sparsity: 65.8976%\n",
      "total_backward_count 264330 real_backward_count 31190  11.800%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  0.590395/  1.040710, val:  72.08%, val_best:  79.17%, tr:  99.49%, tr_best:  99.59%, epoch time: 44.47 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3671%\n",
      "layer   2  Sparsity: 74.2292%\n",
      "layer   3  Sparsity: 66.2932%\n",
      "total_backward_count 269225 real_backward_count 31529  11.711%\n",
      "lif layer 2 self.abs_max_v: 6971.5\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  0.603655/  1.011830, val:  77.92%, val_best:  79.17%, tr:  99.39%, tr_best:  99.59%, epoch time: 45.04 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.2995%\n",
      "layer   2  Sparsity: 74.1838%\n",
      "layer   3  Sparsity: 66.6000%\n",
      "total_backward_count 274120 real_backward_count 31881  11.630%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  0.608157/  1.065141, val:  77.08%, val_best:  79.17%, tr:  99.18%, tr_best:  99.59%, epoch time: 44.38 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3571%\n",
      "layer   2  Sparsity: 73.7605%\n",
      "layer   3  Sparsity: 67.2723%\n",
      "total_backward_count 279015 real_backward_count 32240  11.555%\n",
      "lif layer 1 self.abs_max_v: 19454.0\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  0.597692/  1.015626, val:  76.67%, val_best:  79.17%, tr:  99.39%, tr_best:  99.59%, epoch time: 44.55 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.2973%\n",
      "layer   2  Sparsity: 74.2642%\n",
      "layer   3  Sparsity: 65.8769%\n",
      "total_backward_count 283910 real_backward_count 32587  11.478%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  0.583284/  1.050446, val:  74.17%, val_best:  79.17%, tr:  99.69%, tr_best:  99.69%, epoch time: 44.99 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3564%\n",
      "layer   2  Sparsity: 73.8189%\n",
      "layer   3  Sparsity: 65.6249%\n",
      "total_backward_count 288805 real_backward_count 32901  11.392%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  0.548371/  1.051059, val:  66.25%, val_best:  79.17%, tr:  99.69%, tr_best:  99.69%, epoch time: 44.72 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3146%\n",
      "layer   2  Sparsity: 74.1115%\n",
      "layer   3  Sparsity: 66.1308%\n",
      "total_backward_count 293700 real_backward_count 33215  11.309%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  0.531576/  1.054990, val:  72.08%, val_best:  79.17%, tr:  99.49%, tr_best:  99.69%, epoch time: 44.62 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3202%\n",
      "layer   2  Sparsity: 74.4229%\n",
      "layer   3  Sparsity: 66.9370%\n",
      "total_backward_count 298595 real_backward_count 33491  11.216%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  0.528441/  0.949788, val:  79.17%, val_best:  79.17%, tr:  99.69%, tr_best:  99.69%, epoch time: 45.14 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3600%\n",
      "layer   2  Sparsity: 74.3514%\n",
      "layer   3  Sparsity: 66.8863%\n",
      "total_backward_count 303490 real_backward_count 33813  11.141%\n",
      "fc layer 1 self.abs_max_out: 11104.0\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  0.548527/  0.970660, val:  79.17%, val_best:  79.17%, tr:  99.18%, tr_best:  99.69%, epoch time: 44.72 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3323%\n",
      "layer   2  Sparsity: 74.7357%\n",
      "layer   3  Sparsity: 66.8463%\n",
      "total_backward_count 308385 real_backward_count 34124  11.065%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  0.556892/  1.085832, val:  68.75%, val_best:  79.17%, tr:  99.49%, tr_best:  99.69%, epoch time: 44.86 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.2678%\n",
      "layer   2  Sparsity: 74.8873%\n",
      "layer   3  Sparsity: 66.4107%\n",
      "total_backward_count 313280 real_backward_count 34431  10.990%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  0.523094/  1.025089, val:  74.58%, val_best:  79.17%, tr:  99.69%, tr_best:  99.69%, epoch time: 45.01 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3394%\n",
      "layer   2  Sparsity: 74.6478%\n",
      "layer   3  Sparsity: 67.3288%\n",
      "total_backward_count 318175 real_backward_count 34673  10.897%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  0.530269/  0.999455, val:  72.92%, val_best:  79.17%, tr:  99.59%, tr_best:  99.69%, epoch time: 45.22 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3694%\n",
      "layer   2  Sparsity: 74.4271%\n",
      "layer   3  Sparsity: 67.3996%\n",
      "total_backward_count 323070 real_backward_count 34930  10.812%\n",
      "fc layer 3 self.abs_max_out: 1425.0\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  0.532173/  1.010208, val:  79.17%, val_best:  79.17%, tr:  99.80%, tr_best:  99.80%, epoch time: 44.66 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.4300%\n",
      "layer   2  Sparsity: 74.6378%\n",
      "layer   3  Sparsity: 67.8309%\n",
      "total_backward_count 327965 real_backward_count 35210  10.736%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  0.535555/  0.950910, val:  80.42%, val_best:  80.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 44.68 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3588%\n",
      "layer   2  Sparsity: 74.7058%\n",
      "layer   3  Sparsity: 67.2205%\n",
      "total_backward_count 332860 real_backward_count 35506  10.667%\n",
      "fc layer 1 self.abs_max_out: 11354.0\n",
      "lif layer 1 self.abs_max_v: 19733.0\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  0.500785/  0.953174, val:  80.00%, val_best:  80.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 44.49 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3389%\n",
      "layer   2  Sparsity: 75.2105%\n",
      "layer   3  Sparsity: 67.3691%\n",
      "total_backward_count 337755 real_backward_count 35753  10.585%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  0.492062/  0.965811, val:  80.83%, val_best:  80.83%, tr:  99.69%, tr_best:  99.80%, epoch time: 45.41 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 86.3157%\n",
      "layer   2  Sparsity: 75.1593%\n",
      "layer   3  Sparsity: 67.7670%\n",
      "total_backward_count 342650 real_backward_count 35986  10.502%\n",
      "fc layer 2 self.abs_max_out: 4112.0\n",
      "lif layer 2 self.abs_max_v: 7344.5\n",
      "lif layer 2 self.abs_max_v: 7360.5\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  0.507039/  0.959689, val:  78.75%, val_best:  80.83%, tr:  99.59%, tr_best:  99.80%, epoch time: 44.40 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3500%\n",
      "layer   2  Sparsity: 74.2638%\n",
      "layer   3  Sparsity: 67.2320%\n",
      "total_backward_count 347545 real_backward_count 36243  10.428%\n",
      "lif layer 2 self.abs_max_v: 7386.5\n",
      "fc layer 3 self.abs_max_out: 1427.0\n",
      "lif layer 2 self.abs_max_v: 7427.0\n",
      "lif layer 2 self.abs_max_v: 7752.0\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  0.500443/  1.018352, val:  76.67%, val_best:  80.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 44.92 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3557%\n",
      "layer   2  Sparsity: 74.0918%\n",
      "layer   3  Sparsity: 67.2992%\n",
      "total_backward_count 352440 real_backward_count 36509  10.359%\n",
      "fc layer 2 self.abs_max_out: 4179.0\n",
      "fc layer 2 self.abs_max_out: 4344.0\n",
      "lif layer 2 self.abs_max_v: 7940.0\n",
      "lif layer 2 self.abs_max_v: 8197.0\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  0.484137/  0.902939, val:  82.92%, val_best:  82.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 44.67 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3834%\n",
      "layer   2  Sparsity: 74.3564%\n",
      "layer   3  Sparsity: 67.4331%\n",
      "total_backward_count 357335 real_backward_count 36735  10.280%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  0.490351/  0.989824, val:  72.08%, val_best:  82.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 44.52 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3050%\n",
      "layer   2  Sparsity: 74.4243%\n",
      "layer   3  Sparsity: 67.1434%\n",
      "total_backward_count 362230 real_backward_count 37000  10.215%\n",
      "fc layer 3 self.abs_max_out: 1436.0\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  0.467948/  0.976232, val:  75.42%, val_best:  82.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 44.83 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3712%\n",
      "layer   2  Sparsity: 74.8062%\n",
      "layer   3  Sparsity: 66.9066%\n",
      "total_backward_count 367125 real_backward_count 37239  10.143%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  0.479277/  0.955537, val:  80.83%, val_best:  82.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 44.46 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3064%\n",
      "layer   2  Sparsity: 74.7036%\n",
      "layer   3  Sparsity: 67.6486%\n",
      "total_backward_count 372020 real_backward_count 37479  10.074%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  0.486232/  0.888722, val:  80.42%, val_best:  82.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 44.58 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3780%\n",
      "layer   2  Sparsity: 74.7381%\n",
      "layer   3  Sparsity: 67.1523%\n",
      "total_backward_count 376915 real_backward_count 37713  10.006%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  0.458661/  0.941866, val:  74.58%, val_best:  82.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 44.62 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.4045%\n",
      "layer   2  Sparsity: 74.9445%\n",
      "layer   3  Sparsity: 66.9922%\n",
      "total_backward_count 381810 real_backward_count 37924   9.933%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  0.453728/  0.925958, val:  75.83%, val_best:  82.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 44.47 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3283%\n",
      "layer   2  Sparsity: 74.7972%\n",
      "layer   3  Sparsity: 66.6281%\n",
      "total_backward_count 386705 real_backward_count 38147   9.865%\n",
      "fc layer 3 self.abs_max_out: 1438.0\n",
      "fc layer 3 self.abs_max_out: 1452.0\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  0.436243/  0.904050, val:  77.92%, val_best:  82.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 44.75 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3686%\n",
      "layer   2  Sparsity: 74.5711%\n",
      "layer   3  Sparsity: 66.7945%\n",
      "total_backward_count 391600 real_backward_count 38385   9.802%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  0.449431/  1.030312, val:  70.00%, val_best:  82.92%, tr:  99.39%, tr_best:  99.90%, epoch time: 44.38 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3505%\n",
      "layer   2  Sparsity: 75.4632%\n",
      "layer   3  Sparsity: 66.5497%\n",
      "total_backward_count 396495 real_backward_count 38645   9.747%\n",
      "fc layer 3 self.abs_max_out: 1462.0\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  0.452202/  0.989080, val:  75.42%, val_best:  82.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 44.81 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3731%\n",
      "layer   2  Sparsity: 74.8676%\n",
      "layer   3  Sparsity: 66.3728%\n",
      "total_backward_count 401390 real_backward_count 38890   9.689%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  0.451495/  0.908633, val:  76.67%, val_best:  82.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 44.48 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3254%\n",
      "layer   2  Sparsity: 74.7716%\n",
      "layer   3  Sparsity: 66.1369%\n",
      "total_backward_count 406285 real_backward_count 39130   9.631%\n",
      "fc layer 3 self.abs_max_out: 1471.0\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  0.439149/  0.949360, val:  77.08%, val_best:  82.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 44.85 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3638%\n",
      "layer   2  Sparsity: 74.7423%\n",
      "layer   3  Sparsity: 66.9863%\n",
      "total_backward_count 411180 real_backward_count 39349   9.570%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  0.440960/  0.890865, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.39 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3145%\n",
      "layer   2  Sparsity: 75.0194%\n",
      "layer   3  Sparsity: 67.4034%\n",
      "total_backward_count 416075 real_backward_count 39549   9.505%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  0.433372/  0.890194, val:  79.17%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.42 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3759%\n",
      "layer   2  Sparsity: 74.8989%\n",
      "layer   3  Sparsity: 67.3578%\n",
      "total_backward_count 420970 real_backward_count 39775   9.448%\n",
      "fc layer 3 self.abs_max_out: 1519.0\n",
      "fc layer 3 self.abs_max_out: 1532.0\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  0.438861/  0.879044, val:  80.83%, val_best:  82.92%, tr:  99.39%, tr_best: 100.00%, epoch time: 44.67 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.4098%\n",
      "layer   2  Sparsity: 74.7489%\n",
      "layer   3  Sparsity: 66.8441%\n",
      "total_backward_count 425865 real_backward_count 39996   9.392%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  0.426477/  0.927861, val:  77.50%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.71 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.4104%\n",
      "layer   2  Sparsity: 74.7057%\n",
      "layer   3  Sparsity: 66.8737%\n",
      "total_backward_count 430760 real_backward_count 40206   9.334%\n",
      "fc layer 3 self.abs_max_out: 1537.0\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  0.426627/  0.923489, val:  79.58%, val_best:  82.92%, tr:  99.49%, tr_best: 100.00%, epoch time: 44.85 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3304%\n",
      "layer   2  Sparsity: 74.6217%\n",
      "layer   3  Sparsity: 66.4851%\n",
      "total_backward_count 435655 real_backward_count 40449   9.285%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  0.438503/  0.898294, val:  80.00%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.99 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3146%\n",
      "layer   2  Sparsity: 75.1704%\n",
      "layer   3  Sparsity: 67.0237%\n",
      "total_backward_count 440550 real_backward_count 40678   9.233%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  0.411062/  0.904101, val:  79.58%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.12 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3870%\n",
      "layer   2  Sparsity: 75.1170%\n",
      "layer   3  Sparsity: 67.7610%\n",
      "total_backward_count 445445 real_backward_count 40877   9.177%\n",
      "lif layer 1 self.abs_max_v: 19761.5\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  0.413912/  0.887185, val:  78.33%, val_best:  82.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 44.69 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3600%\n",
      "layer   2  Sparsity: 74.9315%\n",
      "layer   3  Sparsity: 68.0381%\n",
      "total_backward_count 450340 real_backward_count 41080   9.122%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  0.405547/  0.893950, val:  77.92%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.28 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.2960%\n",
      "layer   2  Sparsity: 74.9906%\n",
      "layer   3  Sparsity: 67.3774%\n",
      "total_backward_count 455235 real_backward_count 41270   9.066%\n",
      "fc layer 1 self.abs_max_out: 11363.0\n",
      "lif layer 1 self.abs_max_v: 19817.5\n",
      "lif layer 1 self.abs_max_v: 19992.0\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  0.417141/  0.925857, val:  77.50%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.55 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3658%\n",
      "layer   2  Sparsity: 75.2194%\n",
      "layer   3  Sparsity: 67.2393%\n",
      "total_backward_count 460130 real_backward_count 41474   9.014%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  0.417103/  0.947454, val:  72.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.34 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3198%\n",
      "layer   2  Sparsity: 75.1633%\n",
      "layer   3  Sparsity: 67.3387%\n",
      "total_backward_count 465025 real_backward_count 41665   8.960%\n",
      "fc layer 3 self.abs_max_out: 1563.0\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  0.405143/  0.901322, val:  80.00%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 45.06 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3810%\n",
      "layer   2  Sparsity: 75.1437%\n",
      "layer   3  Sparsity: 67.1827%\n",
      "total_backward_count 469920 real_backward_count 41863   8.909%\n",
      "fc layer 3 self.abs_max_out: 1597.0\n",
      "fc layer 3 self.abs_max_out: 1599.0\n",
      "fc layer 1 self.abs_max_out: 11496.0\n",
      "lif layer 1 self.abs_max_v: 20107.0\n",
      "fc layer 3 self.abs_max_out: 1601.0\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  0.390133/  0.927242, val:  77.08%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.69 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3665%\n",
      "layer   2  Sparsity: 74.9908%\n",
      "layer   3  Sparsity: 67.6730%\n",
      "total_backward_count 474815 real_backward_count 42054   8.857%\n",
      "lif layer 1 self.abs_max_v: 20139.0\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  0.391539/  0.843550, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.76 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.4184%\n",
      "layer   2  Sparsity: 74.9527%\n",
      "layer   3  Sparsity: 67.9116%\n",
      "total_backward_count 479710 real_backward_count 42255   8.808%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  0.388358/  0.896693, val:  78.33%, val_best:  83.33%, tr:  99.49%, tr_best: 100.00%, epoch time: 44.88 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3792%\n",
      "layer   2  Sparsity: 75.0555%\n",
      "layer   3  Sparsity: 68.0462%\n",
      "total_backward_count 484605 real_backward_count 42424   8.754%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  0.402557/  0.916340, val:  80.42%, val_best:  83.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.09 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.3752%\n",
      "layer   2  Sparsity: 74.8463%\n",
      "layer   3  Sparsity: 67.7515%\n",
      "total_backward_count 489500 real_backward_count 42615   8.706%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  0.393185/  0.930662, val:  81.25%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.16 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3319%\n",
      "layer   2  Sparsity: 75.0294%\n",
      "layer   3  Sparsity: 67.8825%\n",
      "total_backward_count 494395 real_backward_count 42776   8.652%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  0.369396/  0.826805, val:  82.50%, val_best:  83.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.23 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3378%\n",
      "layer   2  Sparsity: 75.2024%\n",
      "layer   3  Sparsity: 67.4463%\n",
      "total_backward_count 499290 real_backward_count 42960   8.604%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  0.375883/  0.874540, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.59 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3519%\n",
      "layer   2  Sparsity: 75.3251%\n",
      "layer   3  Sparsity: 66.8740%\n",
      "total_backward_count 504185 real_backward_count 43147   8.558%\n",
      "fc layer 1 self.abs_max_out: 11513.0\n",
      "lif layer 1 self.abs_max_v: 20171.0\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  0.392483/  0.901529, val:  80.42%, val_best:  83.33%, tr:  99.49%, tr_best: 100.00%, epoch time: 44.66 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3297%\n",
      "layer   2  Sparsity: 74.9267%\n",
      "layer   3  Sparsity: 66.9070%\n",
      "total_backward_count 509080 real_backward_count 43349   8.515%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  0.392939/  0.949007, val:  79.58%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.92 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3417%\n",
      "layer   2  Sparsity: 74.7204%\n",
      "layer   3  Sparsity: 67.2853%\n",
      "total_backward_count 513975 real_backward_count 43522   8.468%\n",
      "fc layer 3 self.abs_max_out: 1604.0\n",
      "lif layer 1 self.abs_max_v: 20269.5\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  0.387615/  0.849235, val:  82.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.67 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3272%\n",
      "layer   2  Sparsity: 74.8308%\n",
      "layer   3  Sparsity: 68.2544%\n",
      "total_backward_count 518870 real_backward_count 43683   8.419%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  0.373410/  0.928292, val:  75.83%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.80 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3458%\n",
      "layer   2  Sparsity: 75.1120%\n",
      "layer   3  Sparsity: 68.1041%\n",
      "total_backward_count 523765 real_backward_count 43870   8.376%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  0.359248/  0.952336, val:  75.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.88 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.4035%\n",
      "layer   2  Sparsity: 75.1175%\n",
      "layer   3  Sparsity: 68.3362%\n",
      "total_backward_count 528660 real_backward_count 44013   8.325%\n",
      "fc layer 1 self.abs_max_out: 11540.0\n",
      "lif layer 1 self.abs_max_v: 20325.0\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  0.367567/  0.851753, val:  81.67%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.27 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3340%\n",
      "layer   2  Sparsity: 74.8267%\n",
      "layer   3  Sparsity: 68.6849%\n",
      "total_backward_count 533555 real_backward_count 44156   8.276%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  0.357898/  0.827045, val:  82.50%, val_best:  83.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.44 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3535%\n",
      "layer   2  Sparsity: 75.3075%\n",
      "layer   3  Sparsity: 68.8902%\n",
      "total_backward_count 538450 real_backward_count 44318   8.231%\n",
      "fc layer 3 self.abs_max_out: 1629.0\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  0.346330/  0.849726, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.88 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3435%\n",
      "layer   2  Sparsity: 74.8606%\n",
      "layer   3  Sparsity: 68.9780%\n",
      "total_backward_count 543345 real_backward_count 44462   8.183%\n",
      "fc layer 1 self.abs_max_out: 11551.0\n",
      "lif layer 1 self.abs_max_v: 20375.0\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  0.348224/  0.861040, val:  82.50%, val_best:  83.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.22 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3626%\n",
      "layer   2  Sparsity: 74.7655%\n",
      "layer   3  Sparsity: 68.3682%\n",
      "total_backward_count 548240 real_backward_count 44612   8.137%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  0.346895/  0.891436, val:  79.17%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.73 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3642%\n",
      "layer   2  Sparsity: 74.3404%\n",
      "layer   3  Sparsity: 68.2153%\n",
      "total_backward_count 553135 real_backward_count 44761   8.092%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  0.356913/  0.835487, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.22 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3823%\n",
      "layer   2  Sparsity: 74.0715%\n",
      "layer   3  Sparsity: 68.2445%\n",
      "total_backward_count 558030 real_backward_count 44898   8.046%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  0.350294/  0.832419, val:  80.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.91 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3412%\n",
      "layer   2  Sparsity: 74.4228%\n",
      "layer   3  Sparsity: 68.3838%\n",
      "total_backward_count 562925 real_backward_count 45062   8.005%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  0.334320/  0.864884, val:  77.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.40 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3481%\n",
      "layer   2  Sparsity: 74.7769%\n",
      "layer   3  Sparsity: 68.9836%\n",
      "total_backward_count 567820 real_backward_count 45209   7.962%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  0.346039/  0.842533, val:  79.17%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.95 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.4151%\n",
      "layer   2  Sparsity: 74.9915%\n",
      "layer   3  Sparsity: 68.4911%\n",
      "total_backward_count 572715 real_backward_count 45356   7.919%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  0.341153/  0.857141, val:  81.25%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 45.20 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3540%\n",
      "layer   2  Sparsity: 74.8167%\n",
      "layer   3  Sparsity: 68.8800%\n",
      "total_backward_count 577610 real_backward_count 45504   7.878%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  0.330961/  0.837147, val:  80.42%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3633%\n",
      "layer   2  Sparsity: 75.0156%\n",
      "layer   3  Sparsity: 69.3295%\n",
      "total_backward_count 582505 real_backward_count 45629   7.833%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  0.327695/  0.826603, val:  82.92%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.67 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3910%\n",
      "layer   2  Sparsity: 74.9452%\n",
      "layer   3  Sparsity: 69.3441%\n",
      "total_backward_count 587400 real_backward_count 45773   7.792%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  0.322328/  0.872704, val:  80.83%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.94 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3268%\n",
      "layer   2  Sparsity: 74.6595%\n",
      "layer   3  Sparsity: 69.0007%\n",
      "total_backward_count 592295 real_backward_count 45929   7.754%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  0.337402/  0.963233, val:  69.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.85 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3185%\n",
      "layer   2  Sparsity: 74.6948%\n",
      "layer   3  Sparsity: 68.5021%\n",
      "total_backward_count 597190 real_backward_count 46087   7.717%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  0.341689/  0.868192, val:  79.17%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.81 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3657%\n",
      "layer   2  Sparsity: 74.7868%\n",
      "layer   3  Sparsity: 68.5125%\n",
      "total_backward_count 602085 real_backward_count 46233   7.679%\n",
      "fc layer 3 self.abs_max_out: 1637.0\n",
      "fc layer 3 self.abs_max_out: 1737.0\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  0.329583/  0.802808, val:  83.33%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.46 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3809%\n",
      "layer   2  Sparsity: 74.8188%\n",
      "layer   3  Sparsity: 68.4508%\n",
      "total_backward_count 606980 real_backward_count 46379   7.641%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  0.325657/  0.906980, val:  77.92%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.58 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.2770%\n",
      "layer   2  Sparsity: 74.6411%\n",
      "layer   3  Sparsity: 68.5820%\n",
      "total_backward_count 611875 real_backward_count 46491   7.598%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  0.336099/  0.878838, val:  78.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.52 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.2997%\n",
      "layer   2  Sparsity: 74.7403%\n",
      "layer   3  Sparsity: 68.3537%\n",
      "total_backward_count 616770 real_backward_count 46619   7.559%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  0.323006/  0.858439, val:  80.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.43 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3441%\n",
      "layer   2  Sparsity: 74.7030%\n",
      "layer   3  Sparsity: 68.4516%\n",
      "total_backward_count 621665 real_backward_count 46748   7.520%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  0.317964/  0.837754, val:  79.17%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.37 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3616%\n",
      "layer   2  Sparsity: 74.8330%\n",
      "layer   3  Sparsity: 68.2684%\n",
      "total_backward_count 626560 real_backward_count 46896   7.485%\n",
      "fc layer 1 self.abs_max_out: 11597.0\n",
      "lif layer 1 self.abs_max_v: 20457.0\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  0.319802/  0.807828, val:  80.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.84 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3592%\n",
      "layer   2  Sparsity: 75.0217%\n",
      "layer   3  Sparsity: 68.1598%\n",
      "total_backward_count 631455 real_backward_count 47024   7.447%\n",
      "fc layer 1 self.abs_max_out: 11679.0\n",
      "lif layer 1 self.abs_max_v: 20606.5\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  0.325813/  0.837782, val:  81.67%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.53 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3260%\n",
      "layer   2  Sparsity: 74.9716%\n",
      "layer   3  Sparsity: 68.0409%\n",
      "total_backward_count 636350 real_backward_count 47154   7.410%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  0.331862/  0.841467, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.72 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3589%\n",
      "layer   2  Sparsity: 75.0263%\n",
      "layer   3  Sparsity: 66.9290%\n",
      "total_backward_count 641245 real_backward_count 47272   7.372%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  0.325324/  0.851847, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.43 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3852%\n",
      "layer   2  Sparsity: 75.1854%\n",
      "layer   3  Sparsity: 67.3281%\n",
      "total_backward_count 646140 real_backward_count 47426   7.340%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  0.323341/  0.850466, val:  81.67%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.59 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3398%\n",
      "layer   2  Sparsity: 75.2871%\n",
      "layer   3  Sparsity: 68.4481%\n",
      "total_backward_count 651035 real_backward_count 47563   7.306%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  0.317852/  0.869929, val:  78.75%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.37 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.4084%\n",
      "layer   2  Sparsity: 75.0481%\n",
      "layer   3  Sparsity: 68.9049%\n",
      "total_backward_count 655930 real_backward_count 47684   7.270%\n",
      "fc layer 1 self.abs_max_out: 11714.0\n",
      "lif layer 1 self.abs_max_v: 20649.0\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  0.325310/  0.882404, val:  79.58%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 45.04 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3625%\n",
      "layer   2  Sparsity: 74.6463%\n",
      "layer   3  Sparsity: 68.6427%\n",
      "total_backward_count 660825 real_backward_count 47828   7.238%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  0.319230/  0.851076, val:  78.75%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.45 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3362%\n",
      "layer   2  Sparsity: 74.6883%\n",
      "layer   3  Sparsity: 68.7512%\n",
      "total_backward_count 665720 real_backward_count 47946   7.202%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  0.302684/  0.842177, val:  78.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.78 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3602%\n",
      "layer   2  Sparsity: 74.5888%\n",
      "layer   3  Sparsity: 68.6752%\n",
      "total_backward_count 670615 real_backward_count 48070   7.168%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  0.310597/  0.820176, val:  82.50%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.15 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3283%\n",
      "layer   2  Sparsity: 74.6577%\n",
      "layer   3  Sparsity: 69.4203%\n",
      "total_backward_count 675510 real_backward_count 48186   7.133%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  0.310600/  0.809491, val:  83.33%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.39 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3478%\n",
      "layer   2  Sparsity: 74.6786%\n",
      "layer   3  Sparsity: 68.5710%\n",
      "total_backward_count 680405 real_backward_count 48326   7.103%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  0.293841/  0.835013, val:  79.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.29 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3529%\n",
      "layer   2  Sparsity: 74.7321%\n",
      "layer   3  Sparsity: 69.0075%\n",
      "total_backward_count 685300 real_backward_count 48421   7.066%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  0.296214/  0.811100, val:  83.33%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.52 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3586%\n",
      "layer   2  Sparsity: 74.7220%\n",
      "layer   3  Sparsity: 68.9514%\n",
      "total_backward_count 690195 real_backward_count 48516   7.029%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  0.296481/  0.878097, val:  77.92%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.97 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3433%\n",
      "layer   2  Sparsity: 74.9138%\n",
      "layer   3  Sparsity: 69.2823%\n",
      "total_backward_count 695090 real_backward_count 48622   6.995%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  0.326338/  0.800277, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.50 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3153%\n",
      "layer   2  Sparsity: 75.0430%\n",
      "layer   3  Sparsity: 69.4295%\n",
      "total_backward_count 699985 real_backward_count 48753   6.965%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  0.299735/  0.868048, val:  79.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.67 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3493%\n",
      "layer   2  Sparsity: 75.2089%\n",
      "layer   3  Sparsity: 68.9097%\n",
      "total_backward_count 704880 real_backward_count 48844   6.929%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  0.326531/  0.845967, val:  78.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.08 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.4141%\n",
      "layer   2  Sparsity: 75.1526%\n",
      "layer   3  Sparsity: 68.7477%\n",
      "total_backward_count 709775 real_backward_count 48961   6.898%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  0.317264/  0.839861, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.95 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3883%\n",
      "layer   2  Sparsity: 75.5211%\n",
      "layer   3  Sparsity: 68.8950%\n",
      "total_backward_count 714670 real_backward_count 49046   6.863%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  0.315004/  0.873056, val:  79.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.06 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3083%\n",
      "layer   2  Sparsity: 75.1994%\n",
      "layer   3  Sparsity: 69.3188%\n",
      "total_backward_count 719565 real_backward_count 49178   6.834%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  0.302296/  0.852045, val:  79.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.50 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3229%\n",
      "layer   2  Sparsity: 74.9924%\n",
      "layer   3  Sparsity: 69.2906%\n",
      "total_backward_count 724460 real_backward_count 49283   6.803%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  0.292390/  0.869804, val:  79.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.84 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3067%\n",
      "layer   2  Sparsity: 74.7312%\n",
      "layer   3  Sparsity: 68.8674%\n",
      "total_backward_count 729355 real_backward_count 49378   6.770%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  0.303345/  0.855053, val:  78.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3400%\n",
      "layer   2  Sparsity: 74.9631%\n",
      "layer   3  Sparsity: 69.1286%\n",
      "total_backward_count 734250 real_backward_count 49486   6.740%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  0.306818/  0.846032, val:  79.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.61 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3413%\n",
      "layer   2  Sparsity: 75.0500%\n",
      "layer   3  Sparsity: 68.9664%\n",
      "total_backward_count 739145 real_backward_count 49613   6.712%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  0.302046/  0.799008, val:  81.67%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.47 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3531%\n",
      "layer   2  Sparsity: 75.2107%\n",
      "layer   3  Sparsity: 69.2604%\n",
      "total_backward_count 744040 real_backward_count 49727   6.683%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  0.302927/  0.835658, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.32 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3573%\n",
      "layer   2  Sparsity: 74.9172%\n",
      "layer   3  Sparsity: 68.7776%\n",
      "total_backward_count 748935 real_backward_count 49826   6.653%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  0.313774/  0.819288, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.51 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3915%\n",
      "layer   2  Sparsity: 74.6847%\n",
      "layer   3  Sparsity: 68.9363%\n",
      "total_backward_count 753830 real_backward_count 49952   6.626%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  0.292342/  0.818412, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.27 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.2998%\n",
      "layer   2  Sparsity: 74.6032%\n",
      "layer   3  Sparsity: 69.2950%\n",
      "total_backward_count 758725 real_backward_count 50052   6.597%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  0.313776/  0.862856, val:  80.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.81 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3789%\n",
      "layer   2  Sparsity: 74.9303%\n",
      "layer   3  Sparsity: 68.9469%\n",
      "total_backward_count 763620 real_backward_count 50168   6.570%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  0.303822/  0.898942, val:  76.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.57 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3545%\n",
      "layer   2  Sparsity: 74.8682%\n",
      "layer   3  Sparsity: 68.9389%\n",
      "total_backward_count 768515 real_backward_count 50296   6.545%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  0.289846/  0.785934, val:  83.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.85 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3516%\n",
      "layer   2  Sparsity: 74.6902%\n",
      "layer   3  Sparsity: 69.4965%\n",
      "total_backward_count 773410 real_backward_count 50368   6.512%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  0.301994/  0.825663, val:  80.00%, val_best:  84.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 44.78 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3512%\n",
      "layer   2  Sparsity: 75.0783%\n",
      "layer   3  Sparsity: 68.9658%\n",
      "total_backward_count 778305 real_backward_count 50467   6.484%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  0.301029/  0.904528, val:  77.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.85 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3438%\n",
      "layer   2  Sparsity: 75.0986%\n",
      "layer   3  Sparsity: 68.9016%\n",
      "total_backward_count 783200 real_backward_count 50574   6.457%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  0.311914/  0.811149, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.62 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.4424%\n",
      "layer   2  Sparsity: 74.8827%\n",
      "layer   3  Sparsity: 68.8395%\n",
      "total_backward_count 788095 real_backward_count 50677   6.430%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  0.312588/  0.827802, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.97 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3176%\n",
      "layer   2  Sparsity: 75.1502%\n",
      "layer   3  Sparsity: 68.6976%\n",
      "total_backward_count 792990 real_backward_count 50786   6.404%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  0.291956/  0.847393, val:  80.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.37 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3431%\n",
      "layer   2  Sparsity: 74.9845%\n",
      "layer   3  Sparsity: 68.6219%\n",
      "total_backward_count 797885 real_backward_count 50873   6.376%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  0.287832/  0.811496, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.52 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.4120%\n",
      "layer   2  Sparsity: 74.8610%\n",
      "layer   3  Sparsity: 68.7509%\n",
      "total_backward_count 802780 real_backward_count 50975   6.350%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  0.287730/  0.834133, val:  80.83%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.56 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3082%\n",
      "layer   2  Sparsity: 74.8991%\n",
      "layer   3  Sparsity: 68.4525%\n",
      "total_backward_count 807675 real_backward_count 51083   6.325%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  0.278703/  0.895869, val:  76.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.14 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3613%\n",
      "layer   2  Sparsity: 75.4166%\n",
      "layer   3  Sparsity: 67.8742%\n",
      "total_backward_count 812570 real_backward_count 51171   6.297%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  0.297723/  0.805670, val:  79.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.51 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3961%\n",
      "layer   2  Sparsity: 74.9942%\n",
      "layer   3  Sparsity: 68.0903%\n",
      "total_backward_count 817465 real_backward_count 51287   6.274%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  0.284185/  0.886858, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.74 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3741%\n",
      "layer   2  Sparsity: 75.0262%\n",
      "layer   3  Sparsity: 68.3683%\n",
      "total_backward_count 822360 real_backward_count 51372   6.247%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  0.292225/  0.853083, val:  78.75%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.62 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3096%\n",
      "layer   2  Sparsity: 75.1191%\n",
      "layer   3  Sparsity: 68.3215%\n",
      "total_backward_count 827255 real_backward_count 51463   6.221%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.272448/  0.800668, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.82 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3809%\n",
      "layer   2  Sparsity: 74.9692%\n",
      "layer   3  Sparsity: 69.5847%\n",
      "total_backward_count 832150 real_backward_count 51528   6.192%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.278136/  0.797722, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.36 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.2492%\n",
      "layer   2  Sparsity: 75.1520%\n",
      "layer   3  Sparsity: 69.0054%\n",
      "total_backward_count 837045 real_backward_count 51619   6.167%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  0.278057/  0.825355, val:  80.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.02 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3454%\n",
      "layer   2  Sparsity: 75.1054%\n",
      "layer   3  Sparsity: 69.6145%\n",
      "total_backward_count 841940 real_backward_count 51710   6.142%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  0.282236/  0.845348, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.81 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3483%\n",
      "layer   2  Sparsity: 75.1905%\n",
      "layer   3  Sparsity: 69.5663%\n",
      "total_backward_count 846835 real_backward_count 51810   6.118%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.277793/  0.824272, val:  79.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.88 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3445%\n",
      "layer   2  Sparsity: 75.2533%\n",
      "layer   3  Sparsity: 69.5726%\n",
      "total_backward_count 851730 real_backward_count 51899   6.093%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  0.292550/  0.851166, val:  81.67%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.42 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3700%\n",
      "layer   2  Sparsity: 75.2612%\n",
      "layer   3  Sparsity: 69.9060%\n",
      "total_backward_count 856625 real_backward_count 52004   6.071%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  0.269407/  0.896906, val:  76.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 42.47 seconds, 0.71 minutes\n",
      "layer   1  Sparsity: 86.3900%\n",
      "layer   2  Sparsity: 75.5007%\n",
      "layer   3  Sparsity: 69.9495%\n",
      "total_backward_count 861520 real_backward_count 52090   6.046%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  0.283459/  0.831606, val:  83.75%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 43.95 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.4225%\n",
      "layer   2  Sparsity: 74.8895%\n",
      "layer   3  Sparsity: 69.9651%\n",
      "total_backward_count 866415 real_backward_count 52186   6.023%\n",
      "lif layer 1 self.abs_max_v: 20650.5\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  0.271344/  0.814318, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.69 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3213%\n",
      "layer   2  Sparsity: 74.9930%\n",
      "layer   3  Sparsity: 69.9180%\n",
      "total_backward_count 871310 real_backward_count 52268   5.999%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  0.275569/  0.912790, val:  75.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.46 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3240%\n",
      "layer   2  Sparsity: 74.7675%\n",
      "layer   3  Sparsity: 69.4111%\n",
      "total_backward_count 876205 real_backward_count 52355   5.975%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  0.279338/  0.818978, val:  80.00%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 45.05 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3791%\n",
      "layer   2  Sparsity: 74.9941%\n",
      "layer   3  Sparsity: 69.1935%\n",
      "total_backward_count 881100 real_backward_count 52442   5.952%\n",
      "fc layer 2 self.abs_max_out: 4396.0\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  0.267227/  0.845918, val:  80.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.02 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3265%\n",
      "layer   2  Sparsity: 74.9072%\n",
      "layer   3  Sparsity: 69.7567%\n",
      "total_backward_count 885995 real_backward_count 52521   5.928%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  0.280660/  0.850056, val:  81.25%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 45.10 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.2912%\n",
      "layer   2  Sparsity: 74.9614%\n",
      "layer   3  Sparsity: 69.4086%\n",
      "total_backward_count 890890 real_backward_count 52628   5.907%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  0.275558/  0.817042, val:  80.83%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.25 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3389%\n",
      "layer   2  Sparsity: 74.7886%\n",
      "layer   3  Sparsity: 69.1345%\n",
      "total_backward_count 895785 real_backward_count 52717   5.885%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  0.274498/  0.824630, val:  79.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.44 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3918%\n",
      "layer   2  Sparsity: 74.6704%\n",
      "layer   3  Sparsity: 68.6623%\n",
      "total_backward_count 900680 real_backward_count 52810   5.863%\n",
      "fc layer 3 self.abs_max_out: 1749.0\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  0.261318/  0.816697, val:  80.83%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.00 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.3741%\n",
      "layer   2  Sparsity: 74.4362%\n",
      "layer   3  Sparsity: 69.0506%\n",
      "total_backward_count 905575 real_backward_count 52884   5.840%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  0.278659/  0.788537, val:  81.25%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 44.04 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.2853%\n",
      "layer   2  Sparsity: 74.5688%\n",
      "layer   3  Sparsity: 68.9194%\n",
      "total_backward_count 910470 real_backward_count 52985   5.820%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  0.272018/  0.807768, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 43.88 seconds, 0.73 minutes\n",
      "layer   1  Sparsity: 86.3906%\n",
      "layer   2  Sparsity: 74.5154%\n",
      "layer   3  Sparsity: 69.1692%\n",
      "total_backward_count 915365 real_backward_count 53073   5.798%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  0.266184/  0.781733, val:  82.50%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.94 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3673%\n",
      "layer   2  Sparsity: 74.6355%\n",
      "layer   3  Sparsity: 68.9688%\n",
      "total_backward_count 920260 real_backward_count 53176   5.778%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  0.256866/  0.771189, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.51 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3438%\n",
      "layer   2  Sparsity: 74.8893%\n",
      "layer   3  Sparsity: 68.9835%\n",
      "total_backward_count 925155 real_backward_count 53261   5.757%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  0.262706/  0.843501, val:  77.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.10 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3293%\n",
      "layer   2  Sparsity: 74.8684%\n",
      "layer   3  Sparsity: 68.6289%\n",
      "total_backward_count 930050 real_backward_count 53342   5.735%\n",
      "fc layer 3 self.abs_max_out: 1750.0\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  0.256766/  0.795542, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.23 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3344%\n",
      "layer   2  Sparsity: 74.6077%\n",
      "layer   3  Sparsity: 69.2127%\n",
      "total_backward_count 934945 real_backward_count 53419   5.714%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  0.255413/  0.811278, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.99 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3440%\n",
      "layer   2  Sparsity: 74.5073%\n",
      "layer   3  Sparsity: 69.6838%\n",
      "total_backward_count 939840 real_backward_count 53486   5.691%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  0.250195/  0.802427, val:  81.67%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 44.54 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3345%\n",
      "layer   2  Sparsity: 74.8723%\n",
      "layer   3  Sparsity: 69.8068%\n",
      "total_backward_count 944735 real_backward_count 53565   5.670%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  0.241494/  0.801693, val:  80.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 45.35 seconds, 0.76 minutes\n",
      "layer   1  Sparsity: 86.3612%\n",
      "layer   2  Sparsity: 74.8510%\n",
      "layer   3  Sparsity: 69.8596%\n",
      "total_backward_count 949630 real_backward_count 53641   5.649%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  0.238993/  0.788269, val:  80.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.38 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3611%\n",
      "layer   2  Sparsity: 74.7971%\n",
      "layer   3  Sparsity: 69.5944%\n",
      "total_backward_count 954525 real_backward_count 53705   5.626%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  0.249673/  0.855530, val:  80.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.84 seconds, 0.75 minutes\n",
      "layer   1  Sparsity: 86.3044%\n",
      "layer   2  Sparsity: 74.6292%\n",
      "layer   3  Sparsity: 69.6412%\n",
      "total_backward_count 959420 real_backward_count 53790   5.607%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  0.242646/  0.787208, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.41 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3700%\n",
      "layer   2  Sparsity: 74.6384%\n",
      "layer   3  Sparsity: 69.6005%\n",
      "total_backward_count 964315 real_backward_count 53857   5.585%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  0.233437/  0.827023, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.60 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3801%\n",
      "layer   2  Sparsity: 74.6577%\n",
      "layer   3  Sparsity: 69.3425%\n",
      "total_backward_count 969210 real_backward_count 53915   5.563%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.235608/  0.764278, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.44 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3331%\n",
      "layer   2  Sparsity: 75.0418%\n",
      "layer   3  Sparsity: 69.1107%\n",
      "total_backward_count 974105 real_backward_count 53983   5.542%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.240880/  0.792787, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 44.28 seconds, 0.74 minutes\n",
      "layer   1  Sparsity: 86.3653%\n",
      "layer   2  Sparsity: 74.9006%\n",
      "layer   3  Sparsity: 69.2149%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6dad721a8064d33a1b5bc80d1842212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñà‚ñá‚ñÖ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñà‚ñá‚ñÖ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.24088</td></tr><tr><td>val_acc_best</td><td>0.84583</td></tr><tr><td>val_acc_now</td><td>0.81667</td></tr><tr><td>val_loss</td><td>0.79279</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fast-sweep-6</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7kiuuo0a' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7kiuuo0a</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_013225-7kiuuo0a/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: l0gl0hpu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 75000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_040159-l0gl0hpu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/l0gl0hpu' target=\"_blank\">sweet-sweep-9</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/l0gl0hpu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/l0gl0hpu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251118_040208_501', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 15, 'dvs_duration': 75000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = d42a941a5ec61eeb71eeb0784ba56a39\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 91\n",
      "fc layer 1 self.abs_max_out: 1341.0\n",
      "lif layer 1 self.abs_max_v: 1341.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1774.0\n",
      "lif layer 2 self.abs_max_v: 1774.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 319.0\n",
      "fc layer 1 self.abs_max_out: 1484.0\n",
      "lif layer 1 self.abs_max_v: 1624.5\n",
      "lif layer 2 self.abs_max_v: 1895.0\n",
      "fc layer 3 self.abs_max_out: 603.0\n",
      "lif layer 1 self.abs_max_v: 1832.5\n",
      "lif layer 2 self.abs_max_v: 1934.0\n",
      "fc layer 1 self.abs_max_out: 1662.0\n",
      "lif layer 2 self.abs_max_v: 2278.5\n",
      "lif layer 1 self.abs_max_v: 2004.0\n",
      "fc layer 1 self.abs_max_out: 1809.0\n",
      "lif layer 1 self.abs_max_v: 2233.5\n",
      "fc layer 1 self.abs_max_out: 1992.0\n",
      "lif layer 1 self.abs_max_v: 2358.0\n",
      "lif layer 1 self.abs_max_v: 2596.0\n",
      "smallest_now_T updated: 82\n",
      "fc layer 3 self.abs_max_out: 664.0\n",
      "fc layer 1 self.abs_max_out: 2807.0\n",
      "lif layer 1 self.abs_max_v: 3124.5\n",
      "lif layer 1 self.abs_max_v: 3298.5\n",
      "fc layer 2 self.abs_max_out: 1878.0\n",
      "lif layer 1 self.abs_max_v: 3303.5\n",
      "lif layer 2 self.abs_max_v: 2304.0\n",
      "lif layer 2 self.abs_max_v: 2629.5\n",
      "fc layer 1 self.abs_max_out: 2928.0\n",
      "smallest_now_T updated: 61\n",
      "fc layer 1 self.abs_max_out: 3214.0\n",
      "lif layer 2 self.abs_max_v: 2853.5\n",
      "fc layer 2 self.abs_max_out: 2071.0\n",
      "lif layer 2 self.abs_max_v: 2897.0\n",
      "lif layer 2 self.abs_max_v: 2927.5\n",
      "lif layer 2 self.abs_max_v: 3144.0\n",
      "lif layer 2 self.abs_max_v: 3504.0\n",
      "fc layer 1 self.abs_max_out: 3294.0\n",
      "fc layer 1 self.abs_max_out: 3351.0\n",
      "lif layer 1 self.abs_max_v: 3351.0\n",
      "lif layer 1 self.abs_max_v: 3363.0\n",
      "lif layer 1 self.abs_max_v: 3757.5\n",
      "fc layer 1 self.abs_max_out: 3441.0\n",
      "fc layer 3 self.abs_max_out: 716.0\n",
      "fc layer 3 self.abs_max_out: 720.0\n",
      "fc layer 1 self.abs_max_out: 3621.0\n",
      "lif layer 1 self.abs_max_v: 4274.5\n",
      "fc layer 3 self.abs_max_out: 724.0\n",
      "fc layer 1 self.abs_max_out: 3741.0\n",
      "fc layer 2 self.abs_max_out: 2155.0\n",
      "fc layer 2 self.abs_max_out: 2200.0\n",
      "fc layer 2 self.abs_max_out: 2396.0\n",
      "fc layer 2 self.abs_max_out: 2487.0\n",
      "fc layer 3 self.abs_max_out: 808.0\n",
      "fc layer 1 self.abs_max_out: 4299.0\n",
      "lif layer 1 self.abs_max_v: 4299.0\n",
      "lif layer 1 self.abs_max_v: 4476.5\n",
      "fc layer 1 self.abs_max_out: 4490.0\n",
      "lif layer 1 self.abs_max_v: 4518.0\n",
      "fc layer 1 self.abs_max_out: 4584.0\n",
      "lif layer 1 self.abs_max_v: 4584.0\n",
      "fc layer 2 self.abs_max_out: 2722.0\n",
      "lif layer 1 self.abs_max_v: 4671.5\n",
      "lif layer 2 self.abs_max_v: 3516.0\n",
      "lif layer 2 self.abs_max_v: 3701.5\n",
      "lif layer 2 self.abs_max_v: 4068.0\n",
      "lif layer 2 self.abs_max_v: 4325.0\n",
      "smallest_now_T updated: 51\n",
      "lif layer 1 self.abs_max_v: 4832.5\n",
      "lif layer 1 self.abs_max_v: 5023.0\n",
      "lif layer 1 self.abs_max_v: 5346.0\n",
      "lif layer 1 self.abs_max_v: 5485.5\n",
      "lif layer 1 self.abs_max_v: 6326.0\n",
      "fc layer 1 self.abs_max_out: 4793.0\n",
      "fc layer 1 self.abs_max_out: 4806.0\n",
      "fc layer 2 self.abs_max_out: 2772.0\n",
      "fc layer 1 self.abs_max_out: 4977.0\n",
      "fc layer 2 self.abs_max_out: 2806.0\n",
      "lif layer 2 self.abs_max_v: 4383.0\n",
      "fc layer 3 self.abs_max_out: 836.0\n",
      "lif layer 2 self.abs_max_v: 4449.0\n",
      "fc layer 2 self.abs_max_out: 2807.0\n",
      "lif layer 2 self.abs_max_v: 4468.5\n",
      "fc layer 2 self.abs_max_out: 2846.0\n",
      "lif layer 2 self.abs_max_v: 4667.0\n",
      "fc layer 3 self.abs_max_out: 885.0\n",
      "smallest_now_T updated: 50\n",
      "lif layer 1 self.abs_max_v: 6488.0\n",
      "lif layer 1 self.abs_max_v: 7011.0\n",
      "lif layer 2 self.abs_max_v: 4787.5\n",
      "lif layer 2 self.abs_max_v: 5112.0\n",
      "lif layer 2 self.abs_max_v: 5123.5\n",
      "fc layer 2 self.abs_max_out: 2903.0\n",
      "fc layer 2 self.abs_max_out: 2995.0\n",
      "lif layer 2 self.abs_max_v: 5157.5\n",
      "lif layer 2 self.abs_max_v: 5200.0\n",
      "lif layer 1 self.abs_max_v: 7254.0\n",
      "lif layer 1 self.abs_max_v: 7349.5\n",
      "lif layer 2 self.abs_max_v: 5248.0\n",
      "lif layer 1 self.abs_max_v: 7572.0\n",
      "fc layer 2 self.abs_max_out: 3081.0\n",
      "lif layer 2 self.abs_max_v: 5432.0\n",
      "fc layer 2 self.abs_max_out: 3161.0\n",
      "fc layer 1 self.abs_max_out: 5133.0\n",
      "fc layer 2 self.abs_max_out: 3298.0\n",
      "fc layer 2 self.abs_max_out: 3433.0\n",
      "fc layer 2 self.abs_max_out: 3483.0\n",
      "lif layer 2 self.abs_max_v: 5669.5\n",
      "fc layer 3 self.abs_max_out: 919.0\n",
      "fc layer 1 self.abs_max_out: 5186.0\n",
      "lif layer 1 self.abs_max_v: 7604.5\n",
      "lif layer 1 self.abs_max_v: 8015.5\n",
      "fc layer 1 self.abs_max_out: 5258.0\n",
      "lif layer 1 self.abs_max_v: 8533.5\n",
      "fc layer 2 self.abs_max_out: 3515.0\n",
      "lif layer 2 self.abs_max_v: 5977.5\n",
      "lif layer 1 self.abs_max_v: 8607.5\n",
      "fc layer 1 self.abs_max_out: 5619.0\n",
      "lif layer 1 self.abs_max_v: 9109.0\n",
      "lif layer 1 self.abs_max_v: 9248.5\n",
      "fc layer 1 self.abs_max_out: 6269.0\n",
      "lif layer 1 self.abs_max_v: 9276.5\n",
      "lif layer 1 self.abs_max_v: 9565.5\n",
      "fc layer 3 self.abs_max_out: 966.0\n",
      "fc layer 3 self.abs_max_out: 1051.0\n",
      "lif layer 1 self.abs_max_v: 9647.5\n",
      "fc layer 2 self.abs_max_out: 3547.0\n",
      "lif layer 1 self.abs_max_v: 10003.0\n",
      "lif layer 1 self.abs_max_v: 10387.5\n",
      "fc layer 1 self.abs_max_out: 6872.0\n",
      "fc layer 1 self.abs_max_out: 7026.0\n",
      "fc layer 1 self.abs_max_out: 7189.0\n",
      "fc layer 1 self.abs_max_out: 7286.0\n",
      "lif layer 1 self.abs_max_v: 10826.5\n",
      "lif layer 1 self.abs_max_v: 11607.5\n",
      "fc layer 1 self.abs_max_out: 7868.0\n",
      "lif layer 1 self.abs_max_v: 12057.0\n",
      "lif layer 1 self.abs_max_v: 12161.5\n",
      "lif layer 1 self.abs_max_v: 12743.0\n",
      "smallest_now_T_val updated: 84\n",
      "smallest_now_T_val updated: 69\n",
      "smallest_now_T_val updated: 68\n",
      "smallest_now_T_val updated: 67\n",
      "smallest_now_T_val updated: 55\n",
      "smallest_now_T_val updated: 50\n",
      "lif layer 1 self.abs_max_v: 12811.0\n",
      "lif layer 1 self.abs_max_v: 12828.5\n",
      "lif layer 1 self.abs_max_v: 13310.5\n",
      "fc layer 3 self.abs_max_out: 1057.0\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  1.987769/  2.004645, val:  46.67%, val_best:  46.67%, tr:  78.75%, tr_best:  78.75%, epoch time: 86.50 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4031%\n",
      "layer   2  Sparsity: 72.2001%\n",
      "layer   3  Sparsity: 74.5097%\n",
      "total_backward_count 9790 real_backward_count 3644  37.222%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 3707.0\n",
      "lif layer 1 self.abs_max_v: 13705.0\n",
      "fc layer 3 self.abs_max_out: 1084.0\n",
      "fc layer 3 self.abs_max_out: 1090.0\n",
      "fc layer 3 self.abs_max_out: 1122.0\n",
      "fc layer 3 self.abs_max_out: 1153.0\n",
      "fc layer 2 self.abs_max_out: 3733.0\n",
      "fc layer 1 self.abs_max_out: 7908.0\n",
      "lif layer 1 self.abs_max_v: 13877.5\n",
      "lif layer 1 self.abs_max_v: 14598.0\n",
      "fc layer 2 self.abs_max_out: 3859.0\n",
      "fc layer 3 self.abs_max_out: 1197.0\n",
      "fc layer 2 self.abs_max_out: 3870.0\n",
      "fc layer 1 self.abs_max_out: 7928.0\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  1.911655/  1.986728, val:  50.42%, val_best:  50.42%, tr:  93.67%, tr_best:  93.67%, epoch time: 85.41 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.4081%\n",
      "layer   2  Sparsity: 75.0726%\n",
      "layer   3  Sparsity: 73.3753%\n",
      "total_backward_count 19580 real_backward_count 5840  29.826%\n",
      "fc layer 1 self.abs_max_out: 8187.0\n",
      "lif layer 1 self.abs_max_v: 15032.0\n",
      "fc layer 1 self.abs_max_out: 8941.0\n",
      "lif layer 1 self.abs_max_v: 15592.0\n",
      "lif layer 1 self.abs_max_v: 16036.5\n",
      "fc layer 3 self.abs_max_out: 1274.0\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  1.906819/  1.978345, val:  58.33%, val_best:  58.33%, tr:  97.14%, tr_best:  97.14%, epoch time: 85.17 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.4089%\n",
      "layer   2  Sparsity: 76.9099%\n",
      "layer   3  Sparsity: 74.0788%\n",
      "total_backward_count 29370 real_backward_count 7682  26.156%\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  1.915052/  1.988459, val:  52.50%, val_best:  58.33%, tr:  97.55%, tr_best:  97.55%, epoch time: 85.30 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.4294%\n",
      "layer   2  Sparsity: 77.0882%\n",
      "layer   3  Sparsity: 73.7019%\n",
      "total_backward_count 39160 real_backward_count 9320  23.800%\n",
      "lif layer 2 self.abs_max_v: 6107.5\n",
      "fc layer 2 self.abs_max_out: 3906.0\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  1.907560/  1.987797, val:  54.58%, val_best:  58.33%, tr:  98.47%, tr_best:  98.47%, epoch time: 86.43 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4246%\n",
      "layer   2  Sparsity: 77.2369%\n",
      "layer   3  Sparsity: 73.6670%\n",
      "total_backward_count 48950 real_backward_count 10771  22.004%\n",
      "lif layer 2 self.abs_max_v: 6165.0\n",
      "fc layer 2 self.abs_max_out: 4100.0\n",
      "lif layer 2 self.abs_max_v: 6454.5\n",
      "lif layer 2 self.abs_max_v: 6484.5\n",
      "lif layer 2 self.abs_max_v: 6605.0\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  1.909845/  1.991418, val:  53.33%, val_best:  58.33%, tr:  98.77%, tr_best:  98.77%, epoch time: 86.28 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4140%\n",
      "layer   2  Sparsity: 76.9354%\n",
      "layer   3  Sparsity: 73.7344%\n",
      "total_backward_count 58740 real_backward_count 12136  20.661%\n",
      "fc layer 1 self.abs_max_out: 9231.0\n",
      "fc layer 1 self.abs_max_out: 10361.0\n",
      "lif layer 1 self.abs_max_v: 16083.5\n",
      "lif layer 1 self.abs_max_v: 16490.5\n",
      "lif layer 1 self.abs_max_v: 17128.5\n",
      "lif layer 1 self.abs_max_v: 17230.5\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  1.914757/  1.988541, val:  57.92%, val_best:  58.33%, tr:  99.28%, tr_best:  99.28%, epoch time: 86.65 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4097%\n",
      "layer   2  Sparsity: 77.4141%\n",
      "layer   3  Sparsity: 74.0752%\n",
      "total_backward_count 68530 real_backward_count 13405  19.561%\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  1.906441/  1.970666, val:  57.08%, val_best:  58.33%, tr:  99.59%, tr_best:  99.59%, epoch time: 86.79 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 82.4222%\n",
      "layer   2  Sparsity: 76.6835%\n",
      "layer   3  Sparsity: 74.1532%\n",
      "total_backward_count 78320 real_backward_count 14579  18.615%\n",
      "lif layer 2 self.abs_max_v: 6766.5\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  1.907845/  1.969244, val:  65.42%, val_best:  65.42%, tr:  99.59%, tr_best:  99.59%, epoch time: 85.65 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4317%\n",
      "layer   2  Sparsity: 76.6519%\n",
      "layer   3  Sparsity: 74.3582%\n",
      "total_backward_count 88110 real_backward_count 15717  17.838%\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  1.902298/  1.975671, val:  63.75%, val_best:  65.42%, tr:  99.69%, tr_best:  99.69%, epoch time: 86.25 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4149%\n",
      "layer   2  Sparsity: 76.7594%\n",
      "layer   3  Sparsity: 74.8520%\n",
      "total_backward_count 97900 real_backward_count 16795  17.155%\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  1.904289/  1.975032, val:  57.08%, val_best:  65.42%, tr:  99.80%, tr_best:  99.80%, epoch time: 85.84 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4209%\n",
      "layer   2  Sparsity: 77.0060%\n",
      "layer   3  Sparsity: 75.3073%\n",
      "total_backward_count 107690 real_backward_count 17824  16.551%\n",
      "lif layer 2 self.abs_max_v: 6974.0\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  1.910536/  1.965376, val:  67.50%, val_best:  67.50%, tr:  99.59%, tr_best:  99.80%, epoch time: 84.81 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 82.4181%\n",
      "layer   2  Sparsity: 76.3153%\n",
      "layer   3  Sparsity: 75.2530%\n",
      "total_backward_count 117480 real_backward_count 18869  16.061%\n",
      "fc layer 2 self.abs_max_out: 4204.0\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  1.912361/  1.985450, val:  62.08%, val_best:  67.50%, tr:  99.39%, tr_best:  99.80%, epoch time: 83.01 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 82.4271%\n",
      "layer   2  Sparsity: 76.6508%\n",
      "layer   3  Sparsity: 76.0901%\n",
      "total_backward_count 127270 real_backward_count 19857  15.602%\n",
      "lif layer 2 self.abs_max_v: 6983.5\n",
      "lif layer 1 self.abs_max_v: 17637.5\n",
      "fc layer 2 self.abs_max_out: 4205.0\n",
      "lif layer 1 self.abs_max_v: 17770.5\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  1.914573/  1.971291, val:  64.58%, val_best:  67.50%, tr:  99.69%, tr_best:  99.80%, epoch time: 85.92 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4290%\n",
      "layer   2  Sparsity: 75.7605%\n",
      "layer   3  Sparsity: 76.3400%\n",
      "total_backward_count 137060 real_backward_count 20808  15.182%\n",
      "fc layer 1 self.abs_max_out: 10683.0\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  1.907845/  1.961681, val:  64.17%, val_best:  67.50%, tr:  99.39%, tr_best:  99.80%, epoch time: 85.84 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.3853%\n",
      "layer   2  Sparsity: 75.8096%\n",
      "layer   3  Sparsity: 76.8411%\n",
      "total_backward_count 146850 real_backward_count 21690  14.770%\n",
      "fc layer 1 self.abs_max_out: 10897.0\n",
      "fc layer 1 self.abs_max_out: 10940.0\n",
      "fc layer 2 self.abs_max_out: 4219.0\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  1.915230/  1.990754, val:  63.33%, val_best:  67.50%, tr:  99.69%, tr_best:  99.80%, epoch time: 86.00 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4083%\n",
      "layer   2  Sparsity: 75.5387%\n",
      "layer   3  Sparsity: 77.3877%\n",
      "total_backward_count 156640 real_backward_count 22565  14.406%\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  1.919542/  1.990811, val:  70.83%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.79 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4146%\n",
      "layer   2  Sparsity: 76.2168%\n",
      "layer   3  Sparsity: 77.9192%\n",
      "total_backward_count 166430 real_backward_count 23425  14.075%\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  1.924365/  1.987279, val:  70.00%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.78 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4198%\n",
      "layer   2  Sparsity: 76.1393%\n",
      "layer   3  Sparsity: 78.0205%\n",
      "total_backward_count 176220 real_backward_count 24221  13.745%\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  1.930999/  1.980504, val:  75.00%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.02 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.4222%\n",
      "layer   2  Sparsity: 76.3236%\n",
      "layer   3  Sparsity: 78.6386%\n",
      "total_backward_count 186010 real_backward_count 24997  13.439%\n",
      "lif layer 1 self.abs_max_v: 17910.5\n",
      "fc layer 2 self.abs_max_out: 4221.0\n",
      "fc layer 2 self.abs_max_out: 4313.0\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  1.924157/  1.992413, val:  66.67%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.48 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.4037%\n",
      "layer   2  Sparsity: 75.9834%\n",
      "layer   3  Sparsity: 79.1661%\n",
      "total_backward_count 195800 real_backward_count 25728  13.140%\n",
      "lif layer 1 self.abs_max_v: 18423.5\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  1.927596/  1.995624, val:  71.25%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 86.16 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4121%\n",
      "layer   2  Sparsity: 75.8655%\n",
      "layer   3  Sparsity: 79.6409%\n",
      "total_backward_count 205590 real_backward_count 26468  12.874%\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  1.937777/  1.992406, val:  72.92%, val_best:  75.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 86.46 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4188%\n",
      "layer   2  Sparsity: 75.9856%\n",
      "layer   3  Sparsity: 80.0728%\n",
      "total_backward_count 215380 real_backward_count 27203  12.630%\n",
      "lif layer 1 self.abs_max_v: 18609.5\n",
      "lif layer 1 self.abs_max_v: 18924.0\n",
      "lif layer 2 self.abs_max_v: 7094.5\n",
      "lif layer 2 self.abs_max_v: 7223.5\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  1.938260/  1.999239, val:  70.00%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.74 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4394%\n",
      "layer   2  Sparsity: 75.6798%\n",
      "layer   3  Sparsity: 80.2096%\n",
      "total_backward_count 225170 real_backward_count 27850  12.368%\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  1.936761/  1.995833, val:  73.33%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.57 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4255%\n",
      "layer   2  Sparsity: 75.4049%\n",
      "layer   3  Sparsity: 80.5323%\n",
      "total_backward_count 234960 real_backward_count 28545  12.149%\n",
      "fc layer 1 self.abs_max_out: 11150.0\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  1.938041/  1.996863, val:  68.75%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.79 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4073%\n",
      "layer   2  Sparsity: 75.5915%\n",
      "layer   3  Sparsity: 80.9537%\n",
      "total_backward_count 244750 real_backward_count 29246  11.949%\n",
      "fc layer 1 self.abs_max_out: 11356.0\n",
      "lif layer 2 self.abs_max_v: 7652.5\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  1.936732/  1.992060, val:  72.50%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.14 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4244%\n",
      "layer   2  Sparsity: 75.4149%\n",
      "layer   3  Sparsity: 81.1477%\n",
      "total_backward_count 254540 real_backward_count 29886  11.741%\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  1.939330/  1.998235, val:  76.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.60 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4078%\n",
      "layer   2  Sparsity: 75.5222%\n",
      "layer   3  Sparsity: 81.4249%\n",
      "total_backward_count 264330 real_backward_count 30486  11.533%\n",
      "lif layer 1 self.abs_max_v: 19238.0\n",
      "lif layer 1 self.abs_max_v: 19259.0\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  1.940742/  1.992454, val:  74.58%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.76 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 82.4172%\n",
      "layer   2  Sparsity: 75.1593%\n",
      "layer   3  Sparsity: 81.1333%\n",
      "total_backward_count 274120 real_backward_count 31123  11.354%\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  1.943132/  2.003328, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.74 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 82.4181%\n",
      "layer   2  Sparsity: 75.7050%\n",
      "layer   3  Sparsity: 81.9334%\n",
      "total_backward_count 283910 real_backward_count 31721  11.173%\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  1.939981/  1.993062, val:  77.92%, val_best:  77.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.77 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.3821%\n",
      "layer   2  Sparsity: 75.0153%\n",
      "layer   3  Sparsity: 82.0040%\n",
      "total_backward_count 293700 real_backward_count 32308  11.000%\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  1.943152/  1.991930, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.60 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4134%\n",
      "layer   2  Sparsity: 75.1911%\n",
      "layer   3  Sparsity: 82.1270%\n",
      "total_backward_count 303490 real_backward_count 32871  10.831%\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  1.945462/  2.001422, val:  69.17%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.26 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4192%\n",
      "layer   2  Sparsity: 75.4760%\n",
      "layer   3  Sparsity: 82.3604%\n",
      "total_backward_count 313280 real_backward_count 33419  10.667%\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  1.944071/  2.004871, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.53 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4058%\n",
      "layer   2  Sparsity: 75.1779%\n",
      "layer   3  Sparsity: 82.5250%\n",
      "total_backward_count 323070 real_backward_count 33939  10.505%\n",
      "fc layer 1 self.abs_max_out: 11977.0\n",
      "lif layer 1 self.abs_max_v: 20577.0\n",
      "lif layer 2 self.abs_max_v: 7687.0\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  1.948192/  2.006615, val:  76.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.58 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4212%\n",
      "layer   2  Sparsity: 75.2836%\n",
      "layer   3  Sparsity: 82.9827%\n",
      "total_backward_count 332860 real_backward_count 34479  10.358%\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  1.949647/  2.006474, val:  82.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.16 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4202%\n",
      "layer   2  Sparsity: 75.5603%\n",
      "layer   3  Sparsity: 82.9943%\n",
      "total_backward_count 342650 real_backward_count 34974  10.207%\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  1.956453/  2.013114, val:  71.67%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.75 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4324%\n",
      "layer   2  Sparsity: 75.0180%\n",
      "layer   3  Sparsity: 83.2640%\n",
      "total_backward_count 352440 real_backward_count 35460  10.061%\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  1.953201/  2.006108, val:  80.00%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.30 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.4155%\n",
      "layer   2  Sparsity: 75.0639%\n",
      "layer   3  Sparsity: 83.3630%\n",
      "total_backward_count 362230 real_backward_count 35927   9.918%\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  1.955169/  2.012009, val:  72.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.56 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4195%\n",
      "layer   2  Sparsity: 75.1514%\n",
      "layer   3  Sparsity: 83.3432%\n",
      "total_backward_count 372020 real_backward_count 36360   9.774%\n",
      "lif layer 2 self.abs_max_v: 7760.0\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  1.946593/  2.006522, val:  73.33%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.74 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4162%\n",
      "layer   2  Sparsity: 75.0338%\n",
      "layer   3  Sparsity: 82.9867%\n",
      "total_backward_count 381810 real_backward_count 36818   9.643%\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  1.947948/  2.013455, val:  73.75%, val_best:  82.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.82 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4344%\n",
      "layer   2  Sparsity: 75.2331%\n",
      "layer   3  Sparsity: 83.3514%\n",
      "total_backward_count 391600 real_backward_count 37263   9.516%\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  1.950249/  2.018608, val:  74.17%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.94 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4164%\n",
      "layer   2  Sparsity: 75.5120%\n",
      "layer   3  Sparsity: 83.4266%\n",
      "total_backward_count 401390 real_backward_count 37715   9.396%\n",
      "lif layer 2 self.abs_max_v: 7763.5\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  1.955983/  2.018647, val:  74.58%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.70 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4315%\n",
      "layer   2  Sparsity: 75.5068%\n",
      "layer   3  Sparsity: 83.6338%\n",
      "total_backward_count 411180 real_backward_count 38125   9.272%\n",
      "lif layer 2 self.abs_max_v: 7866.0\n",
      "lif layer 2 self.abs_max_v: 7869.0\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  1.956845/  2.017495, val:  72.50%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.13 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4009%\n",
      "layer   2  Sparsity: 75.2014%\n",
      "layer   3  Sparsity: 83.5570%\n",
      "total_backward_count 420970 real_backward_count 38569   9.162%\n",
      "fc layer 2 self.abs_max_out: 4368.0\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  1.957878/  2.012523, val:  75.83%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.04 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4222%\n",
      "layer   2  Sparsity: 74.8957%\n",
      "layer   3  Sparsity: 83.6523%\n",
      "total_backward_count 430760 real_backward_count 38993   9.052%\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  1.953747/  2.013951, val:  80.42%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.41 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4110%\n",
      "layer   2  Sparsity: 75.0277%\n",
      "layer   3  Sparsity: 83.5158%\n",
      "total_backward_count 440550 real_backward_count 39373   8.937%\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  1.953764/  2.016543, val:  78.75%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.06 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.3923%\n",
      "layer   2  Sparsity: 74.9799%\n",
      "layer   3  Sparsity: 83.5842%\n",
      "total_backward_count 450340 real_backward_count 39752   8.827%\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  1.960185/  2.018122, val:  73.33%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.56 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4145%\n",
      "layer   2  Sparsity: 74.9831%\n",
      "layer   3  Sparsity: 83.6362%\n",
      "total_backward_count 460130 real_backward_count 40120   8.719%\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  1.957766/  2.017383, val:  75.42%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.80 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4122%\n",
      "layer   2  Sparsity: 74.7678%\n",
      "layer   3  Sparsity: 83.8846%\n",
      "total_backward_count 469920 real_backward_count 40482   8.615%\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  1.957460/  2.014981, val:  81.67%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.72 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4153%\n",
      "layer   2  Sparsity: 75.4377%\n",
      "layer   3  Sparsity: 84.1266%\n",
      "total_backward_count 479710 real_backward_count 40845   8.515%\n",
      "lif layer 2 self.abs_max_v: 7908.5\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  1.958778/  2.018742, val:  78.33%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.31 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4389%\n",
      "layer   2  Sparsity: 75.2925%\n",
      "layer   3  Sparsity: 84.0916%\n",
      "total_backward_count 489500 real_backward_count 41175   8.412%\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  1.961355/  2.022439, val:  77.50%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.27 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4156%\n",
      "layer   2  Sparsity: 75.3049%\n",
      "layer   3  Sparsity: 84.2069%\n",
      "total_backward_count 499290 real_backward_count 41540   8.320%\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  1.959865/  2.015153, val:  79.58%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.96 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4255%\n",
      "layer   2  Sparsity: 75.1868%\n",
      "layer   3  Sparsity: 84.4235%\n",
      "total_backward_count 509080 real_backward_count 41888   8.228%\n",
      "fc layer 2 self.abs_max_out: 4371.0\n",
      "lif layer 2 self.abs_max_v: 7936.0\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  1.954481/  2.011883, val:  79.17%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.65 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4331%\n",
      "layer   2  Sparsity: 75.1018%\n",
      "layer   3  Sparsity: 84.2244%\n",
      "total_backward_count 518870 real_backward_count 42214   8.136%\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  1.952646/  2.020014, val:  74.58%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.27 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.3953%\n",
      "layer   2  Sparsity: 74.8402%\n",
      "layer   3  Sparsity: 84.2657%\n",
      "total_backward_count 528660 real_backward_count 42512   8.041%\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  1.952955/  2.020528, val:  81.67%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.22 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4308%\n",
      "layer   2  Sparsity: 75.1078%\n",
      "layer   3  Sparsity: 84.2903%\n",
      "total_backward_count 538450 real_backward_count 42822   7.953%\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  1.957629/  2.023430, val:  77.92%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.41 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4214%\n",
      "layer   2  Sparsity: 75.2997%\n",
      "layer   3  Sparsity: 84.6786%\n",
      "total_backward_count 548240 real_backward_count 43139   7.869%\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  1.956442/  2.009620, val:  77.92%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.41 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.3955%\n",
      "layer   2  Sparsity: 74.8839%\n",
      "layer   3  Sparsity: 84.5964%\n",
      "total_backward_count 558030 real_backward_count 43435   7.784%\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  1.946640/  2.015411, val:  80.83%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.58 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4152%\n",
      "layer   2  Sparsity: 75.5751%\n",
      "layer   3  Sparsity: 84.6242%\n",
      "total_backward_count 567820 real_backward_count 43700   7.696%\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  1.948065/  2.012601, val:  78.75%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.00 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4097%\n",
      "layer   2  Sparsity: 75.0959%\n",
      "layer   3  Sparsity: 84.4306%\n",
      "total_backward_count 577610 real_backward_count 43963   7.611%\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  1.950281/  2.013736, val:  79.17%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.27 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4431%\n",
      "layer   2  Sparsity: 75.1677%\n",
      "layer   3  Sparsity: 84.4956%\n",
      "total_backward_count 587400 real_backward_count 44247   7.533%\n",
      "fc layer 1 self.abs_max_out: 12218.0\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  1.951193/  2.023432, val:  78.75%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.02 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4135%\n",
      "layer   2  Sparsity: 74.9689%\n",
      "layer   3  Sparsity: 84.6652%\n",
      "total_backward_count 597190 real_backward_count 44529   7.456%\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  1.951538/  2.012871, val:  79.17%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.89 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.3980%\n",
      "layer   2  Sparsity: 75.3122%\n",
      "layer   3  Sparsity: 84.7347%\n",
      "total_backward_count 606980 real_backward_count 44812   7.383%\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  1.951698/  2.012942, val:  80.00%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.48 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.4364%\n",
      "layer   2  Sparsity: 75.2495%\n",
      "layer   3  Sparsity: 84.9018%\n",
      "total_backward_count 616770 real_backward_count 45059   7.306%\n",
      "fc layer 2 self.abs_max_out: 4414.0\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  1.950861/  2.011864, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.74 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4033%\n",
      "layer   2  Sparsity: 75.1514%\n",
      "layer   3  Sparsity: 84.8310%\n",
      "total_backward_count 626560 real_backward_count 45347   7.237%\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  1.947690/  2.016776, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.91 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4081%\n",
      "layer   2  Sparsity: 75.0361%\n",
      "layer   3  Sparsity: 84.8054%\n",
      "total_backward_count 636350 real_backward_count 45608   7.167%\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  1.946908/  2.011053, val:  79.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.29 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4166%\n",
      "layer   2  Sparsity: 75.1040%\n",
      "layer   3  Sparsity: 85.0143%\n",
      "total_backward_count 646140 real_backward_count 45848   7.096%\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  1.949034/  2.017833, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.57 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.3970%\n",
      "layer   2  Sparsity: 75.1292%\n",
      "layer   3  Sparsity: 85.2264%\n",
      "total_backward_count 655930 real_backward_count 46083   7.026%\n",
      "fc layer 2 self.abs_max_out: 4473.0\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  1.955069/  2.024686, val:  76.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.02 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4086%\n",
      "layer   2  Sparsity: 75.3178%\n",
      "layer   3  Sparsity: 85.2646%\n",
      "total_backward_count 665720 real_backward_count 46296   6.954%\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  1.956217/  2.020423, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.37 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4141%\n",
      "layer   2  Sparsity: 75.1445%\n",
      "layer   3  Sparsity: 85.2070%\n",
      "total_backward_count 675510 real_backward_count 46529   6.888%\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  1.955454/  2.024243, val:  79.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.41 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4144%\n",
      "layer   2  Sparsity: 75.0767%\n",
      "layer   3  Sparsity: 85.2970%\n",
      "total_backward_count 685300 real_backward_count 46759   6.823%\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  1.958461/  2.023798, val:  77.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.99 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 82.4280%\n",
      "layer   2  Sparsity: 74.9226%\n",
      "layer   3  Sparsity: 85.3264%\n",
      "total_backward_count 695090 real_backward_count 46973   6.758%\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  1.952765/  2.023383, val:  76.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.08 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4222%\n",
      "layer   2  Sparsity: 75.1409%\n",
      "layer   3  Sparsity: 85.2416%\n",
      "total_backward_count 704880 real_backward_count 47221   6.699%\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  1.956360/  2.016436, val:  77.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.63 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4100%\n",
      "layer   2  Sparsity: 75.0582%\n",
      "layer   3  Sparsity: 85.4539%\n",
      "total_backward_count 714670 real_backward_count 47397   6.632%\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  1.957587/  2.018598, val:  77.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.61 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4226%\n",
      "layer   2  Sparsity: 74.9049%\n",
      "layer   3  Sparsity: 85.8141%\n",
      "total_backward_count 724460 real_backward_count 47596   6.570%\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  1.955958/  2.030108, val:  78.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.49 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.4178%\n",
      "layer   2  Sparsity: 74.8095%\n",
      "layer   3  Sparsity: 85.7200%\n",
      "total_backward_count 734250 real_backward_count 47796   6.509%\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  1.956151/  2.020768, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.20 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4102%\n",
      "layer   2  Sparsity: 75.0353%\n",
      "layer   3  Sparsity: 85.7682%\n",
      "total_backward_count 744040 real_backward_count 47999   6.451%\n",
      "lif layer 2 self.abs_max_v: 7994.5\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  1.954279/  2.020731, val:  77.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.76 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 82.4133%\n",
      "layer   2  Sparsity: 74.9640%\n",
      "layer   3  Sparsity: 85.8412%\n",
      "total_backward_count 753830 real_backward_count 48194   6.393%\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  1.954528/  2.025729, val:  77.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.94 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4157%\n",
      "layer   2  Sparsity: 74.9620%\n",
      "layer   3  Sparsity: 85.5493%\n",
      "total_backward_count 763620 real_backward_count 48393   6.337%\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  1.956860/  2.020509, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.12 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4247%\n",
      "layer   2  Sparsity: 75.1683%\n",
      "layer   3  Sparsity: 85.6381%\n",
      "total_backward_count 773410 real_backward_count 48591   6.283%\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  1.957636/  2.022405, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.28 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4050%\n",
      "layer   2  Sparsity: 75.1294%\n",
      "layer   3  Sparsity: 85.6127%\n",
      "total_backward_count 783200 real_backward_count 48778   6.228%\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  1.956035/  2.021517, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.39 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4008%\n",
      "layer   2  Sparsity: 75.1619%\n",
      "layer   3  Sparsity: 85.7323%\n",
      "total_backward_count 792990 real_backward_count 48971   6.175%\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  1.957026/  2.025721, val:  78.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.96 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4258%\n",
      "layer   2  Sparsity: 75.1213%\n",
      "layer   3  Sparsity: 85.9000%\n",
      "total_backward_count 802780 real_backward_count 49127   6.120%\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  1.958660/  2.024146, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.43 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.4091%\n",
      "layer   2  Sparsity: 75.0241%\n",
      "layer   3  Sparsity: 85.9976%\n",
      "total_backward_count 812570 real_backward_count 49311   6.069%\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  1.955971/  2.023319, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.34 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.4335%\n",
      "layer   2  Sparsity: 75.0285%\n",
      "layer   3  Sparsity: 85.8571%\n",
      "total_backward_count 822360 real_backward_count 49476   6.016%\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  1.961529/  2.021632, val:  78.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.22 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.4118%\n",
      "layer   2  Sparsity: 75.0228%\n",
      "layer   3  Sparsity: 85.7967%\n",
      "total_backward_count 832150 real_backward_count 49634   5.965%\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  1.958961/  2.023459, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.91 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4068%\n",
      "layer   2  Sparsity: 75.2358%\n",
      "layer   3  Sparsity: 85.6173%\n",
      "total_backward_count 841940 real_backward_count 49815   5.917%\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  1.958559/  2.023641, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.82 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4044%\n",
      "layer   2  Sparsity: 74.7740%\n",
      "layer   3  Sparsity: 85.7963%\n",
      "total_backward_count 851730 real_backward_count 49988   5.869%\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  1.960946/  2.022344, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.05 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.4266%\n",
      "layer   2  Sparsity: 75.4129%\n",
      "layer   3  Sparsity: 85.9893%\n",
      "total_backward_count 861520 real_backward_count 50144   5.820%\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  1.957650/  2.026726, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.72 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4208%\n",
      "layer   2  Sparsity: 75.3769%\n",
      "layer   3  Sparsity: 85.7896%\n",
      "total_backward_count 871310 real_backward_count 50317   5.775%\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  1.961801/  2.021692, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.94 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4341%\n",
      "layer   2  Sparsity: 75.0420%\n",
      "layer   3  Sparsity: 85.7635%\n",
      "total_backward_count 881100 real_backward_count 50483   5.730%\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  1.963722/  2.026689, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.34 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4174%\n",
      "layer   2  Sparsity: 74.7552%\n",
      "layer   3  Sparsity: 85.8686%\n",
      "total_backward_count 890890 real_backward_count 50671   5.688%\n",
      "fc layer 2 self.abs_max_out: 4483.0\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  1.961253/  2.023195, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.93 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 82.4194%\n",
      "layer   2  Sparsity: 75.0911%\n",
      "layer   3  Sparsity: 85.9594%\n",
      "total_backward_count 900680 real_backward_count 50824   5.643%\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  1.960806/  2.018923, val:  78.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.89 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4175%\n",
      "layer   2  Sparsity: 75.3287%\n",
      "layer   3  Sparsity: 85.8958%\n",
      "total_backward_count 910470 real_backward_count 50992   5.601%\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  1.957128/  2.025265, val:  79.17%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.61 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4224%\n",
      "layer   2  Sparsity: 75.0404%\n",
      "layer   3  Sparsity: 85.9794%\n",
      "total_backward_count 920260 real_backward_count 51144   5.558%\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  1.953307/  2.021556, val:  80.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.62 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4287%\n",
      "layer   2  Sparsity: 75.2261%\n",
      "layer   3  Sparsity: 85.9371%\n",
      "total_backward_count 930050 real_backward_count 51291   5.515%\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  1.958719/  2.022614, val:  73.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.27 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4156%\n",
      "layer   2  Sparsity: 75.2953%\n",
      "layer   3  Sparsity: 86.0116%\n",
      "total_backward_count 939840 real_backward_count 51419   5.471%\n",
      "lif layer 2 self.abs_max_v: 7995.0\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  1.958196/  2.021348, val:  79.58%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.65 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4284%\n",
      "layer   2  Sparsity: 74.8886%\n",
      "layer   3  Sparsity: 86.1883%\n",
      "total_backward_count 949630 real_backward_count 51564   5.430%\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  1.955089/  2.022633, val:  80.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.49 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4052%\n",
      "layer   2  Sparsity: 75.1037%\n",
      "layer   3  Sparsity: 86.1094%\n",
      "total_backward_count 959420 real_backward_count 51711   5.390%\n",
      "fc layer 2 self.abs_max_out: 4527.0\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  1.954758/  2.024600, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.24 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.3999%\n",
      "layer   2  Sparsity: 75.2163%\n",
      "layer   3  Sparsity: 86.1467%\n",
      "total_backward_count 969210 real_backward_count 51855   5.350%\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  1.957499/  2.022073, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.71 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4226%\n",
      "layer   2  Sparsity: 75.3725%\n",
      "layer   3  Sparsity: 86.1375%\n",
      "total_backward_count 979000 real_backward_count 51989   5.310%\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  1.956574/  2.019635, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.34 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.4058%\n",
      "layer   2  Sparsity: 75.3292%\n",
      "layer   3  Sparsity: 85.9625%\n",
      "total_backward_count 988790 real_backward_count 52118   5.271%\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  1.950427/  2.018847, val:  79.58%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.65 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4129%\n",
      "layer   2  Sparsity: 75.5962%\n",
      "layer   3  Sparsity: 85.9492%\n",
      "total_backward_count 998580 real_backward_count 52222   5.230%\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  1.950418/  2.018384, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.58 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4199%\n",
      "layer   2  Sparsity: 75.6014%\n",
      "layer   3  Sparsity: 85.9840%\n",
      "total_backward_count 1008370 real_backward_count 52362   5.193%\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  1.948755/  2.016198, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.02 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4222%\n",
      "layer   2  Sparsity: 75.3395%\n",
      "layer   3  Sparsity: 85.9884%\n",
      "total_backward_count 1018160 real_backward_count 52486   5.155%\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  1.949888/  2.013916, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.31 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.4161%\n",
      "layer   2  Sparsity: 75.0476%\n",
      "layer   3  Sparsity: 85.8956%\n",
      "total_backward_count 1027950 real_backward_count 52606   5.118%\n",
      "fc layer 1 self.abs_max_out: 12262.0\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  1.944981/  2.012069, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.55 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4149%\n",
      "layer   2  Sparsity: 75.2074%\n",
      "layer   3  Sparsity: 85.9188%\n",
      "total_backward_count 1037740 real_backward_count 52727   5.081%\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  1.944427/  2.017937, val:  79.17%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.97 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.3970%\n",
      "layer   2  Sparsity: 75.0221%\n",
      "layer   3  Sparsity: 86.0153%\n",
      "total_backward_count 1047530 real_backward_count 52835   5.044%\n",
      "fc layer 1 self.abs_max_out: 12340.0\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  1.944835/  2.013788, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.22 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4082%\n",
      "layer   2  Sparsity: 74.7820%\n",
      "layer   3  Sparsity: 85.9340%\n",
      "total_backward_count 1057320 real_backward_count 52942   5.007%\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  1.945021/  2.018135, val:  79.58%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.84 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4314%\n",
      "layer   2  Sparsity: 75.1258%\n",
      "layer   3  Sparsity: 86.0675%\n",
      "total_backward_count 1067110 real_backward_count 53061   4.972%\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  1.952326/  2.019357, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.76 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4211%\n",
      "layer   2  Sparsity: 75.4954%\n",
      "layer   3  Sparsity: 86.2876%\n",
      "total_backward_count 1076900 real_backward_count 53174   4.938%\n",
      "fc layer 1 self.abs_max_out: 12397.0\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  1.954340/  2.023776, val:  78.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.31 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.4362%\n",
      "layer   2  Sparsity: 75.2645%\n",
      "layer   3  Sparsity: 86.3668%\n",
      "total_backward_count 1086690 real_backward_count 53298   4.905%\n",
      "fc layer 1 self.abs_max_out: 12567.0\n",
      "lif layer 2 self.abs_max_v: 8259.5\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  1.958647/  2.020377, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.83 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 82.4168%\n",
      "layer   2  Sparsity: 74.8983%\n",
      "layer   3  Sparsity: 86.2562%\n",
      "total_backward_count 1096480 real_backward_count 53423   4.872%\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  1.954761/  2.021676, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.27 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.4025%\n",
      "layer   2  Sparsity: 74.8669%\n",
      "layer   3  Sparsity: 86.1309%\n",
      "total_backward_count 1106270 real_backward_count 53550   4.841%\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  1.952220/  2.021027, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.78 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4317%\n",
      "layer   2  Sparsity: 75.1317%\n",
      "layer   3  Sparsity: 86.1602%\n",
      "total_backward_count 1116060 real_backward_count 53681   4.810%\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  1.947847/  2.019308, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.37 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4157%\n",
      "layer   2  Sparsity: 75.1149%\n",
      "layer   3  Sparsity: 86.2255%\n",
      "total_backward_count 1125850 real_backward_count 53792   4.778%\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  1.948932/  2.018143, val:  79.58%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.31 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.3978%\n",
      "layer   2  Sparsity: 75.2749%\n",
      "layer   3  Sparsity: 86.3421%\n",
      "total_backward_count 1135640 real_backward_count 53882   4.745%\n",
      "lif layer 2 self.abs_max_v: 8373.5\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  1.949025/  2.012563, val:  80.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.78 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4161%\n",
      "layer   2  Sparsity: 75.0611%\n",
      "layer   3  Sparsity: 86.2889%\n",
      "total_backward_count 1145430 real_backward_count 54002   4.715%\n",
      "epoch-117 lr=['0.0009766'], tr/val_loss:  1.946654/  2.016711, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.21 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4092%\n",
      "layer   2  Sparsity: 74.9969%\n",
      "layer   3  Sparsity: 86.2064%\n",
      "total_backward_count 1155220 real_backward_count 54084   4.682%\n",
      "epoch-118 lr=['0.0009766'], tr/val_loss:  1.947773/  2.016245, val:  80.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.65 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4182%\n",
      "layer   2  Sparsity: 75.1474%\n",
      "layer   3  Sparsity: 86.1446%\n",
      "total_backward_count 1165010 real_backward_count 54189   4.651%\n",
      "epoch-119 lr=['0.0009766'], tr/val_loss:  1.943214/  2.012712, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.34 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.4244%\n",
      "layer   2  Sparsity: 74.9615%\n",
      "layer   3  Sparsity: 86.0945%\n",
      "total_backward_count 1174800 real_backward_count 54291   4.621%\n",
      "epoch-120 lr=['0.0009766'], tr/val_loss:  1.946606/  2.014400, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.81 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4233%\n",
      "layer   2  Sparsity: 74.9420%\n",
      "layer   3  Sparsity: 86.1859%\n",
      "total_backward_count 1184590 real_backward_count 54391   4.592%\n",
      "epoch-121 lr=['0.0009766'], tr/val_loss:  1.948239/  2.020561, val:  80.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.77 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4392%\n",
      "layer   2  Sparsity: 75.1502%\n",
      "layer   3  Sparsity: 86.5226%\n",
      "total_backward_count 1194380 real_backward_count 54484   4.562%\n",
      "epoch-122 lr=['0.0009766'], tr/val_loss:  1.951929/  2.029450, val:  79.58%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.04 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4267%\n",
      "layer   2  Sparsity: 75.2184%\n",
      "layer   3  Sparsity: 86.6246%\n",
      "total_backward_count 1204170 real_backward_count 54593   4.534%\n",
      "epoch-123 lr=['0.0009766'], tr/val_loss:  1.955861/  2.021616, val:  78.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.16 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4210%\n",
      "layer   2  Sparsity: 75.2760%\n",
      "layer   3  Sparsity: 86.6370%\n",
      "total_backward_count 1213960 real_backward_count 54692   4.505%\n",
      "epoch-124 lr=['0.0009766'], tr/val_loss:  1.954171/  2.023695, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.15 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4340%\n",
      "layer   2  Sparsity: 75.0322%\n",
      "layer   3  Sparsity: 86.5597%\n",
      "total_backward_count 1223750 real_backward_count 54802   4.478%\n",
      "epoch-125 lr=['0.0009766'], tr/val_loss:  1.957470/  2.024434, val:  78.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.54 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4274%\n",
      "layer   2  Sparsity: 74.9458%\n",
      "layer   3  Sparsity: 86.7181%\n",
      "total_backward_count 1233540 real_backward_count 54910   4.451%\n",
      "epoch-126 lr=['0.0009766'], tr/val_loss:  1.955019/  2.026236, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.69 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4203%\n",
      "layer   2  Sparsity: 74.9700%\n",
      "layer   3  Sparsity: 86.6319%\n",
      "total_backward_count 1243330 real_backward_count 55000   4.424%\n",
      "epoch-127 lr=['0.0009766'], tr/val_loss:  1.954466/  2.021934, val:  78.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.17 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.4116%\n",
      "layer   2  Sparsity: 75.3185%\n",
      "layer   3  Sparsity: 86.6916%\n",
      "total_backward_count 1253120 real_backward_count 55112   4.398%\n",
      "epoch-128 lr=['0.0009766'], tr/val_loss:  1.951944/  2.022479, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.42 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.4194%\n",
      "layer   2  Sparsity: 75.0060%\n",
      "layer   3  Sparsity: 86.5847%\n",
      "total_backward_count 1262910 real_backward_count 55225   4.373%\n",
      "epoch-129 lr=['0.0009766'], tr/val_loss:  1.952634/  2.020017, val:  80.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.32 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4210%\n",
      "layer   2  Sparsity: 75.0620%\n",
      "layer   3  Sparsity: 86.5766%\n",
      "total_backward_count 1272700 real_backward_count 55317   4.346%\n",
      "epoch-130 lr=['0.0009766'], tr/val_loss:  1.953337/  2.019569, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.82 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 82.4137%\n",
      "layer   2  Sparsity: 75.4283%\n",
      "layer   3  Sparsity: 86.7881%\n",
      "total_backward_count 1282490 real_backward_count 55425   4.322%\n",
      "epoch-131 lr=['0.0009766'], tr/val_loss:  1.954189/  2.020712, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.55 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4289%\n",
      "layer   2  Sparsity: 75.5621%\n",
      "layer   3  Sparsity: 86.8928%\n",
      "total_backward_count 1292280 real_backward_count 55512   4.296%\n",
      "epoch-132 lr=['0.0009766'], tr/val_loss:  1.953227/  2.023342, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.68 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.3976%\n",
      "layer   2  Sparsity: 75.6604%\n",
      "layer   3  Sparsity: 86.8899%\n",
      "total_backward_count 1302070 real_backward_count 55620   4.272%\n",
      "epoch-133 lr=['0.0009766'], tr/val_loss:  1.953647/  2.024144, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.48 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.3892%\n",
      "layer   2  Sparsity: 75.4385%\n",
      "layer   3  Sparsity: 86.9827%\n",
      "total_backward_count 1311860 real_backward_count 55716   4.247%\n",
      "epoch-134 lr=['0.0009766'], tr/val_loss:  1.953523/  2.024088, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.28 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.4171%\n",
      "layer   2  Sparsity: 75.0629%\n",
      "layer   3  Sparsity: 86.9039%\n",
      "total_backward_count 1321650 real_backward_count 55827   4.224%\n",
      "fc layer 2 self.abs_max_out: 4697.0\n",
      "epoch-135 lr=['0.0009766'], tr/val_loss:  1.951838/  2.021760, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.94 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.4187%\n",
      "layer   2  Sparsity: 74.8948%\n",
      "layer   3  Sparsity: 86.8946%\n",
      "total_backward_count 1331440 real_backward_count 55939   4.201%\n",
      "epoch-136 lr=['0.0009766'], tr/val_loss:  1.954171/  2.031820, val:  80.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.07 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4103%\n",
      "layer   2  Sparsity: 74.9485%\n",
      "layer   3  Sparsity: 86.9102%\n",
      "total_backward_count 1341230 real_backward_count 56036   4.178%\n",
      "epoch-137 lr=['0.0009766'], tr/val_loss:  1.955687/  2.025961, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.94 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4112%\n",
      "layer   2  Sparsity: 75.1400%\n",
      "layer   3  Sparsity: 86.8917%\n",
      "total_backward_count 1351020 real_backward_count 56129   4.155%\n",
      "epoch-138 lr=['0.0009766'], tr/val_loss:  1.954328/  2.025275, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.63 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4076%\n",
      "layer   2  Sparsity: 75.3457%\n",
      "layer   3  Sparsity: 86.9489%\n",
      "total_backward_count 1360810 real_backward_count 56217   4.131%\n",
      "epoch-139 lr=['0.0009766'], tr/val_loss:  1.952784/  2.024949, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.48 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.4156%\n",
      "layer   2  Sparsity: 75.2461%\n",
      "layer   3  Sparsity: 86.9160%\n",
      "total_backward_count 1370600 real_backward_count 56314   4.109%\n",
      "epoch-140 lr=['0.0009766'], tr/val_loss:  1.950526/  2.019852, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.86 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4159%\n",
      "layer   2  Sparsity: 75.3709%\n",
      "layer   3  Sparsity: 86.9789%\n",
      "total_backward_count 1380390 real_backward_count 56389   4.085%\n",
      "epoch-141 lr=['0.0009766'], tr/val_loss:  1.949008/  2.021877, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.31 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4171%\n",
      "layer   2  Sparsity: 75.3728%\n",
      "layer   3  Sparsity: 86.9566%\n",
      "total_backward_count 1390180 real_backward_count 56461   4.061%\n",
      "epoch-142 lr=['0.0009766'], tr/val_loss:  1.949822/  2.024015, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.12 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4234%\n",
      "layer   2  Sparsity: 75.1750%\n",
      "layer   3  Sparsity: 86.9979%\n",
      "total_backward_count 1399970 real_backward_count 56544   4.039%\n",
      "epoch-143 lr=['0.0009766'], tr/val_loss:  1.950623/  2.025591, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.84 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4277%\n",
      "layer   2  Sparsity: 75.1073%\n",
      "layer   3  Sparsity: 86.9726%\n",
      "total_backward_count 1409760 real_backward_count 56620   4.016%\n",
      "epoch-144 lr=['0.0009766'], tr/val_loss:  1.949500/  2.018193, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.64 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4163%\n",
      "layer   2  Sparsity: 74.8918%\n",
      "layer   3  Sparsity: 86.8987%\n",
      "total_backward_count 1419550 real_backward_count 56706   3.995%\n",
      "epoch-145 lr=['0.0009766'], tr/val_loss:  1.946063/  2.020931, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.12 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4176%\n",
      "layer   2  Sparsity: 74.8071%\n",
      "layer   3  Sparsity: 86.8687%\n",
      "total_backward_count 1429340 real_backward_count 56791   3.973%\n",
      "epoch-146 lr=['0.0009766'], tr/val_loss:  1.946212/  2.018759, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.65 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4219%\n",
      "layer   2  Sparsity: 74.8172%\n",
      "layer   3  Sparsity: 86.9667%\n",
      "total_backward_count 1439130 real_backward_count 56860   3.951%\n",
      "epoch-147 lr=['0.0009766'], tr/val_loss:  1.945447/  2.025927, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.78 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4057%\n",
      "layer   2  Sparsity: 75.0694%\n",
      "layer   3  Sparsity: 87.0081%\n",
      "total_backward_count 1448920 real_backward_count 56933   3.929%\n",
      "epoch-148 lr=['0.0009766'], tr/val_loss:  1.946639/  2.019346, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.21 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4295%\n",
      "layer   2  Sparsity: 75.1571%\n",
      "layer   3  Sparsity: 87.0011%\n",
      "total_backward_count 1458710 real_backward_count 57010   3.908%\n",
      "epoch-149 lr=['0.0009766'], tr/val_loss:  1.945636/  2.020365, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.21 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4148%\n",
      "layer   2  Sparsity: 75.0787%\n",
      "layer   3  Sparsity: 86.9309%\n",
      "total_backward_count 1468500 real_backward_count 57093   3.888%\n",
      "epoch-150 lr=['0.0009766'], tr/val_loss:  1.946447/  2.022998, val:  80.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.50 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.4149%\n",
      "layer   2  Sparsity: 74.8675%\n",
      "layer   3  Sparsity: 86.8797%\n",
      "total_backward_count 1478290 real_backward_count 57184   3.868%\n",
      "epoch-151 lr=['0.0009766'], tr/val_loss:  1.951352/  2.025206, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.91 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4087%\n",
      "layer   2  Sparsity: 74.5538%\n",
      "layer   3  Sparsity: 86.7587%\n",
      "total_backward_count 1488080 real_backward_count 57263   3.848%\n",
      "epoch-152 lr=['0.0009766'], tr/val_loss:  1.947738/  2.018661, val:  79.58%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.87 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4150%\n",
      "layer   2  Sparsity: 74.6775%\n",
      "layer   3  Sparsity: 86.6358%\n",
      "total_backward_count 1497870 real_backward_count 57343   3.828%\n",
      "epoch-153 lr=['0.0009766'], tr/val_loss:  1.948723/  2.014598, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.05 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 82.4298%\n",
      "layer   2  Sparsity: 74.8630%\n",
      "layer   3  Sparsity: 86.7456%\n",
      "total_backward_count 1507660 real_backward_count 57439   3.810%\n",
      "epoch-154 lr=['0.0009766'], tr/val_loss:  1.948011/  2.021490, val:  80.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.29 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.4294%\n",
      "layer   2  Sparsity: 75.0697%\n",
      "layer   3  Sparsity: 86.8251%\n",
      "total_backward_count 1517450 real_backward_count 57525   3.791%\n",
      "epoch-155 lr=['0.0009766'], tr/val_loss:  1.950972/  2.027747, val:  79.58%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.59 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4065%\n",
      "layer   2  Sparsity: 75.1909%\n",
      "layer   3  Sparsity: 86.9022%\n",
      "total_backward_count 1527240 real_backward_count 57600   3.772%\n",
      "epoch-156 lr=['0.0009766'], tr/val_loss:  1.951442/  2.022355, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.19 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4027%\n",
      "layer   2  Sparsity: 75.3587%\n",
      "layer   3  Sparsity: 86.9360%\n",
      "total_backward_count 1537030 real_backward_count 57676   3.752%\n",
      "epoch-157 lr=['0.0009766'], tr/val_loss:  1.947610/  2.020270, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.45 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4181%\n",
      "layer   2  Sparsity: 75.7132%\n",
      "layer   3  Sparsity: 86.7964%\n",
      "total_backward_count 1546820 real_backward_count 57755   3.734%\n",
      "epoch-158 lr=['0.0009766'], tr/val_loss:  1.949033/  2.020179, val:  80.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.04 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.4007%\n",
      "layer   2  Sparsity: 75.7460%\n",
      "layer   3  Sparsity: 86.8910%\n",
      "total_backward_count 1556610 real_backward_count 57831   3.715%\n",
      "epoch-159 lr=['0.0009766'], tr/val_loss:  1.946843/  2.019471, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.84 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 82.4149%\n",
      "layer   2  Sparsity: 75.3317%\n",
      "layer   3  Sparsity: 86.8509%\n",
      "total_backward_count 1566400 real_backward_count 57908   3.697%\n",
      "epoch-160 lr=['0.0009766'], tr/val_loss:  1.948595/  2.021583, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.86 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4234%\n",
      "layer   2  Sparsity: 75.2770%\n",
      "layer   3  Sparsity: 86.8720%\n",
      "total_backward_count 1576190 real_backward_count 58004   3.680%\n",
      "epoch-161 lr=['0.0009766'], tr/val_loss:  1.951643/  2.028679, val:  80.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.90 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4222%\n",
      "layer   2  Sparsity: 75.5657%\n",
      "layer   3  Sparsity: 87.0066%\n",
      "total_backward_count 1585980 real_backward_count 58099   3.663%\n",
      "epoch-162 lr=['0.0009766'], tr/val_loss:  1.952816/  2.022494, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.43 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4215%\n",
      "layer   2  Sparsity: 75.3744%\n",
      "layer   3  Sparsity: 86.9977%\n",
      "total_backward_count 1595770 real_backward_count 58180   3.646%\n",
      "epoch-163 lr=['0.0009766'], tr/val_loss:  1.952847/  2.030020, val:  79.58%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.03 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.4142%\n",
      "layer   2  Sparsity: 75.3926%\n",
      "layer   3  Sparsity: 87.0771%\n",
      "total_backward_count 1605560 real_backward_count 58251   3.628%\n",
      "epoch-164 lr=['0.0009766'], tr/val_loss:  1.951580/  2.023321, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.59 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4082%\n",
      "layer   2  Sparsity: 75.3929%\n",
      "layer   3  Sparsity: 87.0866%\n",
      "total_backward_count 1615350 real_backward_count 58310   3.610%\n",
      "epoch-165 lr=['0.0009766'], tr/val_loss:  1.949052/  2.022455, val:  80.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.85 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4145%\n",
      "layer   2  Sparsity: 75.3830%\n",
      "layer   3  Sparsity: 87.1016%\n",
      "total_backward_count 1625140 real_backward_count 58377   3.592%\n",
      "epoch-166 lr=['0.0009766'], tr/val_loss:  1.950893/  2.025363, val:  80.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.58 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4292%\n",
      "layer   2  Sparsity: 75.1365%\n",
      "layer   3  Sparsity: 87.1458%\n",
      "total_backward_count 1634930 real_backward_count 58440   3.574%\n",
      "epoch-167 lr=['0.0009766'], tr/val_loss:  1.951034/  2.018892, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.70 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4279%\n",
      "layer   2  Sparsity: 75.3574%\n",
      "layer   3  Sparsity: 87.0600%\n",
      "total_backward_count 1644720 real_backward_count 58503   3.557%\n",
      "epoch-168 lr=['0.0009766'], tr/val_loss:  1.947034/  2.018173, val:  80.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.29 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4114%\n",
      "layer   2  Sparsity: 75.3745%\n",
      "layer   3  Sparsity: 86.9144%\n",
      "total_backward_count 1654510 real_backward_count 58579   3.541%\n",
      "epoch-169 lr=['0.0009766'], tr/val_loss:  1.947080/  2.023926, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.07 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4141%\n",
      "layer   2  Sparsity: 75.2634%\n",
      "layer   3  Sparsity: 86.9921%\n",
      "total_backward_count 1664300 real_backward_count 58658   3.524%\n",
      "lif layer 2 self.abs_max_v: 8423.5\n",
      "epoch-170 lr=['0.0009766'], tr/val_loss:  1.947013/  2.021927, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.08 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4251%\n",
      "layer   2  Sparsity: 75.3466%\n",
      "layer   3  Sparsity: 86.9884%\n",
      "total_backward_count 1674090 real_backward_count 58730   3.508%\n",
      "epoch-171 lr=['0.0009766'], tr/val_loss:  1.948288/  2.025386, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.95 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4247%\n",
      "layer   2  Sparsity: 75.1982%\n",
      "layer   3  Sparsity: 87.1676%\n",
      "total_backward_count 1683880 real_backward_count 58789   3.491%\n",
      "epoch-172 lr=['0.0009766'], tr/val_loss:  1.950626/  2.026403, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.95 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4288%\n",
      "layer   2  Sparsity: 75.2122%\n",
      "layer   3  Sparsity: 87.2879%\n",
      "total_backward_count 1693670 real_backward_count 58863   3.475%\n",
      "epoch-173 lr=['0.0009766'], tr/val_loss:  1.949515/  2.022983, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.44 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4087%\n",
      "layer   2  Sparsity: 75.2194%\n",
      "layer   3  Sparsity: 87.3130%\n",
      "total_backward_count 1703460 real_backward_count 58929   3.459%\n",
      "epoch-174 lr=['0.0009766'], tr/val_loss:  1.947232/  2.021712, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.80 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4142%\n",
      "layer   2  Sparsity: 75.1622%\n",
      "layer   3  Sparsity: 87.2480%\n",
      "total_backward_count 1713250 real_backward_count 58993   3.443%\n",
      "epoch-175 lr=['0.0009766'], tr/val_loss:  1.948614/  2.018017, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.42 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4259%\n",
      "layer   2  Sparsity: 75.4101%\n",
      "layer   3  Sparsity: 87.2684%\n",
      "total_backward_count 1723040 real_backward_count 59048   3.427%\n",
      "epoch-176 lr=['0.0009766'], tr/val_loss:  1.946202/  2.017315, val:  80.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.49 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.3892%\n",
      "layer   2  Sparsity: 75.3224%\n",
      "layer   3  Sparsity: 87.2277%\n",
      "total_backward_count 1732830 real_backward_count 59095   3.410%\n",
      "epoch-177 lr=['0.0009766'], tr/val_loss:  1.945184/  2.022133, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.05 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4235%\n",
      "layer   2  Sparsity: 75.0770%\n",
      "layer   3  Sparsity: 87.1948%\n",
      "total_backward_count 1742620 real_backward_count 59151   3.394%\n",
      "epoch-178 lr=['0.0009766'], tr/val_loss:  1.947550/  2.018745, val:  78.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.24 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4251%\n",
      "layer   2  Sparsity: 74.9027%\n",
      "layer   3  Sparsity: 87.2347%\n",
      "total_backward_count 1752410 real_backward_count 59201   3.378%\n",
      "epoch-179 lr=['0.0009766'], tr/val_loss:  1.946509/  2.018505, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.13 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4028%\n",
      "layer   2  Sparsity: 75.0704%\n",
      "layer   3  Sparsity: 87.2268%\n",
      "total_backward_count 1762200 real_backward_count 59266   3.363%\n",
      "epoch-180 lr=['0.0009766'], tr/val_loss:  1.946603/  2.019065, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.03 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4078%\n",
      "layer   2  Sparsity: 75.0342%\n",
      "layer   3  Sparsity: 87.2447%\n",
      "total_backward_count 1771990 real_backward_count 59343   3.349%\n",
      "epoch-181 lr=['0.0009766'], tr/val_loss:  1.943656/  2.018943, val:  80.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.26 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.4187%\n",
      "layer   2  Sparsity: 75.3412%\n",
      "layer   3  Sparsity: 87.2295%\n",
      "total_backward_count 1781780 real_backward_count 59399   3.334%\n",
      "epoch-182 lr=['0.0009766'], tr/val_loss:  1.943018/  2.013591, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.55 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4047%\n",
      "layer   2  Sparsity: 75.3731%\n",
      "layer   3  Sparsity: 87.2112%\n",
      "total_backward_count 1791570 real_backward_count 59465   3.319%\n",
      "epoch-183 lr=['0.0009766'], tr/val_loss:  1.939523/  2.015086, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.12 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.4255%\n",
      "layer   2  Sparsity: 75.4123%\n",
      "layer   3  Sparsity: 87.1032%\n",
      "total_backward_count 1801360 real_backward_count 59511   3.304%\n",
      "epoch-184 lr=['0.0009766'], tr/val_loss:  1.940813/  2.018377, val:  78.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.31 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 82.4199%\n",
      "layer   2  Sparsity: 75.4149%\n",
      "layer   3  Sparsity: 87.0404%\n",
      "total_backward_count 1811150 real_backward_count 59569   3.289%\n",
      "epoch-185 lr=['0.0009766'], tr/val_loss:  1.945538/  2.023211, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.97 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4071%\n",
      "layer   2  Sparsity: 75.3078%\n",
      "layer   3  Sparsity: 87.0683%\n",
      "total_backward_count 1820940 real_backward_count 59638   3.275%\n",
      "epoch-186 lr=['0.0009766'], tr/val_loss:  1.945081/  2.019980, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.78 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4075%\n",
      "layer   2  Sparsity: 75.3450%\n",
      "layer   3  Sparsity: 87.1032%\n",
      "total_backward_count 1830730 real_backward_count 59690   3.260%\n",
      "epoch-187 lr=['0.0009766'], tr/val_loss:  1.945237/  2.018222, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.49 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4293%\n",
      "layer   2  Sparsity: 75.5366%\n",
      "layer   3  Sparsity: 87.1638%\n",
      "total_backward_count 1840520 real_backward_count 59747   3.246%\n",
      "epoch-188 lr=['0.0009766'], tr/val_loss:  1.944214/  2.021079, val:  80.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.07 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4366%\n",
      "layer   2  Sparsity: 75.4034%\n",
      "layer   3  Sparsity: 87.1387%\n",
      "total_backward_count 1850310 real_backward_count 59801   3.232%\n",
      "epoch-189 lr=['0.0009766'], tr/val_loss:  1.943663/  2.019339, val:  80.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.66 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4020%\n",
      "layer   2  Sparsity: 75.3380%\n",
      "layer   3  Sparsity: 87.1105%\n",
      "total_backward_count 1860100 real_backward_count 59850   3.218%\n",
      "epoch-190 lr=['0.0009766'], tr/val_loss:  1.941613/  2.016372, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.33 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4536%\n",
      "layer   2  Sparsity: 75.4320%\n",
      "layer   3  Sparsity: 87.1678%\n",
      "total_backward_count 1869890 real_backward_count 59909   3.204%\n",
      "epoch-191 lr=['0.0009766'], tr/val_loss:  1.943492/  2.018826, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.08 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4251%\n",
      "layer   2  Sparsity: 75.3856%\n",
      "layer   3  Sparsity: 87.2231%\n",
      "total_backward_count 1879680 real_backward_count 59969   3.190%\n",
      "epoch-192 lr=['0.0009766'], tr/val_loss:  1.942275/  2.018305, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.93 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4012%\n",
      "layer   2  Sparsity: 75.2975%\n",
      "layer   3  Sparsity: 87.2152%\n",
      "total_backward_count 1889470 real_backward_count 60027   3.177%\n",
      "epoch-193 lr=['0.0009766'], tr/val_loss:  1.940824/  2.018759, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.16 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.3990%\n",
      "layer   2  Sparsity: 75.2114%\n",
      "layer   3  Sparsity: 87.1965%\n",
      "total_backward_count 1899260 real_backward_count 60095   3.164%\n",
      "epoch-194 lr=['0.0009766'], tr/val_loss:  1.941171/  2.017884, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.01 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 82.4272%\n",
      "layer   2  Sparsity: 75.4766%\n",
      "layer   3  Sparsity: 87.1755%\n",
      "total_backward_count 1909050 real_backward_count 60140   3.150%\n",
      "epoch-195 lr=['0.0009766'], tr/val_loss:  1.941160/  2.019279, val:  78.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.50 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 82.4351%\n",
      "layer   2  Sparsity: 75.5018%\n",
      "layer   3  Sparsity: 87.2096%\n",
      "total_backward_count 1918840 real_backward_count 60190   3.137%\n",
      "epoch-196 lr=['0.0009766'], tr/val_loss:  1.942826/  2.016009, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.35 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4263%\n",
      "layer   2  Sparsity: 75.1221%\n",
      "layer   3  Sparsity: 87.1589%\n",
      "total_backward_count 1928630 real_backward_count 60243   3.124%\n",
      "epoch-197 lr=['0.0009766'], tr/val_loss:  1.940210/  2.017063, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.24 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4084%\n",
      "layer   2  Sparsity: 75.1053%\n",
      "layer   3  Sparsity: 87.1189%\n",
      "total_backward_count 1938420 real_backward_count 60296   3.111%\n",
      "epoch-198 lr=['0.0009766'], tr/val_loss:  1.938960/  2.013195, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.45 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4312%\n",
      "layer   2  Sparsity: 75.0680%\n",
      "layer   3  Sparsity: 86.9882%\n",
      "total_backward_count 1948210 real_backward_count 60354   3.098%\n",
      "epoch-199 lr=['0.0009766'], tr/val_loss:  1.936945/  2.012522, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.46 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 82.4348%\n",
      "layer   2  Sparsity: 75.0705%\n",
      "layer   3  Sparsity: 86.9769%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "787e3efb014a4000b8f4565e20212c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñÉ‚ñÑ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.93695</td></tr><tr><td>val_acc_best</td><td>0.84167</td></tr><tr><td>val_acc_now</td><td>0.8375</td></tr><tr><td>val_loss</td><td>2.01252</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sweet-sweep-9</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/l0gl0hpu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/l0gl0hpu</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_040159-l0gl0hpu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x40lsufk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 50000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_084855-x40lsufk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x40lsufk' target=\"_blank\">vivid-sweep-14</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x40lsufk' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x40lsufk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251118_084904_929', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 25, 'dvs_duration': 50000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 575149142d3019108310063e0e922290\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 139\n",
      "fc layer 1 self.abs_max_out: 202.0\n",
      "lif layer 1 self.abs_max_v: 202.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 534.0\n",
      "lif layer 2 self.abs_max_v: 534.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 158.0\n",
      "fc layer 1 self.abs_max_out: 246.0\n",
      "lif layer 1 self.abs_max_v: 262.0\n",
      "lif layer 2 self.abs_max_v: 607.0\n",
      "fc layer 3 self.abs_max_out: 187.0\n",
      "fc layer 1 self.abs_max_out: 265.0\n",
      "lif layer 1 self.abs_max_v: 336.0\n",
      "lif layer 2 self.abs_max_v: 811.5\n",
      "fc layer 3 self.abs_max_out: 224.0\n",
      "fc layer 1 self.abs_max_out: 301.0\n",
      "lif layer 1 self.abs_max_v: 392.5\n",
      "fc layer 2 self.abs_max_out: 577.0\n",
      "fc layer 3 self.abs_max_out: 312.0\n",
      "fc layer 1 self.abs_max_out: 484.0\n",
      "lif layer 1 self.abs_max_v: 484.0\n",
      "lif layer 2 self.abs_max_v: 918.0\n",
      "lif layer 1 self.abs_max_v: 527.5\n",
      "fc layer 1 self.abs_max_out: 491.0\n",
      "smallest_now_T updated: 125\n",
      "fc layer 2 self.abs_max_out: 680.0\n",
      "fc layer 2 self.abs_max_out: 832.0\n",
      "fc layer 1 self.abs_max_out: 587.0\n",
      "lif layer 1 self.abs_max_v: 587.0\n",
      "lif layer 1 self.abs_max_v: 647.5\n",
      "lif layer 2 self.abs_max_v: 924.5\n",
      "lif layer 1 self.abs_max_v: 655.0\n",
      "fc layer 1 self.abs_max_out: 605.0\n",
      "lif layer 2 self.abs_max_v: 937.0\n",
      "smallest_now_T updated: 94\n",
      "lif layer 2 self.abs_max_v: 1030.5\n",
      "lif layer 1 self.abs_max_v: 678.5\n",
      "lif layer 1 self.abs_max_v: 766.5\n",
      "lif layer 2 self.abs_max_v: 1037.5\n",
      "fc layer 1 self.abs_max_out: 668.0\n",
      "fc layer 1 self.abs_max_out: 765.0\n",
      "lif layer 1 self.abs_max_v: 923.5\n",
      "fc layer 1 self.abs_max_out: 888.0\n",
      "fc layer 1 self.abs_max_out: 1155.0\n",
      "lif layer 1 self.abs_max_v: 1155.0\n",
      "lif layer 1 self.abs_max_v: 1346.5\n",
      "lif layer 2 self.abs_max_v: 1138.5\n",
      "lif layer 2 self.abs_max_v: 1143.5\n",
      "fc layer 3 self.abs_max_out: 336.0\n",
      "fc layer 3 self.abs_max_out: 347.0\n",
      "fc layer 3 self.abs_max_out: 367.0\n",
      "fc layer 2 self.abs_max_out: 939.0\n",
      "fc layer 1 self.abs_max_out: 1295.0\n",
      "lif layer 2 self.abs_max_v: 1194.0\n",
      "lif layer 2 self.abs_max_v: 1216.0\n",
      "lif layer 2 self.abs_max_v: 1218.5\n",
      "lif layer 2 self.abs_max_v: 1219.5\n",
      "lif layer 2 self.abs_max_v: 1398.0\n",
      "lif layer 2 self.abs_max_v: 1409.5\n",
      "lif layer 2 self.abs_max_v: 1522.0\n",
      "smallest_now_T updated: 79\n",
      "lif layer 1 self.abs_max_v: 1367.0\n",
      "lif layer 1 self.abs_max_v: 1424.5\n",
      "lif layer 1 self.abs_max_v: 1477.5\n",
      "lif layer 1 self.abs_max_v: 1518.5\n",
      "fc layer 1 self.abs_max_out: 1330.0\n",
      "lif layer 1 self.abs_max_v: 1540.0\n",
      "fc layer 1 self.abs_max_out: 1443.0\n",
      "lif layer 1 self.abs_max_v: 2113.0\n",
      "lif layer 1 self.abs_max_v: 2388.5\n",
      "fc layer 3 self.abs_max_out: 397.0\n",
      "fc layer 2 self.abs_max_out: 961.0\n",
      "lif layer 2 self.abs_max_v: 1557.0\n",
      "fc layer 2 self.abs_max_out: 1011.0\n",
      "lif layer 2 self.abs_max_v: 1586.0\n",
      "fc layer 2 self.abs_max_out: 1069.0\n",
      "fc layer 2 self.abs_max_out: 1070.0\n",
      "smallest_now_T updated: 73\n",
      "fc layer 2 self.abs_max_out: 1078.0\n",
      "lif layer 2 self.abs_max_v: 1603.5\n",
      "lif layer 2 self.abs_max_v: 1627.0\n",
      "lif layer 2 self.abs_max_v: 1659.5\n",
      "fc layer 2 self.abs_max_out: 1093.0\n",
      "fc layer 2 self.abs_max_out: 1094.0\n",
      "lif layer 2 self.abs_max_v: 1682.5\n",
      "lif layer 2 self.abs_max_v: 1716.5\n",
      "lif layer 2 self.abs_max_v: 1751.5\n",
      "lif layer 2 self.abs_max_v: 1817.0\n",
      "fc layer 3 self.abs_max_out: 475.0\n",
      "fc layer 3 self.abs_max_out: 488.0\n",
      "lif layer 2 self.abs_max_v: 1837.5\n",
      "lif layer 2 self.abs_max_v: 1886.0\n",
      "lif layer 2 self.abs_max_v: 1979.0\n",
      "lif layer 2 self.abs_max_v: 2010.0\n",
      "lif layer 2 self.abs_max_v: 2010.5\n",
      "lif layer 2 self.abs_max_v: 2065.5\n",
      "fc layer 2 self.abs_max_out: 1101.0\n",
      "fc layer 3 self.abs_max_out: 493.0\n",
      "fc layer 2 self.abs_max_out: 1202.0\n",
      "lif layer 2 self.abs_max_v: 2126.0\n",
      "lif layer 2 self.abs_max_v: 2190.0\n",
      "lif layer 2 self.abs_max_v: 2200.0\n",
      "smallest_now_T updated: 65\n",
      "fc layer 1 self.abs_max_out: 1620.0\n",
      "lif layer 1 self.abs_max_v: 2661.0\n",
      "fc layer 2 self.abs_max_out: 1239.0\n",
      "lif layer 2 self.abs_max_v: 2245.5\n",
      "lif layer 2 self.abs_max_v: 2290.5\n",
      "lif layer 2 self.abs_max_v: 2374.5\n",
      "fc layer 2 self.abs_max_out: 1275.0\n",
      "fc layer 2 self.abs_max_out: 1313.0\n",
      "fc layer 2 self.abs_max_out: 1430.0\n",
      "lif layer 2 self.abs_max_v: 2449.5\n",
      "fc layer 3 self.abs_max_out: 514.0\n",
      "fc layer 3 self.abs_max_out: 523.0\n",
      "lif layer 2 self.abs_max_v: 2556.5\n",
      "fc layer 3 self.abs_max_out: 532.0\n",
      "fc layer 2 self.abs_max_out: 1444.0\n",
      "fc layer 2 self.abs_max_out: 1490.0\n",
      "fc layer 3 self.abs_max_out: 549.0\n",
      "fc layer 3 self.abs_max_out: 561.0\n",
      "fc layer 3 self.abs_max_out: 583.0\n",
      "fc layer 1 self.abs_max_out: 1720.0\n",
      "fc layer 1 self.abs_max_out: 2012.0\n",
      "lif layer 1 self.abs_max_v: 2698.0\n",
      "lif layer 1 self.abs_max_v: 3134.5\n",
      "fc layer 3 self.abs_max_out: 586.0\n",
      "fc layer 3 self.abs_max_out: 638.0\n",
      "fc layer 1 self.abs_max_out: 2048.0\n",
      "fc layer 3 self.abs_max_out: 653.0\n",
      "fc layer 2 self.abs_max_out: 1515.0\n",
      "lif layer 2 self.abs_max_v: 2580.5\n",
      "lif layer 2 self.abs_max_v: 2724.5\n",
      "lif layer 2 self.abs_max_v: 2827.0\n",
      "lif layer 2 self.abs_max_v: 2877.5\n",
      "smallest_now_T updated: 56\n",
      "smallest_now_T updated: 50\n",
      "fc layer 2 self.abs_max_out: 1525.0\n",
      "fc layer 2 self.abs_max_out: 1538.0\n",
      "fc layer 2 self.abs_max_out: 1611.0\n",
      "lif layer 2 self.abs_max_v: 3001.0\n",
      "lif layer 2 self.abs_max_v: 3032.0\n",
      "lif layer 2 self.abs_max_v: 3059.5\n",
      "fc layer 2 self.abs_max_out: 1643.0\n",
      "lif layer 1 self.abs_max_v: 3210.0\n",
      "fc layer 1 self.abs_max_out: 2051.0\n",
      "fc layer 1 self.abs_max_out: 2176.0\n",
      "lif layer 1 self.abs_max_v: 3368.5\n",
      "lif layer 1 self.abs_max_v: 3708.0\n",
      "lif layer 1 self.abs_max_v: 3800.0\n",
      "lif layer 1 self.abs_max_v: 4053.0\n",
      "fc layer 2 self.abs_max_out: 1740.0\n",
      "lif layer 2 self.abs_max_v: 3203.0\n",
      "lif layer 2 self.abs_max_v: 3248.5\n",
      "fc layer 1 self.abs_max_out: 2392.0\n",
      "lif layer 1 self.abs_max_v: 4111.0\n",
      "lif layer 1 self.abs_max_v: 4216.5\n",
      "lif layer 1 self.abs_max_v: 4224.5\n",
      "fc layer 2 self.abs_max_out: 1869.0\n",
      "lif layer 2 self.abs_max_v: 3344.5\n",
      "lif layer 2 self.abs_max_v: 3385.0\n",
      "lif layer 2 self.abs_max_v: 3478.5\n",
      "fc layer 1 self.abs_max_out: 2395.0\n",
      "fc layer 1 self.abs_max_out: 2436.0\n",
      "lif layer 1 self.abs_max_v: 4367.0\n",
      "fc layer 1 self.abs_max_out: 2454.0\n",
      "lif layer 1 self.abs_max_v: 4637.5\n",
      "fc layer 1 self.abs_max_out: 2555.0\n",
      "lif layer 1 self.abs_max_v: 4653.0\n",
      "fc layer 1 self.abs_max_out: 2775.0\n",
      "lif layer 1 self.abs_max_v: 5101.5\n",
      "fc layer 3 self.abs_max_out: 697.0\n",
      "fc layer 3 self.abs_max_out: 701.0\n",
      "fc layer 3 self.abs_max_out: 811.0\n",
      "fc layer 3 self.abs_max_out: 813.0\n",
      "smallest_now_T_val updated: 129\n",
      "smallest_now_T_val updated: 106\n",
      "smallest_now_T_val updated: 104\n",
      "smallest_now_T_val updated: 102\n",
      "smallest_now_T_val updated: 85\n",
      "smallest_now_T_val updated: 50\n",
      "fc layer 2 self.abs_max_out: 1967.0\n",
      "fc layer 1 self.abs_max_out: 3353.0\n",
      "lif layer 1 self.abs_max_v: 5322.0\n",
      "lif layer 1 self.abs_max_v: 5325.5\n",
      "lif layer 1 self.abs_max_v: 5467.0\n",
      "lif layer 1 self.abs_max_v: 5476.5\n",
      "lif layer 1 self.abs_max_v: 5990.5\n",
      "lif layer 1 self.abs_max_v: 6301.5\n",
      "lif layer 2 self.abs_max_v: 3543.5\n",
      "fc layer 2 self.abs_max_out: 1968.0\n",
      "lif layer 2 self.abs_max_v: 3662.0\n",
      "lif layer 2 self.abs_max_v: 3697.0\n",
      "lif layer 2 self.abs_max_v: 3768.0\n",
      "fc layer 2 self.abs_max_out: 1970.0\n",
      "lif layer 2 self.abs_max_v: 3790.5\n",
      "fc layer 2 self.abs_max_out: 1980.0\n",
      "lif layer 2 self.abs_max_v: 3875.5\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.372143/  1.747102, val:  34.17%, val_best:  34.17%, tr:  99.28%, tr_best:  99.28%, epoch time: 87.41 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 90.5386%\n",
      "layer   2  Sparsity: 68.3743%\n",
      "layer   3  Sparsity: 64.2394%\n",
      "total_backward_count 9790 real_backward_count 1200  12.257%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1988.0\n",
      "fc layer 2 self.abs_max_out: 1991.0\n",
      "fc layer 2 self.abs_max_out: 2097.0\n",
      "lif layer 2 self.abs_max_v: 3940.5\n",
      "lif layer 2 self.abs_max_v: 4066.5\n",
      "lif layer 2 self.abs_max_v: 4068.0\n",
      "fc layer 2 self.abs_max_out: 2118.0\n",
      "lif layer 2 self.abs_max_v: 4152.0\n",
      "fc layer 2 self.abs_max_out: 2222.0\n",
      "lif layer 2 self.abs_max_v: 4184.0\n",
      "lif layer 2 self.abs_max_v: 4284.0\n",
      "fc layer 2 self.abs_max_out: 2259.0\n",
      "fc layer 1 self.abs_max_out: 3609.0\n",
      "fc layer 1 self.abs_max_out: 4107.0\n",
      "lif layer 1 self.abs_max_v: 6438.5\n",
      "lif layer 1 self.abs_max_v: 6654.5\n",
      "lif layer 1 self.abs_max_v: 6741.0\n",
      "fc layer 2 self.abs_max_out: 2385.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.266372/  1.794399, val:  39.58%, val_best:  39.58%, tr:  99.59%, tr_best:  99.59%, epoch time: 86.78 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 90.5475%\n",
      "layer   2  Sparsity: 72.9478%\n",
      "layer   3  Sparsity: 67.8559%\n",
      "total_backward_count 19580 real_backward_count 2276  11.624%\n",
      "lif layer 2 self.abs_max_v: 4295.5\n",
      "lif layer 2 self.abs_max_v: 4432.0\n",
      "lif layer 2 self.abs_max_v: 4521.0\n",
      "lif layer 2 self.abs_max_v: 4521.5\n",
      "lif layer 2 self.abs_max_v: 4590.0\n",
      "lif layer 2 self.abs_max_v: 4601.0\n",
      "fc layer 2 self.abs_max_out: 2401.0\n",
      "fc layer 3 self.abs_max_out: 847.0\n",
      "fc layer 2 self.abs_max_out: 2461.0\n",
      "fc layer 2 self.abs_max_out: 2564.0\n",
      "lif layer 2 self.abs_max_v: 4859.0\n",
      "fc layer 2 self.abs_max_out: 2578.0\n",
      "lif layer 2 self.abs_max_v: 5007.5\n",
      "fc layer 2 self.abs_max_out: 2833.0\n",
      "lif layer 2 self.abs_max_v: 5085.0\n",
      "lif layer 2 self.abs_max_v: 5354.5\n",
      "lif layer 2 self.abs_max_v: 5372.5\n",
      "fc layer 1 self.abs_max_out: 4430.0\n",
      "lif layer 1 self.abs_max_v: 7052.0\n",
      "fc layer 2 self.abs_max_out: 2870.0\n",
      "lif layer 2 self.abs_max_v: 5395.0\n",
      "fc layer 2 self.abs_max_out: 2923.0\n",
      "lif layer 2 self.abs_max_v: 5620.5\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.235957/  1.672467, val:  47.50%, val_best:  47.50%, tr:  99.69%, tr_best:  99.69%, epoch time: 86.48 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 90.5318%\n",
      "layer   2  Sparsity: 73.8666%\n",
      "layer   3  Sparsity: 69.2401%\n",
      "total_backward_count 29370 real_backward_count 3334  11.352%\n",
      "fc layer 2 self.abs_max_out: 2978.0\n",
      "lif layer 2 self.abs_max_v: 5699.5\n",
      "fc layer 1 self.abs_max_out: 4532.0\n",
      "lif layer 1 self.abs_max_v: 7862.5\n",
      "fc layer 1 self.abs_max_out: 5276.0\n",
      "lif layer 1 self.abs_max_v: 8253.0\n",
      "lif layer 1 self.abs_max_v: 8546.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.194124/  1.621243, val:  43.75%, val_best:  47.50%, tr:  99.28%, tr_best:  99.69%, epoch time: 86.01 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5376%\n",
      "layer   2  Sparsity: 74.2097%\n",
      "layer   3  Sparsity: 68.9459%\n",
      "total_backward_count 39160 real_backward_count 4311  11.009%\n",
      "fc layer 3 self.abs_max_out: 897.0\n",
      "fc layer 3 self.abs_max_out: 899.0\n",
      "fc layer 2 self.abs_max_out: 3025.0\n",
      "fc layer 2 self.abs_max_out: 3236.0\n",
      "lif layer 2 self.abs_max_v: 6080.0\n",
      "lif layer 2 self.abs_max_v: 6129.0\n",
      "lif layer 2 self.abs_max_v: 6209.5\n",
      "lif layer 1 self.abs_max_v: 8969.0\n",
      "fc layer 1 self.abs_max_out: 5779.0\n",
      "lif layer 1 self.abs_max_v: 9550.0\n",
      "lif layer 1 self.abs_max_v: 9783.5\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.168034/  1.635365, val:  50.83%, val_best:  50.83%, tr:  99.80%, tr_best:  99.80%, epoch time: 85.98 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5318%\n",
      "layer   2  Sparsity: 73.9466%\n",
      "layer   3  Sparsity: 69.3755%\n",
      "total_backward_count 48950 real_backward_count 5251  10.727%\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.142903/  1.550949, val:  50.83%, val_best:  50.83%, tr:  99.59%, tr_best:  99.80%, epoch time: 85.63 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5432%\n",
      "layer   2  Sparsity: 73.7463%\n",
      "layer   3  Sparsity: 69.8804%\n",
      "total_backward_count 58740 real_backward_count 6185  10.529%\n",
      "fc layer 3 self.abs_max_out: 972.0\n",
      "fc layer 3 self.abs_max_out: 999.0\n",
      "fc layer 3 self.abs_max_out: 1005.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.143991/  1.535760, val:  56.25%, val_best:  56.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 85.94 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5406%\n",
      "layer   2  Sparsity: 73.7812%\n",
      "layer   3  Sparsity: 70.4929%\n",
      "total_backward_count 68530 real_backward_count 7075  10.324%\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.110269/  1.532386, val:  57.92%, val_best:  57.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 85.38 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5441%\n",
      "layer   2  Sparsity: 73.2562%\n",
      "layer   3  Sparsity: 70.6170%\n",
      "total_backward_count 78320 real_backward_count 7961  10.165%\n",
      "fc layer 1 self.abs_max_out: 5800.0\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.102268/  1.409628, val:  69.58%, val_best:  69.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 85.52 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5342%\n",
      "layer   2  Sparsity: 73.3925%\n",
      "layer   3  Sparsity: 71.0761%\n",
      "total_backward_count 88110 real_backward_count 8876  10.074%\n",
      "fc layer 1 self.abs_max_out: 5844.0\n",
      "lif layer 1 self.abs_max_v: 9810.5\n",
      "lif layer 1 self.abs_max_v: 10309.5\n",
      "lif layer 1 self.abs_max_v: 10399.5\n",
      "fc layer 2 self.abs_max_out: 3278.0\n",
      "fc layer 2 self.abs_max_out: 3376.0\n",
      "lif layer 2 self.abs_max_v: 6432.0\n",
      "fc layer 2 self.abs_max_out: 3386.0\n",
      "lif layer 2 self.abs_max_v: 6601.5\n",
      "fc layer 1 self.abs_max_out: 5942.0\n",
      "fc layer 1 self.abs_max_out: 6018.0\n",
      "fc layer 1 self.abs_max_out: 6093.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.041137/  1.456889, val:  59.58%, val_best:  69.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 85.52 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5422%\n",
      "layer   2  Sparsity: 72.5153%\n",
      "layer   3  Sparsity: 69.9768%\n",
      "total_backward_count 97900 real_backward_count 9705   9.913%\n",
      "lif layer 1 self.abs_max_v: 10584.0\n",
      "fc layer 1 self.abs_max_out: 6129.0\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.007480/  1.421753, val:  61.25%, val_best:  69.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 85.98 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5050%\n",
      "layer   2  Sparsity: 72.1482%\n",
      "layer   3  Sparsity: 69.7696%\n",
      "total_backward_count 107690 real_backward_count 10484   9.735%\n",
      "fc layer 1 self.abs_max_out: 6528.0\n",
      "lif layer 1 self.abs_max_v: 11108.5\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  0.978269/  1.374076, val:  72.92%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.53 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 90.5693%\n",
      "layer   2  Sparsity: 72.1210%\n",
      "layer   3  Sparsity: 71.5356%\n",
      "total_backward_count 117480 real_backward_count 11236   9.564%\n",
      "fc layer 2 self.abs_max_out: 3418.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  0.932962/  1.302947, val:  69.17%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.07 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5568%\n",
      "layer   2  Sparsity: 71.3236%\n",
      "layer   3  Sparsity: 71.3276%\n",
      "total_backward_count 127270 real_backward_count 11891   9.343%\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  0.906082/  1.298913, val:  67.50%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.20 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 90.5434%\n",
      "layer   2  Sparsity: 71.3925%\n",
      "layer   3  Sparsity: 71.2869%\n",
      "total_backward_count 137060 real_backward_count 12551   9.157%\n",
      "fc layer 1 self.abs_max_out: 6841.0\n",
      "lif layer 1 self.abs_max_v: 11886.5\n",
      "fc layer 1 self.abs_max_out: 6967.0\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  0.866178/  1.266426, val:  68.75%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.70 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 90.5255%\n",
      "layer   2  Sparsity: 71.3536%\n",
      "layer   3  Sparsity: 71.7130%\n",
      "total_backward_count 146850 real_backward_count 13180   8.975%\n",
      "fc layer 2 self.abs_max_out: 3427.0\n",
      "fc layer 2 self.abs_max_out: 3507.0\n",
      "lif layer 2 self.abs_max_v: 6715.5\n",
      "fc layer 2 self.abs_max_out: 3631.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  0.866175/  1.214108, val:  75.42%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.41 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 90.5290%\n",
      "layer   2  Sparsity: 71.0095%\n",
      "layer   3  Sparsity: 71.7487%\n",
      "total_backward_count 156640 real_backward_count 13822   8.824%\n",
      "lif layer 2 self.abs_max_v: 6726.5\n",
      "lif layer 2 self.abs_max_v: 6844.5\n",
      "lif layer 2 self.abs_max_v: 6875.5\n",
      "lif layer 2 self.abs_max_v: 6894.0\n",
      "fc layer 3 self.abs_max_out: 1056.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  0.849552/  1.173312, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.39 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 90.5283%\n",
      "layer   2  Sparsity: 70.4519%\n",
      "layer   3  Sparsity: 71.3637%\n",
      "total_backward_count 166430 real_backward_count 14446   8.680%\n",
      "fc layer 1 self.abs_max_out: 7395.0\n",
      "lif layer 1 self.abs_max_v: 12043.0\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  0.826113/  1.135536, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.64 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 90.5446%\n",
      "layer   2  Sparsity: 70.5217%\n",
      "layer   3  Sparsity: 71.9764%\n",
      "total_backward_count 176220 real_backward_count 14968   8.494%\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  0.808892/  1.129620, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.34 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 90.5275%\n",
      "layer   2  Sparsity: 70.7459%\n",
      "layer   3  Sparsity: 71.9001%\n",
      "total_backward_count 186010 real_backward_count 15544   8.357%\n",
      "fc layer 2 self.abs_max_out: 3657.0\n",
      "fc layer 2 self.abs_max_out: 3828.0\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  0.787563/  1.131757, val:  80.42%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.18 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 90.5342%\n",
      "layer   2  Sparsity: 71.2577%\n",
      "layer   3  Sparsity: 72.2649%\n",
      "total_backward_count 195800 real_backward_count 16052   8.198%\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  0.755646/  1.266249, val:  53.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.17 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5188%\n",
      "layer   2  Sparsity: 70.9028%\n",
      "layer   3  Sparsity: 71.2955%\n",
      "total_backward_count 205590 real_backward_count 16559   8.054%\n",
      "fc layer 3 self.abs_max_out: 1100.0\n",
      "fc layer 2 self.abs_max_out: 3853.0\n",
      "lif layer 2 self.abs_max_v: 7167.0\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  0.715753/  1.117528, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.84 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5282%\n",
      "layer   2  Sparsity: 70.5240%\n",
      "layer   3  Sparsity: 72.1393%\n",
      "total_backward_count 215380 real_backward_count 17058   7.920%\n",
      "fc layer 3 self.abs_max_out: 1141.0\n",
      "fc layer 2 self.abs_max_out: 3878.0\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  0.719299/  1.080352, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.07 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5375%\n",
      "layer   2  Sparsity: 70.2260%\n",
      "layer   3  Sparsity: 73.0059%\n",
      "total_backward_count 225170 real_backward_count 17551   7.795%\n",
      "lif layer 1 self.abs_max_v: 12180.5\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  0.709004/  1.073590, val:  79.58%, val_best:  81.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 86.34 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 90.5195%\n",
      "layer   2  Sparsity: 70.6683%\n",
      "layer   3  Sparsity: 73.2226%\n",
      "total_backward_count 234960 real_backward_count 18056   7.685%\n",
      "fc layer 2 self.abs_max_out: 3935.0\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  0.707590/  1.071732, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.23 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 90.5335%\n",
      "layer   2  Sparsity: 70.6377%\n",
      "layer   3  Sparsity: 72.5164%\n",
      "total_backward_count 244750 real_backward_count 18513   7.564%\n",
      "fc layer 2 self.abs_max_out: 3949.0\n",
      "lif layer 1 self.abs_max_v: 12634.5\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  0.703071/  0.988967, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.27 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 90.5519%\n",
      "layer   2  Sparsity: 69.4771%\n",
      "layer   3  Sparsity: 72.7562%\n",
      "total_backward_count 254540 real_backward_count 18939   7.440%\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  0.692540/  0.990224, val:  82.92%, val_best:  83.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 85.82 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5489%\n",
      "layer   2  Sparsity: 69.6961%\n",
      "layer   3  Sparsity: 72.4426%\n",
      "total_backward_count 264330 real_backward_count 19348   7.320%\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  0.685067/  1.025500, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.94 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5337%\n",
      "layer   2  Sparsity: 69.8116%\n",
      "layer   3  Sparsity: 73.0487%\n",
      "total_backward_count 274120 real_backward_count 19769   7.212%\n",
      "fc layer 3 self.abs_max_out: 1154.0\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  0.671215/  1.064396, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.75 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5278%\n",
      "layer   2  Sparsity: 69.6983%\n",
      "layer   3  Sparsity: 73.1176%\n",
      "total_backward_count 283910 real_backward_count 20182   7.109%\n",
      "fc layer 3 self.abs_max_out: 1156.0\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  0.660031/  1.035926, val:  78.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.39 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 90.5415%\n",
      "layer   2  Sparsity: 69.5021%\n",
      "layer   3  Sparsity: 72.6902%\n",
      "total_backward_count 293700 real_backward_count 20583   7.008%\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  0.642664/  1.007585, val:  80.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.89 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5438%\n",
      "layer   2  Sparsity: 69.3986%\n",
      "layer   3  Sparsity: 72.7176%\n",
      "total_backward_count 303490 real_backward_count 20970   6.910%\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  0.661155/  0.981896, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.09 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5769%\n",
      "layer   2  Sparsity: 69.7177%\n",
      "layer   3  Sparsity: 72.3495%\n",
      "total_backward_count 313280 real_backward_count 21339   6.811%\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  0.634431/  0.988821, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.53 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 90.5480%\n",
      "layer   2  Sparsity: 69.6473%\n",
      "layer   3  Sparsity: 72.5816%\n",
      "total_backward_count 323070 real_backward_count 21653   6.702%\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  0.631926/  0.963131, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.40 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 90.5385%\n",
      "layer   2  Sparsity: 69.4276%\n",
      "layer   3  Sparsity: 73.0708%\n",
      "total_backward_count 332860 real_backward_count 21945   6.593%\n",
      "fc layer 3 self.abs_max_out: 1189.0\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  0.614985/  0.989268, val:  79.17%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.39 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 90.5309%\n",
      "layer   2  Sparsity: 69.7502%\n",
      "layer   3  Sparsity: 73.7850%\n",
      "total_backward_count 342650 real_backward_count 22305   6.510%\n",
      "fc layer 1 self.abs_max_out: 7440.0\n",
      "lif layer 1 self.abs_max_v: 13066.5\n",
      "fc layer 1 self.abs_max_out: 7562.0\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  0.600449/  0.963587, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.00 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5353%\n",
      "layer   2  Sparsity: 69.9107%\n",
      "layer   3  Sparsity: 74.0447%\n",
      "total_backward_count 352440 real_backward_count 22632   6.422%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  0.597739/  0.947796, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.35 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 90.5634%\n",
      "layer   2  Sparsity: 69.4090%\n",
      "layer   3  Sparsity: 74.1290%\n",
      "total_backward_count 362230 real_backward_count 22918   6.327%\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  0.581840/  0.999631, val:  77.92%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.81 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5313%\n",
      "layer   2  Sparsity: 69.9094%\n",
      "layer   3  Sparsity: 73.5544%\n",
      "total_backward_count 372020 real_backward_count 23197   6.235%\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  0.570449/  0.992716, val:  78.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.85 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5709%\n",
      "layer   2  Sparsity: 69.7195%\n",
      "layer   3  Sparsity: 73.0388%\n",
      "total_backward_count 381810 real_backward_count 23515   6.159%\n",
      "lif layer 1 self.abs_max_v: 13166.0\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  0.538029/  0.988618, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.04 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5332%\n",
      "layer   2  Sparsity: 69.4638%\n",
      "layer   3  Sparsity: 73.4905%\n",
      "total_backward_count 391600 real_backward_count 23803   6.078%\n",
      "fc layer 3 self.abs_max_out: 1207.0\n",
      "fc layer 1 self.abs_max_out: 7791.0\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  0.569583/  0.969540, val:  79.17%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.08 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5435%\n",
      "layer   2  Sparsity: 69.1850%\n",
      "layer   3  Sparsity: 72.7306%\n",
      "total_backward_count 401390 real_backward_count 24118   6.009%\n",
      "fc layer 3 self.abs_max_out: 1232.0\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  0.551015/  0.920875, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.11 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 90.5845%\n",
      "layer   2  Sparsity: 69.3644%\n",
      "layer   3  Sparsity: 72.5880%\n",
      "total_backward_count 411180 real_backward_count 24411   5.937%\n",
      "fc layer 3 self.abs_max_out: 1247.0\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  0.520823/  0.867180, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.48 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 90.5510%\n",
      "layer   2  Sparsity: 69.5710%\n",
      "layer   3  Sparsity: 72.9793%\n",
      "total_backward_count 420970 real_backward_count 24681   5.863%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  0.510081/  0.865833, val:  84.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.20 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 90.5186%\n",
      "layer   2  Sparsity: 69.3924%\n",
      "layer   3  Sparsity: 72.8518%\n",
      "total_backward_count 430760 real_backward_count 24943   5.790%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  0.510846/  0.906228, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.63 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5802%\n",
      "layer   2  Sparsity: 69.4613%\n",
      "layer   3  Sparsity: 72.9123%\n",
      "total_backward_count 440550 real_backward_count 25209   5.722%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  0.529459/  0.965501, val:  78.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.52 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5342%\n",
      "layer   2  Sparsity: 69.5162%\n",
      "layer   3  Sparsity: 72.2105%\n",
      "total_backward_count 450340 real_backward_count 25503   5.663%\n",
      "lif layer 2 self.abs_max_v: 7179.0\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  0.505665/  0.868126, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.55 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5369%\n",
      "layer   2  Sparsity: 69.5755%\n",
      "layer   3  Sparsity: 73.1000%\n",
      "total_backward_count 460130 real_backward_count 25750   5.596%\n",
      "fc layer 3 self.abs_max_out: 1267.0\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  0.507802/  0.872802, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.08 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5336%\n",
      "layer   2  Sparsity: 69.4436%\n",
      "layer   3  Sparsity: 73.1360%\n",
      "total_backward_count 469920 real_backward_count 25996   5.532%\n",
      "fc layer 3 self.abs_max_out: 1294.0\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  0.501686/  0.863804, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.81 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5575%\n",
      "layer   2  Sparsity: 69.4573%\n",
      "layer   3  Sparsity: 73.6749%\n",
      "total_backward_count 479710 real_backward_count 26227   5.467%\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  0.482723/  0.831027, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.54 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5463%\n",
      "layer   2  Sparsity: 69.2716%\n",
      "layer   3  Sparsity: 73.2788%\n",
      "total_backward_count 489500 real_backward_count 26441   5.402%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  0.474860/  0.882037, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.34 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5351%\n",
      "layer   2  Sparsity: 69.3531%\n",
      "layer   3  Sparsity: 73.0577%\n",
      "total_backward_count 499290 real_backward_count 26663   5.340%\n",
      "fc layer 2 self.abs_max_out: 3960.0\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  0.453455/  0.849104, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.55 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5729%\n",
      "layer   2  Sparsity: 69.3098%\n",
      "layer   3  Sparsity: 72.7588%\n",
      "total_backward_count 509080 real_backward_count 26866   5.277%\n",
      "fc layer 3 self.abs_max_out: 1343.0\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  0.444643/  0.810393, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.99 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5276%\n",
      "layer   2  Sparsity: 69.3357%\n",
      "layer   3  Sparsity: 72.6539%\n",
      "total_backward_count 518870 real_backward_count 27072   5.217%\n",
      "fc layer 1 self.abs_max_out: 7957.0\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  0.445689/  0.864730, val:  80.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.83 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5436%\n",
      "layer   2  Sparsity: 69.6116%\n",
      "layer   3  Sparsity: 72.5299%\n",
      "total_backward_count 528660 real_backward_count 27257   5.156%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  0.432645/  0.808869, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.11 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5039%\n",
      "layer   2  Sparsity: 69.7944%\n",
      "layer   3  Sparsity: 72.2596%\n",
      "total_backward_count 538450 real_backward_count 27474   5.102%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  0.441105/  0.801711, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.79 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5478%\n",
      "layer   2  Sparsity: 69.3737%\n",
      "layer   3  Sparsity: 72.4662%\n",
      "total_backward_count 548240 real_backward_count 27673   5.048%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  0.444974/  0.869108, val:  81.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.98 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5581%\n",
      "layer   2  Sparsity: 69.3596%\n",
      "layer   3  Sparsity: 72.9053%\n",
      "total_backward_count 558030 real_backward_count 27863   4.993%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  0.449853/  0.812420, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.58 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5353%\n",
      "layer   2  Sparsity: 69.7283%\n",
      "layer   3  Sparsity: 73.3890%\n",
      "total_backward_count 567820 real_backward_count 28023   4.935%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  0.436965/  0.855085, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.66 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5369%\n",
      "layer   2  Sparsity: 69.5112%\n",
      "layer   3  Sparsity: 73.3455%\n",
      "total_backward_count 577610 real_backward_count 28241   4.889%\n",
      "lif layer 1 self.abs_max_v: 13209.5\n",
      "fc layer 1 self.abs_max_out: 8003.0\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  0.445065/  0.865293, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.32 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5076%\n",
      "layer   2  Sparsity: 69.5814%\n",
      "layer   3  Sparsity: 72.9595%\n",
      "total_backward_count 587400 real_backward_count 28434   4.841%\n",
      "fc layer 3 self.abs_max_out: 1352.0\n",
      "lif layer 2 self.abs_max_v: 7254.0\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  0.435117/  0.801146, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.57 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5125%\n",
      "layer   2  Sparsity: 69.5890%\n",
      "layer   3  Sparsity: 72.8816%\n",
      "total_backward_count 597190 real_backward_count 28627   4.794%\n",
      "fc layer 3 self.abs_max_out: 1394.0\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  0.416798/  0.794437, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.48 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5412%\n",
      "layer   2  Sparsity: 69.4226%\n",
      "layer   3  Sparsity: 73.3032%\n",
      "total_backward_count 606980 real_backward_count 28794   4.744%\n",
      "lif layer 2 self.abs_max_v: 7329.0\n",
      "fc layer 2 self.abs_max_out: 3962.0\n",
      "fc layer 2 self.abs_max_out: 3977.0\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  0.393162/  0.806387, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.13 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5333%\n",
      "layer   2  Sparsity: 69.6025%\n",
      "layer   3  Sparsity: 73.4103%\n",
      "total_backward_count 616770 real_backward_count 28956   4.695%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  0.403709/  0.758544, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.89 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5443%\n",
      "layer   2  Sparsity: 69.6311%\n",
      "layer   3  Sparsity: 73.0369%\n",
      "total_backward_count 626560 real_backward_count 29112   4.646%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  0.399603/  0.847927, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.61 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5477%\n",
      "layer   2  Sparsity: 69.5581%\n",
      "layer   3  Sparsity: 74.2262%\n",
      "total_backward_count 636350 real_backward_count 29276   4.601%\n",
      "lif layer 1 self.abs_max_v: 13372.0\n",
      "lif layer 1 self.abs_max_v: 13514.0\n",
      "fc layer 1 self.abs_max_out: 8068.0\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  0.402759/  0.846748, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.62 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5455%\n",
      "layer   2  Sparsity: 69.5120%\n",
      "layer   3  Sparsity: 74.0376%\n",
      "total_backward_count 646140 real_backward_count 29436   4.556%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  0.411674/  0.815197, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.62 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5308%\n",
      "layer   2  Sparsity: 69.5331%\n",
      "layer   3  Sparsity: 73.1625%\n",
      "total_backward_count 655930 real_backward_count 29612   4.515%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  0.395877/  0.769919, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.28 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5345%\n",
      "layer   2  Sparsity: 69.6731%\n",
      "layer   3  Sparsity: 73.4411%\n",
      "total_backward_count 665720 real_backward_count 29788   4.475%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  0.399457/  0.832741, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.65 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5227%\n",
      "layer   2  Sparsity: 69.6648%\n",
      "layer   3  Sparsity: 73.3331%\n",
      "total_backward_count 675510 real_backward_count 29935   4.431%\n",
      "lif layer 2 self.abs_max_v: 7384.0\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  0.395425/  0.774041, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.62 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5083%\n",
      "layer   2  Sparsity: 69.8489%\n",
      "layer   3  Sparsity: 73.1461%\n",
      "total_backward_count 685300 real_backward_count 30057   4.386%\n",
      "lif layer 2 self.abs_max_v: 7393.5\n",
      "lif layer 2 self.abs_max_v: 7530.0\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  0.389598/  0.789206, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.71 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5581%\n",
      "layer   2  Sparsity: 69.6893%\n",
      "layer   3  Sparsity: 73.1728%\n",
      "total_backward_count 695090 real_backward_count 30187   4.343%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  0.396101/  0.770310, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.77 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5373%\n",
      "layer   2  Sparsity: 69.5210%\n",
      "layer   3  Sparsity: 72.8825%\n",
      "total_backward_count 704880 real_backward_count 30320   4.301%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  0.385785/  0.758982, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.57 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5276%\n",
      "layer   2  Sparsity: 69.5140%\n",
      "layer   3  Sparsity: 72.8491%\n",
      "total_backward_count 714670 real_backward_count 30444   4.260%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  0.372548/  0.832299, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.90 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5575%\n",
      "layer   2  Sparsity: 69.7563%\n",
      "layer   3  Sparsity: 72.6363%\n",
      "total_backward_count 724460 real_backward_count 30566   4.219%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  0.378667/  0.792146, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.23 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 90.5283%\n",
      "layer   2  Sparsity: 69.4970%\n",
      "layer   3  Sparsity: 72.4969%\n",
      "total_backward_count 734250 real_backward_count 30686   4.179%\n",
      "fc layer 3 self.abs_max_out: 1404.0\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  0.373150/  0.749286, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.71 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5217%\n",
      "layer   2  Sparsity: 69.2677%\n",
      "layer   3  Sparsity: 72.1512%\n",
      "total_backward_count 744040 real_backward_count 30817   4.142%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  0.368999/  0.757958, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.94 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5165%\n",
      "layer   2  Sparsity: 69.3559%\n",
      "layer   3  Sparsity: 72.4292%\n",
      "total_backward_count 753830 real_backward_count 30958   4.107%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  0.359220/  0.813839, val:  82.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.18 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5676%\n",
      "layer   2  Sparsity: 69.3106%\n",
      "layer   3  Sparsity: 72.8471%\n",
      "total_backward_count 763620 real_backward_count 31089   4.071%\n",
      "fc layer 2 self.abs_max_out: 4028.0\n",
      "lif layer 2 self.abs_max_v: 7561.5\n",
      "fc layer 3 self.abs_max_out: 1420.0\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  0.345157/  0.755939, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.20 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5196%\n",
      "layer   2  Sparsity: 69.2864%\n",
      "layer   3  Sparsity: 73.9027%\n",
      "total_backward_count 773410 real_backward_count 31223   4.037%\n",
      "fc layer 2 self.abs_max_out: 4124.0\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  0.331831/  0.719040, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.21 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 90.5398%\n",
      "layer   2  Sparsity: 69.7672%\n",
      "layer   3  Sparsity: 74.2438%\n",
      "total_backward_count 783200 real_backward_count 31324   3.999%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  0.334852/  0.764871, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.05 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5230%\n",
      "layer   2  Sparsity: 69.6861%\n",
      "layer   3  Sparsity: 74.2475%\n",
      "total_backward_count 792990 real_backward_count 31430   3.963%\n",
      "fc layer 3 self.abs_max_out: 1435.0\n",
      "fc layer 3 self.abs_max_out: 1469.0\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  0.331440/  0.760024, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.43 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5525%\n",
      "layer   2  Sparsity: 69.3488%\n",
      "layer   3  Sparsity: 73.8395%\n",
      "total_backward_count 802780 real_backward_count 31544   3.929%\n",
      "fc layer 3 self.abs_max_out: 1494.0\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  0.331471/  0.763168, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.56 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5608%\n",
      "layer   2  Sparsity: 69.2290%\n",
      "layer   3  Sparsity: 73.6659%\n",
      "total_backward_count 812570 real_backward_count 31673   3.898%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  0.323753/  0.739782, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.77 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5402%\n",
      "layer   2  Sparsity: 69.3998%\n",
      "layer   3  Sparsity: 73.9762%\n",
      "total_backward_count 822360 real_backward_count 31782   3.865%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  0.323146/  0.748598, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.05 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5764%\n",
      "layer   2  Sparsity: 69.1477%\n",
      "layer   3  Sparsity: 74.3900%\n",
      "total_backward_count 832150 real_backward_count 31902   3.834%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  0.318678/  0.741272, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.25 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5321%\n",
      "layer   2  Sparsity: 69.5105%\n",
      "layer   3  Sparsity: 74.1817%\n",
      "total_backward_count 841940 real_backward_count 31999   3.801%\n",
      "fc layer 1 self.abs_max_out: 8123.0\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  0.315454/  0.707505, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.17 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5377%\n",
      "layer   2  Sparsity: 69.3572%\n",
      "layer   3  Sparsity: 73.9257%\n",
      "total_backward_count 851730 real_backward_count 32085   3.767%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  0.306392/  0.723630, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.89 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5271%\n",
      "layer   2  Sparsity: 69.2494%\n",
      "layer   3  Sparsity: 74.0432%\n",
      "total_backward_count 861520 real_backward_count 32166   3.734%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  0.312725/  0.749864, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.46 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5376%\n",
      "layer   2  Sparsity: 68.9513%\n",
      "layer   3  Sparsity: 74.3245%\n",
      "total_backward_count 871310 real_backward_count 32268   3.703%\n",
      "fc layer 1 self.abs_max_out: 8170.0\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  0.310721/  0.734383, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.19 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5447%\n",
      "layer   2  Sparsity: 69.0778%\n",
      "layer   3  Sparsity: 74.5386%\n",
      "total_backward_count 881100 real_backward_count 32368   3.674%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  0.320675/  0.730156, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.93 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5336%\n",
      "layer   2  Sparsity: 69.2499%\n",
      "layer   3  Sparsity: 74.5626%\n",
      "total_backward_count 890890 real_backward_count 32463   3.644%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  0.305959/  0.753558, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.05 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 90.5404%\n",
      "layer   2  Sparsity: 69.1253%\n",
      "layer   3  Sparsity: 74.8345%\n",
      "total_backward_count 900680 real_backward_count 32561   3.615%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  0.297176/  0.749488, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.28 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5467%\n",
      "layer   2  Sparsity: 69.2915%\n",
      "layer   3  Sparsity: 75.2085%\n",
      "total_backward_count 910470 real_backward_count 32641   3.585%\n",
      "lif layer 2 self.abs_max_v: 7669.0\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  0.306183/  0.738597, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.61 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5532%\n",
      "layer   2  Sparsity: 69.2002%\n",
      "layer   3  Sparsity: 75.3845%\n",
      "total_backward_count 920260 real_backward_count 32744   3.558%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  0.300720/  0.776202, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.35 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5551%\n",
      "layer   2  Sparsity: 69.0357%\n",
      "layer   3  Sparsity: 74.9837%\n",
      "total_backward_count 930050 real_backward_count 32829   3.530%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  0.296366/  0.744046, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.04 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5141%\n",
      "layer   2  Sparsity: 69.1034%\n",
      "layer   3  Sparsity: 75.0052%\n",
      "total_backward_count 939840 real_backward_count 32894   3.500%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  0.299770/  0.747273, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.55 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5321%\n",
      "layer   2  Sparsity: 69.0291%\n",
      "layer   3  Sparsity: 74.8722%\n",
      "total_backward_count 949630 real_backward_count 32970   3.472%\n",
      "fc layer 2 self.abs_max_out: 4199.0\n",
      "lif layer 2 self.abs_max_v: 7741.5\n",
      "lif layer 2 self.abs_max_v: 7747.0\n",
      "lif layer 2 self.abs_max_v: 7762.5\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  0.290196/  0.762596, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.47 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5259%\n",
      "layer   2  Sparsity: 69.1604%\n",
      "layer   3  Sparsity: 74.7230%\n",
      "total_backward_count 959420 real_backward_count 33053   3.445%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  0.289836/  0.746365, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.52 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5405%\n",
      "layer   2  Sparsity: 69.1397%\n",
      "layer   3  Sparsity: 75.0173%\n",
      "total_backward_count 969210 real_backward_count 33123   3.418%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  0.291605/  0.725268, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.50 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 90.5430%\n",
      "layer   2  Sparsity: 69.2670%\n",
      "layer   3  Sparsity: 75.1007%\n",
      "total_backward_count 979000 real_backward_count 33197   3.391%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  0.287354/  0.752266, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.00 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 90.5046%\n",
      "layer   2  Sparsity: 68.9389%\n",
      "layer   3  Sparsity: 75.0338%\n",
      "total_backward_count 988790 real_backward_count 33256   3.363%\n",
      "lif layer 2 self.abs_max_v: 7884.5\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  0.299658/  0.728828, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.54 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5274%\n",
      "layer   2  Sparsity: 69.0917%\n",
      "layer   3  Sparsity: 74.5455%\n",
      "total_backward_count 998580 real_backward_count 33347   3.339%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  0.292337/  0.749370, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.20 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 90.5376%\n",
      "layer   2  Sparsity: 69.2466%\n",
      "layer   3  Sparsity: 74.5475%\n",
      "total_backward_count 1008370 real_backward_count 33409   3.313%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  0.305778/  0.707646, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.35 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5115%\n",
      "layer   2  Sparsity: 69.0243%\n",
      "layer   3  Sparsity: 74.7564%\n",
      "total_backward_count 1018160 real_backward_count 33506   3.291%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  0.293465/  0.711725, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.71 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5242%\n",
      "layer   2  Sparsity: 69.0470%\n",
      "layer   3  Sparsity: 74.7681%\n",
      "total_backward_count 1027950 real_backward_count 33583   3.267%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  0.296198/  0.726513, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.37 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5310%\n",
      "layer   2  Sparsity: 69.0178%\n",
      "layer   3  Sparsity: 75.0256%\n",
      "total_backward_count 1037740 real_backward_count 33650   3.243%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  0.310770/  0.702974, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.23 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5393%\n",
      "layer   2  Sparsity: 68.8237%\n",
      "layer   3  Sparsity: 74.6748%\n",
      "total_backward_count 1047530 real_backward_count 33733   3.220%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  0.306487/  0.737741, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.99 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5332%\n",
      "layer   2  Sparsity: 68.7652%\n",
      "layer   3  Sparsity: 74.2528%\n",
      "total_backward_count 1057320 real_backward_count 33801   3.197%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  0.306018/  0.723662, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.74 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5332%\n",
      "layer   2  Sparsity: 68.7824%\n",
      "layer   3  Sparsity: 74.8470%\n",
      "total_backward_count 1067110 real_backward_count 33876   3.175%\n",
      "fc layer 3 self.abs_max_out: 1496.0\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  0.309681/  0.712113, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.92 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5306%\n",
      "layer   2  Sparsity: 69.0055%\n",
      "layer   3  Sparsity: 74.8377%\n",
      "total_backward_count 1076900 real_backward_count 33954   3.153%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  0.304853/  0.711636, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.40 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5344%\n",
      "layer   2  Sparsity: 69.0767%\n",
      "layer   3  Sparsity: 74.8820%\n",
      "total_backward_count 1086690 real_backward_count 34027   3.131%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  0.297934/  0.724266, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.49 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5318%\n",
      "layer   2  Sparsity: 69.0799%\n",
      "layer   3  Sparsity: 75.4037%\n",
      "total_backward_count 1096480 real_backward_count 34081   3.108%\n",
      "fc layer 1 self.abs_max_out: 8190.0\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  0.308162/  0.745488, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.41 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5515%\n",
      "layer   2  Sparsity: 68.6876%\n",
      "layer   3  Sparsity: 75.5765%\n",
      "total_backward_count 1106270 real_backward_count 34159   3.088%\n",
      "fc layer 3 self.abs_max_out: 1510.0\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  0.316475/  0.756974, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.32 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5490%\n",
      "layer   2  Sparsity: 68.6530%\n",
      "layer   3  Sparsity: 75.0792%\n",
      "total_backward_count 1116060 real_backward_count 34230   3.067%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  0.300367/  0.744682, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.53 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5494%\n",
      "layer   2  Sparsity: 68.7266%\n",
      "layer   3  Sparsity: 74.5879%\n",
      "total_backward_count 1125850 real_backward_count 34295   3.046%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  0.295148/  0.732029, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.97 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5202%\n",
      "layer   2  Sparsity: 68.8208%\n",
      "layer   3  Sparsity: 74.3955%\n",
      "total_backward_count 1135640 real_backward_count 34362   3.026%\n",
      "fc layer 3 self.abs_max_out: 1520.0\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  0.295844/  0.728465, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.15 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5521%\n",
      "layer   2  Sparsity: 68.8827%\n",
      "layer   3  Sparsity: 74.3749%\n",
      "total_backward_count 1145430 real_backward_count 34412   3.004%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  0.289777/  0.728245, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.05 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5545%\n",
      "layer   2  Sparsity: 68.8433%\n",
      "layer   3  Sparsity: 74.8020%\n",
      "total_backward_count 1155220 real_backward_count 34472   2.984%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  0.284727/  0.726609, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.85 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5138%\n",
      "layer   2  Sparsity: 68.9193%\n",
      "layer   3  Sparsity: 75.1097%\n",
      "total_backward_count 1165010 real_backward_count 34526   2.964%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  0.285394/  0.715775, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.09 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5355%\n",
      "layer   2  Sparsity: 68.5641%\n",
      "layer   3  Sparsity: 75.0971%\n",
      "total_backward_count 1174800 real_backward_count 34575   2.943%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  0.293375/  0.738094, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.87 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5468%\n",
      "layer   2  Sparsity: 68.8689%\n",
      "layer   3  Sparsity: 74.7177%\n",
      "total_backward_count 1184590 real_backward_count 34651   2.925%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  0.285058/  0.728056, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.43 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5356%\n",
      "layer   2  Sparsity: 68.9811%\n",
      "layer   3  Sparsity: 74.4917%\n",
      "total_backward_count 1194380 real_backward_count 34699   2.905%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  0.287826/  0.712948, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.37 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5637%\n",
      "layer   2  Sparsity: 68.9166%\n",
      "layer   3  Sparsity: 74.3196%\n",
      "total_backward_count 1204170 real_backward_count 34760   2.887%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  0.290248/  0.746358, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.61 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5194%\n",
      "layer   2  Sparsity: 68.7741%\n",
      "layer   3  Sparsity: 74.5389%\n",
      "total_backward_count 1213960 real_backward_count 34818   2.868%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  0.286421/  0.712857, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.23 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5375%\n",
      "layer   2  Sparsity: 68.9478%\n",
      "layer   3  Sparsity: 74.8844%\n",
      "total_backward_count 1223750 real_backward_count 34874   2.850%\n",
      "fc layer 1 self.abs_max_out: 8244.0\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  0.282944/  0.712261, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.48 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5109%\n",
      "layer   2  Sparsity: 68.9314%\n",
      "layer   3  Sparsity: 75.2438%\n",
      "total_backward_count 1233540 real_backward_count 34919   2.831%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  0.279184/  0.728207, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.16 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5101%\n",
      "layer   2  Sparsity: 69.1340%\n",
      "layer   3  Sparsity: 75.0509%\n",
      "total_backward_count 1243330 real_backward_count 34979   2.813%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  0.284286/  0.752257, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.72 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5413%\n",
      "layer   2  Sparsity: 69.1279%\n",
      "layer   3  Sparsity: 75.1160%\n",
      "total_backward_count 1253120 real_backward_count 35038   2.796%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  0.281069/  0.764535, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.33 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5470%\n",
      "layer   2  Sparsity: 69.0319%\n",
      "layer   3  Sparsity: 75.0326%\n",
      "total_backward_count 1262910 real_backward_count 35093   2.779%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  0.278613/  0.744082, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.29 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5258%\n",
      "layer   2  Sparsity: 69.1018%\n",
      "layer   3  Sparsity: 74.5537%\n",
      "total_backward_count 1272700 real_backward_count 35139   2.761%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  0.278201/  0.694973, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.77 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5454%\n",
      "layer   2  Sparsity: 68.7842%\n",
      "layer   3  Sparsity: 74.5122%\n",
      "total_backward_count 1282490 real_backward_count 35205   2.745%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  0.267557/  0.700296, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.46 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5446%\n",
      "layer   2  Sparsity: 68.9232%\n",
      "layer   3  Sparsity: 74.9365%\n",
      "total_backward_count 1292280 real_backward_count 35269   2.729%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  0.265070/  0.711326, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.61 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5261%\n",
      "layer   2  Sparsity: 68.7856%\n",
      "layer   3  Sparsity: 75.5498%\n",
      "total_backward_count 1302070 real_backward_count 35311   2.712%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  0.271152/  0.700191, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.26 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5297%\n",
      "layer   2  Sparsity: 68.2630%\n",
      "layer   3  Sparsity: 75.7054%\n",
      "total_backward_count 1311860 real_backward_count 35352   2.695%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  0.278415/  0.725361, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.91 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5483%\n",
      "layer   2  Sparsity: 67.9357%\n",
      "layer   3  Sparsity: 75.4698%\n",
      "total_backward_count 1321650 real_backward_count 35399   2.678%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  0.274138/  0.710129, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.82 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5372%\n",
      "layer   2  Sparsity: 68.3511%\n",
      "layer   3  Sparsity: 75.0669%\n",
      "total_backward_count 1331440 real_backward_count 35462   2.663%\n",
      "lif layer 1 self.abs_max_v: 13544.5\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  0.262409/  0.708054, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.32 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5352%\n",
      "layer   2  Sparsity: 68.7901%\n",
      "layer   3  Sparsity: 74.9361%\n",
      "total_backward_count 1341230 real_backward_count 35516   2.648%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  0.257968/  0.683984, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.78 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5518%\n",
      "layer   2  Sparsity: 68.8402%\n",
      "layer   3  Sparsity: 75.0776%\n",
      "total_backward_count 1351020 real_backward_count 35578   2.633%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  0.260383/  0.688708, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.40 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5472%\n",
      "layer   2  Sparsity: 68.7763%\n",
      "layer   3  Sparsity: 74.7562%\n",
      "total_backward_count 1360810 real_backward_count 35637   2.619%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  0.253451/  0.685389, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.94 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5630%\n",
      "layer   2  Sparsity: 68.6776%\n",
      "layer   3  Sparsity: 74.7942%\n",
      "total_backward_count 1370600 real_backward_count 35680   2.603%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  0.253983/  0.751293, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.65 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5487%\n",
      "layer   2  Sparsity: 68.6724%\n",
      "layer   3  Sparsity: 74.2507%\n",
      "total_backward_count 1380390 real_backward_count 35735   2.589%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  0.247534/  0.680054, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.53 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5460%\n",
      "layer   2  Sparsity: 68.7422%\n",
      "layer   3  Sparsity: 74.4732%\n",
      "total_backward_count 1390180 real_backward_count 35780   2.574%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  0.251179/  0.696488, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.92 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5340%\n",
      "layer   2  Sparsity: 68.5741%\n",
      "layer   3  Sparsity: 74.8566%\n",
      "total_backward_count 1399970 real_backward_count 35812   2.558%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  0.250502/  0.702647, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.23 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5648%\n",
      "layer   2  Sparsity: 68.2223%\n",
      "layer   3  Sparsity: 75.0514%\n",
      "total_backward_count 1409760 real_backward_count 35838   2.542%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  0.250947/  0.701446, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.82 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5148%\n",
      "layer   2  Sparsity: 68.4324%\n",
      "layer   3  Sparsity: 75.0603%\n",
      "total_backward_count 1419550 real_backward_count 35873   2.527%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  0.259784/  0.733496, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.02 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5436%\n",
      "layer   2  Sparsity: 68.6185%\n",
      "layer   3  Sparsity: 74.8095%\n",
      "total_backward_count 1429340 real_backward_count 35924   2.513%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  0.250403/  0.677379, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.75 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5554%\n",
      "layer   2  Sparsity: 68.6034%\n",
      "layer   3  Sparsity: 74.7746%\n",
      "total_backward_count 1439130 real_backward_count 35965   2.499%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  0.251816/  0.669286, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.40 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5612%\n",
      "layer   2  Sparsity: 68.6733%\n",
      "layer   3  Sparsity: 74.4621%\n",
      "total_backward_count 1448920 real_backward_count 35998   2.484%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  0.244441/  0.679103, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.00 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5479%\n",
      "layer   2  Sparsity: 68.7420%\n",
      "layer   3  Sparsity: 74.5642%\n",
      "total_backward_count 1458710 real_backward_count 36018   2.469%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  0.247734/  0.714412, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.54 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5201%\n",
      "layer   2  Sparsity: 68.8888%\n",
      "layer   3  Sparsity: 74.6655%\n",
      "total_backward_count 1468500 real_backward_count 36045   2.455%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  0.245601/  0.707250, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.68 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5369%\n",
      "layer   2  Sparsity: 68.7928%\n",
      "layer   3  Sparsity: 75.0908%\n",
      "total_backward_count 1478290 real_backward_count 36080   2.441%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  0.250452/  0.715473, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.89 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5606%\n",
      "layer   2  Sparsity: 68.8327%\n",
      "layer   3  Sparsity: 75.6276%\n",
      "total_backward_count 1488080 real_backward_count 36114   2.427%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  0.241174/  0.707102, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.37 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5513%\n",
      "layer   2  Sparsity: 68.9205%\n",
      "layer   3  Sparsity: 75.7491%\n",
      "total_backward_count 1497870 real_backward_count 36140   2.413%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  0.243825/  0.719323, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.48 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5127%\n",
      "layer   2  Sparsity: 68.8870%\n",
      "layer   3  Sparsity: 75.7116%\n",
      "total_backward_count 1507660 real_backward_count 36174   2.399%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  0.241890/  0.691113, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.34 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5361%\n",
      "layer   2  Sparsity: 68.9996%\n",
      "layer   3  Sparsity: 75.3608%\n",
      "total_backward_count 1517450 real_backward_count 36208   2.386%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  0.245363/  0.724050, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.16 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5454%\n",
      "layer   2  Sparsity: 68.8825%\n",
      "layer   3  Sparsity: 75.1580%\n",
      "total_backward_count 1527240 real_backward_count 36251   2.374%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  0.250785/  0.733088, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.11 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5609%\n",
      "layer   2  Sparsity: 68.7696%\n",
      "layer   3  Sparsity: 74.9619%\n",
      "total_backward_count 1537030 real_backward_count 36287   2.361%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  0.239876/  0.701585, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.25 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5328%\n",
      "layer   2  Sparsity: 68.6290%\n",
      "layer   3  Sparsity: 74.9292%\n",
      "total_backward_count 1546820 real_backward_count 36316   2.348%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  0.240705/  0.689175, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.20 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5450%\n",
      "layer   2  Sparsity: 68.5380%\n",
      "layer   3  Sparsity: 75.1537%\n",
      "total_backward_count 1556610 real_backward_count 36346   2.335%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  0.236281/  0.687765, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.30 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5234%\n",
      "layer   2  Sparsity: 68.6143%\n",
      "layer   3  Sparsity: 74.9725%\n",
      "total_backward_count 1566400 real_backward_count 36370   2.322%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  0.238850/  0.697515, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.16 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5534%\n",
      "layer   2  Sparsity: 68.6899%\n",
      "layer   3  Sparsity: 75.0658%\n",
      "total_backward_count 1576190 real_backward_count 36421   2.311%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  0.241973/  0.726047, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.74 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5567%\n",
      "layer   2  Sparsity: 68.7625%\n",
      "layer   3  Sparsity: 74.9867%\n",
      "total_backward_count 1585980 real_backward_count 36467   2.299%\n",
      "fc layer 3 self.abs_max_out: 1537.0\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  0.237124/  0.704269, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.13 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5246%\n",
      "layer   2  Sparsity: 68.7718%\n",
      "layer   3  Sparsity: 75.2602%\n",
      "total_backward_count 1595770 real_backward_count 36497   2.287%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  0.240289/  0.691467, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.49 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5644%\n",
      "layer   2  Sparsity: 68.6602%\n",
      "layer   3  Sparsity: 75.3215%\n",
      "total_backward_count 1605560 real_backward_count 36541   2.276%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  0.236500/  0.680865, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.70 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5292%\n",
      "layer   2  Sparsity: 68.6786%\n",
      "layer   3  Sparsity: 75.3014%\n",
      "total_backward_count 1615350 real_backward_count 36570   2.264%\n",
      "fc layer 3 self.abs_max_out: 1538.0\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  0.243011/  0.690930, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.45 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5609%\n",
      "layer   2  Sparsity: 68.5824%\n",
      "layer   3  Sparsity: 75.3530%\n",
      "total_backward_count 1625140 real_backward_count 36592   2.252%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  0.242696/  0.698871, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.05 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5346%\n",
      "layer   2  Sparsity: 68.5098%\n",
      "layer   3  Sparsity: 75.6175%\n",
      "total_backward_count 1634930 real_backward_count 36614   2.239%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  0.239521/  0.676975, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.02 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5311%\n",
      "layer   2  Sparsity: 68.5695%\n",
      "layer   3  Sparsity: 75.7445%\n",
      "total_backward_count 1644720 real_backward_count 36632   2.227%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  0.241339/  0.695293, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.97 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5526%\n",
      "layer   2  Sparsity: 68.5402%\n",
      "layer   3  Sparsity: 75.6661%\n",
      "total_backward_count 1654510 real_backward_count 36656   2.216%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.246921/  0.691205, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.83 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5290%\n",
      "layer   2  Sparsity: 68.5250%\n",
      "layer   3  Sparsity: 75.4726%\n",
      "total_backward_count 1664300 real_backward_count 36682   2.204%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.243102/  0.703927, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.96 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5655%\n",
      "layer   2  Sparsity: 68.6497%\n",
      "layer   3  Sparsity: 75.3932%\n",
      "total_backward_count 1674090 real_backward_count 36716   2.193%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  0.245036/  0.693577, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.20 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5527%\n",
      "layer   2  Sparsity: 68.5322%\n",
      "layer   3  Sparsity: 75.2356%\n",
      "total_backward_count 1683880 real_backward_count 36740   2.182%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  0.244403/  0.696077, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.79 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 90.5494%\n",
      "layer   2  Sparsity: 68.5818%\n",
      "layer   3  Sparsity: 75.3699%\n",
      "total_backward_count 1693670 real_backward_count 36786   2.172%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.241026/  0.705023, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.73 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5323%\n",
      "layer   2  Sparsity: 68.3527%\n",
      "layer   3  Sparsity: 75.7833%\n",
      "total_backward_count 1703460 real_backward_count 36827   2.162%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  0.240539/  0.680606, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.83 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5585%\n",
      "layer   2  Sparsity: 68.3738%\n",
      "layer   3  Sparsity: 75.5937%\n",
      "total_backward_count 1713250 real_backward_count 36871   2.152%\n",
      "fc layer 3 self.abs_max_out: 1553.0\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  0.237470/  0.699754, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.48 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5418%\n",
      "layer   2  Sparsity: 68.6449%\n",
      "layer   3  Sparsity: 75.2913%\n",
      "total_backward_count 1723040 real_backward_count 36900   2.142%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  0.231523/  0.683041, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.86 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5238%\n",
      "layer   2  Sparsity: 68.6674%\n",
      "layer   3  Sparsity: 75.2276%\n",
      "total_backward_count 1732830 real_backward_count 36926   2.131%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  0.232858/  0.689552, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.89 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5316%\n",
      "layer   2  Sparsity: 68.8628%\n",
      "layer   3  Sparsity: 75.0829%\n",
      "total_backward_count 1742620 real_backward_count 36948   2.120%\n",
      "fc layer 3 self.abs_max_out: 1593.0\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  0.225279/  0.682865, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.39 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5376%\n",
      "layer   2  Sparsity: 68.8624%\n",
      "layer   3  Sparsity: 75.6417%\n",
      "total_backward_count 1752410 real_backward_count 36973   2.110%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  0.230153/  0.712328, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.15 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5102%\n",
      "layer   2  Sparsity: 68.7884%\n",
      "layer   3  Sparsity: 75.6477%\n",
      "total_backward_count 1762200 real_backward_count 36999   2.100%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  0.235496/  0.678819, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.97 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5484%\n",
      "layer   2  Sparsity: 68.8109%\n",
      "layer   3  Sparsity: 75.3970%\n",
      "total_backward_count 1771990 real_backward_count 37034   2.090%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  0.227246/  0.692945, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.48 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5176%\n",
      "layer   2  Sparsity: 68.8883%\n",
      "layer   3  Sparsity: 74.9843%\n",
      "total_backward_count 1781780 real_backward_count 37059   2.080%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  0.236899/  0.677626, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.21 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5293%\n",
      "layer   2  Sparsity: 68.9902%\n",
      "layer   3  Sparsity: 74.7857%\n",
      "total_backward_count 1791570 real_backward_count 37079   2.070%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  0.227717/  0.678806, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.44 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5415%\n",
      "layer   2  Sparsity: 69.0684%\n",
      "layer   3  Sparsity: 74.7933%\n",
      "total_backward_count 1801360 real_backward_count 37103   2.060%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  0.228441/  0.689374, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.48 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5303%\n",
      "layer   2  Sparsity: 69.0454%\n",
      "layer   3  Sparsity: 74.5857%\n",
      "total_backward_count 1811150 real_backward_count 37126   2.050%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  0.226246/  0.686305, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.04 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5738%\n",
      "layer   2  Sparsity: 69.0718%\n",
      "layer   3  Sparsity: 74.6642%\n",
      "total_backward_count 1820940 real_backward_count 37153   2.040%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  0.231477/  0.722592, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.16 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5579%\n",
      "layer   2  Sparsity: 69.0109%\n",
      "layer   3  Sparsity: 75.0616%\n",
      "total_backward_count 1830730 real_backward_count 37181   2.031%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  0.228180/  0.693283, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.22 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5499%\n",
      "layer   2  Sparsity: 68.9776%\n",
      "layer   3  Sparsity: 75.2014%\n",
      "total_backward_count 1840520 real_backward_count 37206   2.021%\n",
      "fc layer 3 self.abs_max_out: 1619.0\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  0.224783/  0.668769, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.15 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5590%\n",
      "layer   2  Sparsity: 69.0372%\n",
      "layer   3  Sparsity: 75.1872%\n",
      "total_backward_count 1850310 real_backward_count 37227   2.012%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  0.226436/  0.664624, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.93 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5525%\n",
      "layer   2  Sparsity: 69.1234%\n",
      "layer   3  Sparsity: 74.9914%\n",
      "total_backward_count 1860100 real_backward_count 37250   2.003%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  0.225480/  0.698752, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.70 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5283%\n",
      "layer   2  Sparsity: 68.8653%\n",
      "layer   3  Sparsity: 75.0226%\n",
      "total_backward_count 1869890 real_backward_count 37275   1.993%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  0.222276/  0.705670, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.25 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5610%\n",
      "layer   2  Sparsity: 68.6260%\n",
      "layer   3  Sparsity: 75.0144%\n",
      "total_backward_count 1879680 real_backward_count 37300   1.984%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  0.221351/  0.691919, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.02 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5446%\n",
      "layer   2  Sparsity: 68.7063%\n",
      "layer   3  Sparsity: 74.9835%\n",
      "total_backward_count 1889470 real_backward_count 37316   1.975%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  0.215307/  0.676773, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.02 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5487%\n",
      "layer   2  Sparsity: 68.8645%\n",
      "layer   3  Sparsity: 75.2859%\n",
      "total_backward_count 1899260 real_backward_count 37346   1.966%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  0.215664/  0.724124, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.01 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.4961%\n",
      "layer   2  Sparsity: 68.8896%\n",
      "layer   3  Sparsity: 75.3093%\n",
      "total_backward_count 1909050 real_backward_count 37362   1.957%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  0.215802/  0.688698, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.20 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 90.5461%\n",
      "layer   2  Sparsity: 68.8584%\n",
      "layer   3  Sparsity: 75.6117%\n",
      "total_backward_count 1918840 real_backward_count 37378   1.948%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  0.211784/  0.689531, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.96 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5412%\n",
      "layer   2  Sparsity: 68.8908%\n",
      "layer   3  Sparsity: 76.0779%\n",
      "total_backward_count 1928630 real_backward_count 37394   1.939%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  0.212088/  0.681700, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.59 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5223%\n",
      "layer   2  Sparsity: 69.0696%\n",
      "layer   3  Sparsity: 75.8726%\n",
      "total_backward_count 1938420 real_backward_count 37411   1.930%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.219539/  0.732183, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.59 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 90.5459%\n",
      "layer   2  Sparsity: 69.0779%\n",
      "layer   3  Sparsity: 75.7380%\n",
      "total_backward_count 1948210 real_backward_count 37450   1.922%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.219963/  0.728258, val:  84.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.71 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 90.5410%\n",
      "layer   2  Sparsity: 69.3071%\n",
      "layer   3  Sparsity: 75.6884%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eddece25416446e9c9cff302af9bc99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÅ‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.21996</td></tr><tr><td>val_acc_best</td><td>0.9</td></tr><tr><td>val_acc_now</td><td>0.84583</td></tr><tr><td>val_loss</td><td>0.72826</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vivid-sweep-14</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x40lsufk' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x40lsufk</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_084855-x40lsufk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: y5h1ej3h with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 50000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_133410-y5h1ej3h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y5h1ej3h' target=\"_blank\">fiery-sweep-17</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y5h1ej3h' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y5h1ej3h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251118_133419_314', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 50000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 575149142d3019108310063e0e922290\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 139\n",
      "fc layer 1 self.abs_max_out: 477.0\n",
      "lif layer 1 self.abs_max_v: 477.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 504.0\n",
      "lif layer 2 self.abs_max_v: 504.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 196.0\n",
      "lif layer 1 self.abs_max_v: 489.5\n",
      "lif layer 2 self.abs_max_v: 685.5\n",
      "fc layer 1 self.abs_max_out: 485.0\n",
      "lif layer 1 self.abs_max_v: 543.5\n",
      "lif layer 2 self.abs_max_v: 727.0\n",
      "fc layer 1 self.abs_max_out: 491.0\n",
      "lif layer 1 self.abs_max_v: 662.0\n",
      "fc layer 2 self.abs_max_out: 554.0\n",
      "lif layer 2 self.abs_max_v: 809.5\n",
      "fc layer 3 self.abs_max_out: 211.0\n",
      "fc layer 1 self.abs_max_out: 774.0\n",
      "lif layer 1 self.abs_max_v: 774.0\n",
      "fc layer 2 self.abs_max_out: 576.0\n",
      "fc layer 3 self.abs_max_out: 215.0\n",
      "fc layer 1 self.abs_max_out: 1134.0\n",
      "lif layer 1 self.abs_max_v: 1134.0\n",
      "fc layer 3 self.abs_max_out: 235.0\n",
      "lif layer 1 self.abs_max_v: 1141.0\n",
      "fc layer 3 self.abs_max_out: 240.0\n",
      "lif layer 1 self.abs_max_v: 1261.0\n",
      "fc layer 3 self.abs_max_out: 246.0\n",
      "lif layer 1 self.abs_max_v: 1393.5\n",
      "lif layer 2 self.abs_max_v: 867.5\n",
      "smallest_now_T updated: 125\n",
      "fc layer 2 self.abs_max_out: 604.0\n",
      "fc layer 3 self.abs_max_out: 326.0\n",
      "fc layer 1 self.abs_max_out: 1225.0\n",
      "fc layer 2 self.abs_max_out: 684.0\n",
      "lif layer 1 self.abs_max_v: 1415.5\n",
      "lif layer 2 self.abs_max_v: 869.0\n",
      "lif layer 1 self.abs_max_v: 1490.0\n",
      "lif layer 2 self.abs_max_v: 887.5\n",
      "lif layer 2 self.abs_max_v: 949.0\n",
      "smallest_now_T updated: 94\n",
      "fc layer 1 self.abs_max_out: 1278.0\n",
      "fc layer 1 self.abs_max_out: 1304.0\n",
      "lif layer 1 self.abs_max_v: 1527.0\n",
      "lif layer 1 self.abs_max_v: 1722.5\n",
      "lif layer 1 self.abs_max_v: 1775.5\n",
      "fc layer 1 self.abs_max_out: 1350.0\n",
      "lif layer 1 self.abs_max_v: 1841.0\n",
      "lif layer 2 self.abs_max_v: 1004.0\n",
      "fc layer 1 self.abs_max_out: 1360.0\n",
      "lif layer 1 self.abs_max_v: 1906.5\n",
      "fc layer 1 self.abs_max_out: 1407.0\n",
      "fc layer 1 self.abs_max_out: 1519.0\n",
      "fc layer 1 self.abs_max_out: 1936.0\n",
      "lif layer 1 self.abs_max_v: 2177.5\n",
      "lif layer 2 self.abs_max_v: 1028.5\n",
      "fc layer 1 self.abs_max_out: 2164.0\n",
      "lif layer 1 self.abs_max_v: 2617.0\n",
      "lif layer 1 self.abs_max_v: 2664.5\n",
      "lif layer 1 self.abs_max_v: 2769.5\n",
      "fc layer 1 self.abs_max_out: 2242.0\n",
      "lif layer 1 self.abs_max_v: 2880.0\n",
      "lif layer 2 self.abs_max_v: 1073.0\n",
      "lif layer 2 self.abs_max_v: 1123.5\n",
      "fc layer 1 self.abs_max_out: 2308.0\n",
      "fc layer 2 self.abs_max_out: 723.0\n",
      "fc layer 1 self.abs_max_out: 2335.0\n",
      "lif layer 2 self.abs_max_v: 1131.0\n",
      "lif layer 1 self.abs_max_v: 3002.5\n",
      "lif layer 2 self.abs_max_v: 1238.0\n",
      "lif layer 2 self.abs_max_v: 1248.0\n",
      "fc layer 2 self.abs_max_out: 762.0\n",
      "lif layer 2 self.abs_max_v: 1386.0\n",
      "lif layer 1 self.abs_max_v: 3139.0\n",
      "fc layer 2 self.abs_max_out: 778.0\n",
      "fc layer 2 self.abs_max_out: 809.0\n",
      "fc layer 2 self.abs_max_out: 848.0\n",
      "fc layer 2 self.abs_max_out: 877.0\n",
      "fc layer 2 self.abs_max_out: 902.0\n",
      "smallest_now_T updated: 79\n",
      "fc layer 2 self.abs_max_out: 907.0\n",
      "lif layer 2 self.abs_max_v: 1395.5\n",
      "lif layer 2 self.abs_max_v: 1398.5\n",
      "lif layer 2 self.abs_max_v: 1421.0\n",
      "lif layer 2 self.abs_max_v: 1429.5\n",
      "lif layer 2 self.abs_max_v: 1453.0\n",
      "lif layer 2 self.abs_max_v: 1477.0\n",
      "lif layer 2 self.abs_max_v: 1507.5\n",
      "smallest_now_T updated: 73\n",
      "fc layer 1 self.abs_max_out: 2371.0\n",
      "lif layer 2 self.abs_max_v: 1514.5\n",
      "lif layer 2 self.abs_max_v: 1527.0\n",
      "lif layer 2 self.abs_max_v: 1546.5\n",
      "lif layer 2 self.abs_max_v: 1553.5\n",
      "fc layer 2 self.abs_max_out: 911.0\n",
      "lif layer 2 self.abs_max_v: 1675.0\n",
      "lif layer 2 self.abs_max_v: 1688.5\n",
      "lif layer 2 self.abs_max_v: 1707.5\n",
      "fc layer 1 self.abs_max_out: 2455.0\n",
      "fc layer 2 self.abs_max_out: 915.0\n",
      "lif layer 2 self.abs_max_v: 1748.5\n",
      "fc layer 2 self.abs_max_out: 944.0\n",
      "smallest_now_T updated: 65\n",
      "lif layer 1 self.abs_max_v: 3151.0\n",
      "fc layer 1 self.abs_max_out: 2687.0\n",
      "fc layer 1 self.abs_max_out: 2714.0\n",
      "fc layer 1 self.abs_max_out: 2774.0\n",
      "lif layer 1 self.abs_max_v: 3617.5\n",
      "fc layer 1 self.abs_max_out: 2778.0\n",
      "lif layer 1 self.abs_max_v: 3815.0\n",
      "lif layer 1 self.abs_max_v: 3873.5\n",
      "lif layer 1 self.abs_max_v: 3932.5\n",
      "lif layer 1 self.abs_max_v: 4017.0\n",
      "smallest_now_T updated: 56\n",
      "smallest_now_T updated: 50\n",
      "fc layer 2 self.abs_max_out: 966.0\n",
      "fc layer 2 self.abs_max_out: 1027.0\n",
      "fc layer 3 self.abs_max_out: 330.0\n",
      "lif layer 1 self.abs_max_v: 4043.5\n",
      "lif layer 1 self.abs_max_v: 4199.5\n",
      "lif layer 1 self.abs_max_v: 4456.0\n",
      "lif layer 1 self.abs_max_v: 4511.0\n",
      "lif layer 2 self.abs_max_v: 1757.5\n",
      "lif layer 2 self.abs_max_v: 1775.0\n",
      "fc layer 3 self.abs_max_out: 337.0\n",
      "smallest_now_T_val updated: 129\n",
      "smallest_now_T_val updated: 106\n",
      "smallest_now_T_val updated: 104\n",
      "smallest_now_T_val updated: 102\n",
      "smallest_now_T_val updated: 85\n",
      "smallest_now_T_val updated: 50\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.863430/  2.141083, val:  21.67%, val_best:  21.67%, tr:  98.77%, tr_best:  98.77%, epoch time: 85.35 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6392%\n",
      "layer   2  Sparsity: 53.2050%\n",
      "layer   3  Sparsity: 44.6443%\n",
      "total_backward_count 9790 real_backward_count 1097  11.205%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 352.0\n",
      "fc layer 3 self.abs_max_out: 358.0\n",
      "lif layer 1 self.abs_max_v: 4541.5\n",
      "lif layer 1 self.abs_max_v: 4600.5\n",
      "lif layer 1 self.abs_max_v: 4626.5\n",
      "fc layer 3 self.abs_max_out: 360.0\n",
      "fc layer 2 self.abs_max_out: 1040.0\n",
      "fc layer 2 self.abs_max_out: 1062.0\n",
      "lif layer 2 self.abs_max_v: 1784.5\n",
      "lif layer 1 self.abs_max_v: 4681.0\n",
      "lif layer 1 self.abs_max_v: 4734.0\n",
      "lif layer 1 self.abs_max_v: 4896.5\n",
      "lif layer 2 self.abs_max_v: 1806.5\n",
      "lif layer 2 self.abs_max_v: 1823.5\n",
      "lif layer 2 self.abs_max_v: 1833.0\n",
      "fc layer 1 self.abs_max_out: 2843.0\n",
      "lif layer 1 self.abs_max_v: 4910.0\n",
      "lif layer 1 self.abs_max_v: 5060.0\n",
      "fc layer 3 self.abs_max_out: 391.0\n",
      "fc layer 1 self.abs_max_out: 2949.0\n",
      "lif layer 1 self.abs_max_v: 5400.5\n",
      "fc layer 2 self.abs_max_out: 1110.0\n",
      "lif layer 2 self.abs_max_v: 1842.0\n",
      "lif layer 2 self.abs_max_v: 1861.0\n",
      "lif layer 2 self.abs_max_v: 1870.5\n",
      "lif layer 2 self.abs_max_v: 1875.5\n",
      "fc layer 2 self.abs_max_out: 1203.0\n",
      "lif layer 2 self.abs_max_v: 2014.5\n",
      "lif layer 2 self.abs_max_v: 2098.5\n",
      "fc layer 2 self.abs_max_out: 1215.0\n",
      "lif layer 2 self.abs_max_v: 2264.5\n",
      "lif layer 2 self.abs_max_v: 2282.5\n",
      "lif layer 2 self.abs_max_v: 2332.5\n",
      "lif layer 2 self.abs_max_v: 2371.5\n",
      "fc layer 1 self.abs_max_out: 2989.0\n",
      "fc layer 1 self.abs_max_out: 3018.0\n",
      "lif layer 1 self.abs_max_v: 5440.5\n",
      "fc layer 1 self.abs_max_out: 3103.0\n",
      "fc layer 1 self.abs_max_out: 3198.0\n",
      "lif layer 1 self.abs_max_v: 5646.0\n",
      "lif layer 1 self.abs_max_v: 5812.0\n",
      "lif layer 1 self.abs_max_v: 5885.0\n",
      "lif layer 1 self.abs_max_v: 6060.5\n",
      "lif layer 1 self.abs_max_v: 6125.5\n",
      "fc layer 1 self.abs_max_out: 3238.0\n",
      "lif layer 1 self.abs_max_v: 6237.5\n",
      "fc layer 1 self.abs_max_out: 3399.0\n",
      "lif layer 1 self.abs_max_v: 6518.0\n",
      "fc layer 3 self.abs_max_out: 399.0\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.704232/  2.017745, val:  34.17%, val_best:  34.17%, tr:  99.08%, tr_best:  99.08%, epoch time: 84.65 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6704%\n",
      "layer   2  Sparsity: 64.5755%\n",
      "layer   3  Sparsity: 51.1319%\n",
      "total_backward_count 19580 real_backward_count 2215  11.313%\n",
      "fc layer 3 self.abs_max_out: 450.0\n",
      "fc layer 3 self.abs_max_out: 465.0\n",
      "fc layer 2 self.abs_max_out: 1238.0\n",
      "fc layer 1 self.abs_max_out: 3401.0\n",
      "fc layer 1 self.abs_max_out: 3447.0\n",
      "fc layer 1 self.abs_max_out: 3762.0\n",
      "lif layer 1 self.abs_max_v: 6716.0\n",
      "fc layer 2 self.abs_max_out: 1286.0\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.625274/  1.948864, val:  42.08%, val_best:  42.08%, tr:  99.59%, tr_best:  99.59%, epoch time: 84.98 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6202%\n",
      "layer   2  Sparsity: 66.7429%\n",
      "layer   3  Sparsity: 52.7909%\n",
      "total_backward_count 29370 real_backward_count 3325  11.321%\n",
      "fc layer 3 self.abs_max_out: 486.0\n",
      "fc layer 3 self.abs_max_out: 500.0\n",
      "fc layer 3 self.abs_max_out: 518.0\n",
      "fc layer 3 self.abs_max_out: 532.0\n",
      "lif layer 2 self.abs_max_v: 2443.5\n",
      "lif layer 2 self.abs_max_v: 2470.5\n",
      "fc layer 1 self.abs_max_out: 3815.0\n",
      "lif layer 1 self.abs_max_v: 6716.5\n",
      "lif layer 1 self.abs_max_v: 6742.5\n",
      "fc layer 2 self.abs_max_out: 1323.0\n",
      "fc layer 2 self.abs_max_out: 1361.0\n",
      "lif layer 1 self.abs_max_v: 6815.0\n",
      "lif layer 1 self.abs_max_v: 7087.5\n",
      "fc layer 2 self.abs_max_out: 1423.0\n",
      "lif layer 1 self.abs_max_v: 7100.0\n",
      "fc layer 1 self.abs_max_out: 3926.0\n",
      "lif layer 1 self.abs_max_v: 7350.5\n",
      "fc layer 1 self.abs_max_out: 3971.0\n",
      "fc layer 1 self.abs_max_out: 4286.0\n",
      "lif layer 1 self.abs_max_v: 7678.5\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.584267/  1.917979, val:  34.17%, val_best:  42.08%, tr:  99.49%, tr_best:  99.59%, epoch time: 85.47 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6495%\n",
      "layer   2  Sparsity: 70.0176%\n",
      "layer   3  Sparsity: 54.1676%\n",
      "total_backward_count 39160 real_backward_count 4433  11.320%\n",
      "fc layer 3 self.abs_max_out: 535.0\n",
      "fc layer 3 self.abs_max_out: 539.0\n",
      "lif layer 1 self.abs_max_v: 7872.0\n",
      "lif layer 1 self.abs_max_v: 7915.0\n",
      "lif layer 1 self.abs_max_v: 7939.5\n",
      "lif layer 1 self.abs_max_v: 8126.0\n",
      "fc layer 1 self.abs_max_out: 4324.0\n",
      "fc layer 1 self.abs_max_out: 4475.0\n",
      "lif layer 1 self.abs_max_v: 8337.0\n",
      "lif layer 1 self.abs_max_v: 8398.0\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.572279/  1.918231, val:  43.33%, val_best:  43.33%, tr:  99.69%, tr_best:  99.69%, epoch time: 83.83 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 72.6628%\n",
      "layer   2  Sparsity: 72.0905%\n",
      "layer   3  Sparsity: 56.5894%\n",
      "total_backward_count 48950 real_backward_count 5511  11.258%\n",
      "lif layer 2 self.abs_max_v: 2520.0\n",
      "lif layer 2 self.abs_max_v: 2528.0\n",
      "lif layer 2 self.abs_max_v: 2589.0\n",
      "lif layer 2 self.abs_max_v: 2592.0\n",
      "lif layer 2 self.abs_max_v: 2645.5\n",
      "lif layer 2 self.abs_max_v: 2676.0\n",
      "fc layer 2 self.abs_max_out: 1464.0\n",
      "lif layer 2 self.abs_max_v: 2701.0\n",
      "fc layer 2 self.abs_max_out: 1493.0\n",
      "lif layer 2 self.abs_max_v: 2843.5\n",
      "fc layer 2 self.abs_max_out: 1494.0\n",
      "lif layer 2 self.abs_max_v: 2916.0\n",
      "fc layer 2 self.abs_max_out: 1730.0\n",
      "lif layer 2 self.abs_max_v: 3081.5\n",
      "lif layer 2 self.abs_max_v: 3195.0\n",
      "lif layer 2 self.abs_max_v: 3309.5\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.595141/  1.913316, val:  41.25%, val_best:  43.33%, tr:  99.59%, tr_best:  99.69%, epoch time: 83.37 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 72.6482%\n",
      "layer   2  Sparsity: 72.0737%\n",
      "layer   3  Sparsity: 58.2107%\n",
      "total_backward_count 58740 real_backward_count 6597  11.231%\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.578063/  1.876446, val:  40.42%, val_best:  43.33%, tr:  99.28%, tr_best:  99.69%, epoch time: 85.35 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6748%\n",
      "layer   2  Sparsity: 71.4563%\n",
      "layer   3  Sparsity: 58.2495%\n",
      "total_backward_count 68530 real_backward_count 7621  11.121%\n",
      "fc layer 3 self.abs_max_out: 550.0\n",
      "fc layer 3 self.abs_max_out: 578.0\n",
      "fc layer 3 self.abs_max_out: 592.0\n",
      "fc layer 3 self.abs_max_out: 595.0\n",
      "fc layer 3 self.abs_max_out: 612.0\n",
      "fc layer 2 self.abs_max_out: 1806.0\n",
      "lif layer 2 self.abs_max_v: 3381.0\n",
      "lif layer 2 self.abs_max_v: 3437.5\n",
      "fc layer 1 self.abs_max_out: 4498.0\n",
      "fc layer 1 self.abs_max_out: 4590.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.549038/  1.877221, val:  48.75%, val_best:  48.75%, tr:  99.69%, tr_best:  99.69%, epoch time: 85.03 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6568%\n",
      "layer   2  Sparsity: 73.5304%\n",
      "layer   3  Sparsity: 57.6880%\n",
      "total_backward_count 78320 real_backward_count 8656  11.052%\n",
      "fc layer 1 self.abs_max_out: 4721.0\n",
      "lif layer 1 self.abs_max_v: 8462.0\n",
      "lif layer 1 self.abs_max_v: 8761.0\n",
      "fc layer 2 self.abs_max_out: 1829.0\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.522818/  1.834964, val:  44.17%, val_best:  48.75%, tr:  99.69%, tr_best:  99.69%, epoch time: 84.90 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6736%\n",
      "layer   2  Sparsity: 75.4840%\n",
      "layer   3  Sparsity: 58.4976%\n",
      "total_backward_count 88110 real_backward_count 9756  11.073%\n",
      "fc layer 1 self.abs_max_out: 4856.0\n",
      "lif layer 1 self.abs_max_v: 8813.0\n",
      "fc layer 1 self.abs_max_out: 4886.0\n",
      "fc layer 3 self.abs_max_out: 620.0\n",
      "fc layer 3 self.abs_max_out: 621.0\n",
      "fc layer 3 self.abs_max_out: 627.0\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.472234/  1.828532, val:  41.25%, val_best:  48.75%, tr:  99.69%, tr_best:  99.69%, epoch time: 85.02 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.5898%\n",
      "layer   2  Sparsity: 75.1813%\n",
      "layer   3  Sparsity: 58.0661%\n",
      "total_backward_count 97900 real_backward_count 10785  11.016%\n",
      "fc layer 1 self.abs_max_out: 4888.0\n",
      "lif layer 1 self.abs_max_v: 8869.0\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.459566/  1.814683, val:  37.50%, val_best:  48.75%, tr:  99.59%, tr_best:  99.69%, epoch time: 84.83 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.5438%\n",
      "layer   2  Sparsity: 75.5865%\n",
      "layer   3  Sparsity: 57.4605%\n",
      "total_backward_count 107690 real_backward_count 11793  10.951%\n",
      "fc layer 1 self.abs_max_out: 5134.0\n",
      "lif layer 1 self.abs_max_v: 9364.5\n",
      "fc layer 2 self.abs_max_out: 1847.0\n",
      "fc layer 2 self.abs_max_out: 1873.0\n",
      "lif layer 2 self.abs_max_v: 3487.0\n",
      "lif layer 2 self.abs_max_v: 3575.5\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.438786/  1.774664, val:  45.83%, val_best:  48.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 84.58 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.7522%\n",
      "layer   2  Sparsity: 74.1856%\n",
      "layer   3  Sparsity: 57.0497%\n",
      "total_backward_count 117480 real_backward_count 12763  10.864%\n",
      "fc layer 3 self.abs_max_out: 647.0\n",
      "fc layer 3 self.abs_max_out: 661.0\n",
      "lif layer 2 self.abs_max_v: 3594.5\n",
      "fc layer 1 self.abs_max_out: 5291.0\n",
      "lif layer 1 self.abs_max_v: 9707.5\n",
      "lif layer 1 self.abs_max_v: 9719.0\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.397220/  1.808135, val:  36.67%, val_best:  48.75%, tr:  99.28%, tr_best:  99.80%, epoch time: 84.96 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6948%\n",
      "layer   2  Sparsity: 73.8303%\n",
      "layer   3  Sparsity: 57.4858%\n",
      "total_backward_count 127270 real_backward_count 13768  10.818%\n",
      "fc layer 2 self.abs_max_out: 1885.0\n",
      "fc layer 1 self.abs_max_out: 5578.0\n",
      "lif layer 1 self.abs_max_v: 10063.0\n",
      "lif layer 1 self.abs_max_v: 10312.0\n",
      "fc layer 3 self.abs_max_out: 680.0\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.436531/  1.839019, val:  36.67%, val_best:  48.75%, tr:  99.59%, tr_best:  99.80%, epoch time: 85.54 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 72.6502%\n",
      "layer   2  Sparsity: 74.1174%\n",
      "layer   3  Sparsity: 57.5414%\n",
      "total_backward_count 137060 real_backward_count 14759  10.768%\n",
      "fc layer 1 self.abs_max_out: 6198.0\n",
      "lif layer 1 self.abs_max_v: 10648.5\n",
      "lif layer 1 self.abs_max_v: 11215.5\n",
      "lif layer 1 self.abs_max_v: 11477.0\n",
      "lif layer 1 self.abs_max_v: 11543.5\n",
      "fc layer 1 self.abs_max_out: 6300.0\n",
      "lif layer 1 self.abs_max_v: 12072.0\n",
      "lif layer 1 self.abs_max_v: 12183.0\n",
      "fc layer 1 self.abs_max_out: 6775.0\n",
      "lif layer 1 self.abs_max_v: 12866.5\n",
      "lif layer 1 self.abs_max_v: 13139.5\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.407664/  1.749323, val:  42.08%, val_best:  48.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 85.32 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.5848%\n",
      "layer   2  Sparsity: 74.2881%\n",
      "layer   3  Sparsity: 58.5986%\n",
      "total_backward_count 146850 real_backward_count 15744  10.721%\n",
      "fc layer 1 self.abs_max_out: 7444.0\n",
      "lif layer 1 self.abs_max_v: 13603.0\n",
      "lif layer 1 self.abs_max_v: 13895.5\n",
      "fc layer 2 self.abs_max_out: 1927.0\n",
      "lif layer 2 self.abs_max_v: 3620.0\n",
      "lif layer 2 self.abs_max_v: 3672.0\n",
      "lif layer 2 self.abs_max_v: 3691.0\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.382313/  1.700484, val:  51.25%, val_best:  51.25%, tr:  99.80%, tr_best:  99.80%, epoch time: 85.25 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6026%\n",
      "layer   2  Sparsity: 73.6216%\n",
      "layer   3  Sparsity: 57.6027%\n",
      "total_backward_count 156640 real_backward_count 16762  10.701%\n",
      "fc layer 1 self.abs_max_out: 7815.0\n",
      "lif layer 1 self.abs_max_v: 14329.5\n",
      "lif layer 1 self.abs_max_v: 14633.0\n",
      "lif layer 1 self.abs_max_v: 14705.5\n",
      "lif layer 1 self.abs_max_v: 15048.0\n",
      "fc layer 2 self.abs_max_out: 1963.0\n",
      "fc layer 2 self.abs_max_out: 2002.0\n",
      "lif layer 2 self.abs_max_v: 3705.0\n",
      "fc layer 2 self.abs_max_out: 2030.0\n",
      "lif layer 2 self.abs_max_v: 3880.0\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.391814/  1.684728, val:  50.00%, val_best:  51.25%, tr:  99.18%, tr_best:  99.80%, epoch time: 85.61 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 72.5872%\n",
      "layer   2  Sparsity: 73.3348%\n",
      "layer   3  Sparsity: 58.3759%\n",
      "total_backward_count 166430 real_backward_count 17772  10.678%\n",
      "fc layer 1 self.abs_max_out: 8430.0\n",
      "lif layer 1 self.abs_max_v: 15539.5\n",
      "lif layer 1 self.abs_max_v: 15850.5\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.394434/  1.697133, val:  43.33%, val_best:  51.25%, tr:  99.49%, tr_best:  99.80%, epoch time: 85.45 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6378%\n",
      "layer   2  Sparsity: 75.5272%\n",
      "layer   3  Sparsity: 59.1934%\n",
      "total_backward_count 176220 real_backward_count 18728  10.628%\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.391101/  1.793085, val:  40.42%, val_best:  51.25%, tr:  99.80%, tr_best:  99.80%, epoch time: 85.70 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 72.5897%\n",
      "layer   2  Sparsity: 75.0831%\n",
      "layer   3  Sparsity: 59.3360%\n",
      "total_backward_count 186010 real_backward_count 19787  10.638%\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.384076/  1.719247, val:  38.75%, val_best:  51.25%, tr:  99.80%, tr_best:  99.80%, epoch time: 85.19 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6375%\n",
      "layer   2  Sparsity: 75.2826%\n",
      "layer   3  Sparsity: 59.0180%\n",
      "total_backward_count 195800 real_backward_count 20726  10.585%\n",
      "fc layer 3 self.abs_max_out: 737.0\n",
      "fc layer 3 self.abs_max_out: 743.0\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.365502/  1.696067, val:  46.67%, val_best:  51.25%, tr:  99.59%, tr_best:  99.80%, epoch time: 85.15 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.5902%\n",
      "layer   2  Sparsity: 75.0508%\n",
      "layer   3  Sparsity: 59.2337%\n",
      "total_backward_count 205590 real_backward_count 21703  10.556%\n",
      "fc layer 3 self.abs_max_out: 767.0\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.342080/  1.680479, val:  49.17%, val_best:  51.25%, tr:  99.59%, tr_best:  99.80%, epoch time: 84.25 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 72.6127%\n",
      "layer   2  Sparsity: 74.4435%\n",
      "layer   3  Sparsity: 59.5629%\n",
      "total_backward_count 215380 real_backward_count 22734  10.555%\n",
      "lif layer 1 self.abs_max_v: 15983.0\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.382075/  1.656048, val:  52.92%, val_best:  52.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 83.82 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 72.6035%\n",
      "layer   2  Sparsity: 73.9946%\n",
      "layer   3  Sparsity: 58.6573%\n",
      "total_backward_count 225170 real_backward_count 23720  10.534%\n",
      "fc layer 1 self.abs_max_out: 8547.0\n",
      "lif layer 1 self.abs_max_v: 16084.5\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.354526/  1.706757, val:  51.67%, val_best:  52.92%, tr:  99.39%, tr_best:  99.90%, epoch time: 84.54 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6153%\n",
      "layer   2  Sparsity: 73.5158%\n",
      "layer   3  Sparsity: 58.3751%\n",
      "total_backward_count 234960 real_backward_count 24718  10.520%\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.322658/  1.615308, val:  58.33%, val_best:  58.33%, tr:  99.90%, tr_best:  99.90%, epoch time: 84.71 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6482%\n",
      "layer   2  Sparsity: 72.7012%\n",
      "layer   3  Sparsity: 57.5576%\n",
      "total_backward_count 244750 real_backward_count 25682  10.493%\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.340269/  1.599720, val:  62.08%, val_best:  62.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 84.84 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6437%\n",
      "layer   2  Sparsity: 71.9853%\n",
      "layer   3  Sparsity: 58.3126%\n",
      "total_backward_count 254540 real_backward_count 26613  10.455%\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.331130/  1.628959, val:  50.42%, val_best:  62.08%, tr:  99.18%, tr_best:  99.90%, epoch time: 84.94 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6676%\n",
      "layer   2  Sparsity: 72.1363%\n",
      "layer   3  Sparsity: 59.2701%\n",
      "total_backward_count 264330 real_backward_count 27561  10.427%\n",
      "fc layer 2 self.abs_max_out: 2040.0\n",
      "lif layer 2 self.abs_max_v: 3901.0\n",
      "fc layer 2 self.abs_max_out: 2102.0\n",
      "fc layer 2 self.abs_max_out: 2172.0\n",
      "lif layer 2 self.abs_max_v: 4061.0\n",
      "lif layer 2 self.abs_max_v: 4180.5\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.344938/  1.663645, val:  51.25%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.17 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6173%\n",
      "layer   2  Sparsity: 70.6934%\n",
      "layer   3  Sparsity: 59.6861%\n",
      "total_backward_count 274120 real_backward_count 28550  10.415%\n",
      "fc layer 2 self.abs_max_out: 2237.0\n",
      "fc layer 2 self.abs_max_out: 2249.0\n",
      "lif layer 2 self.abs_max_v: 4314.0\n",
      "fc layer 2 self.abs_max_out: 2345.0\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.352006/  1.739866, val:  41.67%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.30 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.5792%\n",
      "layer   2  Sparsity: 71.1673%\n",
      "layer   3  Sparsity: 60.5680%\n",
      "total_backward_count 283910 real_backward_count 29496  10.389%\n",
      "lif layer 2 self.abs_max_v: 4355.5\n",
      "lif layer 2 self.abs_max_v: 4498.5\n",
      "lif layer 2 self.abs_max_v: 4533.5\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.351396/  1.669310, val:  47.92%, val_best:  62.08%, tr:  99.39%, tr_best: 100.00%, epoch time: 85.24 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6173%\n",
      "layer   2  Sparsity: 71.5005%\n",
      "layer   3  Sparsity: 61.3963%\n",
      "total_backward_count 293700 real_backward_count 30432  10.362%\n",
      "fc layer 2 self.abs_max_out: 2408.0\n",
      "fc layer 2 self.abs_max_out: 2552.0\n",
      "lif layer 2 self.abs_max_v: 4694.5\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.330082/  1.603372, val:  61.25%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 84.85 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6229%\n",
      "layer   2  Sparsity: 70.0686%\n",
      "layer   3  Sparsity: 60.3303%\n",
      "total_backward_count 303490 real_backward_count 31416  10.352%\n",
      "lif layer 2 self.abs_max_v: 4767.5\n",
      "fc layer 3 self.abs_max_out: 783.0\n",
      "fc layer 3 self.abs_max_out: 790.0\n",
      "lif layer 2 self.abs_max_v: 4801.0\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.370118/  1.713615, val:  47.50%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.93 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.7000%\n",
      "layer   2  Sparsity: 69.2995%\n",
      "layer   3  Sparsity: 61.8589%\n",
      "total_backward_count 313280 real_backward_count 32309  10.313%\n",
      "lif layer 2 self.abs_max_v: 4848.0\n",
      "lif layer 2 self.abs_max_v: 4910.0\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.435435/  1.769008, val:  54.17%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 84.93 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6737%\n",
      "layer   2  Sparsity: 69.3980%\n",
      "layer   3  Sparsity: 63.1482%\n",
      "total_backward_count 323070 real_backward_count 33205  10.278%\n",
      "fc layer 2 self.abs_max_out: 2553.0\n",
      "fc layer 2 self.abs_max_out: 2554.0\n",
      "fc layer 2 self.abs_max_out: 2617.0\n",
      "lif layer 2 self.abs_max_v: 4977.5\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.416785/  1.701713, val:  51.67%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 85.34 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6835%\n",
      "layer   2  Sparsity: 69.6645%\n",
      "layer   3  Sparsity: 63.0739%\n",
      "total_backward_count 332860 real_backward_count 34153  10.260%\n",
      "fc layer 2 self.abs_max_out: 2737.0\n",
      "lif layer 2 self.abs_max_v: 5216.0\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.396279/  1.658043, val:  51.25%, val_best:  62.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 85.72 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 72.6058%\n",
      "layer   2  Sparsity: 69.2130%\n",
      "layer   3  Sparsity: 64.0449%\n",
      "total_backward_count 342650 real_backward_count 35063  10.233%\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.369042/  1.668641, val:  54.58%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.14 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6404%\n",
      "layer   2  Sparsity: 70.2857%\n",
      "layer   3  Sparsity: 63.1270%\n",
      "total_backward_count 352440 real_backward_count 35969  10.206%\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.365275/  1.711493, val:  46.67%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.19 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.7000%\n",
      "layer   2  Sparsity: 69.4730%\n",
      "layer   3  Sparsity: 61.8725%\n",
      "total_backward_count 362230 real_backward_count 36850  10.173%\n",
      "fc layer 2 self.abs_max_out: 2761.0\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.349111/  1.654248, val:  52.50%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.69 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.5941%\n",
      "layer   2  Sparsity: 69.3189%\n",
      "layer   3  Sparsity: 62.2087%\n",
      "total_backward_count 372020 real_backward_count 37744  10.146%\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.354430/  1.700549, val:  49.17%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.35 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6804%\n",
      "layer   2  Sparsity: 69.5826%\n",
      "layer   3  Sparsity: 62.2282%\n",
      "total_backward_count 381810 real_backward_count 38676  10.130%\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.355900/  1.714933, val:  53.75%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.70 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6410%\n",
      "layer   2  Sparsity: 69.8946%\n",
      "layer   3  Sparsity: 63.2483%\n",
      "total_backward_count 391600 real_backward_count 39576  10.106%\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.369911/  1.652305, val:  57.50%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 85.02 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6463%\n",
      "layer   2  Sparsity: 71.0732%\n",
      "layer   3  Sparsity: 64.7198%\n",
      "total_backward_count 401390 real_backward_count 40491  10.088%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.381765/  1.642019, val:  56.67%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.80 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 72.6721%\n",
      "layer   2  Sparsity: 71.3251%\n",
      "layer   3  Sparsity: 64.7523%\n",
      "total_backward_count 411180 real_backward_count 41349  10.056%\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.371046/  1.648852, val:  57.92%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.58 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 72.6851%\n",
      "layer   2  Sparsity: 71.0685%\n",
      "layer   3  Sparsity: 64.5509%\n",
      "total_backward_count 420970 real_backward_count 42191  10.022%\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.375721/  1.634812, val:  58.33%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.11 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.5553%\n",
      "layer   2  Sparsity: 71.7317%\n",
      "layer   3  Sparsity: 64.2839%\n",
      "total_backward_count 430760 real_backward_count 43038   9.991%\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.382056/  1.640543, val:  57.08%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.01 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.7334%\n",
      "layer   2  Sparsity: 71.3673%\n",
      "layer   3  Sparsity: 64.4593%\n",
      "total_backward_count 440550 real_backward_count 43926   9.971%\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.365049/  1.677188, val:  57.08%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 85.65 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 72.6750%\n",
      "layer   2  Sparsity: 71.3252%\n",
      "layer   3  Sparsity: 63.5688%\n",
      "total_backward_count 450340 real_backward_count 44809   9.950%\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.377587/  1.658758, val:  58.33%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.46 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6192%\n",
      "layer   2  Sparsity: 71.9467%\n",
      "layer   3  Sparsity: 64.3129%\n",
      "total_backward_count 460130 real_backward_count 45746   9.942%\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.376376/  1.672232, val:  58.33%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.77 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6133%\n",
      "layer   2  Sparsity: 72.1916%\n",
      "layer   3  Sparsity: 65.4927%\n",
      "total_backward_count 469920 real_backward_count 46621   9.921%\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.349644/  1.659327, val:  50.83%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.38 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.7066%\n",
      "layer   2  Sparsity: 72.5242%\n",
      "layer   3  Sparsity: 65.5934%\n",
      "total_backward_count 479710 real_backward_count 47476   9.897%\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.365773/  1.673571, val:  54.17%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.57 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6800%\n",
      "layer   2  Sparsity: 72.8130%\n",
      "layer   3  Sparsity: 65.2271%\n",
      "total_backward_count 489500 real_backward_count 48314   9.870%\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.373158/  1.650424, val:  52.50%, val_best:  62.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 84.99 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6441%\n",
      "layer   2  Sparsity: 71.9160%\n",
      "layer   3  Sparsity: 64.8214%\n",
      "total_backward_count 499290 real_backward_count 49147   9.843%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.302192/  1.576765, val:  57.50%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.34 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6775%\n",
      "layer   2  Sparsity: 71.9735%\n",
      "layer   3  Sparsity: 63.7305%\n",
      "total_backward_count 509080 real_backward_count 49976   9.817%\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.319680/  1.626744, val:  52.50%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 85.60 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 72.6001%\n",
      "layer   2  Sparsity: 72.1529%\n",
      "layer   3  Sparsity: 64.5500%\n",
      "total_backward_count 518870 real_backward_count 50843   9.799%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.299429/  1.631138, val:  43.33%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 85.04 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6456%\n",
      "layer   2  Sparsity: 72.2101%\n",
      "layer   3  Sparsity: 63.7483%\n",
      "total_backward_count 528660 real_backward_count 51773   9.793%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.323733/  1.651705, val:  56.67%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.04 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6191%\n",
      "layer   2  Sparsity: 73.2913%\n",
      "layer   3  Sparsity: 63.4414%\n",
      "total_backward_count 538450 real_backward_count 52689   9.785%\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.327014/  1.629336, val:  55.42%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.04 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6540%\n",
      "layer   2  Sparsity: 73.4005%\n",
      "layer   3  Sparsity: 63.9899%\n",
      "total_backward_count 548240 real_backward_count 53546   9.767%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.306989/  1.637571, val:  42.08%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.31 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6322%\n",
      "layer   2  Sparsity: 73.2904%\n",
      "layer   3  Sparsity: 63.8013%\n",
      "total_backward_count 558030 real_backward_count 54441   9.756%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.302112/  1.713152, val:  36.25%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 84.63 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6283%\n",
      "layer   2  Sparsity: 73.5422%\n",
      "layer   3  Sparsity: 63.7974%\n",
      "total_backward_count 567820 real_backward_count 55276   9.735%\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.306378/  1.606056, val:  51.25%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.24 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6282%\n",
      "layer   2  Sparsity: 73.6782%\n",
      "layer   3  Sparsity: 64.5865%\n",
      "total_backward_count 577610 real_backward_count 56147   9.721%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.299540/  1.640500, val:  51.67%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.69 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6056%\n",
      "layer   2  Sparsity: 73.9067%\n",
      "layer   3  Sparsity: 64.5050%\n",
      "total_backward_count 587400 real_backward_count 56999   9.704%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.300656/  1.644922, val:  54.58%, val_best:  62.08%, tr:  99.28%, tr_best: 100.00%, epoch time: 85.10 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.5789%\n",
      "layer   2  Sparsity: 74.4522%\n",
      "layer   3  Sparsity: 64.4485%\n",
      "total_backward_count 597190 real_backward_count 57839   9.685%\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.308074/  1.595751, val:  53.75%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.02 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6606%\n",
      "layer   2  Sparsity: 74.5418%\n",
      "layer   3  Sparsity: 65.5824%\n",
      "total_backward_count 606980 real_backward_count 58696   9.670%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.312929/  1.626688, val:  52.08%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.08 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6235%\n",
      "layer   2  Sparsity: 74.1668%\n",
      "layer   3  Sparsity: 65.8039%\n",
      "total_backward_count 616770 real_backward_count 59508   9.648%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.298389/  1.576412, val:  57.08%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.68 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6469%\n",
      "layer   2  Sparsity: 74.1778%\n",
      "layer   3  Sparsity: 65.6306%\n",
      "total_backward_count 626560 real_backward_count 60306   9.625%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.262284/  1.602701, val:  53.33%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.84 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6688%\n",
      "layer   2  Sparsity: 74.9337%\n",
      "layer   3  Sparsity: 64.6300%\n",
      "total_backward_count 636350 real_backward_count 61115   9.604%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.254388/  1.564784, val:  54.58%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.88 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6492%\n",
      "layer   2  Sparsity: 75.2505%\n",
      "layer   3  Sparsity: 63.9980%\n",
      "total_backward_count 646140 real_backward_count 61942   9.586%\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.220607/  1.573992, val:  49.17%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.61 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.5982%\n",
      "layer   2  Sparsity: 74.3794%\n",
      "layer   3  Sparsity: 63.0651%\n",
      "total_backward_count 655930 real_backward_count 62766   9.569%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.194190/  1.511023, val:  60.00%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 85.33 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6488%\n",
      "layer   2  Sparsity: 74.1962%\n",
      "layer   3  Sparsity: 62.5468%\n",
      "total_backward_count 665720 real_backward_count 63557   9.547%\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.198023/  1.530574, val:  55.83%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.53 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 72.6337%\n",
      "layer   2  Sparsity: 74.0863%\n",
      "layer   3  Sparsity: 63.2570%\n",
      "total_backward_count 675510 real_backward_count 64373   9.530%\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.211632/  1.531706, val:  58.33%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.33 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6086%\n",
      "layer   2  Sparsity: 74.2514%\n",
      "layer   3  Sparsity: 62.9273%\n",
      "total_backward_count 685300 real_backward_count 65158   9.508%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.216176/  1.547287, val:  60.42%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.89 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6627%\n",
      "layer   2  Sparsity: 73.7932%\n",
      "layer   3  Sparsity: 61.5475%\n",
      "total_backward_count 695090 real_backward_count 66023   9.498%\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.223094/  1.572420, val:  55.42%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.65 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6397%\n",
      "layer   2  Sparsity: 74.0493%\n",
      "layer   3  Sparsity: 61.2934%\n",
      "total_backward_count 704880 real_backward_count 66798   9.477%\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.217478/  1.521912, val:  62.92%, val_best:  62.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 84.16 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 72.5908%\n",
      "layer   2  Sparsity: 73.4593%\n",
      "layer   3  Sparsity: 61.7671%\n",
      "total_backward_count 714670 real_backward_count 67615   9.461%\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.180153/  1.596603, val:  41.67%, val_best:  62.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.33 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6696%\n",
      "layer   2  Sparsity: 73.6720%\n",
      "layer   3  Sparsity: 61.6853%\n",
      "total_backward_count 724460 real_backward_count 68431   9.446%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.188543/  1.581576, val:  48.75%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.51 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6020%\n",
      "layer   2  Sparsity: 73.3696%\n",
      "layer   3  Sparsity: 61.2007%\n",
      "total_backward_count 734250 real_backward_count 69286   9.436%\n",
      "fc layer 3 self.abs_max_out: 792.0\n",
      "fc layer 3 self.abs_max_out: 809.0\n",
      "fc layer 3 self.abs_max_out: 813.0\n",
      "fc layer 3 self.abs_max_out: 814.0\n",
      "fc layer 3 self.abs_max_out: 852.0\n",
      "fc layer 3 self.abs_max_out: 889.0\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.194303/  1.563094, val:  47.92%, val_best:  62.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.31 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6030%\n",
      "layer   2  Sparsity: 73.2133%\n",
      "layer   3  Sparsity: 61.8255%\n",
      "total_backward_count 744040 real_backward_count 70088   9.420%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.183366/  1.517410, val:  55.00%, val_best:  62.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.64 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 72.6328%\n",
      "layer   2  Sparsity: 73.8441%\n",
      "layer   3  Sparsity: 61.6166%\n",
      "total_backward_count 753830 real_backward_count 70888   9.404%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.166978/  1.473276, val:  60.00%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.24 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6265%\n",
      "layer   2  Sparsity: 73.7530%\n",
      "layer   3  Sparsity: 61.7978%\n",
      "total_backward_count 763620 real_backward_count 71626   9.380%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.166845/  1.549119, val:  57.08%, val_best:  62.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.53 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 72.6027%\n",
      "layer   2  Sparsity: 73.3748%\n",
      "layer   3  Sparsity: 62.4204%\n",
      "total_backward_count 773410 real_backward_count 72397   9.361%\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.151901/  1.529914, val:  58.33%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.28 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6250%\n",
      "layer   2  Sparsity: 73.5900%\n",
      "layer   3  Sparsity: 62.4355%\n",
      "total_backward_count 783200 real_backward_count 73177   9.343%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.156792/  1.448869, val:  60.83%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.85 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6154%\n",
      "layer   2  Sparsity: 73.4808%\n",
      "layer   3  Sparsity: 61.9428%\n",
      "total_backward_count 792990 real_backward_count 73968   9.328%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.148784/  1.489235, val:  57.92%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.27 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 72.6683%\n",
      "layer   2  Sparsity: 72.9056%\n",
      "layer   3  Sparsity: 61.8328%\n",
      "total_backward_count 802780 real_backward_count 74721   9.308%\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  1.183203/  1.537084, val:  50.42%, val_best:  62.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 85.05 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6492%\n",
      "layer   2  Sparsity: 72.9820%\n",
      "layer   3  Sparsity: 61.9628%\n",
      "total_backward_count 812570 real_backward_count 75513   9.293%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  1.167082/  1.448460, val:  58.33%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.12 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6359%\n",
      "layer   2  Sparsity: 72.8339%\n",
      "layer   3  Sparsity: 62.6047%\n",
      "total_backward_count 822360 real_backward_count 76332   9.282%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  1.151780/  1.497391, val:  57.92%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.84 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.7012%\n",
      "layer   2  Sparsity: 72.6673%\n",
      "layer   3  Sparsity: 61.8445%\n",
      "total_backward_count 832150 real_backward_count 77129   9.269%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  1.179676/  1.461651, val:  59.17%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.18 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.5937%\n",
      "layer   2  Sparsity: 72.3389%\n",
      "layer   3  Sparsity: 61.6050%\n",
      "total_backward_count 841940 real_backward_count 77858   9.247%\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  1.147017/  1.526806, val:  58.33%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.07 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6482%\n",
      "layer   2  Sparsity: 72.3388%\n",
      "layer   3  Sparsity: 61.8622%\n",
      "total_backward_count 851730 real_backward_count 78645   9.234%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  1.159276/  1.487894, val:  53.33%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.83 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 72.6212%\n",
      "layer   2  Sparsity: 72.4039%\n",
      "layer   3  Sparsity: 61.9669%\n",
      "total_backward_count 861520 real_backward_count 79412   9.218%\n",
      "fc layer 2 self.abs_max_out: 2762.0\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  1.141752/  1.479436, val:  63.33%, val_best:  63.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.81 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 72.6407%\n",
      "layer   2  Sparsity: 72.3362%\n",
      "layer   3  Sparsity: 62.0948%\n",
      "total_backward_count 871310 real_backward_count 80188   9.203%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  1.158380/  1.495275, val:  55.83%, val_best:  63.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 85.07 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6261%\n",
      "layer   2  Sparsity: 72.3943%\n",
      "layer   3  Sparsity: 61.2483%\n",
      "total_backward_count 881100 real_backward_count 80977   9.190%\n",
      "lif layer 2 self.abs_max_v: 5226.0\n",
      "lif layer 2 self.abs_max_v: 5290.0\n",
      "lif layer 2 self.abs_max_v: 5308.0\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  1.144695/  1.533306, val:  55.83%, val_best:  63.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.30 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.5802%\n",
      "layer   2  Sparsity: 72.5110%\n",
      "layer   3  Sparsity: 62.8985%\n",
      "total_backward_count 890890 real_backward_count 81761   9.177%\n",
      "fc layer 2 self.abs_max_out: 2802.0\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  1.167546/  1.538015, val:  63.33%, val_best:  63.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 84.91 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6603%\n",
      "layer   2  Sparsity: 71.6342%\n",
      "layer   3  Sparsity: 62.7421%\n",
      "total_backward_count 900680 real_backward_count 82562   9.167%\n",
      "fc layer 2 self.abs_max_out: 2918.0\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  1.202930/  1.503718, val:  60.00%, val_best:  63.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 84.15 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 72.6935%\n",
      "layer   2  Sparsity: 72.3792%\n",
      "layer   3  Sparsity: 62.2516%\n",
      "total_backward_count 910470 real_backward_count 83332   9.153%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  1.175548/  1.485359, val:  56.67%, val_best:  63.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.48 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6774%\n",
      "layer   2  Sparsity: 71.9540%\n",
      "layer   3  Sparsity: 61.8152%\n",
      "total_backward_count 920260 real_backward_count 84108   9.140%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  1.147635/  1.481510, val:  57.08%, val_best:  63.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.00 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6619%\n",
      "layer   2  Sparsity: 71.5533%\n",
      "layer   3  Sparsity: 61.9600%\n",
      "total_backward_count 930050 real_backward_count 84872   9.126%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  1.126056/  1.454548, val:  60.42%, val_best:  63.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.24 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6169%\n",
      "layer   2  Sparsity: 71.1315%\n",
      "layer   3  Sparsity: 61.2321%\n",
      "total_backward_count 939840 real_backward_count 85638   9.112%\n",
      "fc layer 2 self.abs_max_out: 3008.0\n",
      "lif layer 2 self.abs_max_v: 5313.5\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  1.138958/  1.500196, val:  60.00%, val_best:  63.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 84.95 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6271%\n",
      "layer   2  Sparsity: 70.6765%\n",
      "layer   3  Sparsity: 62.2573%\n",
      "total_backward_count 949630 real_backward_count 86369   9.095%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  1.135828/  1.501219, val:  60.00%, val_best:  63.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 84.49 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6158%\n",
      "layer   2  Sparsity: 70.8348%\n",
      "layer   3  Sparsity: 62.5619%\n",
      "total_backward_count 959420 real_backward_count 87109   9.079%\n",
      "lif layer 2 self.abs_max_v: 5319.0\n",
      "lif layer 2 self.abs_max_v: 5340.5\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  1.126118/  1.496214, val:  55.00%, val_best:  63.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 84.33 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6256%\n",
      "layer   2  Sparsity: 70.4553%\n",
      "layer   3  Sparsity: 63.0911%\n",
      "total_backward_count 969210 real_backward_count 87843   9.063%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  1.094158/  1.536689, val:  61.67%, val_best:  63.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.79 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.5890%\n",
      "layer   2  Sparsity: 70.3413%\n",
      "layer   3  Sparsity: 63.3191%\n",
      "total_backward_count 979000 real_backward_count 88569   9.047%\n",
      "lif layer 2 self.abs_max_v: 5397.0\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  1.147501/  1.521493, val:  60.83%, val_best:  63.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.38 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.5811%\n",
      "layer   2  Sparsity: 70.4738%\n",
      "layer   3  Sparsity: 64.4634%\n",
      "total_backward_count 988790 real_backward_count 89303   9.032%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  1.182218/  1.534556, val:  62.50%, val_best:  63.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 85.08 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6182%\n",
      "layer   2  Sparsity: 71.1010%\n",
      "layer   3  Sparsity: 64.3493%\n",
      "total_backward_count 998580 real_backward_count 90054   9.018%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  1.175779/  1.528419, val:  53.33%, val_best:  63.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.26 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6207%\n",
      "layer   2  Sparsity: 70.4925%\n",
      "layer   3  Sparsity: 64.2959%\n",
      "total_backward_count 1008370 real_backward_count 90787   9.003%\n",
      "fc layer 1 self.abs_max_out: 8716.0\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  1.118362/  1.446846, val:  58.75%, val_best:  63.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 84.90 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6086%\n",
      "layer   2  Sparsity: 69.9819%\n",
      "layer   3  Sparsity: 63.1941%\n",
      "total_backward_count 1018160 real_backward_count 91529   8.990%\n",
      "lif layer 2 self.abs_max_v: 5412.5\n",
      "fc layer 1 self.abs_max_out: 9217.0\n",
      "lif layer 1 self.abs_max_v: 16609.5\n",
      "lif layer 1 self.abs_max_v: 16919.0\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  1.146329/  1.497231, val:  52.50%, val_best:  63.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.93 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6620%\n",
      "layer   2  Sparsity: 71.1895%\n",
      "layer   3  Sparsity: 63.8031%\n",
      "total_backward_count 1027950 real_backward_count 92282   8.977%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  1.094618/  1.424795, val:  62.50%, val_best:  63.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.97 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6398%\n",
      "layer   2  Sparsity: 71.2561%\n",
      "layer   3  Sparsity: 64.5631%\n",
      "total_backward_count 1037740 real_backward_count 93046   8.966%\n",
      "fc layer 3 self.abs_max_out: 898.0\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  1.100871/  1.446765, val:  60.00%, val_best:  63.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 84.80 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6493%\n",
      "layer   2  Sparsity: 70.3457%\n",
      "layer   3  Sparsity: 63.9876%\n",
      "total_backward_count 1047530 real_backward_count 93766   8.951%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  1.105905/  1.501386, val:  48.33%, val_best:  63.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.71 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6326%\n",
      "layer   2  Sparsity: 70.6063%\n",
      "layer   3  Sparsity: 64.3422%\n",
      "total_backward_count 1057320 real_backward_count 94565   8.944%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  1.108663/  1.463236, val:  56.67%, val_best:  63.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.89 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6308%\n",
      "layer   2  Sparsity: 70.9254%\n",
      "layer   3  Sparsity: 64.6669%\n",
      "total_backward_count 1067110 real_backward_count 95280   8.929%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  1.105958/  1.478566, val:  54.58%, val_best:  63.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 84.87 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6536%\n",
      "layer   2  Sparsity: 70.6115%\n",
      "layer   3  Sparsity: 64.2144%\n",
      "total_backward_count 1076900 real_backward_count 96027   8.917%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  1.124705/  1.393969, val:  65.00%, val_best:  65.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 83.18 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 72.6017%\n",
      "layer   2  Sparsity: 70.1200%\n",
      "layer   3  Sparsity: 64.0916%\n",
      "total_backward_count 1086690 real_backward_count 96787   8.907%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  1.114726/  1.537148, val:  53.75%, val_best:  65.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.56 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.5948%\n",
      "layer   2  Sparsity: 69.9014%\n",
      "layer   3  Sparsity: 63.7980%\n",
      "total_backward_count 1096480 real_backward_count 97501   8.892%\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  1.096306/  1.433815, val:  60.83%, val_best:  65.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.71 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6887%\n",
      "layer   2  Sparsity: 70.0007%\n",
      "layer   3  Sparsity: 63.5531%\n",
      "total_backward_count 1106270 real_backward_count 98211   8.878%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  1.092942/  1.460285, val:  55.83%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 84.39 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6406%\n",
      "layer   2  Sparsity: 70.3527%\n",
      "layer   3  Sparsity: 63.6526%\n",
      "total_backward_count 1116060 real_backward_count 98916   8.863%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  1.071507/  1.436669, val:  62.50%, val_best:  65.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.03 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6723%\n",
      "layer   2  Sparsity: 70.3340%\n",
      "layer   3  Sparsity: 63.3036%\n",
      "total_backward_count 1125850 real_backward_count 99637   8.850%\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  1.075369/  1.406105, val:  57.50%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.25 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6069%\n",
      "layer   2  Sparsity: 70.0114%\n",
      "layer   3  Sparsity: 63.0643%\n",
      "total_backward_count 1135640 real_backward_count 100348   8.836%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  1.084268/  1.485615, val:  54.58%, val_best:  65.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.07 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6065%\n",
      "layer   2  Sparsity: 69.7992%\n",
      "layer   3  Sparsity: 63.2117%\n",
      "total_backward_count 1145430 real_backward_count 101081   8.825%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  1.075564/  1.423128, val:  59.58%, val_best:  65.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 85.37 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6681%\n",
      "layer   2  Sparsity: 69.9245%\n",
      "layer   3  Sparsity: 63.3900%\n",
      "total_backward_count 1155220 real_backward_count 101787   8.811%\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  1.062935/  1.449162, val:  56.67%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 84.53 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.5685%\n",
      "layer   2  Sparsity: 70.1904%\n",
      "layer   3  Sparsity: 63.1545%\n",
      "total_backward_count 1165010 real_backward_count 102505   8.799%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  1.056049/  1.399462, val:  67.92%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.22 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6211%\n",
      "layer   2  Sparsity: 70.4943%\n",
      "layer   3  Sparsity: 63.6710%\n",
      "total_backward_count 1174800 real_backward_count 103215   8.786%\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  1.066756/  1.426278, val:  62.08%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.17 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 72.6698%\n",
      "layer   2  Sparsity: 70.5970%\n",
      "layer   3  Sparsity: 63.4517%\n",
      "total_backward_count 1184590 real_backward_count 103923   8.773%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  1.060024/  1.459683, val:  60.00%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.13 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 72.6622%\n",
      "layer   2  Sparsity: 70.2745%\n",
      "layer   3  Sparsity: 62.8982%\n",
      "total_backward_count 1194380 real_backward_count 104581   8.756%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  1.075222/  1.471397, val:  66.25%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.53 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6802%\n",
      "layer   2  Sparsity: 69.9182%\n",
      "layer   3  Sparsity: 63.7523%\n",
      "total_backward_count 1204170 real_backward_count 105327   8.747%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  1.105927/  1.426473, val:  60.00%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.58 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6305%\n",
      "layer   2  Sparsity: 70.2926%\n",
      "layer   3  Sparsity: 64.7301%\n",
      "total_backward_count 1213960 real_backward_count 106046   8.736%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  1.075803/  1.402792, val:  60.42%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.96 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6596%\n",
      "layer   2  Sparsity: 70.5568%\n",
      "layer   3  Sparsity: 64.3553%\n",
      "total_backward_count 1223750 real_backward_count 106799   8.727%\n",
      "lif layer 2 self.abs_max_v: 5498.0\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  1.065608/  1.478488, val:  57.50%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.12 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.5966%\n",
      "layer   2  Sparsity: 70.6539%\n",
      "layer   3  Sparsity: 63.2098%\n",
      "total_backward_count 1233540 real_backward_count 107500   8.715%\n",
      "lif layer 2 self.abs_max_v: 5592.5\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  1.057731/  1.480511, val:  58.75%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.39 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6047%\n",
      "layer   2  Sparsity: 71.0332%\n",
      "layer   3  Sparsity: 63.3784%\n",
      "total_backward_count 1243330 real_backward_count 108156   8.699%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  1.040623/  1.398249, val:  63.33%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.92 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6253%\n",
      "layer   2  Sparsity: 71.5141%\n",
      "layer   3  Sparsity: 63.7853%\n",
      "total_backward_count 1253120 real_backward_count 108817   8.684%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  1.055767/  1.409831, val:  59.17%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.98 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6594%\n",
      "layer   2  Sparsity: 71.2678%\n",
      "layer   3  Sparsity: 64.1685%\n",
      "total_backward_count 1262910 real_backward_count 109481   8.669%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  1.055905/  1.394493, val:  63.33%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.13 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6332%\n",
      "layer   2  Sparsity: 71.3429%\n",
      "layer   3  Sparsity: 63.8470%\n",
      "total_backward_count 1272700 real_backward_count 110130   8.653%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  1.078745/  1.449123, val:  57.92%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 84.55 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6582%\n",
      "layer   2  Sparsity: 71.5919%\n",
      "layer   3  Sparsity: 64.7991%\n",
      "total_backward_count 1282490 real_backward_count 110831   8.642%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  1.071753/  1.443888, val:  55.42%, val_best:  67.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 85.58 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 72.6547%\n",
      "layer   2  Sparsity: 72.2233%\n",
      "layer   3  Sparsity: 64.4797%\n",
      "total_backward_count 1292280 real_backward_count 111593   8.635%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  1.064252/  1.471746, val:  58.75%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.12 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6365%\n",
      "layer   2  Sparsity: 72.2635%\n",
      "layer   3  Sparsity: 63.4821%\n",
      "total_backward_count 1302070 real_backward_count 112295   8.624%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  1.061286/  1.409439, val:  58.33%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.40 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6567%\n",
      "layer   2  Sparsity: 72.1925%\n",
      "layer   3  Sparsity: 62.3587%\n",
      "total_backward_count 1311860 real_backward_count 113039   8.617%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  1.063139/  1.478937, val:  52.50%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.25 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6561%\n",
      "layer   2  Sparsity: 72.5696%\n",
      "layer   3  Sparsity: 63.0820%\n",
      "total_backward_count 1321650 real_backward_count 113665   8.600%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  1.086695/  1.415835, val:  60.83%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.27 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6175%\n",
      "layer   2  Sparsity: 71.5121%\n",
      "layer   3  Sparsity: 63.6310%\n",
      "total_backward_count 1331440 real_backward_count 114349   8.588%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  1.034710/  1.375929, val:  63.75%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.02 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6153%\n",
      "layer   2  Sparsity: 70.7778%\n",
      "layer   3  Sparsity: 63.3975%\n",
      "total_backward_count 1341230 real_backward_count 115003   8.574%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  1.051088/  1.423538, val:  54.58%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 84.46 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6490%\n",
      "layer   2  Sparsity: 70.9573%\n",
      "layer   3  Sparsity: 64.0374%\n",
      "total_backward_count 1351020 real_backward_count 115660   8.561%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  1.053051/  1.384295, val:  66.25%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.02 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6381%\n",
      "layer   2  Sparsity: 71.0447%\n",
      "layer   3  Sparsity: 64.2180%\n",
      "total_backward_count 1360810 real_backward_count 116315   8.547%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  1.045448/  1.426730, val:  59.58%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.74 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6646%\n",
      "layer   2  Sparsity: 70.6678%\n",
      "layer   3  Sparsity: 64.7057%\n",
      "total_backward_count 1370600 real_backward_count 117017   8.538%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  1.049449/  1.494286, val:  44.58%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.92 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 72.6992%\n",
      "layer   2  Sparsity: 71.0179%\n",
      "layer   3  Sparsity: 64.3911%\n",
      "total_backward_count 1380390 real_backward_count 117703   8.527%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  1.044743/  1.422509, val:  58.75%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.46 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 72.6338%\n",
      "layer   2  Sparsity: 71.1048%\n",
      "layer   3  Sparsity: 64.9072%\n",
      "total_backward_count 1390180 real_backward_count 118373   8.515%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  1.060667/  1.428755, val:  60.00%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.43 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.6268%\n",
      "layer   2  Sparsity: 71.7406%\n",
      "layer   3  Sparsity: 64.2602%\n",
      "total_backward_count 1399970 real_backward_count 119069   8.505%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  1.029885/  1.426294, val:  57.50%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.82 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.7636%\n",
      "layer   2  Sparsity: 71.8019%\n",
      "layer   3  Sparsity: 64.6546%\n",
      "total_backward_count 1409760 real_backward_count 119782   8.497%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  1.041521/  1.396243, val:  56.25%, val_best:  67.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 84.49 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 72.5945%\n",
      "layer   2  Sparsity: 72.7164%\n",
      "layer   3  Sparsity: 65.1182%\n",
      "total_backward_count 1419550 real_backward_count 120474   8.487%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  1.026494/  1.350483, val:  59.17%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 72.6449%\n",
      "layer   2  Sparsity: 72.5986%\n",
      "layer   3  Sparsity: 65.0030%\n",
      "total_backward_count 1429340 real_backward_count 121147   8.476%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  1.027862/  1.415671, val:  54.17%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.44 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 72.6854%\n",
      "layer   2  Sparsity: 72.5114%\n",
      "layer   3  Sparsity: 64.8087%\n",
      "total_backward_count 1439130 real_backward_count 121814   8.464%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  1.039165/  1.431813, val:  58.33%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.48 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 72.6595%\n",
      "layer   2  Sparsity: 72.4562%\n",
      "layer   3  Sparsity: 63.6850%\n",
      "total_backward_count 1448920 real_backward_count 122442   8.451%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  1.049294/  1.402607, val:  62.50%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.71 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 72.7154%\n",
      "layer   2  Sparsity: 72.8199%\n",
      "layer   3  Sparsity: 63.2490%\n",
      "total_backward_count 1458710 real_backward_count 123117   8.440%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  1.009433/  1.401445, val:  57.92%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.46 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 72.5867%\n",
      "layer   2  Sparsity: 72.5904%\n",
      "layer   3  Sparsity: 63.8959%\n",
      "total_backward_count 1468500 real_backward_count 123793   8.430%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  1.031782/  1.417664, val:  54.58%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 72.6164%\n",
      "layer   2  Sparsity: 71.7217%\n",
      "layer   3  Sparsity: 64.3720%\n",
      "total_backward_count 1478290 real_backward_count 124415   8.416%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  1.020653/  1.428440, val:  59.17%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.66 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 72.6563%\n",
      "layer   2  Sparsity: 71.6717%\n",
      "layer   3  Sparsity: 63.0455%\n",
      "total_backward_count 1488080 real_backward_count 125082   8.406%\n",
      "fc layer 1 self.abs_max_out: 9311.0\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  1.001413/  1.363771, val:  60.00%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 72.6552%\n",
      "layer   2  Sparsity: 71.7751%\n",
      "layer   3  Sparsity: 63.7194%\n",
      "total_backward_count 1497870 real_backward_count 125705   8.392%\n",
      "fc layer 3 self.abs_max_out: 907.0\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  1.033691/  1.370070, val:  62.92%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 72.5711%\n",
      "layer   2  Sparsity: 71.6981%\n",
      "layer   3  Sparsity: 63.0486%\n",
      "total_backward_count 1507660 real_backward_count 126419   8.385%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  1.001526/  1.395069, val:  58.75%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 72.6151%\n",
      "layer   2  Sparsity: 72.0097%\n",
      "layer   3  Sparsity: 63.2345%\n",
      "total_backward_count 1517450 real_backward_count 127032   8.371%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  1.006604/  1.506233, val:  55.00%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 72.6652%\n",
      "layer   2  Sparsity: 71.7894%\n",
      "layer   3  Sparsity: 63.2726%\n",
      "total_backward_count 1527240 real_backward_count 127668   8.359%\n",
      "fc layer 1 self.abs_max_out: 9322.0\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  1.015447/  1.402348, val:  63.33%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 72.6707%\n",
      "layer   2  Sparsity: 71.6421%\n",
      "layer   3  Sparsity: 63.4926%\n",
      "total_backward_count 1537030 real_backward_count 128351   8.351%\n",
      "fc layer 1 self.abs_max_out: 9348.0\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  1.002318/  1.399264, val:  60.42%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 72.6260%\n",
      "layer   2  Sparsity: 71.7078%\n",
      "layer   3  Sparsity: 63.6285%\n",
      "total_backward_count 1546820 real_backward_count 128990   8.339%\n",
      "fc layer 2 self.abs_max_out: 3014.0\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  1.017734/  1.442457, val:  55.00%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 72.6354%\n",
      "layer   2  Sparsity: 71.3640%\n",
      "layer   3  Sparsity: 64.3250%\n",
      "total_backward_count 1556610 real_backward_count 129633   8.328%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  1.031576/  1.398483, val:  61.67%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 72.6297%\n",
      "layer   2  Sparsity: 71.5231%\n",
      "layer   3  Sparsity: 64.1964%\n",
      "total_backward_count 1566400 real_backward_count 130260   8.316%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  1.038828/  1.375164, val:  61.25%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 72.6670%\n",
      "layer   2  Sparsity: 71.3815%\n",
      "layer   3  Sparsity: 63.1564%\n",
      "total_backward_count 1576190 real_backward_count 130936   8.307%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  1.015956/  1.372880, val:  66.25%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 72.6794%\n",
      "layer   2  Sparsity: 71.4567%\n",
      "layer   3  Sparsity: 64.3665%\n",
      "total_backward_count 1585980 real_backward_count 131572   8.296%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  1.002678/  1.373023, val:  61.67%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 72.5615%\n",
      "layer   2  Sparsity: 71.0904%\n",
      "layer   3  Sparsity: 64.6672%\n",
      "total_backward_count 1595770 real_backward_count 132245   8.287%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  1.003553/  1.376055, val:  60.42%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 72.6425%\n",
      "layer   2  Sparsity: 70.8322%\n",
      "layer   3  Sparsity: 65.1203%\n",
      "total_backward_count 1605560 real_backward_count 132862   8.275%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  1.006388/  1.392396, val:  63.33%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 72.6163%\n",
      "layer   2  Sparsity: 71.2006%\n",
      "layer   3  Sparsity: 64.3271%\n",
      "total_backward_count 1615350 real_backward_count 133462   8.262%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  1.030760/  1.410252, val:  56.25%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 72.6328%\n",
      "layer   2  Sparsity: 70.8328%\n",
      "layer   3  Sparsity: 64.2115%\n",
      "total_backward_count 1625140 real_backward_count 134098   8.251%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  0.993050/  1.413153, val:  57.08%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 72.5866%\n",
      "layer   2  Sparsity: 70.9256%\n",
      "layer   3  Sparsity: 64.0290%\n",
      "total_backward_count 1634930 real_backward_count 134716   8.240%\n",
      "fc layer 3 self.abs_max_out: 913.0\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  1.007142/  1.380198, val:  58.33%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 72.6269%\n",
      "layer   2  Sparsity: 71.2638%\n",
      "layer   3  Sparsity: 64.0659%\n",
      "total_backward_count 1644720 real_backward_count 135324   8.228%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  0.987266/  1.379644, val:  61.67%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 72.6322%\n",
      "layer   2  Sparsity: 71.5003%\n",
      "layer   3  Sparsity: 64.2632%\n",
      "total_backward_count 1654510 real_backward_count 135952   8.217%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  0.981298/  1.380411, val:  62.08%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 72.6166%\n",
      "layer   2  Sparsity: 72.1393%\n",
      "layer   3  Sparsity: 63.5311%\n",
      "total_backward_count 1664300 real_backward_count 136539   8.204%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  0.979849/  1.415363, val:  56.67%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 72.6379%\n",
      "layer   2  Sparsity: 71.4723%\n",
      "layer   3  Sparsity: 64.1775%\n",
      "total_backward_count 1674090 real_backward_count 137171   8.194%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  0.994039/  1.391912, val:  58.75%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 72.6536%\n",
      "layer   2  Sparsity: 71.4070%\n",
      "layer   3  Sparsity: 64.5834%\n",
      "total_backward_count 1683880 real_backward_count 137778   8.182%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  0.980540/  1.401395, val:  60.00%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 72.7138%\n",
      "layer   2  Sparsity: 71.2887%\n",
      "layer   3  Sparsity: 64.1310%\n",
      "total_backward_count 1693670 real_backward_count 138448   8.174%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  0.985922/  1.376008, val:  66.25%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 72.6450%\n",
      "layer   2  Sparsity: 71.4148%\n",
      "layer   3  Sparsity: 64.0003%\n",
      "total_backward_count 1703460 real_backward_count 139027   8.161%\n",
      "fc layer 1 self.abs_max_out: 9396.0\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  0.987284/  1.422606, val:  65.42%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 72.6834%\n",
      "layer   2  Sparsity: 71.3927%\n",
      "layer   3  Sparsity: 64.4865%\n",
      "total_backward_count 1713250 real_backward_count 139633   8.150%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  0.988395/  1.379635, val:  67.92%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 72.6870%\n",
      "layer   2  Sparsity: 71.3427%\n",
      "layer   3  Sparsity: 63.5752%\n",
      "total_backward_count 1723040 real_backward_count 140237   8.139%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  0.973685/  1.390977, val:  60.00%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 72.6369%\n",
      "layer   2  Sparsity: 70.8758%\n",
      "layer   3  Sparsity: 64.3975%\n",
      "total_backward_count 1732830 real_backward_count 140837   8.128%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  0.977585/  1.369076, val:  65.83%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 72.6372%\n",
      "layer   2  Sparsity: 70.9895%\n",
      "layer   3  Sparsity: 63.9964%\n",
      "total_backward_count 1742620 real_backward_count 141407   8.115%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  1.003925/  1.451265, val:  60.42%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 72.6540%\n",
      "layer   2  Sparsity: 70.8511%\n",
      "layer   3  Sparsity: 64.0512%\n",
      "total_backward_count 1752410 real_backward_count 141999   8.103%\n",
      "fc layer 1 self.abs_max_out: 9476.0\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  0.985486/  1.335347, val:  64.17%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 72.6427%\n",
      "layer   2  Sparsity: 71.1210%\n",
      "layer   3  Sparsity: 64.4250%\n",
      "total_backward_count 1762200 real_backward_count 142579   8.091%\n",
      "fc layer 1 self.abs_max_out: 9712.0\n",
      "lif layer 1 self.abs_max_v: 17112.5\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  0.962656/  1.380657, val:  60.00%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 72.6791%\n",
      "layer   2  Sparsity: 70.9175%\n",
      "layer   3  Sparsity: 64.0840%\n",
      "total_backward_count 1771990 real_backward_count 143163   8.079%\n",
      "fc layer 1 self.abs_max_out: 10024.0\n",
      "lif layer 1 self.abs_max_v: 17215.0\n",
      "lif layer 1 self.abs_max_v: 17668.5\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  0.984364/  1.338742, val:  65.42%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 72.6146%\n",
      "layer   2  Sparsity: 70.3279%\n",
      "layer   3  Sparsity: 64.9303%\n",
      "total_backward_count 1781780 real_backward_count 143757   8.068%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  0.983945/  1.345489, val:  61.25%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 72.6120%\n",
      "layer   2  Sparsity: 70.3475%\n",
      "layer   3  Sparsity: 64.5778%\n",
      "total_backward_count 1791570 real_backward_count 144372   8.058%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  0.967645/  1.392212, val:  58.75%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 72.5973%\n",
      "layer   2  Sparsity: 70.6433%\n",
      "layer   3  Sparsity: 64.2934%\n",
      "total_backward_count 1801360 real_backward_count 144961   8.047%\n",
      "fc layer 1 self.abs_max_out: 10078.0\n",
      "lif layer 1 self.abs_max_v: 17766.5\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  0.983951/  1.375453, val:  58.75%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 72.6782%\n",
      "layer   2  Sparsity: 70.8870%\n",
      "layer   3  Sparsity: 65.0922%\n",
      "total_backward_count 1811150 real_backward_count 145554   8.037%\n",
      "fc layer 1 self.abs_max_out: 10311.0\n",
      "lif layer 1 self.abs_max_v: 18179.0\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  0.982900/  1.348551, val:  62.92%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 72.7171%\n",
      "layer   2  Sparsity: 71.4710%\n",
      "layer   3  Sparsity: 64.9420%\n",
      "total_backward_count 1820940 real_backward_count 146178   8.028%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  0.983210/  1.374962, val:  60.83%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 72.7005%\n",
      "layer   2  Sparsity: 71.6787%\n",
      "layer   3  Sparsity: 65.8012%\n",
      "total_backward_count 1830730 real_backward_count 146788   8.018%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  0.961156/  1.421666, val:  53.33%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 72.6187%\n",
      "layer   2  Sparsity: 71.5217%\n",
      "layer   3  Sparsity: 65.5106%\n",
      "total_backward_count 1840520 real_backward_count 147434   8.010%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  0.945939/  1.361095, val:  58.33%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 72.7009%\n",
      "layer   2  Sparsity: 71.8496%\n",
      "layer   3  Sparsity: 66.0283%\n",
      "total_backward_count 1850310 real_backward_count 148050   8.001%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  0.949562/  1.331308, val:  62.92%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 72.6713%\n",
      "layer   2  Sparsity: 72.2059%\n",
      "layer   3  Sparsity: 65.8614%\n",
      "total_backward_count 1860100 real_backward_count 148663   7.992%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  0.952084/  1.349371, val:  59.58%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 72.6665%\n",
      "layer   2  Sparsity: 72.6330%\n",
      "layer   3  Sparsity: 65.9789%\n",
      "total_backward_count 1869890 real_backward_count 149264   7.983%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  0.943239/  1.393744, val:  54.58%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 72.6413%\n",
      "layer   2  Sparsity: 72.7360%\n",
      "layer   3  Sparsity: 65.5657%\n",
      "total_backward_count 1879680 real_backward_count 149854   7.972%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  0.960358/  1.308254, val:  67.08%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 72.6645%\n",
      "layer   2  Sparsity: 72.3571%\n",
      "layer   3  Sparsity: 65.4865%\n",
      "total_backward_count 1889470 real_backward_count 150434   7.962%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  0.952967/  1.344174, val:  67.08%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 72.6672%\n",
      "layer   2  Sparsity: 72.4030%\n",
      "layer   3  Sparsity: 65.7263%\n",
      "total_backward_count 1899260 real_backward_count 151012   7.951%\n",
      "fc layer 3 self.abs_max_out: 923.0\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  0.960581/  1.359694, val:  57.08%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 72.5853%\n",
      "layer   2  Sparsity: 72.2767%\n",
      "layer   3  Sparsity: 65.7734%\n",
      "total_backward_count 1909050 real_backward_count 151631   7.943%\n",
      "fc layer 3 self.abs_max_out: 939.0\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  0.919658/  1.309332, val:  64.58%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 72.6574%\n",
      "layer   2  Sparsity: 71.9994%\n",
      "layer   3  Sparsity: 65.1189%\n",
      "total_backward_count 1918840 real_backward_count 152221   7.933%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  0.933618/  1.342439, val:  60.42%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 72.6284%\n",
      "layer   2  Sparsity: 72.1100%\n",
      "layer   3  Sparsity: 65.6664%\n",
      "total_backward_count 1928630 real_backward_count 152767   7.921%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  0.943336/  1.333975, val:  61.25%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 72.6200%\n",
      "layer   2  Sparsity: 72.0827%\n",
      "layer   3  Sparsity: 65.2437%\n",
      "total_backward_count 1938420 real_backward_count 153348   7.911%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  0.946587/  1.381923, val:  57.92%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 72.6842%\n",
      "layer   2  Sparsity: 71.9442%\n",
      "layer   3  Sparsity: 65.5616%\n",
      "total_backward_count 1948210 real_backward_count 153931   7.901%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  0.925989/  1.377393, val:  58.33%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 72.6501%\n",
      "layer   2  Sparsity: 71.7000%\n",
      "layer   3  Sparsity: 65.7067%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4297cdad2da4491295b8fd441b7de9e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñà‚ñÜ‚ñÖ‚ñà‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñà‚ñÜ‚ñá‚ñà‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñà‚ñÜ‚ñÖ‚ñà‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99898</td></tr><tr><td>tr_epoch_loss</td><td>0.92599</td></tr><tr><td>val_acc_best</td><td>0.67917</td></tr><tr><td>val_acc_now</td><td>0.58333</td></tr><tr><td>val_loss</td><td>1.37739</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fiery-sweep-17</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y5h1ej3h' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y5h1ej3h</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_133410-y5h1ej3h/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: skvsfzg8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 50000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3161905769a459286642c615b10deba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112977944624921, max=1.0‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_181046-skvsfzg8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/skvsfzg8' target=\"_blank\">upbeat-sweep-21</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/skvsfzg8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/skvsfzg8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251118_181055_798', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 50000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 575149142d3019108310063e0e922290\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 139\n",
      "fc layer 1 self.abs_max_out: 407.0\n",
      "lif layer 1 self.abs_max_v: 407.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 453.0\n",
      "lif layer 1 self.abs_max_v: 549.5\n",
      "fc layer 2 self.abs_max_out: 72.0\n",
      "lif layer 2 self.abs_max_v: 72.0\n",
      "lif layer 1 self.abs_max_v: 687.0\n",
      "lif layer 1 self.abs_max_v: 710.5\n",
      "fc layer 2 self.abs_max_out: 266.0\n",
      "lif layer 2 self.abs_max_v: 252.0\n",
      "fc layer 1 self.abs_max_out: 702.0\n",
      "lif layer 1 self.abs_max_v: 896.5\n",
      "fc layer 2 self.abs_max_out: 347.0\n",
      "lif layer 2 self.abs_max_v: 393.0\n",
      "fc layer 1 self.abs_max_out: 1005.0\n",
      "lif layer 1 self.abs_max_v: 1167.0\n",
      "fc layer 2 self.abs_max_out: 383.0\n",
      "lif layer 2 self.abs_max_v: 510.5\n",
      "lif layer 2 self.abs_max_v: 609.5\n",
      "fc layer 3 self.abs_max_out: 31.0\n",
      "smallest_now_T updated: 125\n",
      "fc layer 1 self.abs_max_out: 1536.0\n",
      "lif layer 1 self.abs_max_v: 1701.5\n",
      "fc layer 2 self.abs_max_out: 538.0\n",
      "lif layer 2 self.abs_max_v: 638.5\n",
      "fc layer 2 self.abs_max_out: 569.0\n",
      "lif layer 2 self.abs_max_v: 774.5\n",
      "fc layer 3 self.abs_max_out: 135.0\n",
      "lif layer 2 self.abs_max_v: 883.5\n",
      "fc layer 1 self.abs_max_out: 1654.0\n",
      "fc layer 2 self.abs_max_out: 587.0\n",
      "lif layer 2 self.abs_max_v: 971.5\n",
      "fc layer 1 self.abs_max_out: 1737.0\n",
      "lif layer 1 self.abs_max_v: 1737.0\n",
      "lif layer 2 self.abs_max_v: 1050.0\n",
      "smallest_now_T updated: 94\n",
      "fc layer 2 self.abs_max_out: 663.0\n",
      "lif layer 2 self.abs_max_v: 1068.5\n",
      "fc layer 2 self.abs_max_out: 699.0\n",
      "lif layer 2 self.abs_max_v: 1134.5\n",
      "fc layer 3 self.abs_max_out: 142.0\n",
      "fc layer 2 self.abs_max_out: 735.0\n",
      "fc layer 3 self.abs_max_out: 150.0\n",
      "fc layer 1 self.abs_max_out: 1826.0\n",
      "lif layer 1 self.abs_max_v: 1826.0\n",
      "fc layer 1 self.abs_max_out: 2022.0\n",
      "lif layer 1 self.abs_max_v: 2022.0\n",
      "fc layer 2 self.abs_max_out: 881.0\n",
      "fc layer 3 self.abs_max_out: 153.0\n",
      "lif layer 2 self.abs_max_v: 1235.5\n",
      "fc layer 2 self.abs_max_out: 951.0\n",
      "fc layer 2 self.abs_max_out: 978.0\n",
      "lif layer 2 self.abs_max_v: 1243.0\n",
      "fc layer 1 self.abs_max_out: 2065.0\n",
      "lif layer 1 self.abs_max_v: 2065.0\n",
      "fc layer 2 self.abs_max_out: 1027.0\n",
      "lif layer 2 self.abs_max_v: 1253.0\n",
      "fc layer 2 self.abs_max_out: 1118.0\n",
      "fc layer 2 self.abs_max_out: 1167.0\n",
      "fc layer 2 self.abs_max_out: 1369.0\n",
      "lif layer 2 self.abs_max_v: 1369.0\n",
      "lif layer 2 self.abs_max_v: 1503.5\n",
      "fc layer 3 self.abs_max_out: 190.0\n",
      "fc layer 3 self.abs_max_out: 215.0\n",
      "smallest_now_T updated: 79\n",
      "fc layer 3 self.abs_max_out: 298.0\n",
      "fc layer 1 self.abs_max_out: 2318.0\n",
      "lif layer 1 self.abs_max_v: 2318.0\n",
      "fc layer 2 self.abs_max_out: 1571.0\n",
      "lif layer 2 self.abs_max_v: 1571.0\n",
      "lif layer 2 self.abs_max_v: 1579.5\n",
      "fc layer 1 self.abs_max_out: 2748.0\n",
      "lif layer 1 self.abs_max_v: 2748.0\n",
      "lif layer 2 self.abs_max_v: 1724.0\n",
      "lif layer 2 self.abs_max_v: 1804.0\n",
      "fc layer 3 self.abs_max_out: 304.0\n",
      "lif layer 2 self.abs_max_v: 1815.0\n",
      "fc layer 3 self.abs_max_out: 352.0\n",
      "fc layer 3 self.abs_max_out: 357.0\n",
      "lif layer 2 self.abs_max_v: 1828.0\n",
      "lif layer 2 self.abs_max_v: 1834.5\n",
      "smallest_now_T updated: 73\n",
      "lif layer 2 self.abs_max_v: 1894.5\n",
      "lif layer 2 self.abs_max_v: 1920.5\n",
      "fc layer 2 self.abs_max_out: 1613.0\n",
      "lif layer 2 self.abs_max_v: 1968.5\n",
      "lif layer 2 self.abs_max_v: 1970.0\n",
      "lif layer 1 self.abs_max_v: 2952.0\n",
      "lif layer 2 self.abs_max_v: 2034.0\n",
      "fc layer 3 self.abs_max_out: 396.0\n",
      "fc layer 2 self.abs_max_out: 1743.0\n",
      "smallest_now_T updated: 65\n",
      "fc layer 3 self.abs_max_out: 471.0\n",
      "fc layer 2 self.abs_max_out: 1750.0\n",
      "fc layer 2 self.abs_max_out: 1802.0\n",
      "fc layer 2 self.abs_max_out: 1867.0\n",
      "lif layer 2 self.abs_max_v: 2120.5\n",
      "fc layer 2 self.abs_max_out: 1984.0\n",
      "lif layer 2 self.abs_max_v: 2130.0\n",
      "fc layer 1 self.abs_max_out: 3041.0\n",
      "lif layer 1 self.abs_max_v: 3041.0\n",
      "fc layer 3 self.abs_max_out: 480.0\n",
      "fc layer 3 self.abs_max_out: 592.0\n",
      "fc layer 1 self.abs_max_out: 3316.0\n",
      "lif layer 1 self.abs_max_v: 3316.0\n",
      "smallest_now_T updated: 56\n",
      "fc layer 2 self.abs_max_out: 1998.0\n",
      "smallest_now_T updated: 50\n",
      "fc layer 2 self.abs_max_out: 2066.0\n",
      "fc layer 2 self.abs_max_out: 2077.0\n",
      "fc layer 2 self.abs_max_out: 2151.0\n",
      "lif layer 2 self.abs_max_v: 2151.0\n",
      "fc layer 2 self.abs_max_out: 2177.0\n",
      "lif layer 2 self.abs_max_v: 2177.0\n",
      "fc layer 1 self.abs_max_out: 3402.0\n",
      "lif layer 1 self.abs_max_v: 3402.0\n",
      "fc layer 1 self.abs_max_out: 3650.0\n",
      "lif layer 1 self.abs_max_v: 3650.0\n",
      "fc layer 1 self.abs_max_out: 3828.0\n",
      "lif layer 1 self.abs_max_v: 3828.0\n",
      "lif layer 1 self.abs_max_v: 4041.0\n",
      "lif layer 1 self.abs_max_v: 4181.5\n",
      "lif layer 1 self.abs_max_v: 4398.0\n",
      "fc layer 1 self.abs_max_out: 4004.0\n",
      "fc layer 2 self.abs_max_out: 2217.0\n",
      "lif layer 2 self.abs_max_v: 2217.0\n",
      "fc layer 2 self.abs_max_out: 2229.0\n",
      "lif layer 2 self.abs_max_v: 2229.0\n",
      "fc layer 2 self.abs_max_out: 2267.0\n",
      "lif layer 2 self.abs_max_v: 2267.0\n",
      "fc layer 2 self.abs_max_out: 2371.0\n",
      "lif layer 2 self.abs_max_v: 2371.0\n",
      "fc layer 2 self.abs_max_out: 2373.0\n",
      "lif layer 2 self.abs_max_v: 2373.0\n",
      "lif layer 2 self.abs_max_v: 2413.0\n",
      "lif layer 2 self.abs_max_v: 2501.5\n",
      "fc layer 2 self.abs_max_out: 2399.0\n",
      "fc layer 2 self.abs_max_out: 2410.0\n",
      "fc layer 2 self.abs_max_out: 2493.0\n",
      "lif layer 2 self.abs_max_v: 2708.0\n",
      "fc layer 2 self.abs_max_out: 2505.0\n",
      "fc layer 1 self.abs_max_out: 4056.0\n",
      "fc layer 2 self.abs_max_out: 2647.0\n",
      "fc layer 1 self.abs_max_out: 4572.0\n",
      "lif layer 1 self.abs_max_v: 4572.0\n",
      "fc layer 1 self.abs_max_out: 4593.0\n",
      "lif layer 1 self.abs_max_v: 4593.0\n",
      "fc layer 2 self.abs_max_out: 2711.0\n",
      "lif layer 2 self.abs_max_v: 2711.0\n",
      "fc layer 2 self.abs_max_out: 2949.0\n",
      "lif layer 2 self.abs_max_v: 2949.0\n",
      "fc layer 1 self.abs_max_out: 4612.0\n",
      "lif layer 1 self.abs_max_v: 4612.0\n",
      "fc layer 1 self.abs_max_out: 4634.0\n",
      "lif layer 1 self.abs_max_v: 4634.0\n",
      "lif layer 1 self.abs_max_v: 4636.0\n",
      "lif layer 1 self.abs_max_v: 4904.0\n",
      "lif layer 1 self.abs_max_v: 5132.5\n",
      "lif layer 1 self.abs_max_v: 5170.5\n",
      "lif layer 1 self.abs_max_v: 5235.5\n",
      "lif layer 1 self.abs_max_v: 5258.5\n",
      "lif layer 1 self.abs_max_v: 5456.5\n",
      "fc layer 1 self.abs_max_out: 4652.0\n",
      "smallest_now_T_val updated: 129\n",
      "smallest_now_T_val updated: 106\n",
      "smallest_now_T_val updated: 104\n",
      "smallest_now_T_val updated: 102\n",
      "smallest_now_T_val updated: 85\n",
      "smallest_now_T_val updated: 50\n",
      "lif layer 1 self.abs_max_v: 5663.0\n",
      "lif layer 1 self.abs_max_v: 5685.0\n",
      "lif layer 1 self.abs_max_v: 5849.5\n",
      "lif layer 1 self.abs_max_v: 5872.0\n",
      "lif layer 1 self.abs_max_v: 6197.5\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.963758/  2.054298, val:  32.50%, val_best:  32.50%, tr:  92.75%, tr_best:  92.75%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.7142%\n",
      "layer   2  Sparsity: 80.4162%\n",
      "layer   3  Sparsity: 85.1948%\n",
      "total_backward_count 9790 real_backward_count 2670  27.273%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 3233.5\n",
      "lif layer 2 self.abs_max_v: 3399.0\n",
      "fc layer 1 self.abs_max_out: 4819.0\n",
      "fc layer 1 self.abs_max_out: 5034.0\n",
      "fc layer 1 self.abs_max_out: 5071.0\n",
      "fc layer 1 self.abs_max_out: 5080.0\n",
      "fc layer 1 self.abs_max_out: 5227.0\n",
      "fc layer 1 self.abs_max_out: 5430.0\n",
      "fc layer 2 self.abs_max_out: 3003.0\n",
      "lif layer 2 self.abs_max_v: 3406.0\n",
      "lif layer 2 self.abs_max_v: 3500.0\n",
      "lif layer 2 self.abs_max_v: 3913.0\n",
      "lif layer 2 self.abs_max_v: 4038.5\n",
      "fc layer 2 self.abs_max_out: 3006.0\n",
      "fc layer 2 self.abs_max_out: 3012.0\n",
      "fc layer 2 self.abs_max_out: 3078.0\n",
      "lif layer 1 self.abs_max_v: 6260.0\n",
      "lif layer 1 self.abs_max_v: 6759.0\n",
      "lif layer 1 self.abs_max_v: 7065.5\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.907547/  2.049723, val:  44.58%, val_best:  44.58%, tr:  99.59%, tr_best:  99.59%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.7239%\n",
      "layer   2  Sparsity: 78.8114%\n",
      "layer   3  Sparsity: 82.8210%\n",
      "total_backward_count 19580 real_backward_count 4291  21.915%\n",
      "fc layer 2 self.abs_max_out: 3258.0\n",
      "fc layer 3 self.abs_max_out: 612.0\n",
      "fc layer 1 self.abs_max_out: 5446.0\n",
      "fc layer 1 self.abs_max_out: 5516.0\n",
      "fc layer 1 self.abs_max_out: 5621.0\n",
      "fc layer 1 self.abs_max_out: 5714.0\n",
      "fc layer 1 self.abs_max_out: 5738.0\n",
      "lif layer 2 self.abs_max_v: 4079.0\n",
      "lif layer 2 self.abs_max_v: 4160.5\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.879046/  2.021144, val:  50.83%, val_best:  50.83%, tr:  99.49%, tr_best:  99.59%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.7092%\n",
      "layer   2  Sparsity: 78.4660%\n",
      "layer   3  Sparsity: 82.1619%\n",
      "total_backward_count 29370 real_backward_count 5747  19.568%\n",
      "fc layer 1 self.abs_max_out: 5788.0\n",
      "fc layer 2 self.abs_max_out: 3369.0\n",
      "fc layer 1 self.abs_max_out: 6016.0\n",
      "fc layer 1 self.abs_max_out: 6048.0\n",
      "fc layer 1 self.abs_max_out: 6109.0\n",
      "fc layer 2 self.abs_max_out: 3399.0\n",
      "lif layer 1 self.abs_max_v: 7230.0\n",
      "lif layer 1 self.abs_max_v: 7972.5\n",
      "lif layer 1 self.abs_max_v: 8242.5\n",
      "lif layer 1 self.abs_max_v: 8510.5\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.859493/  2.020025, val:  43.33%, val_best:  50.83%, tr:  99.59%, tr_best:  99.59%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.7133%\n",
      "layer   2  Sparsity: 78.2731%\n",
      "layer   3  Sparsity: 81.9370%\n",
      "total_backward_count 39160 real_backward_count 7122  18.187%\n",
      "fc layer 2 self.abs_max_out: 3451.0\n",
      "fc layer 1 self.abs_max_out: 6297.0\n",
      "fc layer 1 self.abs_max_out: 6483.0\n",
      "fc layer 1 self.abs_max_out: 6555.0\n",
      "fc layer 1 self.abs_max_out: 6756.0\n",
      "lif layer 1 self.abs_max_v: 8668.5\n",
      "lif layer 1 self.abs_max_v: 8703.5\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.858034/  1.998983, val:  45.83%, val_best:  50.83%, tr:  99.28%, tr_best:  99.59%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.7046%\n",
      "layer   2  Sparsity: 77.5566%\n",
      "layer   3  Sparsity: 81.9482%\n",
      "total_backward_count 48950 real_backward_count 8477  17.318%\n",
      "lif layer 1 self.abs_max_v: 8963.5\n",
      "lif layer 1 self.abs_max_v: 9205.0\n",
      "lif layer 2 self.abs_max_v: 4285.0\n",
      "lif layer 1 self.abs_max_v: 9215.5\n",
      "lif layer 1 self.abs_max_v: 9319.0\n",
      "lif layer 1 self.abs_max_v: 9793.5\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.820724/  1.968954, val:  59.17%, val_best:  59.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.7205%\n",
      "layer   2  Sparsity: 76.8654%\n",
      "layer   3  Sparsity: 82.0196%\n",
      "total_backward_count 58740 real_backward_count 9702  16.517%\n",
      "fc layer 1 self.abs_max_out: 6943.0\n",
      "fc layer 3 self.abs_max_out: 629.0\n",
      "fc layer 3 self.abs_max_out: 633.0\n",
      "lif layer 2 self.abs_max_v: 4290.0\n",
      "fc layer 2 self.abs_max_out: 3605.0\n",
      "fc layer 2 self.abs_max_out: 3668.0\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.833296/  1.962528, val:  58.75%, val_best:  59.17%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.7149%\n",
      "layer   2  Sparsity: 76.7092%\n",
      "layer   3  Sparsity: 82.4555%\n",
      "total_backward_count 68530 real_backward_count 10927  15.945%\n",
      "fc layer 1 self.abs_max_out: 6970.0\n",
      "fc layer 1 self.abs_max_out: 7176.0\n",
      "lif layer 2 self.abs_max_v: 4406.5\n",
      "fc layer 3 self.abs_max_out: 663.0\n",
      "fc layer 3 self.abs_max_out: 675.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.800774/  1.927655, val:  65.00%, val_best:  65.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.7181%\n",
      "layer   2  Sparsity: 76.7623%\n",
      "layer   3  Sparsity: 82.3221%\n",
      "total_backward_count 78320 real_backward_count 12061  15.400%\n",
      "fc layer 3 self.abs_max_out: 677.0\n",
      "fc layer 2 self.abs_max_out: 3679.0\n",
      "fc layer 2 self.abs_max_out: 3853.0\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.780038/  1.906302, val:  67.08%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.7124%\n",
      "layer   2  Sparsity: 76.9000%\n",
      "layer   3  Sparsity: 82.6615%\n",
      "total_backward_count 88110 real_backward_count 13227  15.012%\n",
      "fc layer 3 self.abs_max_out: 689.0\n",
      "fc layer 1 self.abs_max_out: 7265.0\n",
      "fc layer 1 self.abs_max_out: 7484.0\n",
      "lif layer 1 self.abs_max_v: 9797.5\n",
      "fc layer 3 self.abs_max_out: 711.0\n",
      "fc layer 3 self.abs_max_out: 720.0\n",
      "fc layer 3 self.abs_max_out: 729.0\n",
      "fc layer 3 self.abs_max_out: 748.0\n",
      "fc layer 3 self.abs_max_out: 813.0\n",
      "lif layer 1 self.abs_max_v: 9840.5\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.753093/  1.902227, val:  62.50%, val_best:  67.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.7179%\n",
      "layer   2  Sparsity: 76.3083%\n",
      "layer   3  Sparsity: 82.4274%\n",
      "total_backward_count 97900 real_backward_count 14323  14.630%\n",
      "fc layer 1 self.abs_max_out: 7665.0\n",
      "lif layer 2 self.abs_max_v: 4417.0\n",
      "lif layer 2 self.abs_max_v: 4758.5\n",
      "lif layer 1 self.abs_max_v: 10058.5\n",
      "lif layer 1 self.abs_max_v: 10217.5\n",
      "lif layer 1 self.abs_max_v: 10453.0\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.739884/  1.874009, val:  61.67%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.6858%\n",
      "layer   2  Sparsity: 76.6427%\n",
      "layer   3  Sparsity: 82.3599%\n",
      "total_backward_count 107690 real_backward_count 15371  14.273%\n",
      "fc layer 1 self.abs_max_out: 7726.0\n",
      "lif layer 1 self.abs_max_v: 10876.0\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.729254/  1.888644, val:  72.50%, val_best:  72.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.7387%\n",
      "layer   2  Sparsity: 75.9333%\n",
      "layer   3  Sparsity: 82.2746%\n",
      "total_backward_count 117480 real_backward_count 16317  13.889%\n",
      "fc layer 1 self.abs_max_out: 7742.0\n",
      "lif layer 1 self.abs_max_v: 11181.5\n",
      "lif layer 1 self.abs_max_v: 11187.5\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.713579/  1.882598, val:  70.00%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.7286%\n",
      "layer   2  Sparsity: 76.2710%\n",
      "layer   3  Sparsity: 82.8358%\n",
      "total_backward_count 127270 real_backward_count 17252  13.555%\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.722713/  1.874925, val:  70.83%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.7143%\n",
      "layer   2  Sparsity: 76.0128%\n",
      "layer   3  Sparsity: 82.9307%\n",
      "total_backward_count 137060 real_backward_count 18169  13.256%\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.726464/  1.853685, val:  66.25%, val_best:  72.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.7027%\n",
      "layer   2  Sparsity: 76.2170%\n",
      "layer   3  Sparsity: 83.0518%\n",
      "total_backward_count 146850 real_backward_count 19048  12.971%\n",
      "fc layer 1 self.abs_max_out: 7896.0\n",
      "lif layer 1 self.abs_max_v: 11771.5\n",
      "lif layer 1 self.abs_max_v: 12055.5\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.722386/  1.852113, val:  73.75%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.7028%\n",
      "layer   2  Sparsity: 76.1069%\n",
      "layer   3  Sparsity: 83.0476%\n",
      "total_backward_count 156640 real_backward_count 19929  12.723%\n",
      "lif layer 2 self.abs_max_v: 4860.0\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.726155/  1.840474, val:  75.83%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.7081%\n",
      "layer   2  Sparsity: 75.7513%\n",
      "layer   3  Sparsity: 83.1861%\n",
      "total_backward_count 166430 real_backward_count 20734  12.458%\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.708929/  1.831677, val:  84.58%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.7177%\n",
      "layer   2  Sparsity: 75.8439%\n",
      "layer   3  Sparsity: 83.1750%\n",
      "total_backward_count 176220 real_backward_count 21561  12.235%\n",
      "fc layer 1 self.abs_max_out: 8069.0\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.703312/  1.819486, val:  80.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.44 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.7041%\n",
      "layer   2  Sparsity: 75.6438%\n",
      "layer   3  Sparsity: 83.4594%\n",
      "total_backward_count 186010 real_backward_count 22394  12.039%\n",
      "lif layer 2 self.abs_max_v: 4955.5\n",
      "lif layer 1 self.abs_max_v: 12324.5\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.707570/  1.870132, val:  72.08%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.7112%\n",
      "layer   2  Sparsity: 75.6898%\n",
      "layer   3  Sparsity: 83.8196%\n",
      "total_backward_count 195800 real_backward_count 23172  11.835%\n",
      "lif layer 1 self.abs_max_v: 12512.0\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.718095/  1.867593, val:  72.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.93 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.6944%\n",
      "layer   2  Sparsity: 75.4349%\n",
      "layer   3  Sparsity: 83.7280%\n",
      "total_backward_count 205590 real_backward_count 23932  11.641%\n",
      "lif layer 2 self.abs_max_v: 5024.0\n",
      "lif layer 2 self.abs_max_v: 5148.0\n",
      "lif layer 2 self.abs_max_v: 5157.0\n",
      "fc layer 1 self.abs_max_out: 8079.0\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.730768/  1.858604, val:  80.42%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.7064%\n",
      "layer   2  Sparsity: 75.7097%\n",
      "layer   3  Sparsity: 83.4490%\n",
      "total_backward_count 215380 real_backward_count 24706  11.471%\n",
      "fc layer 1 self.abs_max_out: 8125.0\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.712844/  1.809391, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.7146%\n",
      "layer   2  Sparsity: 75.9629%\n",
      "layer   3  Sparsity: 83.4337%\n",
      "total_backward_count 225170 real_backward_count 25445  11.300%\n",
      "fc layer 2 self.abs_max_out: 3890.0\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.683763/  1.818635, val:  79.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.6966%\n",
      "layer   2  Sparsity: 75.6406%\n",
      "layer   3  Sparsity: 83.2216%\n",
      "total_backward_count 234960 real_backward_count 26141  11.126%\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.679048/  1.808653, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.7077%\n",
      "layer   2  Sparsity: 75.6354%\n",
      "layer   3  Sparsity: 83.0695%\n",
      "total_backward_count 244750 real_backward_count 26825  10.960%\n",
      "fc layer 1 self.abs_max_out: 8191.0\n",
      "lif layer 1 self.abs_max_v: 12552.0\n",
      "lif layer 1 self.abs_max_v: 12621.0\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.677596/  1.800074, val:  84.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.7253%\n",
      "layer   2  Sparsity: 75.6164%\n",
      "layer   3  Sparsity: 83.0662%\n",
      "total_backward_count 254540 real_backward_count 27523  10.813%\n",
      "fc layer 2 self.abs_max_out: 3947.0\n",
      "lif layer 1 self.abs_max_v: 12873.5\n",
      "fc layer 1 self.abs_max_out: 8219.0\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.663583/  1.797738, val:  79.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.7256%\n",
      "layer   2  Sparsity: 75.1050%\n",
      "layer   3  Sparsity: 82.6858%\n",
      "total_backward_count 264330 real_backward_count 28221  10.676%\n",
      "fc layer 1 self.abs_max_out: 8251.0\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.661791/  1.777858, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.80 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.7115%\n",
      "layer   2  Sparsity: 75.1031%\n",
      "layer   3  Sparsity: 82.5482%\n",
      "total_backward_count 274120 real_backward_count 28946  10.560%\n",
      "fc layer 1 self.abs_max_out: 8295.0\n",
      "lif layer 1 self.abs_max_v: 13203.5\n",
      "lif layer 1 self.abs_max_v: 13816.5\n",
      "lif layer 1 self.abs_max_v: 13901.5\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.655785/  1.780308, val:  83.33%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.7051%\n",
      "layer   2  Sparsity: 75.2822%\n",
      "layer   3  Sparsity: 82.9552%\n",
      "total_backward_count 283910 real_backward_count 29561  10.412%\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.655600/  1.799353, val:  66.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.7148%\n",
      "layer   2  Sparsity: 74.9723%\n",
      "layer   3  Sparsity: 83.3210%\n",
      "total_backward_count 293700 real_backward_count 30155  10.267%\n",
      "fc layer 1 self.abs_max_out: 8337.0\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.655158/  1.773813, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.95 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.7220%\n",
      "layer   2  Sparsity: 74.9010%\n",
      "layer   3  Sparsity: 82.9285%\n",
      "total_backward_count 303490 real_backward_count 30762  10.136%\n",
      "fc layer 1 self.abs_max_out: 8341.0\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.645978/  1.791581, val:  79.17%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 83.19 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 91.7479%\n",
      "layer   2  Sparsity: 74.8338%\n",
      "layer   3  Sparsity: 83.0596%\n",
      "total_backward_count 313280 real_backward_count 31400  10.023%\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.639766/  1.778079, val:  80.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.71 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7220%\n",
      "layer   2  Sparsity: 75.1126%\n",
      "layer   3  Sparsity: 83.4136%\n",
      "total_backward_count 323070 real_backward_count 31944   9.888%\n",
      "fc layer 1 self.abs_max_out: 8366.0\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.635616/  1.751996, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.7122%\n",
      "layer   2  Sparsity: 75.0339%\n",
      "layer   3  Sparsity: 83.6040%\n",
      "total_backward_count 332860 real_backward_count 32505   9.765%\n",
      "lif layer 2 self.abs_max_v: 5380.5\n",
      "fc layer 3 self.abs_max_out: 817.0\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.626884/  1.763161, val:  81.67%, val_best:  85.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.7084%\n",
      "layer   2  Sparsity: 74.7678%\n",
      "layer   3  Sparsity: 83.4452%\n",
      "total_backward_count 342650 real_backward_count 33048   9.645%\n",
      "fc layer 3 self.abs_max_out: 822.0\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.641368/  1.767895, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.7100%\n",
      "layer   2  Sparsity: 75.1684%\n",
      "layer   3  Sparsity: 83.4895%\n",
      "total_backward_count 352440 real_backward_count 33616   9.538%\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.637353/  1.757305, val:  83.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.7391%\n",
      "layer   2  Sparsity: 75.0539%\n",
      "layer   3  Sparsity: 83.6722%\n",
      "total_backward_count 362230 real_backward_count 34169   9.433%\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.630431/  1.774944, val:  80.00%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.7082%\n",
      "layer   2  Sparsity: 75.2420%\n",
      "layer   3  Sparsity: 84.2859%\n",
      "total_backward_count 372020 real_backward_count 34658   9.316%\n",
      "fc layer 1 self.abs_max_out: 8371.0\n",
      "fc layer 3 self.abs_max_out: 829.0\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.630116/  1.751701, val:  80.83%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.7426%\n",
      "layer   2  Sparsity: 75.2947%\n",
      "layer   3  Sparsity: 83.8023%\n",
      "total_backward_count 381810 real_backward_count 35190   9.217%\n",
      "fc layer 1 self.abs_max_out: 8387.0\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.621412/  1.758922, val:  86.67%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.7070%\n",
      "layer   2  Sparsity: 74.9386%\n",
      "layer   3  Sparsity: 83.7631%\n",
      "total_backward_count 391600 real_backward_count 35688   9.113%\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.618248/  1.765033, val:  78.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.34 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.7197%\n",
      "layer   2  Sparsity: 74.8969%\n",
      "layer   3  Sparsity: 82.8744%\n",
      "total_backward_count 401390 real_backward_count 36204   9.020%\n",
      "fc layer 3 self.abs_max_out: 846.0\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.609677/  1.732393, val:  79.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.13 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.7546%\n",
      "layer   2  Sparsity: 75.1197%\n",
      "layer   3  Sparsity: 82.9934%\n",
      "total_backward_count 411180 real_backward_count 36747   8.937%\n",
      "fc layer 3 self.abs_max_out: 858.0\n",
      "fc layer 3 self.abs_max_out: 867.0\n",
      "fc layer 3 self.abs_max_out: 893.0\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.598954/  1.745182, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.7265%\n",
      "layer   2  Sparsity: 75.1370%\n",
      "layer   3  Sparsity: 83.1326%\n",
      "total_backward_count 420970 real_backward_count 37253   8.849%\n",
      "fc layer 1 self.abs_max_out: 8420.0\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.612747/  1.749330, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.7000%\n",
      "layer   2  Sparsity: 75.1103%\n",
      "layer   3  Sparsity: 83.2211%\n",
      "total_backward_count 430760 real_backward_count 37730   8.759%\n",
      "fc layer 1 self.abs_max_out: 8449.0\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.604506/  1.728327, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.7487%\n",
      "layer   2  Sparsity: 74.8372%\n",
      "layer   3  Sparsity: 83.5426%\n",
      "total_backward_count 440550 real_backward_count 38202   8.671%\n",
      "fc layer 1 self.abs_max_out: 8457.0\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.587200/  1.701947, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.7103%\n",
      "layer   2  Sparsity: 74.9969%\n",
      "layer   3  Sparsity: 82.9101%\n",
      "total_backward_count 450340 real_backward_count 38678   8.589%\n",
      "lif layer 2 self.abs_max_v: 5608.0\n",
      "lif layer 2 self.abs_max_v: 5623.0\n",
      "lif layer 2 self.abs_max_v: 5734.5\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.581909/  1.710794, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.7145%\n",
      "layer   2  Sparsity: 74.9498%\n",
      "layer   3  Sparsity: 83.1788%\n",
      "total_backward_count 460130 real_backward_count 39089   8.495%\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.568270/  1.714762, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.7113%\n",
      "layer   2  Sparsity: 74.8471%\n",
      "layer   3  Sparsity: 82.8812%\n",
      "total_backward_count 469920 real_backward_count 39484   8.402%\n",
      "fc layer 2 self.abs_max_out: 3969.0\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.559369/  1.691921, val:  87.92%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.85 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.7303%\n",
      "layer   2  Sparsity: 74.9457%\n",
      "layer   3  Sparsity: 83.0901%\n",
      "total_backward_count 479710 real_backward_count 39933   8.324%\n",
      "fc layer 3 self.abs_max_out: 927.0\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.543340/  1.688145, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.7200%\n",
      "layer   2  Sparsity: 74.9589%\n",
      "layer   3  Sparsity: 82.7597%\n",
      "total_backward_count 489500 real_backward_count 40369   8.247%\n",
      "fc layer 1 self.abs_max_out: 8544.0\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.545160/  1.675300, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.7079%\n",
      "layer   2  Sparsity: 74.9258%\n",
      "layer   3  Sparsity: 82.6214%\n",
      "total_backward_count 499290 real_backward_count 40790   8.170%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.543027/  1.695075, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.7442%\n",
      "layer   2  Sparsity: 75.0454%\n",
      "layer   3  Sparsity: 82.9348%\n",
      "total_backward_count 509080 real_backward_count 41181   8.089%\n",
      "fc layer 3 self.abs_max_out: 933.0\n",
      "fc layer 3 self.abs_max_out: 956.0\n",
      "fc layer 3 self.abs_max_out: 958.0\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.534141/  1.688458, val:  81.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.60 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.7051%\n",
      "layer   2  Sparsity: 75.2622%\n",
      "layer   3  Sparsity: 83.1964%\n",
      "total_backward_count 518870 real_backward_count 41567   8.011%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.532923/  1.705850, val:  78.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.7201%\n",
      "layer   2  Sparsity: 75.0446%\n",
      "layer   3  Sparsity: 83.0888%\n",
      "total_backward_count 528660 real_backward_count 41950   7.935%\n",
      "fc layer 3 self.abs_max_out: 965.0\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.554271/  1.710886, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.6783%\n",
      "layer   2  Sparsity: 74.8231%\n",
      "layer   3  Sparsity: 83.1530%\n",
      "total_backward_count 538450 real_backward_count 42393   7.873%\n",
      "fc layer 2 self.abs_max_out: 4031.0\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.549063/  1.692246, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.7238%\n",
      "layer   2  Sparsity: 74.7581%\n",
      "layer   3  Sparsity: 83.1661%\n",
      "total_backward_count 548240 real_backward_count 42779   7.803%\n",
      "fc layer 1 self.abs_max_out: 8546.0\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.550263/  1.691993, val:  80.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.7335%\n",
      "layer   2  Sparsity: 74.8066%\n",
      "layer   3  Sparsity: 82.7995%\n",
      "total_backward_count 558030 real_backward_count 43161   7.735%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.541776/  1.701414, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.7103%\n",
      "layer   2  Sparsity: 75.0307%\n",
      "layer   3  Sparsity: 83.2893%\n",
      "total_backward_count 567820 real_backward_count 43539   7.668%\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.550424/  1.704667, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.7143%\n",
      "layer   2  Sparsity: 74.9683%\n",
      "layer   3  Sparsity: 83.4388%\n",
      "total_backward_count 577610 real_backward_count 43930   7.605%\n",
      "fc layer 2 self.abs_max_out: 4035.0\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.554753/  1.703902, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.49 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.6860%\n",
      "layer   2  Sparsity: 74.9381%\n",
      "layer   3  Sparsity: 83.4160%\n",
      "total_backward_count 587400 real_backward_count 44297   7.541%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.541766/  1.676620, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.6909%\n",
      "layer   2  Sparsity: 75.0382%\n",
      "layer   3  Sparsity: 83.2209%\n",
      "total_backward_count 597190 real_backward_count 44658   7.478%\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.534180/  1.658175, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.7110%\n",
      "layer   2  Sparsity: 75.1569%\n",
      "layer   3  Sparsity: 83.1838%\n",
      "total_backward_count 606980 real_backward_count 45045   7.421%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.523590/  1.676014, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.7057%\n",
      "layer   2  Sparsity: 74.9609%\n",
      "layer   3  Sparsity: 83.7230%\n",
      "total_backward_count 616770 real_backward_count 45394   7.360%\n",
      "fc layer 2 self.abs_max_out: 4066.0\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.544964/  1.687624, val:  82.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.7199%\n",
      "layer   2  Sparsity: 75.0619%\n",
      "layer   3  Sparsity: 83.6471%\n",
      "total_backward_count 626560 real_backward_count 45748   7.301%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.543155/  1.690097, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.7200%\n",
      "layer   2  Sparsity: 74.9125%\n",
      "layer   3  Sparsity: 83.6389%\n",
      "total_backward_count 636350 real_backward_count 46129   7.249%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.545069/  1.690643, val:  80.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.7235%\n",
      "layer   2  Sparsity: 75.1103%\n",
      "layer   3  Sparsity: 83.9885%\n",
      "total_backward_count 646140 real_backward_count 46451   7.189%\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.524304/  1.679744, val:  88.33%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.7077%\n",
      "layer   2  Sparsity: 75.1726%\n",
      "layer   3  Sparsity: 84.0761%\n",
      "total_backward_count 655930 real_backward_count 46792   7.134%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.528640/  1.675920, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.7053%\n",
      "layer   2  Sparsity: 75.0374%\n",
      "layer   3  Sparsity: 84.0691%\n",
      "total_backward_count 665720 real_backward_count 47114   7.077%\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.529390/  1.666394, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.48 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.6956%\n",
      "layer   2  Sparsity: 75.0233%\n",
      "layer   3  Sparsity: 83.8468%\n",
      "total_backward_count 675510 real_backward_count 47440   7.023%\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.527674/  1.686264, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 82.77 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 91.6855%\n",
      "layer   2  Sparsity: 74.9434%\n",
      "layer   3  Sparsity: 84.2587%\n",
      "total_backward_count 685300 real_backward_count 47732   6.965%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.537358/  1.698219, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.51 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 91.7294%\n",
      "layer   2  Sparsity: 74.9326%\n",
      "layer   3  Sparsity: 84.5713%\n",
      "total_backward_count 695090 real_backward_count 48042   6.912%\n",
      "lif layer 2 self.abs_max_v: 5772.0\n",
      "lif layer 2 self.abs_max_v: 5867.0\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.539216/  1.695153, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.19 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 91.7132%\n",
      "layer   2  Sparsity: 74.9809%\n",
      "layer   3  Sparsity: 84.4700%\n",
      "total_backward_count 704880 real_backward_count 48364   6.861%\n",
      "fc layer 2 self.abs_max_out: 4098.0\n",
      "fc layer 2 self.abs_max_out: 4103.0\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.532178/  1.674120, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 82.98 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 91.7051%\n",
      "layer   2  Sparsity: 74.7135%\n",
      "layer   3  Sparsity: 83.8731%\n",
      "total_backward_count 714670 real_backward_count 48674   6.811%\n",
      "fc layer 2 self.abs_max_out: 4193.0\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.509167/  1.659422, val:  84.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7351%\n",
      "layer   2  Sparsity: 74.7025%\n",
      "layer   3  Sparsity: 83.3041%\n",
      "total_backward_count 724460 real_backward_count 48974   6.760%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.510104/  1.668145, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.7065%\n",
      "layer   2  Sparsity: 74.7332%\n",
      "layer   3  Sparsity: 83.3764%\n",
      "total_backward_count 734250 real_backward_count 49285   6.712%\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.496902/  1.643574, val:  86.67%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.6966%\n",
      "layer   2  Sparsity: 74.7894%\n",
      "layer   3  Sparsity: 83.4697%\n",
      "total_backward_count 744040 real_backward_count 49582   6.664%\n",
      "fc layer 3 self.abs_max_out: 973.0\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.496573/  1.653450, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.6949%\n",
      "layer   2  Sparsity: 74.5938%\n",
      "layer   3  Sparsity: 83.6431%\n",
      "total_backward_count 753830 real_backward_count 49862   6.614%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.486060/  1.663855, val:  80.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.7438%\n",
      "layer   2  Sparsity: 74.4631%\n",
      "layer   3  Sparsity: 83.2249%\n",
      "total_backward_count 763620 real_backward_count 50157   6.568%\n",
      "fc layer 1 self.abs_max_out: 8559.0\n",
      "lif layer 2 self.abs_max_v: 6022.0\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.494354/  1.663873, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.56 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.6957%\n",
      "layer   2  Sparsity: 74.7018%\n",
      "layer   3  Sparsity: 83.7100%\n",
      "total_backward_count 773410 real_backward_count 50438   6.522%\n",
      "fc layer 3 self.abs_max_out: 1009.0\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.485829/  1.642119, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.57 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.7142%\n",
      "layer   2  Sparsity: 75.0487%\n",
      "layer   3  Sparsity: 83.7575%\n",
      "total_backward_count 783200 real_backward_count 50736   6.478%\n",
      "fc layer 1 self.abs_max_out: 8571.0\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.502292/  1.655306, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.7042%\n",
      "layer   2  Sparsity: 75.1120%\n",
      "layer   3  Sparsity: 84.0628%\n",
      "total_backward_count 792990 real_backward_count 51028   6.435%\n",
      "fc layer 1 self.abs_max_out: 8585.0\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.501089/  1.655756, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 82.07 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 91.7295%\n",
      "layer   2  Sparsity: 74.9432%\n",
      "layer   3  Sparsity: 83.7507%\n",
      "total_backward_count 802780 real_backward_count 51314   6.392%\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  1.493075/  1.641464, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 82.50 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 91.7335%\n",
      "layer   2  Sparsity: 75.0008%\n",
      "layer   3  Sparsity: 83.9134%\n",
      "total_backward_count 812570 real_backward_count 51601   6.350%\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        # \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [5, 10,]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.5, 0.25, 0.125, 0.0625]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [4.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "        \"learning_rate\": {\"values\": [1/128, 1/256, 1/512, 1/1024]}, \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [5, 10, 15, 20, 25, 30]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [12_000, 25_000, 50_000, 75_000, 100_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [-1]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [-11,-10,-9]},\n",
    "        # \"scale_exp_1w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        # \"scale_exp_2w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        # \"scale_exp_3w\": {\"values\": [-9]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"4\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w + 1,wandb.config.scale_exp_1w + 1]],\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'pyz704uj'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
