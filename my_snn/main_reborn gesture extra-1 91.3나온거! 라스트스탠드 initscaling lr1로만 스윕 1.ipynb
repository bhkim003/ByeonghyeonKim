{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31082/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    lif_layer_sg_width2 = None,\n",
    "                    lif_layer_v_threshold2 = None,\n",
    "                    learning_rate2 = None,\n",
    "                    init_scaling = None,\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp,\n",
    "                    ANPI_MODE=False,\n",
    "                    lif_layer_sg_width2=lif_layer_sg_width2,\n",
    "                    lif_layer_v_threshold2=lif_layer_v_threshold2,\n",
    "                    init_scaling=init_scaling).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                # lr = group['lr']\n",
    "\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        lr = learning_rate\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        lr = learning_rate2\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        lr = 1.0\n",
    "\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "                now_T = inputs.shape[1]\n",
    "                now_time_steps = temporal_filter*TIME\n",
    "                # start_idx = random.randint(0, now_T - now_time_steps)\n",
    "                start_idx = random.choice(range(0, now_T - now_time_steps + 1, now_time_steps))\n",
    "                # start_idx = random.choice([i for i in range(0, now_T - now_time_steps + 1, now_time_steps)])\n",
    "                inputs = inputs[:, start_idx : start_idx + now_time_steps]\n",
    "                if dvs_clipping != 0:\n",
    "                    inputs[inputs<dvs_clipping] = 0.0\n",
    "                    inputs[inputs>=dvs_clipping] = 1.0\n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            now_T = inputs_val.shape[1]\n",
    "                            now_time_steps = temporal_filter*TIME\n",
    "                            start_idx = 0\n",
    "                            inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if dvs_clipping != 0:\n",
    "                                inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "\n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    assert val_acc_best > 0.2\n",
    "                elif epoch > 10:\n",
    "                    assert val_acc_best > 0.4\n",
    "                elif epoch > 30:\n",
    "                    assert val_acc_best > 0.5\n",
    "                elif epoch > 100:\n",
    "                    assert val_acc_best > 0.6\n",
    "                    \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"5\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 2871,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 8, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 200,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = -1, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[0,0],[0,0],[0,0]], \n",
    "#                 lif_layer_sg_width2 = 4.0,\n",
    "#                 lif_layer_v_threshold2 = 8,\n",
    "#                 learning_rate2 = 8,\n",
    "#                 init_scaling = [1/2,1/2,1/2],\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: liga4sas with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.046875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0234375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_010835-liga4sas</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/liga4sas' target=\"_blank\">dauntless-sweep-2</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/liga4sas' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/liga4sas</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': '20251214_010844_185', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 32, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 1, 'lif_layer_v_threshold2': 128, 'init_scaling': [0.03125, 0.046875, 0.0234375], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 2, self.v_threshold 32\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 1, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=32, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 46.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 6.0\n",
      "lif layer 2 self.abs_max_v: 6.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 59.5\n",
      "fc layer 2 self.abs_max_out: 36.0\n",
      "lif layer 2 self.abs_max_v: 35.5\n",
      "fc layer 1 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 69.0\n",
      "fc layer 2 self.abs_max_out: 48.0\n",
      "lif layer 2 self.abs_max_v: 53.0\n",
      "fc layer 1 self.abs_max_out: 62.0\n",
      "lif layer 1 self.abs_max_v: 90.0\n",
      "lif layer 2 self.abs_max_v: 59.5\n",
      "fc layer 1 self.abs_max_out: 101.0\n",
      "lif layer 1 self.abs_max_v: 138.0\n",
      "fc layer 2 self.abs_max_out: 54.0\n",
      "lif layer 2 self.abs_max_v: 72.0\n",
      "fc layer 1 self.abs_max_out: 147.0\n",
      "lif layer 1 self.abs_max_v: 210.0\n",
      "fc layer 2 self.abs_max_out: 76.0\n",
      "lif layer 2 self.abs_max_v: 112.0\n",
      "fc layer 1 self.abs_max_out: 200.0\n",
      "lif layer 1 self.abs_max_v: 270.0\n",
      "fc layer 2 self.abs_max_out: 105.0\n",
      "lif layer 2 self.abs_max_v: 161.0\n",
      "fc layer 1 self.abs_max_out: 225.0\n",
      "lif layer 1 self.abs_max_v: 360.0\n",
      "fc layer 2 self.abs_max_out: 121.0\n",
      "lif layer 2 self.abs_max_v: 201.5\n",
      "fc layer 3 self.abs_max_out: 9.0\n",
      "fc layer 1 self.abs_max_out: 277.0\n",
      "lif layer 1 self.abs_max_v: 424.0\n",
      "fc layer 2 self.abs_max_out: 140.0\n",
      "lif layer 2 self.abs_max_v: 226.0\n",
      "fc layer 3 self.abs_max_out: 11.0\n",
      "fc layer 1 self.abs_max_out: 297.0\n",
      "fc layer 2 self.abs_max_out: 152.0\n",
      "fc layer 3 self.abs_max_out: 14.0\n",
      "fc layer 2 self.abs_max_out: 165.0\n",
      "lif layer 2 self.abs_max_v: 228.0\n",
      "fc layer 3 self.abs_max_out: 16.0\n",
      "fc layer 2 self.abs_max_out: 200.0\n",
      "lif layer 2 self.abs_max_v: 281.0\n",
      "fc layer 3 self.abs_max_out: 21.0\n",
      "fc layer 1 self.abs_max_out: 386.0\n",
      "lif layer 1 self.abs_max_v: 529.0\n",
      "lif layer 2 self.abs_max_v: 309.0\n",
      "fc layer 1 self.abs_max_out: 504.0\n",
      "lif layer 1 self.abs_max_v: 689.5\n",
      "fc layer 2 self.abs_max_out: 202.0\n",
      "lif layer 2 self.abs_max_v: 328.5\n",
      "lif layer 1 self.abs_max_v: 695.0\n",
      "fc layer 2 self.abs_max_out: 244.0\n",
      "lif layer 2 self.abs_max_v: 341.5\n",
      "lif layer 2 self.abs_max_v: 349.0\n",
      "lif layer 2 self.abs_max_v: 365.5\n",
      "fc layer 3 self.abs_max_out: 29.0\n",
      "fc layer 3 self.abs_max_out: 31.0\n",
      "lif layer 2 self.abs_max_v: 391.5\n",
      "fc layer 3 self.abs_max_out: 36.0\n",
      "fc layer 1 self.abs_max_out: 507.0\n",
      "fc layer 1 self.abs_max_out: 595.0\n",
      "fc layer 2 self.abs_max_out: 280.0\n",
      "fc layer 3 self.abs_max_out: 46.0\n",
      "fc layer 3 self.abs_max_out: 49.0\n",
      "fc layer 2 self.abs_max_out: 287.0\n",
      "fc layer 2 self.abs_max_out: 321.0\n",
      "lif layer 2 self.abs_max_v: 412.0\n",
      "lif layer 2 self.abs_max_v: 416.5\n",
      "lif layer 2 self.abs_max_v: 430.5\n",
      "lif layer 1 self.abs_max_v: 721.0\n",
      "lif layer 2 self.abs_max_v: 445.5\n",
      "lif layer 2 self.abs_max_v: 497.5\n",
      "lif layer 1 self.abs_max_v: 786.0\n",
      "lif layer 2 self.abs_max_v: 509.5\n",
      "lif layer 2 self.abs_max_v: 547.5\n",
      "fc layer 3 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 799.5\n",
      "fc layer 1 self.abs_max_out: 687.0\n",
      "lif layer 1 self.abs_max_v: 1087.0\n",
      "lif layer 1 self.abs_max_v: 1207.5\n",
      "lif layer 2 self.abs_max_v: 561.5\n",
      "fc layer 1 self.abs_max_out: 688.0\n",
      "lif layer 1 self.abs_max_v: 1287.5\n",
      "fc layer 2 self.abs_max_out: 339.0\n",
      "lif layer 2 self.abs_max_v: 617.0\n",
      "fc layer 1 self.abs_max_out: 749.0\n",
      "lif layer 1 self.abs_max_v: 1369.0\n",
      "fc layer 2 self.abs_max_out: 362.0\n",
      "lif layer 2 self.abs_max_v: 670.5\n",
      "lif layer 2 self.abs_max_v: 680.5\n",
      "fc layer 1 self.abs_max_out: 779.0\n",
      "lif layer 2 self.abs_max_v: 688.0\n",
      "fc layer 3 self.abs_max_out: 52.0\n",
      "fc layer 2 self.abs_max_out: 365.0\n",
      "lif layer 2 self.abs_max_v: 709.0\n",
      "fc layer 3 self.abs_max_out: 54.0\n",
      "fc layer 2 self.abs_max_out: 407.0\n",
      "fc layer 2 self.abs_max_out: 414.0\n",
      "fc layer 3 self.abs_max_out: 69.0\n",
      "fc layer 2 self.abs_max_out: 429.0\n",
      "fc layer 1 self.abs_max_out: 825.0\n",
      "fc layer 3 self.abs_max_out: 80.0\n",
      "fc layer 2 self.abs_max_out: 504.0\n",
      "lif layer 1 self.abs_max_v: 1406.0\n",
      "fc layer 2 self.abs_max_out: 507.0\n",
      "fc layer 3 self.abs_max_out: 108.0\n",
      "fc layer 3 self.abs_max_out: 109.0\n",
      "lif layer 2 self.abs_max_v: 739.5\n",
      "lif layer 2 self.abs_max_v: 752.5\n",
      "lif layer 2 self.abs_max_v: 755.5\n",
      "fc layer 1 self.abs_max_out: 829.0\n",
      "fc layer 1 self.abs_max_out: 859.0\n",
      "fc layer 1 self.abs_max_out: 1007.0\n",
      "fc layer 3 self.abs_max_out: 111.0\n",
      "fc layer 2 self.abs_max_out: 509.0\n",
      "fc layer 2 self.abs_max_out: 519.0\n",
      "lif layer 2 self.abs_max_v: 765.5\n",
      "lif layer 2 self.abs_max_v: 773.0\n",
      "lif layer 2 self.abs_max_v: 775.5\n",
      "lif layer 2 self.abs_max_v: 787.0\n",
      "lif layer 2 self.abs_max_v: 832.5\n",
      "fc layer 3 self.abs_max_out: 130.0\n",
      "lif layer 2 self.abs_max_v: 893.0\n",
      "fc layer 2 self.abs_max_out: 526.0\n",
      "fc layer 2 self.abs_max_out: 551.0\n",
      "lif layer 2 self.abs_max_v: 906.0\n",
      "lif layer 2 self.abs_max_v: 985.0\n",
      "fc layer 3 self.abs_max_out: 164.0\n",
      "fc layer 2 self.abs_max_out: 571.0\n",
      "fc layer 2 self.abs_max_out: 602.0\n",
      "fc layer 1 self.abs_max_out: 1161.0\n",
      "lif layer 1 self.abs_max_v: 1477.5\n",
      "fc layer 1 self.abs_max_out: 1222.0\n",
      "lif layer 1 self.abs_max_v: 1961.0\n",
      "lif layer 1 self.abs_max_v: 1999.5\n",
      "lif layer 2 self.abs_max_v: 1008.0\n",
      "fc layer 2 self.abs_max_out: 633.0\n",
      "fc layer 2 self.abs_max_out: 653.0\n",
      "fc layer 3 self.abs_max_out: 166.0\n",
      "lif layer 2 self.abs_max_v: 1009.0\n",
      "fc layer 2 self.abs_max_out: 670.0\n",
      "fc layer 2 self.abs_max_out: 695.0\n",
      "fc layer 1 self.abs_max_out: 1322.0\n",
      "lif layer 1 self.abs_max_v: 2229.5\n",
      "lif layer 1 self.abs_max_v: 2304.5\n",
      "lif layer 1 self.abs_max_v: 2372.5\n",
      "fc layer 1 self.abs_max_out: 1391.0\n",
      "lif layer 2 self.abs_max_v: 1084.0\n",
      "lif layer 2 self.abs_max_v: 1090.0\n",
      "lif layer 2 self.abs_max_v: 1093.5\n",
      "lif layer 2 self.abs_max_v: 1117.5\n",
      "fc layer 3 self.abs_max_out: 169.0\n",
      "lif layer 2 self.abs_max_v: 1120.0\n",
      "lif layer 2 self.abs_max_v: 1210.0\n",
      "lif layer 2 self.abs_max_v: 1240.5\n",
      "lif layer 2 self.abs_max_v: 1280.5\n",
      "fc layer 2 self.abs_max_out: 704.0\n",
      "lif layer 2 self.abs_max_v: 1299.0\n",
      "fc layer 2 self.abs_max_out: 710.0\n",
      "fc layer 2 self.abs_max_out: 727.0\n",
      "fc layer 2 self.abs_max_out: 733.0\n",
      "fc layer 2 self.abs_max_out: 741.0\n",
      "fc layer 3 self.abs_max_out: 173.0\n",
      "fc layer 3 self.abs_max_out: 201.0\n",
      "fc layer 3 self.abs_max_out: 202.0\n",
      "fc layer 2 self.abs_max_out: 789.0\n",
      "fc layer 1 self.abs_max_out: 1392.0\n",
      "fc layer 1 self.abs_max_out: 1487.0\n",
      "fc layer 1 self.abs_max_out: 1497.0\n",
      "fc layer 3 self.abs_max_out: 205.0\n",
      "fc layer 2 self.abs_max_out: 794.0\n",
      "fc layer 1 self.abs_max_out: 1513.0\n",
      "fc layer 2 self.abs_max_out: 801.0\n",
      "fc layer 2 self.abs_max_out: 823.0\n",
      "fc layer 2 self.abs_max_out: 855.0\n",
      "lif layer 2 self.abs_max_v: 1318.5\n",
      "lif layer 2 self.abs_max_v: 1377.0\n",
      "lif layer 2 self.abs_max_v: 1407.5\n",
      "lif layer 2 self.abs_max_v: 1420.5\n",
      "lif layer 2 self.abs_max_v: 1429.5\n",
      "lif layer 2 self.abs_max_v: 1524.5\n",
      "lif layer 2 self.abs_max_v: 1562.5\n",
      "lif layer 2 self.abs_max_v: 1586.5\n",
      "fc layer 3 self.abs_max_out: 217.0\n",
      "fc layer 1 self.abs_max_out: 1643.0\n",
      "fc layer 2 self.abs_max_out: 964.0\n",
      "fc layer 2 self.abs_max_out: 980.0\n",
      "fc layer 3 self.abs_max_out: 229.0\n",
      "fc layer 2 self.abs_max_out: 1010.0\n",
      "fc layer 1 self.abs_max_out: 1747.0\n",
      "fc layer 2 self.abs_max_out: 1030.0\n",
      "fc layer 2 self.abs_max_out: 1165.0\n",
      "fc layer 2 self.abs_max_out: 1188.0\n",
      "fc layer 1 self.abs_max_out: 1795.0\n",
      "fc layer 3 self.abs_max_out: 231.0\n",
      "lif layer 1 self.abs_max_v: 2390.5\n",
      "lif layer 1 self.abs_max_v: 2689.5\n",
      "fc layer 1 self.abs_max_out: 2004.0\n",
      "lif layer 1 self.abs_max_v: 2862.0\n",
      "lif layer 1 self.abs_max_v: 2982.0\n",
      "fc layer 2 self.abs_max_out: 1195.0\n",
      "fc layer 3 self.abs_max_out: 236.0\n",
      "fc layer 3 self.abs_max_out: 237.0\n",
      "fc layer 3 self.abs_max_out: 252.0\n",
      "fc layer 3 self.abs_max_out: 253.0\n",
      "lif layer 2 self.abs_max_v: 1607.5\n",
      "lif layer 2 self.abs_max_v: 1670.0\n",
      "lif layer 2 self.abs_max_v: 1701.0\n",
      "lif layer 2 self.abs_max_v: 1703.5\n",
      "lif layer 2 self.abs_max_v: 1718.5\n",
      "lif layer 2 self.abs_max_v: 1726.0\n",
      "fc layer 3 self.abs_max_out: 264.0\n",
      "fc layer 3 self.abs_max_out: 267.0\n",
      "fc layer 3 self.abs_max_out: 268.0\n",
      "fc layer 3 self.abs_max_out: 296.0\n",
      "lif layer 1 self.abs_max_v: 2997.0\n",
      "lif layer 1 self.abs_max_v: 3152.0\n",
      "fc layer 2 self.abs_max_out: 1200.0\n",
      "fc layer 3 self.abs_max_out: 312.0\n",
      "fc layer 2 self.abs_max_out: 1231.0\n",
      "fc layer 2 self.abs_max_out: 1330.0\n",
      "fc layer 2 self.abs_max_out: 1378.0\n",
      "fc layer 2 self.abs_max_out: 1499.0\n",
      "fc layer 2 self.abs_max_out: 1571.0\n",
      "fc layer 2 self.abs_max_out: 1585.0\n",
      "fc layer 2 self.abs_max_out: 1600.0\n",
      "lif layer 2 self.abs_max_v: 1746.5\n",
      "lif layer 2 self.abs_max_v: 1776.0\n",
      "lif layer 2 self.abs_max_v: 1797.0\n",
      "lif layer 1 self.abs_max_v: 3168.0\n",
      "lif layer 1 self.abs_max_v: 3216.5\n",
      "lif layer 1 self.abs_max_v: 3340.5\n",
      "fc layer 1 self.abs_max_out: 2103.0\n",
      "lif layer 1 self.abs_max_v: 3526.0\n",
      "lif layer 2 self.abs_max_v: 1812.0\n",
      "lif layer 2 self.abs_max_v: 1859.0\n",
      "lif layer 2 self.abs_max_v: 1882.5\n",
      "lif layer 2 self.abs_max_v: 1894.5\n",
      "lif layer 2 self.abs_max_v: 1900.5\n",
      "lif layer 2 self.abs_max_v: 1903.5\n",
      "fc layer 2 self.abs_max_out: 1638.0\n",
      "lif layer 1 self.abs_max_v: 3592.0\n",
      "fc layer 1 self.abs_max_out: 2141.0\n",
      "lif layer 1 self.abs_max_v: 3840.5\n",
      "fc layer 1 self.abs_max_out: 2173.0\n",
      "lif layer 1 self.abs_max_v: 3994.0\n",
      "lif layer 1 self.abs_max_v: 4121.0\n",
      "lif layer 1 self.abs_max_v: 4136.5\n",
      "fc layer 1 self.abs_max_out: 2199.0\n",
      "fc layer 1 self.abs_max_out: 2428.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss:  9.330625/ 57.628151, val:  35.83%, val_best:  35.83%, tr:  99.08%, tr_best:  99.08%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.9046%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3389%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 1734  17.712%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1687.0\n",
      "fc layer 2 self.abs_max_out: 1737.0\n",
      "fc layer 2 self.abs_max_out: 1783.0\n",
      "lif layer 2 self.abs_max_v: 1904.0\n",
      "fc layer 2 self.abs_max_out: 1815.0\n",
      "lif layer 2 self.abs_max_v: 1908.5\n",
      "lif layer 2 self.abs_max_v: 1925.5\n",
      "lif layer 2 self.abs_max_v: 1927.0\n",
      "lif layer 2 self.abs_max_v: 1934.5\n",
      "fc layer 2 self.abs_max_out: 1936.0\n",
      "lif layer 2 self.abs_max_v: 1936.0\n",
      "lif layer 2 self.abs_max_v: 1953.5\n",
      "lif layer 2 self.abs_max_v: 1972.0\n",
      "lif layer 2 self.abs_max_v: 2050.0\n",
      "fc layer 3 self.abs_max_out: 313.0\n",
      "fc layer 3 self.abs_max_out: 316.0\n",
      "lif layer 2 self.abs_max_v: 2119.5\n",
      "lif layer 2 self.abs_max_v: 2168.0\n",
      "fc layer 2 self.abs_max_out: 1943.0\n",
      "fc layer 2 self.abs_max_out: 1944.0\n",
      "fc layer 2 self.abs_max_out: 1984.0\n",
      "fc layer 2 self.abs_max_out: 2113.0\n",
      "fc layer 2 self.abs_max_out: 2135.0\n",
      "lif layer 2 self.abs_max_v: 2186.0\n",
      "lif layer 2 self.abs_max_v: 2198.5\n",
      "fc layer 2 self.abs_max_out: 2153.0\n",
      "lif layer 2 self.abs_max_v: 2260.0\n",
      "lif layer 2 self.abs_max_v: 2365.0\n",
      "lif layer 2 self.abs_max_v: 2555.5\n",
      "lif layer 2 self.abs_max_v: 2651.0\n",
      "fc layer 3 self.abs_max_out: 324.0\n",
      "lif layer 2 self.abs_max_v: 2681.5\n",
      "fc layer 3 self.abs_max_out: 358.0\n",
      "fc layer 3 self.abs_max_out: 367.0\n",
      "lif layer 2 self.abs_max_v: 2687.0\n",
      "lif layer 2 self.abs_max_v: 2735.5\n",
      "fc layer 2 self.abs_max_out: 2170.0\n",
      "lif layer 2 self.abs_max_v: 2804.0\n",
      "lif layer 2 self.abs_max_v: 2805.0\n",
      "lif layer 2 self.abs_max_v: 2849.5\n",
      "lif layer 2 self.abs_max_v: 2872.0\n",
      "lif layer 2 self.abs_max_v: 2877.0\n",
      "lif layer 2 self.abs_max_v: 2902.5\n",
      "lif layer 2 self.abs_max_v: 2915.5\n",
      "fc layer 2 self.abs_max_out: 2254.0\n",
      "fc layer 3 self.abs_max_out: 395.0\n",
      "fc layer 1 self.abs_max_out: 2617.0\n",
      "fc layer 1 self.abs_max_out: 2988.0\n",
      "lif layer 1 self.abs_max_v: 4296.5\n",
      "lif layer 1 self.abs_max_v: 4826.5\n",
      "lif layer 1 self.abs_max_v: 5153.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 10.573953/ 58.210800, val:  32.50%, val_best:  35.83%, tr:  98.37%, tr_best:  99.08%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7350%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7970%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3286  16.782%\n",
      "lif layer 2 self.abs_max_v: 2925.0\n",
      "fc layer 2 self.abs_max_out: 2260.0\n",
      "fc layer 2 self.abs_max_out: 2275.0\n",
      "fc layer 2 self.abs_max_out: 2287.0\n",
      "lif layer 2 self.abs_max_v: 3016.0\n",
      "lif layer 2 self.abs_max_v: 3063.0\n",
      "fc layer 2 self.abs_max_out: 2292.0\n",
      "fc layer 2 self.abs_max_out: 2308.0\n",
      "fc layer 2 self.abs_max_out: 2466.0\n",
      "fc layer 2 self.abs_max_out: 2497.0\n",
      "lif layer 2 self.abs_max_v: 3108.0\n",
      "lif layer 2 self.abs_max_v: 3224.5\n",
      "lif layer 2 self.abs_max_v: 3249.5\n",
      "lif layer 2 self.abs_max_v: 3344.0\n",
      "lif layer 2 self.abs_max_v: 3420.0\n",
      "fc layer 2 self.abs_max_out: 2569.0\n",
      "fc layer 1 self.abs_max_out: 3114.0\n",
      "fc layer 1 self.abs_max_out: 3534.0\n",
      "lif layer 1 self.abs_max_v: 5832.5\n",
      "lif layer 1 self.abs_max_v: 6269.5\n",
      "lif layer 2 self.abs_max_v: 3474.0\n",
      "lif layer 2 self.abs_max_v: 3566.0\n",
      "lif layer 2 self.abs_max_v: 3612.0\n",
      "lif layer 2 self.abs_max_v: 3635.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 10.020522/ 64.205688, val:  39.17%, val_best:  39.17%, tr:  98.16%, tr_best:  99.08%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1267%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2622%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 4771  16.244%\n",
      "lif layer 2 self.abs_max_v: 3668.5\n",
      "lif layer 2 self.abs_max_v: 3886.0\n",
      "lif layer 2 self.abs_max_v: 3959.5\n",
      "lif layer 2 self.abs_max_v: 4165.0\n",
      "lif layer 2 self.abs_max_v: 4424.5\n",
      "lif layer 2 self.abs_max_v: 4592.5\n",
      "lif layer 2 self.abs_max_v: 4676.5\n",
      "lif layer 2 self.abs_max_v: 4718.5\n",
      "lif layer 2 self.abs_max_v: 4739.5\n",
      "lif layer 2 self.abs_max_v: 4750.0\n",
      "fc layer 2 self.abs_max_out: 2576.0\n",
      "fc layer 2 self.abs_max_out: 2578.0\n",
      "fc layer 2 self.abs_max_out: 2649.0\n",
      "fc layer 2 self.abs_max_out: 2733.0\n",
      "fc layer 2 self.abs_max_out: 2777.0\n",
      "fc layer 2 self.abs_max_out: 2815.0\n",
      "lif layer 2 self.abs_max_v: 4921.5\n",
      "fc layer 1 self.abs_max_out: 3696.0\n",
      "lif layer 1 self.abs_max_v: 6592.0\n",
      "fc layer 1 self.abs_max_out: 3724.0\n",
      "lif layer 1 self.abs_max_v: 6611.5\n",
      "lif layer 2 self.abs_max_v: 5047.0\n",
      "lif layer 2 self.abs_max_v: 5079.5\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  9.552220/ 69.180252, val:  31.25%, val_best:  39.17%, tr:  98.47%, tr_best:  99.08%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5680%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5304%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6301  16.090%\n",
      "fc layer 2 self.abs_max_out: 2821.0\n",
      "lif layer 2 self.abs_max_v: 5148.0\n",
      "lif layer 2 self.abs_max_v: 5369.0\n",
      "lif layer 2 self.abs_max_v: 5505.5\n",
      "lif layer 2 self.abs_max_v: 5574.0\n",
      "lif layer 2 self.abs_max_v: 5608.0\n",
      "fc layer 2 self.abs_max_out: 2949.0\n",
      "lif layer 2 self.abs_max_v: 5618.5\n",
      "fc layer 2 self.abs_max_out: 2979.0\n",
      "lif layer 2 self.abs_max_v: 5788.5\n",
      "fc layer 1 self.abs_max_out: 3843.0\n",
      "fc layer 2 self.abs_max_out: 3008.0\n",
      "fc layer 2 self.abs_max_out: 3097.0\n",
      "lif layer 2 self.abs_max_v: 5841.5\n",
      "lif layer 2 self.abs_max_v: 5882.0\n",
      "lif layer 2 self.abs_max_v: 5894.0\n",
      "lif layer 2 self.abs_max_v: 5991.0\n",
      "fc layer 2 self.abs_max_out: 3155.0\n",
      "lif layer 1 self.abs_max_v: 6757.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  8.423879/ 43.143578, val:  35.83%, val_best:  39.17%, tr:  98.37%, tr_best:  99.08%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3097%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.0276%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 7724  15.779%\n",
      "fc layer 2 self.abs_max_out: 3187.0\n",
      "fc layer 2 self.abs_max_out: 3273.0\n",
      "lif layer 2 self.abs_max_v: 6076.5\n",
      "lif layer 2 self.abs_max_v: 6144.5\n",
      "fc layer 2 self.abs_max_out: 3646.0\n",
      "fc layer 1 self.abs_max_out: 4676.0\n",
      "lif layer 1 self.abs_max_v: 7163.5\n",
      "lif layer 1 self.abs_max_v: 7479.5\n",
      "lif layer 1 self.abs_max_v: 7942.5\n",
      "lif layer 2 self.abs_max_v: 6446.5\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  7.917520/ 50.876240, val:  30.42%, val_best:  39.17%, tr:  97.96%, tr_best:  99.08%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.1637%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.9616%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9165  15.603%\n",
      "lif layer 2 self.abs_max_v: 6478.0\n",
      "lif layer 2 self.abs_max_v: 6537.0\n",
      "lif layer 2 self.abs_max_v: 6542.5\n",
      "lif layer 2 self.abs_max_v: 6695.5\n",
      "lif layer 2 self.abs_max_v: 6939.0\n",
      "lif layer 2 self.abs_max_v: 7060.5\n",
      "lif layer 2 self.abs_max_v: 7091.5\n",
      "fc layer 2 self.abs_max_out: 3662.0\n",
      "fc layer 2 self.abs_max_out: 3664.0\n",
      "fc layer 2 self.abs_max_out: 3718.0\n",
      "lif layer 2 self.abs_max_v: 7235.0\n",
      "fc layer 2 self.abs_max_out: 3755.0\n",
      "fc layer 2 self.abs_max_out: 4086.0\n",
      "fc layer 2 self.abs_max_out: 4197.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  7.563658/ 44.919388, val:  40.00%, val_best:  40.00%, tr:  98.37%, tr_best:  99.08%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6518%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.7151%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10609  15.481%\n",
      "lif layer 2 self.abs_max_v: 7243.5\n",
      "lif layer 1 self.abs_max_v: 8107.5\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  7.178793/ 37.599998, val:  42.50%, val_best:  42.50%, tr:  97.65%, tr_best:  99.08%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.1230%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.1793%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 11939  15.244%\n",
      "lif layer 2 self.abs_max_v: 7547.5\n",
      "lif layer 2 self.abs_max_v: 7719.0\n",
      "fc layer 1 self.abs_max_out: 4971.0\n",
      "fc layer 1 self.abs_max_out: 5222.0\n",
      "lif layer 1 self.abs_max_v: 8681.5\n",
      "lif layer 1 self.abs_max_v: 9104.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  7.224070/ 54.776264, val:  40.00%, val_best:  42.50%, tr:  98.16%, tr_best:  99.08%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7771%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.5586%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 13321  15.119%\n",
      "lif layer 2 self.abs_max_v: 7884.5\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  7.269179/ 45.003441, val:  35.83%, val_best:  42.50%, tr:  97.85%, tr_best:  99.08%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6183%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.4084%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 14700  15.015%\n",
      "fc layer 2 self.abs_max_out: 4211.0\n",
      "fc layer 2 self.abs_max_out: 4288.0\n",
      "fc layer 2 self.abs_max_out: 4431.0\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  6.923165/ 40.764488, val:  37.08%, val_best:  42.50%, tr:  97.65%, tr_best:  99.08%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3733%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.2269%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 16075  14.927%\n",
      "lif layer 2 self.abs_max_v: 8089.5\n",
      "lif layer 2 self.abs_max_v: 8121.0\n",
      "fc layer 1 self.abs_max_out: 5451.0\n",
      "lif layer 1 self.abs_max_v: 9374.5\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  7.066158/ 46.471573, val:  42.50%, val_best:  42.50%, tr:  98.06%, tr_best:  99.08%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3555%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.7125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 17499  14.895%\n",
      "lif layer 1 self.abs_max_v: 9414.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  6.590438/ 34.954075, val:  36.25%, val_best:  42.50%, tr:  97.96%, tr_best:  99.08%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8063%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.1428%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 18860  14.819%\n",
      "lif layer 1 self.abs_max_v: 9729.5\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  6.848401/ 63.023067, val:  35.83%, val_best:  42.50%, tr:  98.16%, tr_best:  99.08%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0965%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.2049%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 20276  14.794%\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  6.557399/ 40.604950, val:  35.83%, val_best:  42.50%, tr:  97.75%, tr_best:  99.08%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8206%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.3777%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 21642  14.737%\n",
      "fc layer 2 self.abs_max_out: 4482.0\n",
      "lif layer 2 self.abs_max_v: 8262.0\n",
      "fc layer 1 self.abs_max_out: 5466.0\n",
      "fc layer 1 self.abs_max_out: 5689.0\n",
      "lif layer 1 self.abs_max_v: 10261.5\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  6.672442/ 41.162914, val:  35.83%, val_best:  42.50%, tr:  98.26%, tr_best:  99.08%, epoch time: 73.38 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1828%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.8698%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 23053  14.717%\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  6.445009/ 37.499737, val:  43.75%, val_best:  43.75%, tr:  97.65%, tr_best:  99.08%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8607%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.7076%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 24410  14.667%\n",
      "fc layer 1 self.abs_max_out: 5887.0\n",
      "lif layer 1 self.abs_max_v: 10589.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  6.566326/ 28.560289, val:  50.00%, val_best:  50.00%, tr:  98.57%, tr_best:  99.08%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9534%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.1527%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 25823  14.654%\n",
      "fc layer 2 self.abs_max_out: 4544.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  6.976124/ 51.336678, val:  35.83%, val_best:  50.00%, tr:  98.16%, tr_best:  99.08%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1115%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.9739%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 27258  14.654%\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  6.496148/ 55.090229, val:  32.08%, val_best:  50.00%, tr:  98.77%, tr_best:  99.08%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0234%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.5497%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 28605  14.609%\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  6.481365/ 52.260990, val:  38.33%, val_best:  50.00%, tr:  98.57%, tr_best:  99.08%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7079%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.0448%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 30003  14.594%\n",
      "fc layer 2 self.abs_max_out: 4612.0\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  6.803692/ 40.151936, val:  44.17%, val_best:  50.00%, tr:  99.08%, tr_best:  99.08%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3735%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.0533%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 31430  14.593%\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  6.391627/ 41.478760, val:  42.08%, val_best:  50.00%, tr:  98.88%, tr_best:  99.08%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4006%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.1581%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 32805  14.569%\n",
      "fc layer 1 self.abs_max_out: 5920.0\n",
      "lif layer 1 self.abs_max_v: 10639.5\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  6.468401/ 31.331781, val:  50.83%, val_best:  50.83%, tr:  98.77%, tr_best:  99.08%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6398%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.4476%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 34171  14.543%\n",
      "fc layer 1 self.abs_max_out: 6111.0\n",
      "lif layer 1 self.abs_max_v: 11037.5\n",
      "fc layer 2 self.abs_max_out: 4724.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  6.479379/ 39.492981, val:  38.75%, val_best:  50.83%, tr:  98.06%, tr_best:  99.08%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7541%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.9612%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 35555  14.527%\n",
      "fc layer 1 self.abs_max_out: 6267.0\n",
      "lif layer 1 self.abs_max_v: 11287.5\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  6.424814/ 32.194828, val:  42.92%, val_best:  50.83%, tr:  98.77%, tr_best:  99.08%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4907%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.8134%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 36986  14.531%\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  6.502487/ 22.763765, val:  48.75%, val_best:  50.83%, tr:  98.77%, tr_best:  99.08%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5726%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.8639%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 38368  14.515%\n",
      "lif layer 2 self.abs_max_v: 8289.5\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  6.548391/ 30.590986, val:  41.25%, val_best:  50.83%, tr:  98.98%, tr_best:  99.08%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4061%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.8770%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 39772  14.509%\n",
      "lif layer 1 self.abs_max_v: 11342.0\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  6.318634/ 54.769878, val:  34.58%, val_best:  50.83%, tr:  98.88%, tr_best:  99.08%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5157%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.2501%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 41124  14.485%\n",
      "lif layer 2 self.abs_max_v: 8401.0\n",
      "fc layer 1 self.abs_max_out: 6272.0\n",
      "lif layer 1 self.abs_max_v: 11369.0\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  6.548314/ 58.450722, val:  40.83%, val_best:  50.83%, tr:  99.39%, tr_best:  99.39%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4676%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.0789%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 42491  14.467%\n",
      "fc layer 1 self.abs_max_out: 6298.0\n",
      "lif layer 1 self.abs_max_v: 11392.0\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  6.576680/ 33.164028, val:  45.00%, val_best:  50.83%, tr:  98.67%, tr_best:  99.39%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7308%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.1891%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 43857  14.451%\n",
      "fc layer 1 self.abs_max_out: 6344.0\n",
      "lif layer 1 self.abs_max_v: 11533.5\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  6.485057/ 44.012878, val:  42.50%, val_best:  50.83%, tr:  98.67%, tr_best:  99.39%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6087%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.8035%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 45209  14.431%\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  6.179301/ 36.972935, val:  49.58%, val_best:  50.83%, tr:  99.08%, tr_best:  99.39%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8156%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.1747%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 46510  14.396%\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  6.261185/ 53.165699, val:  37.50%, val_best:  50.83%, tr:  99.28%, tr_best:  99.39%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.5637%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.5451%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 47854  14.377%\n",
      "fc layer 1 self.abs_max_out: 6636.0\n",
      "lif layer 1 self.abs_max_v: 11954.5\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  6.084156/ 39.648617, val:  40.42%, val_best:  50.83%, tr:  98.67%, tr_best:  99.39%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.8322%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.0644%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 49178  14.352%\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  5.918658/ 28.301531, val:  47.50%, val_best:  50.83%, tr:  99.28%, tr_best:  99.39%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0119%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.8342%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 50474  14.321%\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  5.966211/ 33.269104, val:  45.42%, val_best:  50.83%, tr:  98.98%, tr_best:  99.39%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0991%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.8360%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 51768  14.291%\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  5.986721/ 38.813862, val:  49.58%, val_best:  50.83%, tr:  99.08%, tr_best:  99.39%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.2670%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.0094%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 53077  14.267%\n",
      "fc layer 1 self.abs_max_out: 6761.0\n",
      "lif layer 1 self.abs_max_v: 12193.0\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  6.422868/ 35.984756, val:  42.08%, val_best:  50.83%, tr:  99.08%, tr_best:  99.39%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1779%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.8375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 54437  14.258%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  5.957887/ 37.226257, val:  48.33%, val_best:  50.83%, tr:  98.47%, tr_best:  99.39%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0870%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.7044%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 55731  14.232%\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  6.011582/ 36.474346, val:  43.75%, val_best:  50.83%, tr:  98.98%, tr_best:  99.39%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.4655%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.9240%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 57042  14.211%\n",
      "lif layer 2 self.abs_max_v: 8572.0\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  6.067180/ 35.903446, val:  39.58%, val_best:  50.83%, tr:  98.37%, tr_best:  99.39%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0596%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.7235%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 58362  14.194%\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  5.691220/ 44.060341, val:  43.33%, val_best:  50.83%, tr:  98.88%, tr_best:  99.39%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9012%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.9345%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 59595  14.157%\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  6.010692/ 34.541267, val:  55.42%, val_best:  55.42%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.6483%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.0955%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 60937  14.146%\n",
      "fc layer 2 self.abs_max_out: 4916.0\n",
      "lif layer 2 self.abs_max_v: 8736.0\n",
      "lif layer 2 self.abs_max_v: 9284.0\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  6.299552/ 46.840469, val:  43.33%, val_best:  55.42%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0769%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.8009%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 62293  14.140%\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  6.097278/ 42.995777, val:  44.17%, val_best:  55.42%, tr:  99.08%, tr_best:  99.80%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.2176%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.1546%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 63599  14.122%\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  6.205384/ 45.742897, val:  45.00%, val_best:  55.42%, tr:  99.18%, tr_best:  99.80%, epoch time: 75.80 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8614%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.9668%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 64936  14.113%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  5.892577/ 40.013081, val:  44.17%, val_best:  55.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0173%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.0870%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 66260  14.100%\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  6.106565/ 37.439819, val:  52.50%, val_best:  55.42%, tr:  99.18%, tr_best:  99.80%, epoch time: 75.77 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9719%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.1492%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 67621  14.096%\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  5.985271/ 35.098293, val:  50.83%, val_best:  55.42%, tr:  98.88%, tr_best:  99.80%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0155%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.1238%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 68957  14.087%\n",
      "fc layer 2 self.abs_max_out: 4928.0\n",
      "lif layer 1 self.abs_max_v: 12287.5\n",
      "fc layer 2 self.abs_max_out: 5104.0\n",
      "lif layer 2 self.abs_max_v: 9422.5\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  5.911438/ 34.351837, val:  49.17%, val_best:  55.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9606%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.5928%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 70268  14.074%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  5.757256/ 34.920063, val:  49.17%, val_best:  55.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.2961%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.2059%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 71560  14.057%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  5.782211/ 33.770157, val:  50.00%, val_best:  55.42%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8860%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.7760%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 72866  14.043%\n",
      "fc layer 2 self.abs_max_out: 5160.0\n",
      "lif layer 2 self.abs_max_v: 9443.5\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  5.884788/ 40.435696, val:  42.92%, val_best:  55.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.3919%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.3451%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 74199  14.035%\n",
      "fc layer 1 self.abs_max_out: 6819.0\n",
      "fc layer 2 self.abs_max_out: 5184.0\n",
      "lif layer 2 self.abs_max_v: 9564.0\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  5.954492/ 24.875870, val:  49.17%, val_best:  55.42%, tr:  99.08%, tr_best:  99.80%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4176%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.4792%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 75543  14.030%\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  6.365177/ 33.883472, val:  45.42%, val_best:  55.42%, tr:  98.67%, tr_best:  99.80%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5006%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.5934%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 76902  14.027%\n",
      "fc layer 1 self.abs_max_out: 6879.0\n",
      "lif layer 1 self.abs_max_v: 12544.0\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  6.014273/ 35.020920, val:  45.42%, val_best:  55.42%, tr:  98.77%, tr_best:  99.80%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9260%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.8425%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 78223  14.018%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  5.853073/ 35.558495, val:  43.75%, val_best:  55.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6049%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.5243%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 79523  14.005%\n",
      "fc layer 1 self.abs_max_out: 7074.0\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  5.742723/ 37.577522, val:  45.83%, val_best:  55.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4815%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.2852%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 80826  13.993%\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  6.117914/ 43.574455, val:  42.92%, val_best:  55.42%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8329%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.2320%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 82151  13.986%\n",
      "fc layer 1 self.abs_max_out: 7136.0\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  6.157043/ 36.701481, val:  43.75%, val_best:  55.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7024%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.2205%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 83483  13.979%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  6.181971/ 38.148975, val:  47.92%, val_best:  55.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1025%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.4914%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 84862  13.981%\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  6.060568/ 29.836407, val:  51.25%, val_best:  55.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.3003%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.2983%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 86194  13.975%\n",
      "fc layer 1 self.abs_max_out: 7148.0\n",
      "lif layer 1 self.abs_max_v: 12732.5\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  5.984407/ 52.918152, val:  42.08%, val_best:  55.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9597%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.1081%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 87482  13.962%\n",
      "fc layer 1 self.abs_max_out: 7538.0\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  6.108615/ 40.269512, val:  54.58%, val_best:  55.42%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8493%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.8763%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 88814  13.957%\n",
      "fc layer 1 self.abs_max_out: 7750.0\n",
      "lif layer 1 self.abs_max_v: 12787.5\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  5.943784/ 41.470814, val:  48.75%, val_best:  55.42%, tr:  99.08%, tr_best:  99.80%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5486%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.4894%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 90128  13.949%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  5.787141/ 31.653946, val:  49.58%, val_best:  55.42%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0444%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.4598%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 91417  13.937%\n",
      "fc layer 1 self.abs_max_out: 7851.0\n",
      "lif layer 1 self.abs_max_v: 12844.0\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  6.022789/ 35.137760, val:  49.17%, val_best:  55.42%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.4422%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 92742  13.931%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  5.863365/ 33.552414, val:  55.83%, val_best:  55.83%, tr:  98.77%, tr_best:  99.80%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0236%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.3356%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 94066  13.925%\n",
      "lif layer 2 self.abs_max_v: 9573.0\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  5.924049/ 41.230934, val:  43.33%, val_best:  55.83%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.3556%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.1403%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 95370  13.917%\n",
      "fc layer 2 self.abs_max_out: 5211.0\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  6.067772/ 33.129490, val:  52.92%, val_best:  55.83%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1014%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.2017%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 96698  13.912%\n",
      "fc layer 2 self.abs_max_out: 5212.0\n",
      "fc layer 2 self.abs_max_out: 5361.0\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  5.784109/ 38.178097, val:  56.25%, val_best:  56.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7627%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.3803%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 97980  13.900%\n",
      "lif layer 1 self.abs_max_v: 13084.5\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  5.907248/ 32.860111, val:  49.58%, val_best:  56.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 74.57 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9199%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.5241%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 99280  13.892%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  5.691941/ 41.656197, val:  52.08%, val_best:  56.25%, tr:  98.88%, tr_best:  99.80%, epoch time: 74.99 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6426%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.1677%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 100560  13.881%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  5.902976/ 31.874451, val:  53.75%, val_best:  56.25%, tr:  99.18%, tr_best:  99.80%, epoch time: 74.73 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6443%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.0745%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 101850  13.871%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  5.784824/ 27.491163, val:  55.83%, val_best:  56.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 74.55 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8275%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.9189%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 103102  13.857%\n",
      "fc layer 2 self.abs_max_out: 5893.0\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  6.029727/ 26.059320, val:  55.83%, val_best:  56.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5057%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.8463%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 104410  13.851%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  5.695824/ 43.337578, val:  41.25%, val_best:  56.25%, tr:  99.59%, tr_best:  99.80%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9558%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.3342%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 105679  13.839%\n",
      "lif layer 1 self.abs_max_v: 13103.0\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  5.943890/ 32.682976, val:  47.92%, val_best:  56.25%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8153%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.7883%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 106990  13.834%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  5.496351/ 38.343075, val:  42.08%, val_best:  56.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1843%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.7851%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 108214  13.817%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  5.528330/ 33.533524, val:  53.75%, val_best:  56.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.2799%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.3011%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 109506  13.809%\n",
      "fc layer 1 self.abs_max_out: 8158.0\n",
      "lif layer 1 self.abs_max_v: 13165.0\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  5.275093/ 33.756725, val:  46.25%, val_best:  56.25%, tr:  98.98%, tr_best:  99.80%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8567%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.3162%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 110746  13.795%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  5.684516/ 30.740789, val:  56.25%, val_best:  56.25%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9292%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.1976%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 112046  13.789%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  5.585030/ 40.510391, val:  37.50%, val_best:  56.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8470%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.3507%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 113351  13.784%\n",
      "fc layer 1 self.abs_max_out: 8297.0\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  5.640975/ 26.200016, val:  55.83%, val_best:  56.25%, tr:  98.77%, tr_best:  99.80%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4974%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.0529%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 114627  13.775%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  5.838839/ 37.477085, val:  35.42%, val_best:  56.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0973%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.3514%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 115975  13.775%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  5.420178/ 20.501492, val:  68.33%, val_best:  68.33%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5637%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.7027%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 117204  13.761%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  5.751469/ 27.974052, val:  60.42%, val_best:  68.33%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4105%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.8148%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 118482  13.753%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  5.587103/ 28.294842, val:  51.67%, val_best:  68.33%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1932%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.0315%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 119771  13.746%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  5.539083/ 26.603455, val:  51.25%, val_best:  68.33%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5912%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.7148%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 121068  13.741%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  5.643982/ 36.201073, val:  50.42%, val_best:  68.33%, tr:  98.98%, tr_best:  99.80%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4345%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.8433%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 122389  13.738%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  5.347200/ 26.057501, val:  52.92%, val_best:  68.33%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3075%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.7148%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 123632  13.727%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  5.791101/ 30.419739, val:  51.25%, val_best:  68.33%, tr:  99.59%, tr_best:  99.80%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2683%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.1476%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 124955  13.724%\n",
      "lif layer 1 self.abs_max_v: 13382.5\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  5.491590/ 28.132862, val:  51.67%, val_best:  68.33%, tr:  99.08%, tr_best:  99.80%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7977%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.4638%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 126227  13.716%\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  5.257410/ 36.295368, val:  48.75%, val_best:  68.33%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4455%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.1000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 127454  13.704%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  5.650300/ 34.395420, val:  51.25%, val_best:  68.33%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0835%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.3637%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 128766  13.701%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  5.436754/ 37.746181, val:  47.08%, val_best:  68.33%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6654%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.5639%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 130040  13.694%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  5.281369/ 32.828510, val:  55.00%, val_best:  68.33%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8178%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.5870%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 131288  13.684%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  5.300906/ 37.489994, val:  37.92%, val_best:  68.33%, tr:  99.08%, tr_best:  99.80%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2920%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.7525%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 132505  13.671%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  5.325086/ 24.251471, val:  56.67%, val_best:  68.33%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7449%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.5237%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 133771  13.664%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  5.029271/ 27.803724, val:  61.67%, val_best:  68.33%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1733%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.5612%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 134984  13.651%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  5.450311/ 18.848087, val:  59.17%, val_best:  68.33%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2626%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.8146%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 136230  13.642%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  4.866374/ 27.955498, val:  58.33%, val_best:  68.33%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5315%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.8964%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 137391  13.625%\n",
      "lif layer 1 self.abs_max_v: 13617.5\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  5.014132/ 32.541817, val:  51.25%, val_best:  68.33%, tr:  98.88%, tr_best:  99.80%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5605%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.6308%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 138581  13.611%\n",
      "fc layer 1 self.abs_max_out: 8305.0\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  5.612960/ 25.876862, val:  60.83%, val_best:  68.33%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0347%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.0920%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 139896  13.609%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  5.226383/ 29.543289, val:  51.67%, val_best:  68.33%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0692%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.9970%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 141154  13.602%\n",
      "fc layer 1 self.abs_max_out: 8322.0\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  5.241009/ 33.821793, val:  46.25%, val_best:  68.33%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4904%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.2056%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 142381  13.592%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  5.127820/ 30.536390, val:  52.92%, val_best:  68.33%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0135%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.9988%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 143612  13.583%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  5.304765/ 22.657379, val:  58.75%, val_best:  68.33%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4732%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.7032%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 144836  13.573%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  4.944471/ 24.040661, val:  52.92%, val_best:  68.33%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1323%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.2441%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 146029  13.560%\n",
      "fc layer 1 self.abs_max_out: 8364.0\n",
      "fc layer 1 self.abs_max_out: 8806.0\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  5.109689/ 29.584068, val:  52.50%, val_best:  68.33%, tr:  98.88%, tr_best:  99.80%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9878%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.3424%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 147238  13.549%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  4.959108/ 25.690756, val:  61.67%, val_best:  68.33%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6784%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.8277%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 148468  13.540%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  5.015182/ 29.798695, val:  62.92%, val_best:  68.33%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6969%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.6068%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 149703  13.532%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  4.977878/ 37.873146, val:  52.92%, val_best:  68.33%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9764%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.2874%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 150904  13.521%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  5.179710/ 38.826672, val:  46.25%, val_best:  68.33%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1724%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.3943%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 152115  13.511%\n",
      "lif layer 1 self.abs_max_v: 13769.5\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  5.137968/ 27.222143, val:  56.67%, val_best:  68.33%, tr:  99.08%, tr_best:  99.80%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9999%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.6864%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 153374  13.506%\n",
      "fc layer 1 self.abs_max_out: 8917.0\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  4.848581/ 36.393192, val:  50.00%, val_best:  68.33%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8475%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.6792%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 154576  13.495%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  4.921933/ 32.413296, val:  48.33%, val_best:  68.33%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9016%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.4609%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 155784  13.485%\n",
      "lif layer 1 self.abs_max_v: 13786.5\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  5.067216/ 27.661312, val:  52.50%, val_best:  68.33%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5418%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.5904%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 157009  13.477%\n",
      "fc layer 1 self.abs_max_out: 9102.0\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  4.844306/ 30.230951, val:  52.92%, val_best:  68.33%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7689%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.6804%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 158200  13.466%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  4.955034/ 21.177032, val:  64.58%, val_best:  68.33%, tr:  98.98%, tr_best:  99.90%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9609%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.1965%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 159423  13.458%\n",
      "lif layer 1 self.abs_max_v: 13837.0\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  4.787163/ 35.157063, val:  43.75%, val_best:  68.33%, tr:  99.39%, tr_best:  99.90%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7772%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.0114%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 160623  13.448%\n",
      "lif layer 1 self.abs_max_v: 13891.0\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  4.943918/ 30.339325, val:  55.42%, val_best:  68.33%, tr:  99.18%, tr_best:  99.90%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0927%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.7600%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 161827  13.439%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  5.255761/ 19.854258, val:  62.08%, val_best:  68.33%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0142%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.7171%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 163084  13.434%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  4.827585/ 24.035788, val:  64.58%, val_best:  68.33%, tr:  98.67%, tr_best:  99.90%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8484%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.3420%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 164269  13.423%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  4.874372/ 25.690886, val:  48.75%, val_best:  68.33%, tr:  99.08%, tr_best:  99.90%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1055%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.8304%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 165493  13.416%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  4.866053/ 30.853662, val:  58.75%, val_best:  68.33%, tr:  99.39%, tr_best:  99.90%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.4198%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.4865%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 166724  13.409%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  4.566260/ 37.373947, val:  42.50%, val_best:  68.33%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0845%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.2941%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 167867  13.396%\n",
      "fc layer 1 self.abs_max_out: 9168.0\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  4.746778/ 19.393898, val:  56.67%, val_best:  68.33%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9662%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.3273%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 169052  13.386%\n",
      "fc layer 1 self.abs_max_out: 9458.0\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  4.352972/ 23.824514, val:  59.58%, val_best:  68.33%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9475%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.7338%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 170157  13.370%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  4.683610/ 38.131931, val:  49.17%, val_best:  68.33%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7009%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.3674%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 171373  13.363%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  4.862088/ 37.352814, val:  50.42%, val_best:  68.33%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0032%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.1461%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 172597  13.356%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  4.763543/ 30.700348, val:  49.58%, val_best:  68.33%, tr:  99.18%, tr_best:  99.90%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0790%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.4778%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 173769  13.346%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  4.656822/ 24.868258, val:  53.75%, val_best:  68.33%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.4495%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.4693%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 174949  13.336%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  4.754384/ 23.023645, val:  57.50%, val_best:  68.33%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3368%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.0935%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 176112  13.325%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  4.625244/ 23.397287, val:  62.92%, val_best:  68.33%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0990%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.4581%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 177256  13.313%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  4.423244/ 24.735298, val:  55.42%, val_best:  68.33%, tr:  98.98%, tr_best:  99.90%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8100%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.0543%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 178386  13.300%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  4.442867/ 22.609375, val:  53.75%, val_best:  68.33%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3635%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.8538%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 179520  13.288%\n",
      "lif layer 1 self.abs_max_v: 13976.0\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  4.507846/ 26.229990, val:  55.42%, val_best:  68.33%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8846%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.4544%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 180665  13.276%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  4.732857/ 30.719913, val:  52.50%, val_best:  68.33%, tr:  99.18%, tr_best:  99.90%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9009%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.9943%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 181868  13.269%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  4.653636/ 22.854670, val:  58.75%, val_best:  68.33%, tr:  98.98%, tr_best:  99.90%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.4121%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.0577%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 183040  13.260%\n",
      "lif layer 1 self.abs_max_v: 14373.0\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  4.493553/ 22.111290, val:  50.42%, val_best:  68.33%, tr:  98.88%, tr_best:  99.90%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0880%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.3990%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 184208  13.251%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  4.446980/ 33.708321, val:  52.92%, val_best:  68.33%, tr:  99.18%, tr_best:  99.90%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0482%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.5525%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 185404  13.243%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  4.773275/ 30.367302, val:  45.00%, val_best:  68.33%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8546%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.2118%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 186613  13.237%\n",
      "fc layer 1 self.abs_max_out: 9658.0\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  4.438701/ 31.469742, val:  53.75%, val_best:  68.33%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.4457%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.0533%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 187784  13.228%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  4.464254/ 24.701204, val:  47.92%, val_best:  68.33%, tr:  99.08%, tr_best:  99.90%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.4320%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.2743%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 188922  13.217%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  4.420433/ 26.445616, val:  47.08%, val_best:  68.33%, tr:  99.18%, tr_best:  99.90%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8068%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8712%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 190100  13.209%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  4.121651/ 36.297543, val:  36.67%, val_best:  68.33%, tr:  99.18%, tr_best:  99.90%, epoch time: 74.99 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0294%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.7715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 191171  13.194%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  4.279263/ 28.054407, val:  42.08%, val_best:  68.33%, tr:  98.88%, tr_best:  99.90%, epoch time: 75.42 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6301%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.6845%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 192278  13.181%\n",
      "lif layer 1 self.abs_max_v: 14485.5\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  4.319854/ 25.891607, val:  51.25%, val_best:  68.33%, tr:  99.28%, tr_best:  99.90%, epoch time: 75.39 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6862%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.5332%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 193399  13.170%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  4.191783/ 29.599224, val:  47.92%, val_best:  68.33%, tr:  99.80%, tr_best:  99.90%, epoch time: 75.08 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7010%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0542%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 194535  13.159%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  4.259119/ 27.164490, val:  60.83%, val_best:  68.33%, tr:  98.88%, tr_best:  99.90%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3221%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.4859%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 195681  13.150%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  4.531172/ 29.691992, val:  61.25%, val_best:  68.33%, tr:  99.08%, tr_best:  99.90%, epoch time: 75.41 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.4056%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.3613%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 196833  13.141%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  4.436828/ 26.181196, val:  55.42%, val_best:  68.33%, tr:  98.98%, tr_best:  99.90%, epoch time: 74.01 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6677%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.7639%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 198022  13.134%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  4.395767/ 18.853220, val:  57.92%, val_best:  68.33%, tr:  99.59%, tr_best:  99.90%, epoch time: 75.46 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5800%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8763%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 199156  13.124%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  4.273107/ 29.960932, val:  44.17%, val_best:  68.33%, tr:  99.49%, tr_best:  99.90%, epoch time: 75.16 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.1707%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9164%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 200287  13.114%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  4.197575/ 26.384548, val:  55.42%, val_best:  68.33%, tr:  99.18%, tr_best:  99.90%, epoch time: 74.59 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8256%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9417%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 201406  13.104%\n",
      "fc layer 2 self.abs_max_out: 5939.0\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  4.253244/ 18.593538, val:  59.17%, val_best:  68.33%, tr:  98.88%, tr_best:  99.90%, epoch time: 75.16 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9320%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.7445%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 202518  13.093%\n",
      "lif layer 1 self.abs_max_v: 14532.0\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  4.086474/ 20.180489, val:  63.75%, val_best:  68.33%, tr:  99.39%, tr_best:  99.90%, epoch time: 75.00 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1467%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8630%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 203625  13.081%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  4.334739/ 26.109108, val:  47.08%, val_best:  68.33%, tr:  98.47%, tr_best:  99.90%, epoch time: 74.77 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2060%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2020%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 204806  13.075%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  3.873841/ 17.933355, val:  51.25%, val_best:  68.33%, tr:  99.59%, tr_best:  99.90%, epoch time: 74.72 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1038%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1627%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 205871  13.061%\n",
      "lif layer 1 self.abs_max_v: 14542.0\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  4.390350/ 27.294275, val:  50.00%, val_best:  68.33%, tr:  99.28%, tr_best:  99.90%, epoch time: 74.52 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8106%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.5803%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 207031  13.054%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  4.281899/ 25.751385, val:  56.25%, val_best:  68.33%, tr:  99.08%, tr_best:  99.90%, epoch time: 75.07 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9384%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0845%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 208178  13.046%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  4.268472/ 26.945297, val:  53.33%, val_best:  68.33%, tr:  99.28%, tr_best:  99.90%, epoch time: 74.47 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9129%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.5747%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 209316  13.037%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  4.118229/ 31.241589, val:  57.08%, val_best:  68.33%, tr:  98.98%, tr_best:  99.90%, epoch time: 74.32 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9006%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9484%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 210391  13.024%\n",
      "lif layer 1 self.abs_max_v: 14670.0\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  4.371994/ 32.026711, val:  46.67%, val_best:  68.33%, tr:  98.98%, tr_best:  99.90%, epoch time: 74.34 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1210%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0034%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 211547  13.017%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  4.002063/ 28.747084, val:  47.92%, val_best:  68.33%, tr:  98.98%, tr_best:  99.90%, epoch time: 74.98 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0263%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0411%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 212616  13.005%\n",
      "lif layer 1 self.abs_max_v: 14821.5\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  4.297435/ 25.541531, val:  52.50%, val_best:  68.33%, tr:  99.28%, tr_best:  99.90%, epoch time: 74.57 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0061%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.6392%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 213756  12.996%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  4.129869/ 26.279804, val:  57.92%, val_best:  68.33%, tr:  99.18%, tr_best:  99.90%, epoch time: 74.06 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8817%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.7171%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 214823  12.984%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  3.982104/ 22.578619, val:  60.83%, val_best:  68.33%, tr:  99.08%, tr_best:  99.90%, epoch time: 73.84 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4108%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2619%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 215883  12.971%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  4.098102/ 29.432688, val:  45.00%, val_best:  68.33%, tr:  98.67%, tr_best:  99.90%, epoch time: 73.05 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1141%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1646%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3057%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 216992  12.962%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  3.787402/ 32.494843, val:  48.75%, val_best:  68.33%, tr:  99.28%, tr_best:  99.90%, epoch time: 73.83 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1533%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2040%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 218029  12.948%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  4.373018/ 21.582172, val:  55.83%, val_best:  68.33%, tr:  99.18%, tr_best:  99.90%, epoch time: 74.19 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8579%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9196%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 219160  12.940%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  4.169790/ 35.025436, val:  46.67%, val_best:  68.33%, tr:  99.49%, tr_best:  99.90%, epoch time: 74.50 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9527%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.7184%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 220250  12.930%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  3.962861/ 29.365211, val:  50.83%, val_best:  68.33%, tr:  99.39%, tr_best:  99.90%, epoch time: 74.42 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8184%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0392%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 221319  12.918%\n",
      "lif layer 1 self.abs_max_v: 14896.0\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  4.358421/ 26.902964, val:  57.50%, val_best:  68.33%, tr:  99.28%, tr_best:  99.90%, epoch time: 75.46 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1352%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0307%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 222437  12.910%\n",
      "lif layer 1 self.abs_max_v: 15041.0\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  4.212426/ 23.083036, val:  45.42%, val_best:  68.33%, tr:  99.39%, tr_best:  99.90%, epoch time: 74.35 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1181%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9973%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 223533  12.900%\n",
      "fc layer 1 self.abs_max_out: 9841.0\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  3.869901/ 26.630260, val:  48.75%, val_best:  68.33%, tr:  99.18%, tr_best:  99.90%, epoch time: 74.47 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0312%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0356%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2820%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 224613  12.889%\n",
      "fc layer 1 self.abs_max_out: 9863.0\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  4.049967/ 29.666025, val:  51.67%, val_best:  68.33%, tr:  98.98%, tr_best:  99.90%, epoch time: 74.46 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0376%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1101%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1202%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 225683  12.878%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  3.957721/ 21.473991, val:  61.25%, val_best:  68.33%, tr:  99.69%, tr_best:  99.90%, epoch time: 74.67 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8164%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9607%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 226725  12.866%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  4.163121/ 23.646286, val:  62.50%, val_best:  68.33%, tr:  98.98%, tr_best:  99.90%, epoch time: 74.29 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7714%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0051%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 227846  12.858%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  3.930608/ 22.412935, val:  50.83%, val_best:  68.33%, tr:  99.18%, tr_best:  99.90%, epoch time: 74.95 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8431%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2968%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 228875  12.845%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  3.695577/ 20.569374, val:  52.92%, val_best:  68.33%, tr:  99.08%, tr_best:  99.90%, epoch time: 74.76 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1190%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2913%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 229889  12.832%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  3.911599/ 26.034113, val:  50.83%, val_best:  68.33%, tr:  99.28%, tr_best:  99.90%, epoch time: 74.30 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0426%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0550%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8159%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 230918  12.819%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  4.049095/ 26.081535, val:  48.75%, val_best:  68.33%, tr:  99.39%, tr_best:  99.90%, epoch time: 74.63 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0278%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8427%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 231969  12.808%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  4.126348/ 25.701017, val:  55.00%, val_best:  68.33%, tr:  99.28%, tr_best:  99.90%, epoch time: 75.54 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9137%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1177%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 233050  12.798%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  4.016161/ 26.960697, val:  55.00%, val_best:  68.33%, tr:  99.39%, tr_best:  99.90%, epoch time: 75.35 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0427%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2677%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 234137  12.789%\n",
      "lif layer 1 self.abs_max_v: 15225.5\n",
      "lif layer 1 self.abs_max_v: 15357.5\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  3.715723/ 27.419420, val:  50.42%, val_best:  68.33%, tr:  99.69%, tr_best:  99.90%, epoch time: 74.71 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4159%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7479%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 235179  12.778%\n",
      "lif layer 1 self.abs_max_v: 15568.5\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  3.870103/ 26.278414, val:  60.00%, val_best:  68.33%, tr:  99.18%, tr_best:  99.90%, epoch time: 74.56 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5460%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4480%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 236224  12.767%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  3.854368/ 30.088846, val:  52.08%, val_best:  68.33%, tr:  98.88%, tr_best:  99.90%, epoch time: 74.65 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6671%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6754%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 237291  12.757%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  3.854482/ 20.450645, val:  59.58%, val_best:  68.33%, tr:  99.39%, tr_best:  99.90%, epoch time: 75.22 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7306%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0753%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 238309  12.745%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  4.134491/ 30.149939, val:  39.58%, val_best:  68.33%, tr:  99.18%, tr_best:  99.90%, epoch time: 74.87 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9602%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2562%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 239398  12.736%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  4.001867/ 19.929148, val:  57.50%, val_best:  68.33%, tr:  98.88%, tr_best:  99.90%, epoch time: 75.20 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5759%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4299%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 240463  12.726%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  4.023937/ 26.593082, val:  50.83%, val_best:  68.33%, tr:  98.57%, tr_best:  99.90%, epoch time: 75.40 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7155%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6767%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 241530  12.717%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  3.819183/ 24.022112, val:  53.33%, val_best:  68.33%, tr:  98.88%, tr_best:  99.90%, epoch time: 74.78 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7604%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 242553  12.705%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  3.869593/ 25.601835, val:  55.83%, val_best:  68.33%, tr:  98.88%, tr_best:  99.90%, epoch time: 74.69 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7858%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7040%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 243621  12.696%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  3.807830/ 23.151628, val:  60.00%, val_best:  68.33%, tr:  99.59%, tr_best:  99.90%, epoch time: 75.06 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9684%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8412%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 244665  12.686%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  3.763540/ 25.909359, val:  52.08%, val_best:  68.33%, tr:  99.28%, tr_best:  99.90%, epoch time: 73.99 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7038%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8047%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 245719  12.676%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  3.827147/ 16.583485, val:  64.58%, val_best:  68.33%, tr:  99.18%, tr_best:  99.90%, epoch time: 74.32 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0788%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8155%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 246744  12.665%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  3.696708/ 22.772530, val:  57.08%, val_best:  68.33%, tr:  99.18%, tr_best:  99.90%, epoch time: 74.30 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9618%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7188%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08adcd917e05478baa9b5dab00a10a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÉ‚ñÖ‚ñà‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÇ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñà‚ñÉ‚ñÜ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÉ‚ñÖ‚ñà‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÇ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99183</td></tr><tr><td>tr_epoch_loss</td><td>3.69671</td></tr><tr><td>val_acc_best</td><td>0.68333</td></tr><tr><td>val_acc_now</td><td>0.57083</td></tr><tr><td>val_loss</td><td>22.77253</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dauntless-sweep-2</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/liga4sas' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/liga4sas</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_010835-liga4sas/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: w5ditk4p with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.046875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0234375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_052338-w5ditk4p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/w5ditk4p' target=\"_blank\">dry-sweep-6</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/w5ditk4p' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/w5ditk4p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': '20251214_052347_440', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 16, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 1, 'lif_layer_v_threshold2': 256, 'init_scaling': [0.03125, 0.046875, 0.0234375], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 16\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 1, self.v_threshold 256\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=16, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=256, v_reset=10000, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 46.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 41.0\n",
      "lif layer 2 self.abs_max_v: 41.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 51.0\n",
      "fc layer 2 self.abs_max_out: 80.0\n",
      "lif layer 2 self.abs_max_v: 94.5\n",
      "fc layer 1 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 69.0\n",
      "lif layer 2 self.abs_max_v: 107.5\n",
      "fc layer 1 self.abs_max_out: 62.0\n",
      "lif layer 1 self.abs_max_v: 90.0\n",
      "fc layer 2 self.abs_max_out: 82.0\n",
      "lif layer 2 self.abs_max_v: 121.5\n",
      "fc layer 1 self.abs_max_out: 101.0\n",
      "lif layer 1 self.abs_max_v: 138.0\n",
      "fc layer 2 self.abs_max_out: 87.0\n",
      "lif layer 2 self.abs_max_v: 142.5\n",
      "fc layer 1 self.abs_max_out: 147.0\n",
      "lif layer 1 self.abs_max_v: 210.0\n",
      "fc layer 2 self.abs_max_out: 126.0\n",
      "lif layer 2 self.abs_max_v: 194.5\n",
      "fc layer 1 self.abs_max_out: 200.0\n",
      "lif layer 1 self.abs_max_v: 270.0\n",
      "fc layer 2 self.abs_max_out: 195.0\n",
      "lif layer 2 self.abs_max_v: 292.5\n",
      "fc layer 1 self.abs_max_out: 225.0\n",
      "lif layer 1 self.abs_max_v: 360.0\n",
      "fc layer 2 self.abs_max_out: 219.0\n",
      "lif layer 2 self.abs_max_v: 365.5\n",
      "fc layer 3 self.abs_max_out: 12.0\n",
      "fc layer 1 self.abs_max_out: 277.0\n",
      "lif layer 1 self.abs_max_v: 405.0\n",
      "fc layer 2 self.abs_max_out: 228.0\n",
      "lif layer 2 self.abs_max_v: 393.0\n",
      "lif layer 2 self.abs_max_v: 409.5\n",
      "fc layer 1 self.abs_max_out: 297.0\n",
      "fc layer 2 self.abs_max_out: 271.0\n",
      "fc layer 2 self.abs_max_out: 294.0\n",
      "fc layer 3 self.abs_max_out: 30.0\n",
      "fc layer 1 self.abs_max_out: 392.0\n",
      "fc layer 1 self.abs_max_out: 481.0\n",
      "lif layer 1 self.abs_max_v: 511.0\n",
      "fc layer 2 self.abs_max_out: 321.0\n",
      "lif layer 1 self.abs_max_v: 518.5\n",
      "lif layer 2 self.abs_max_v: 417.5\n",
      "lif layer 2 self.abs_max_v: 418.0\n",
      "fc layer 3 self.abs_max_out: 31.0\n",
      "lif layer 2 self.abs_max_v: 425.5\n",
      "fc layer 2 self.abs_max_out: 327.0\n",
      "lif layer 2 self.abs_max_v: 440.0\n",
      "lif layer 1 self.abs_max_v: 532.5\n",
      "lif layer 2 self.abs_max_v: 450.0\n",
      "lif layer 2 self.abs_max_v: 458.0\n",
      "lif layer 2 self.abs_max_v: 470.0\n",
      "lif layer 2 self.abs_max_v: 496.0\n",
      "lif layer 2 self.abs_max_v: 522.0\n",
      "lif layer 2 self.abs_max_v: 523.0\n",
      "lif layer 2 self.abs_max_v: 568.5\n",
      "lif layer 2 self.abs_max_v: 587.5\n",
      "fc layer 1 self.abs_max_out: 485.0\n",
      "lif layer 2 self.abs_max_v: 600.5\n",
      "lif layer 2 self.abs_max_v: 610.0\n",
      "lif layer 2 self.abs_max_v: 624.0\n",
      "fc layer 1 self.abs_max_out: 550.0\n",
      "lif layer 1 self.abs_max_v: 550.0\n",
      "fc layer 2 self.abs_max_out: 336.0\n",
      "fc layer 3 self.abs_max_out: 36.0\n",
      "fc layer 2 self.abs_max_out: 363.0\n",
      "lif layer 2 self.abs_max_v: 674.5\n",
      "lif layer 2 self.abs_max_v: 696.5\n",
      "fc layer 2 self.abs_max_out: 396.0\n",
      "lif layer 2 self.abs_max_v: 731.0\n",
      "fc layer 3 self.abs_max_out: 47.0\n",
      "fc layer 2 self.abs_max_out: 407.0\n",
      "lif layer 2 self.abs_max_v: 772.5\n",
      "fc layer 2 self.abs_max_out: 410.0\n",
      "lif layer 2 self.abs_max_v: 780.5\n",
      "lif layer 2 self.abs_max_v: 799.5\n",
      "fc layer 2 self.abs_max_out: 432.0\n",
      "lif layer 2 self.abs_max_v: 816.0\n",
      "fc layer 1 self.abs_max_out: 561.0\n",
      "lif layer 1 self.abs_max_v: 561.0\n",
      "lif layer 2 self.abs_max_v: 821.0\n",
      "fc layer 3 self.abs_max_out: 52.0\n",
      "fc layer 2 self.abs_max_out: 493.0\n",
      "lif layer 2 self.abs_max_v: 848.0\n",
      "lif layer 2 self.abs_max_v: 868.0\n",
      "lif layer 2 self.abs_max_v: 913.0\n",
      "fc layer 2 self.abs_max_out: 532.0\n",
      "lif layer 2 self.abs_max_v: 932.0\n",
      "lif layer 2 self.abs_max_v: 945.0\n",
      "lif layer 2 self.abs_max_v: 951.5\n",
      "lif layer 2 self.abs_max_v: 980.0\n",
      "fc layer 2 self.abs_max_out: 592.0\n",
      "lif layer 2 self.abs_max_v: 996.5\n",
      "lif layer 2 self.abs_max_v: 1044.5\n",
      "lif layer 2 self.abs_max_v: 1059.5\n",
      "lif layer 2 self.abs_max_v: 1073.5\n",
      "lif layer 2 self.abs_max_v: 1112.0\n",
      "lif layer 2 self.abs_max_v: 1116.0\n",
      "lif layer 2 self.abs_max_v: 1129.0\n",
      "lif layer 2 self.abs_max_v: 1142.5\n",
      "fc layer 2 self.abs_max_out: 751.0\n",
      "lif layer 1 self.abs_max_v: 567.5\n",
      "lif layer 2 self.abs_max_v: 1161.5\n",
      "lif layer 2 self.abs_max_v: 1235.0\n",
      "lif layer 2 self.abs_max_v: 1326.5\n",
      "lif layer 2 self.abs_max_v: 1330.5\n",
      "fc layer 1 self.abs_max_out: 586.0\n",
      "lif layer 1 self.abs_max_v: 586.0\n",
      "fc layer 1 self.abs_max_out: 706.0\n",
      "lif layer 1 self.abs_max_v: 706.0\n",
      "fc layer 1 self.abs_max_out: 775.0\n",
      "lif layer 1 self.abs_max_v: 775.0\n",
      "lif layer 1 self.abs_max_v: 830.5\n",
      "lif layer 1 self.abs_max_v: 910.5\n",
      "fc layer 2 self.abs_max_out: 753.0\n",
      "lif layer 2 self.abs_max_v: 1403.0\n",
      "fc layer 2 self.abs_max_out: 758.0\n",
      "lif layer 2 self.abs_max_v: 1459.5\n",
      "fc layer 3 self.abs_max_out: 67.0\n",
      "fc layer 2 self.abs_max_out: 796.0\n",
      "fc layer 1 self.abs_max_out: 794.0\n",
      "fc layer 3 self.abs_max_out: 68.0\n",
      "fc layer 3 self.abs_max_out: 74.0\n",
      "fc layer 3 self.abs_max_out: 81.0\n",
      "fc layer 3 self.abs_max_out: 85.0\n",
      "lif layer 1 self.abs_max_v: 947.5\n",
      "fc layer 3 self.abs_max_out: 92.0\n",
      "fc layer 2 self.abs_max_out: 816.0\n",
      "fc layer 3 self.abs_max_out: 101.0\n",
      "fc layer 3 self.abs_max_out: 106.0\n",
      "fc layer 3 self.abs_max_out: 125.0\n",
      "lif layer 1 self.abs_max_v: 962.0\n",
      "fc layer 2 self.abs_max_out: 817.0\n",
      "fc layer 2 self.abs_max_out: 819.0\n",
      "lif layer 1 self.abs_max_v: 1000.5\n",
      "fc layer 1 self.abs_max_out: 800.0\n",
      "lif layer 1 self.abs_max_v: 1055.5\n",
      "lif layer 1 self.abs_max_v: 1187.0\n",
      "fc layer 1 self.abs_max_out: 802.0\n",
      "fc layer 1 self.abs_max_out: 841.0\n",
      "lif layer 2 self.abs_max_v: 1472.5\n",
      "lif layer 2 self.abs_max_v: 1476.0\n",
      "lif layer 2 self.abs_max_v: 1503.0\n",
      "fc layer 3 self.abs_max_out: 126.0\n",
      "fc layer 3 self.abs_max_out: 134.0\n",
      "fc layer 3 self.abs_max_out: 136.0\n",
      "fc layer 2 self.abs_max_out: 874.0\n",
      "lif layer 1 self.abs_max_v: 1189.0\n",
      "lif layer 1 self.abs_max_v: 1249.0\n",
      "fc layer 2 self.abs_max_out: 886.0\n",
      "lif layer 2 self.abs_max_v: 1521.5\n",
      "lif layer 2 self.abs_max_v: 1539.5\n",
      "fc layer 2 self.abs_max_out: 959.0\n",
      "lif layer 2 self.abs_max_v: 1584.5\n",
      "lif layer 2 self.abs_max_v: 1637.0\n",
      "lif layer 1 self.abs_max_v: 1267.5\n",
      "lif layer 1 self.abs_max_v: 1308.0\n",
      "fc layer 1 self.abs_max_out: 958.0\n",
      "lif layer 1 self.abs_max_v: 1462.0\n",
      "lif layer 2 self.abs_max_v: 1673.0\n",
      "lif layer 2 self.abs_max_v: 1688.5\n",
      "fc layer 3 self.abs_max_out: 146.0\n",
      "fc layer 3 self.abs_max_out: 150.0\n",
      "fc layer 3 self.abs_max_out: 163.0\n",
      "lif layer 1 self.abs_max_v: 1509.0\n",
      "lif layer 1 self.abs_max_v: 1549.0\n",
      "fc layer 2 self.abs_max_out: 986.0\n",
      "fc layer 2 self.abs_max_out: 1024.0\n",
      "fc layer 3 self.abs_max_out: 176.0\n",
      "fc layer 1 self.abs_max_out: 1001.0\n",
      "lif layer 1 self.abs_max_v: 1614.5\n",
      "fc layer 1 self.abs_max_out: 1051.0\n",
      "lif layer 1 self.abs_max_v: 1852.5\n",
      "lif layer 1 self.abs_max_v: 1884.5\n",
      "lif layer 1 self.abs_max_v: 1948.5\n",
      "lif layer 2 self.abs_max_v: 1709.5\n",
      "fc layer 3 self.abs_max_out: 184.0\n",
      "fc layer 2 self.abs_max_out: 1075.0\n",
      "fc layer 3 self.abs_max_out: 189.0\n",
      "fc layer 2 self.abs_max_out: 1193.0\n",
      "fc layer 3 self.abs_max_out: 199.0\n",
      "lif layer 2 self.abs_max_v: 1715.5\n",
      "lif layer 2 self.abs_max_v: 1816.0\n",
      "fc layer 3 self.abs_max_out: 204.0\n",
      "fc layer 1 self.abs_max_out: 1079.0\n",
      "fc layer 2 self.abs_max_out: 1262.0\n",
      "lif layer 2 self.abs_max_v: 1904.5\n",
      "lif layer 2 self.abs_max_v: 1930.5\n",
      "fc layer 1 self.abs_max_out: 1085.0\n",
      "fc layer 3 self.abs_max_out: 205.0\n",
      "fc layer 2 self.abs_max_out: 1296.0\n",
      "lif layer 1 self.abs_max_v: 2014.0\n",
      "fc layer 1 self.abs_max_out: 1088.0\n",
      "lif layer 1 self.abs_max_v: 2095.0\n",
      "lif layer 1 self.abs_max_v: 2117.5\n",
      "fc layer 1 self.abs_max_out: 1152.0\n",
      "fc layer 2 self.abs_max_out: 1300.0\n",
      "fc layer 2 self.abs_max_out: 1336.0\n",
      "lif layer 2 self.abs_max_v: 1956.0\n",
      "lif layer 2 self.abs_max_v: 1974.0\n",
      "lif layer 2 self.abs_max_v: 2024.0\n",
      "fc layer 1 self.abs_max_out: 1158.0\n",
      "lif layer 1 self.abs_max_v: 2164.5\n",
      "fc layer 1 self.abs_max_out: 1179.0\n",
      "fc layer 1 self.abs_max_out: 1270.0\n",
      "fc layer 1 self.abs_max_out: 1312.0\n",
      "lif layer 1 self.abs_max_v: 2241.0\n",
      "lif layer 1 self.abs_max_v: 2395.5\n",
      "lif layer 1 self.abs_max_v: 2458.0\n",
      "lif layer 1 self.abs_max_v: 2461.0\n",
      "fc layer 1 self.abs_max_out: 1340.0\n",
      "lif layer 2 self.abs_max_v: 2037.0\n",
      "fc layer 2 self.abs_max_out: 1342.0\n",
      "fc layer 2 self.abs_max_out: 1344.0\n",
      "fc layer 2 self.abs_max_out: 1352.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss:  6.965924/ 49.080685, val:  28.33%, val_best:  28.33%, tr:  98.26%, tr_best:  98.26%, epoch time: 74.65 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1884%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.8637%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 1721  17.579%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1380.0\n",
      "fc layer 2 self.abs_max_out: 1382.0\n",
      "fc layer 2 self.abs_max_out: 1476.0\n",
      "fc layer 3 self.abs_max_out: 216.0\n",
      "fc layer 2 self.abs_max_out: 1518.0\n",
      "fc layer 2 self.abs_max_out: 1523.0\n",
      "fc layer 2 self.abs_max_out: 1525.0\n",
      "fc layer 2 self.abs_max_out: 1526.0\n",
      "fc layer 3 self.abs_max_out: 243.0\n",
      "fc layer 2 self.abs_max_out: 1584.0\n",
      "fc layer 2 self.abs_max_out: 1596.0\n",
      "fc layer 2 self.abs_max_out: 1622.0\n",
      "fc layer 2 self.abs_max_out: 1666.0\n",
      "fc layer 2 self.abs_max_out: 1671.0\n",
      "fc layer 2 self.abs_max_out: 1738.0\n",
      "lif layer 2 self.abs_max_v: 2101.5\n",
      "lif layer 2 self.abs_max_v: 2130.5\n",
      "lif layer 2 self.abs_max_v: 2172.0\n",
      "lif layer 2 self.abs_max_v: 2183.5\n",
      "fc layer 2 self.abs_max_out: 1781.0\n",
      "fc layer 1 self.abs_max_out: 1361.0\n",
      "lif layer 2 self.abs_max_v: 2199.5\n",
      "lif layer 2 self.abs_max_v: 2277.5\n",
      "fc layer 3 self.abs_max_out: 257.0\n",
      "lif layer 2 self.abs_max_v: 2368.5\n",
      "lif layer 2 self.abs_max_v: 2389.5\n",
      "lif layer 2 self.abs_max_v: 2395.0\n",
      "fc layer 1 self.abs_max_out: 1435.0\n",
      "lif layer 1 self.abs_max_v: 2464.0\n",
      "fc layer 3 self.abs_max_out: 266.0\n",
      "fc layer 3 self.abs_max_out: 269.0\n",
      "lif layer 2 self.abs_max_v: 2409.5\n",
      "lif layer 2 self.abs_max_v: 2464.0\n",
      "fc layer 2 self.abs_max_out: 1799.0\n",
      "fc layer 2 self.abs_max_out: 1811.0\n",
      "fc layer 2 self.abs_max_out: 1852.0\n",
      "fc layer 2 self.abs_max_out: 1874.0\n",
      "fc layer 2 self.abs_max_out: 1877.0\n",
      "fc layer 2 self.abs_max_out: 1884.0\n",
      "fc layer 2 self.abs_max_out: 1914.0\n",
      "fc layer 2 self.abs_max_out: 1950.0\n",
      "lif layer 2 self.abs_max_v: 2485.5\n",
      "lif layer 2 self.abs_max_v: 2649.0\n",
      "lif layer 2 self.abs_max_v: 2675.0\n",
      "lif layer 2 self.abs_max_v: 2736.5\n",
      "fc layer 2 self.abs_max_out: 1952.0\n",
      "fc layer 3 self.abs_max_out: 276.0\n",
      "fc layer 2 self.abs_max_out: 1981.0\n",
      "fc layer 2 self.abs_max_out: 2039.0\n",
      "fc layer 2 self.abs_max_out: 2066.0\n",
      "fc layer 2 self.abs_max_out: 2067.0\n",
      "fc layer 2 self.abs_max_out: 2103.0\n",
      "fc layer 2 self.abs_max_out: 2129.0\n",
      "fc layer 2 self.abs_max_out: 2141.0\n",
      "fc layer 2 self.abs_max_out: 2180.0\n",
      "lif layer 2 self.abs_max_v: 2777.0\n",
      "fc layer 2 self.abs_max_out: 2197.0\n",
      "fc layer 1 self.abs_max_out: 1470.0\n",
      "fc layer 1 self.abs_max_out: 1479.0\n",
      "lif layer 1 self.abs_max_v: 2528.5\n",
      "lif layer 1 self.abs_max_v: 2706.5\n",
      "fc layer 1 self.abs_max_out: 1575.0\n",
      "lif layer 1 self.abs_max_v: 2823.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss:  8.218301/ 38.810112, val:  41.67%, val_best:  41.67%, tr:  98.88%, tr_best:  98.88%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0584%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.6623%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3316  16.936%\n",
      "fc layer 3 self.abs_max_out: 287.0\n",
      "lif layer 2 self.abs_max_v: 2798.0\n",
      "fc layer 2 self.abs_max_out: 2232.0\n",
      "lif layer 2 self.abs_max_v: 2842.5\n",
      "fc layer 2 self.abs_max_out: 2248.0\n",
      "fc layer 3 self.abs_max_out: 308.0\n",
      "lif layer 2 self.abs_max_v: 2897.0\n",
      "lif layer 2 self.abs_max_v: 3046.5\n",
      "fc layer 3 self.abs_max_out: 310.0\n",
      "fc layer 2 self.abs_max_out: 2261.0\n",
      "fc layer 2 self.abs_max_out: 2262.0\n",
      "fc layer 2 self.abs_max_out: 2304.0\n",
      "fc layer 2 self.abs_max_out: 2343.0\n",
      "fc layer 2 self.abs_max_out: 2354.0\n",
      "lif layer 2 self.abs_max_v: 3119.0\n",
      "fc layer 2 self.abs_max_out: 2365.0\n",
      "lif layer 2 self.abs_max_v: 3216.0\n",
      "lif layer 2 self.abs_max_v: 3264.0\n",
      "lif layer 2 self.abs_max_v: 3268.0\n",
      "lif layer 2 self.abs_max_v: 3305.0\n",
      "lif layer 2 self.abs_max_v: 3372.0\n",
      "lif layer 2 self.abs_max_v: 3519.5\n",
      "fc layer 2 self.abs_max_out: 2378.0\n",
      "fc layer 2 self.abs_max_out: 2424.0\n",
      "fc layer 2 self.abs_max_out: 2442.0\n",
      "fc layer 1 self.abs_max_out: 1581.0\n",
      "fc layer 2 self.abs_max_out: 2468.0\n",
      "lif layer 2 self.abs_max_v: 3652.0\n",
      "lif layer 2 self.abs_max_v: 3710.5\n",
      "fc layer 2 self.abs_max_out: 2513.0\n",
      "fc layer 2 self.abs_max_out: 2576.0\n",
      "lif layer 2 self.abs_max_v: 3762.5\n",
      "lif layer 2 self.abs_max_v: 3853.5\n",
      "lif layer 2 self.abs_max_v: 3891.0\n",
      "fc layer 2 self.abs_max_out: 2594.0\n",
      "fc layer 2 self.abs_max_out: 2598.0\n",
      "fc layer 2 self.abs_max_out: 2654.0\n",
      "fc layer 1 self.abs_max_out: 1612.0\n",
      "fc layer 1 self.abs_max_out: 1695.0\n",
      "lif layer 1 self.abs_max_v: 2838.5\n",
      "fc layer 1 self.abs_max_out: 1757.0\n",
      "lif layer 1 self.abs_max_v: 3176.5\n",
      "lif layer 2 self.abs_max_v: 3987.5\n",
      "lif layer 2 self.abs_max_v: 4056.5\n",
      "lif layer 2 self.abs_max_v: 4078.5\n",
      "lif layer 2 self.abs_max_v: 4178.5\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss:  8.260167/ 57.048294, val:  38.75%, val_best:  41.67%, tr:  98.57%, tr_best:  98.88%, epoch time: 74.50 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.2721%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.1963%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 4743  16.149%\n",
      "fc layer 1 self.abs_max_out: 1785.0\n",
      "lif layer 1 self.abs_max_v: 3229.0\n",
      "lif layer 2 self.abs_max_v: 4254.5\n",
      "fc layer 3 self.abs_max_out: 338.0\n",
      "lif layer 2 self.abs_max_v: 4368.0\n",
      "lif layer 2 self.abs_max_v: 4552.0\n",
      "lif layer 2 self.abs_max_v: 4610.5\n",
      "lif layer 2 self.abs_max_v: 4655.0\n",
      "fc layer 1 self.abs_max_out: 1882.0\n",
      "fc layer 1 self.abs_max_out: 1909.0\n",
      "lif layer 1 self.abs_max_v: 3229.5\n",
      "fc layer 1 self.abs_max_out: 1927.0\n",
      "lif layer 1 self.abs_max_v: 3533.0\n",
      "fc layer 1 self.abs_max_out: 1975.0\n",
      "lif layer 1 self.abs_max_v: 3618.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  8.094145/ 59.559101, val:  28.33%, val_best:  41.67%, tr:  98.37%, tr_best:  98.88%, epoch time: 74.69 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2751%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.1411%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6117  15.621%\n",
      "fc layer 2 self.abs_max_out: 2671.0\n",
      "fc layer 2 self.abs_max_out: 2688.0\n",
      "fc layer 2 self.abs_max_out: 2803.0\n",
      "lif layer 2 self.abs_max_v: 4720.0\n",
      "fc layer 1 self.abs_max_out: 2062.0\n",
      "lif layer 1 self.abs_max_v: 3751.5\n",
      "fc layer 1 self.abs_max_out: 2140.0\n",
      "lif layer 1 self.abs_max_v: 3776.0\n",
      "fc layer 1 self.abs_max_out: 2253.0\n",
      "lif layer 1 self.abs_max_v: 4141.0\n",
      "lif layer 2 self.abs_max_v: 4862.5\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  7.642323/ 57.603603, val:  32.50%, val_best:  41.67%, tr:  98.88%, tr_best:  98.88%, epoch time: 74.81 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7208%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.9217%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 7477  15.275%\n",
      "fc layer 1 self.abs_max_out: 2319.0\n",
      "lif layer 2 self.abs_max_v: 4902.5\n",
      "lif layer 2 self.abs_max_v: 4937.5\n",
      "fc layer 2 self.abs_max_out: 3075.0\n",
      "lif layer 2 self.abs_max_v: 5037.5\n",
      "lif layer 2 self.abs_max_v: 5215.0\n",
      "fc layer 1 self.abs_max_out: 2385.0\n",
      "fc layer 1 self.abs_max_out: 2410.0\n",
      "lif layer 1 self.abs_max_v: 4432.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  7.538672/ 39.904667, val:  39.58%, val_best:  41.67%, tr:  98.57%, tr_best:  98.88%, epoch time: 75.03 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5173%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.0771%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 8847  15.061%\n",
      "lif layer 2 self.abs_max_v: 5269.0\n",
      "lif layer 2 self.abs_max_v: 5423.5\n",
      "lif layer 2 self.abs_max_v: 5571.0\n",
      "fc layer 2 self.abs_max_out: 3077.0\n",
      "lif layer 2 self.abs_max_v: 5829.0\n",
      "fc layer 1 self.abs_max_out: 2417.0\n",
      "lif layer 1 self.abs_max_v: 4446.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  6.637997/ 58.432556, val:  39.17%, val_best:  41.67%, tr:  98.37%, tr_best:  98.88%, epoch time: 75.16 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9159%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.4424%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10187  14.865%\n",
      "fc layer 2 self.abs_max_out: 3226.0\n",
      "lif layer 2 self.abs_max_v: 5839.5\n",
      "fc layer 2 self.abs_max_out: 3264.0\n",
      "lif layer 2 self.abs_max_v: 6039.0\n",
      "lif layer 2 self.abs_max_v: 6113.5\n",
      "fc layer 2 self.abs_max_out: 3303.0\n",
      "fc layer 2 self.abs_max_out: 3336.0\n",
      "fc layer 1 self.abs_max_out: 2652.0\n",
      "fc layer 1 self.abs_max_out: 2791.0\n",
      "lif layer 1 self.abs_max_v: 4685.5\n",
      "lif layer 1 self.abs_max_v: 5064.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  6.856523/ 45.967438, val:  41.67%, val_best:  41.67%, tr:  98.06%, tr_best:  98.88%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5185%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.9281%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 11530  14.722%\n",
      "fc layer 2 self.abs_max_out: 3452.0\n",
      "lif layer 2 self.abs_max_v: 6274.0\n",
      "lif layer 2 self.abs_max_v: 6455.0\n",
      "lif layer 2 self.abs_max_v: 6562.5\n",
      "lif layer 2 self.abs_max_v: 6573.5\n",
      "lif layer 2 self.abs_max_v: 6600.0\n",
      "lif layer 1 self.abs_max_v: 5070.5\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  6.501037/ 33.811302, val:  43.33%, val_best:  43.33%, tr:  99.08%, tr_best:  99.08%, epoch time: 74.95 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0760%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.1579%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 12894  14.634%\n",
      "fc layer 2 self.abs_max_out: 3479.0\n",
      "fc layer 2 self.abs_max_out: 3484.0\n",
      "lif layer 2 self.abs_max_v: 6604.5\n",
      "fc layer 2 self.abs_max_out: 3643.0\n",
      "fc layer 2 self.abs_max_out: 3685.0\n",
      "fc layer 2 self.abs_max_out: 3699.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  6.020388/ 32.520302, val:  48.33%, val_best:  48.33%, tr:  98.67%, tr_best:  99.08%, epoch time: 74.51 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0934%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.3476%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 14195  14.499%\n",
      "fc layer 2 self.abs_max_out: 3770.0\n",
      "lif layer 2 self.abs_max_v: 6708.0\n",
      "lif layer 2 self.abs_max_v: 6945.0\n",
      "lif layer 2 self.abs_max_v: 7074.5\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  5.795158/ 37.025291, val:  44.58%, val_best:  48.33%, tr:  98.37%, tr_best:  99.08%, epoch time: 74.93 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.2438%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.2123%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 15511  14.403%\n",
      "fc layer 1 self.abs_max_out: 2829.0\n",
      "fc layer 2 self.abs_max_out: 3791.0\n",
      "fc layer 2 self.abs_max_out: 3918.0\n",
      "fc layer 1 self.abs_max_out: 2862.0\n",
      "fc layer 1 self.abs_max_out: 3033.0\n",
      "lif layer 1 self.abs_max_v: 5085.0\n",
      "lif layer 1 self.abs_max_v: 5495.5\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  5.548516/ 28.624247, val:  54.17%, val_best:  54.17%, tr:  99.08%, tr_best:  99.08%, epoch time: 75.09 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0095%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.7097%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 16780  14.283%\n",
      "fc layer 2 self.abs_max_out: 4000.0\n",
      "fc layer 2 self.abs_max_out: 4162.0\n",
      "fc layer 1 self.abs_max_out: 3066.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  5.178112/ 33.205166, val:  42.50%, val_best:  54.17%, tr:  98.77%, tr_best:  99.08%, epoch time: 74.83 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0483%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.1867%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 18012  14.153%\n",
      "lif layer 2 self.abs_max_v: 7089.5\n",
      "fc layer 1 self.abs_max_out: 3113.0\n",
      "lif layer 1 self.abs_max_v: 5663.0\n",
      "fc layer 2 self.abs_max_out: 4165.0\n",
      "fc layer 2 self.abs_max_out: 4234.0\n",
      "lif layer 2 self.abs_max_v: 7503.5\n",
      "lif layer 2 self.abs_max_v: 7514.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  5.131362/ 35.435867, val:  43.75%, val_best:  54.17%, tr:  98.16%, tr_best:  99.08%, epoch time: 74.84 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.2619%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.5502%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 19254  14.048%\n",
      "fc layer 2 self.abs_max_out: 4414.0\n",
      "fc layer 1 self.abs_max_out: 3160.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  5.184893/ 29.482996, val:  45.42%, val_best:  54.17%, tr:  98.67%, tr_best:  99.08%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6139%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.7083%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 20546  13.991%\n",
      "fc layer 2 self.abs_max_out: 4417.0\n",
      "lif layer 2 self.abs_max_v: 7569.5\n",
      "lif layer 2 self.abs_max_v: 7775.0\n",
      "lif layer 2 self.abs_max_v: 7963.0\n",
      "fc layer 1 self.abs_max_out: 3249.0\n",
      "fc layer 1 self.abs_max_out: 3250.0\n",
      "lif layer 1 self.abs_max_v: 5916.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  4.864833/ 19.592583, val:  53.33%, val_best:  54.17%, tr:  98.47%, tr_best:  99.08%, epoch time: 74.92 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5459%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.0326%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 21748  13.884%\n",
      "fc layer 1 self.abs_max_out: 3303.0\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  4.641924/ 32.516914, val:  50.42%, val_best:  54.17%, tr:  98.88%, tr_best:  99.08%, epoch time: 74.68 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7731%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.5373%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 22905  13.763%\n",
      "lif layer 2 self.abs_max_v: 8156.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  4.557277/ 21.640379, val:  57.08%, val_best:  57.08%, tr:  98.57%, tr_best:  99.08%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.1977%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.7795%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 24088  13.669%\n",
      "fc layer 2 self.abs_max_out: 4548.0\n",
      "lif layer 2 self.abs_max_v: 8184.5\n",
      "lif layer 2 self.abs_max_v: 8241.5\n",
      "fc layer 1 self.abs_max_out: 3416.0\n",
      "fc layer 1 self.abs_max_out: 3437.0\n",
      "lif layer 1 self.abs_max_v: 6258.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  4.642508/ 26.836388, val:  45.83%, val_best:  57.08%, tr:  99.18%, tr_best:  99.18%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3664%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.7137%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 25312  13.608%\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  4.388690/ 24.542500, val:  44.17%, val_best:  57.08%, tr:  98.77%, tr_best:  99.18%, epoch time: 74.94 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3628%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.8747%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 26455  13.511%\n",
      "fc layer 2 self.abs_max_out: 4622.0\n",
      "lif layer 2 self.abs_max_v: 8245.5\n",
      "lif layer 2 self.abs_max_v: 8463.0\n",
      "fc layer 1 self.abs_max_out: 3539.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  4.308469/ 27.226566, val:  51.67%, val_best:  57.08%, tr:  98.57%, tr_best:  99.18%, epoch time: 75.32 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8065%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.8089%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 27578  13.414%\n",
      "fc layer 2 self.abs_max_out: 4631.0\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  4.449396/ 18.476889, val:  58.75%, val_best:  58.75%, tr:  98.47%, tr_best:  99.18%, epoch time: 71.73 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8522%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.1155%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 28761  13.354%\n",
      "lif layer 2 self.abs_max_v: 8489.5\n",
      "lif layer 2 self.abs_max_v: 8531.0\n",
      "fc layer 1 self.abs_max_out: 3584.0\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  4.425652/ 17.166084, val:  62.92%, val_best:  62.92%, tr:  98.26%, tr_best:  99.18%, epoch time: 73.21 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7336%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.9483%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 29911  13.284%\n",
      "lif layer 2 self.abs_max_v: 8580.0\n",
      "lif layer 2 self.abs_max_v: 8634.5\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  4.402909/ 23.602581, val:  52.50%, val_best:  62.92%, tr:  98.98%, tr_best:  99.18%, epoch time: 73.73 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0713%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.1233%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 31045  13.213%\n",
      "fc layer 2 self.abs_max_out: 4691.0\n",
      "fc layer 1 self.abs_max_out: 3807.0\n",
      "lif layer 1 self.abs_max_v: 6332.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  4.449677/ 25.923918, val:  47.92%, val_best:  62.92%, tr:  98.98%, tr_best:  99.18%, epoch time: 73.86 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7848%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.2304%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 32192  13.153%\n",
      "lif layer 2 self.abs_max_v: 8640.5\n",
      "fc layer 2 self.abs_max_out: 4719.0\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  4.585789/ 22.762154, val:  55.00%, val_best:  62.92%, tr:  98.47%, tr_best:  99.18%, epoch time: 74.35 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8386%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.2495%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 33409  13.125%\n",
      "fc layer 1 self.abs_max_out: 3875.0\n",
      "lif layer 1 self.abs_max_v: 6534.5\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  4.227386/ 23.849228, val:  58.33%, val_best:  62.92%, tr:  98.67%, tr_best:  99.18%, epoch time: 74.51 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6042%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.2401%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 34472  13.041%\n",
      "lif layer 2 self.abs_max_v: 8701.5\n",
      "lif layer 2 self.abs_max_v: 8705.5\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  4.247909/ 23.786709, val:  59.17%, val_best:  62.92%, tr:  98.77%, tr_best:  99.18%, epoch time: 75.44 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2509%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.5758%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 35596  12.986%\n",
      "fc layer 2 self.abs_max_out: 4730.0\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  4.159892/ 18.742487, val:  63.33%, val_best:  63.33%, tr:  99.18%, tr_best:  99.18%, epoch time: 74.68 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9452%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.7276%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 36694  12.925%\n",
      "fc layer 1 self.abs_max_out: 3876.0\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  4.075986/ 32.617603, val:  52.92%, val_best:  63.33%, tr:  98.98%, tr_best:  99.18%, epoch time: 75.01 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9128%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.7400%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 37791  12.867%\n",
      "fc layer 1 self.abs_max_out: 3909.0\n",
      "lif layer 1 self.abs_max_v: 6598.0\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  4.014257/ 23.487812, val:  60.00%, val_best:  63.33%, tr:  99.08%, tr_best:  99.18%, epoch time: 74.64 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0128%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8829%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 38839  12.797%\n",
      "fc layer 1 self.abs_max_out: 3949.0\n",
      "lif layer 1 self.abs_max_v: 6632.0\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  4.005191/ 22.927637, val:  54.58%, val_best:  63.33%, tr:  99.28%, tr_best:  99.28%, epoch time: 74.59 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6025%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0895%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 39924  12.744%\n",
      "fc layer 1 self.abs_max_out: 4021.0\n",
      "lif layer 1 self.abs_max_v: 6827.5\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  3.848790/ 28.814156, val:  46.25%, val_best:  63.33%, tr:  98.98%, tr_best:  99.28%, epoch time: 74.22 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2792%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2900%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 40981  12.685%\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  4.202685/ 24.862587, val:  52.50%, val_best:  63.33%, tr:  98.57%, tr_best:  99.28%, epoch time: 74.42 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2911%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9550%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 42109  12.651%\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  3.905316/ 28.814066, val:  50.83%, val_best:  63.33%, tr:  98.88%, tr_best:  99.28%, epoch time: 74.84 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8509%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3121%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 43154  12.594%\n",
      "fc layer 1 self.abs_max_out: 4081.0\n",
      "lif layer 1 self.abs_max_v: 6924.0\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  3.807679/ 23.792355, val:  41.25%, val_best:  63.33%, tr:  98.88%, tr_best:  99.28%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.1206%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6222%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 44207  12.543%\n",
      "fc layer 1 self.abs_max_out: 4112.0\n",
      "lif layer 1 self.abs_max_v: 6962.0\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  3.721684/ 21.013954, val:  63.33%, val_best:  63.33%, tr:  98.77%, tr_best:  99.28%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6198%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5640%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 45239  12.489%\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  3.598440/ 15.905412, val:  65.00%, val_best:  65.00%, tr:  98.98%, tr_best:  99.28%, epoch time: 74.82 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1107%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4619%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 46221  12.424%\n",
      "fc layer 2 self.abs_max_out: 4807.0\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  3.854264/ 25.056892, val:  56.25%, val_best:  65.00%, tr:  99.08%, tr_best:  99.28%, epoch time: 74.47 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2811%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4006%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 47259  12.378%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  3.574104/ 25.907396, val:  46.25%, val_best:  65.00%, tr:  99.39%, tr_best:  99.39%, epoch time: 74.17 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3164%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5136%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 48229  12.316%\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  3.662677/ 15.840083, val:  56.67%, val_best:  65.00%, tr:  99.28%, tr_best:  99.39%, epoch time: 74.33 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8374%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3838%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 49225  12.264%\n",
      "fc layer 1 self.abs_max_out: 4139.0\n",
      "lif layer 1 self.abs_max_v: 6987.5\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  3.933856/ 23.628988, val:  52.50%, val_best:  65.00%, tr:  99.28%, tr_best:  99.39%, epoch time: 74.59 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5213%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0227%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 50258  12.223%\n",
      "fc layer 2 self.abs_max_out: 4835.0\n",
      "fc layer 1 self.abs_max_out: 4154.0\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  3.841076/ 24.458361, val:  50.42%, val_best:  65.00%, tr:  99.28%, tr_best:  99.39%, epoch time: 73.96 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6225%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5224%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 51261  12.177%\n",
      "fc layer 1 self.abs_max_out: 4246.0\n",
      "lif layer 1 self.abs_max_v: 7196.0\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  3.612981/ 20.722315, val:  57.50%, val_best:  65.00%, tr:  99.18%, tr_best:  99.39%, epoch time: 74.86 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5914%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9088%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 52268  12.134%\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  3.632451/ 22.598852, val:  60.42%, val_best:  65.00%, tr:  98.98%, tr_best:  99.39%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1213%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9947%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 53291  12.096%\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  3.531158/ 26.843563, val:  50.83%, val_best:  65.00%, tr:  99.28%, tr_best:  99.39%, epoch time: 75.13 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7142%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2189%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 54302  12.058%\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  3.614990/ 20.102428, val:  60.83%, val_best:  65.00%, tr:  99.08%, tr_best:  99.39%, epoch time: 74.55 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2849%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7595%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 55298  12.018%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  3.687632/ 29.679403, val:  49.17%, val_best:  65.00%, tr:  99.18%, tr_best:  99.39%, epoch time: 75.61 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3444%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8383%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 56323  11.986%\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  3.293340/ 21.726908, val:  57.92%, val_best:  65.00%, tr:  99.49%, tr_best:  99.49%, epoch time: 74.49 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6369%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3913%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 57281  11.941%\n",
      "fc layer 2 self.abs_max_out: 4989.0\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  3.495325/ 18.438522, val:  66.67%, val_best:  66.67%, tr:  98.67%, tr_best:  99.49%, epoch time: 75.27 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2946%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3303%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 58287  11.907%\n",
      "fc layer 1 self.abs_max_out: 4360.0\n",
      "lif layer 1 self.abs_max_v: 7449.5\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  3.594155/ 18.281477, val:  57.08%, val_best:  66.67%, tr:  98.88%, tr_best:  99.49%, epoch time: 74.51 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5842%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5160%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 59292  11.875%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  3.615306/ 22.490038, val:  50.83%, val_best:  66.67%, tr:  99.49%, tr_best:  99.49%, epoch time: 75.44 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7626%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0062%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 60299  11.845%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  3.588657/ 16.122152, val:  57.08%, val_best:  66.67%, tr:  99.39%, tr_best:  99.49%, epoch time: 75.77 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6532%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3104%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 61308  11.816%\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  3.415055/ 20.216803, val:  57.50%, val_best:  66.67%, tr:  98.77%, tr_best:  99.49%, epoch time: 74.71 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4657%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6320%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 62297  11.784%\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  3.434166/ 24.513285, val:  53.75%, val_best:  66.67%, tr:  98.98%, tr_best:  99.49%, epoch time: 74.41 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2028%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4400%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 63279  11.752%\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  3.771423/ 16.050232, val:  66.25%, val_best:  66.67%, tr:  98.98%, tr_best:  99.49%, epoch time: 75.19 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0601%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1151%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 64293  11.727%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  3.490534/ 20.686142, val:  57.92%, val_best:  66.67%, tr:  99.59%, tr_best:  99.59%, epoch time: 74.92 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1689%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0694%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 65273  11.697%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  3.400648/ 23.359474, val:  57.92%, val_best:  66.67%, tr:  98.98%, tr_best:  99.59%, epoch time: 74.91 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9766%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2526%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 66242  11.666%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  3.501972/ 18.231678, val:  67.08%, val_best:  67.08%, tr:  98.77%, tr_best:  99.59%, epoch time: 74.23 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0959%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0115%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 67204  11.635%\n",
      "fc layer 1 self.abs_max_out: 4530.0\n",
      "lif layer 1 self.abs_max_v: 7821.0\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  3.542205/ 27.248207, val:  48.75%, val_best:  67.08%, tr:  99.39%, tr_best:  99.59%, epoch time: 74.52 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0799%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 68186  11.608%\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  3.488564/ 28.954987, val:  49.17%, val_best:  67.08%, tr:  98.77%, tr_best:  99.59%, epoch time: 74.43 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3624%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2338%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 69118  11.574%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  3.482769/ 17.411860, val:  69.58%, val_best:  69.58%, tr:  99.39%, tr_best:  99.59%, epoch time: 74.67 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2025%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0843%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 70078  11.545%\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  3.395637/ 23.129152, val:  60.83%, val_best:  69.58%, tr:  99.08%, tr_best:  99.59%, epoch time: 74.54 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4505%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6652%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 71047  11.519%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  3.173119/ 18.754475, val:  59.58%, val_best:  69.58%, tr:  99.39%, tr_best:  99.59%, epoch time: 74.77 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0827%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6468%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 71951  11.483%\n",
      "fc layer 1 self.abs_max_out: 4609.0\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  3.651934/ 27.985325, val:  40.83%, val_best:  69.58%, tr:  99.08%, tr_best:  99.59%, epoch time: 74.38 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4384%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2258%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 72930  11.461%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  3.356709/ 15.204924, val:  65.83%, val_best:  69.58%, tr:  98.98%, tr_best:  99.59%, epoch time: 74.65 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6388%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3630%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 73860  11.431%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  3.435344/ 23.567507, val:  59.17%, val_best:  69.58%, tr:  98.77%, tr_best:  99.59%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6411%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3939%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 74814  11.406%\n",
      "fc layer 1 self.abs_max_out: 4654.0\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  3.344776/ 21.062731, val:  61.25%, val_best:  69.58%, tr:  99.59%, tr_best:  99.59%, epoch time: 74.77 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3109%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4744%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 75736  11.377%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  3.378099/ 15.916752, val:  62.50%, val_best:  69.58%, tr:  99.08%, tr_best:  99.59%, epoch time: 75.37 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4756%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4377%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 76705  11.355%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  2.996153/ 16.081667, val:  57.08%, val_best:  69.58%, tr:  99.18%, tr_best:  99.59%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4538%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8131%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 77557  11.317%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  3.350399/ 14.811749, val:  66.67%, val_best:  69.58%, tr:  99.08%, tr_best:  99.59%, epoch time: 75.09 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5236%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8257%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 78506  11.294%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  3.369477/ 21.414753, val:  53.75%, val_best:  69.58%, tr:  98.98%, tr_best:  99.59%, epoch time: 74.40 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6311%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9128%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 79452  11.272%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  3.062429/ 20.076960, val:  66.25%, val_best:  69.58%, tr:  99.08%, tr_best:  99.59%, epoch time: 74.65 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6736%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6963%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 80305  11.237%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  3.071279/ 20.483593, val:  61.25%, val_best:  69.58%, tr:  99.08%, tr_best:  99.59%, epoch time: 74.21 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5588%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.7747%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 81179  11.205%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  3.239206/ 20.828085, val:  63.33%, val_best:  69.58%, tr:  99.39%, tr_best:  99.59%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8793%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5617%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 82074  11.178%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  3.225450/ 18.640059, val:  68.33%, val_best:  69.58%, tr:  98.98%, tr_best:  99.59%, epoch time: 74.83 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7468%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4947%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 82963  11.150%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  3.444096/ 18.830564, val:  58.75%, val_best:  69.58%, tr:  99.08%, tr_best:  99.59%, epoch time: 74.37 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6453%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5598%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 83901  11.130%\n",
      "fc layer 1 self.abs_max_out: 4680.0\n",
      "lif layer 1 self.abs_max_v: 8111.5\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  3.185546/ 25.530457, val:  52.92%, val_best:  69.58%, tr:  98.98%, tr_best:  99.59%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3728%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6452%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 84787  11.103%\n",
      "fc layer 1 self.abs_max_out: 4699.0\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  3.146338/ 23.473206, val:  59.17%, val_best:  69.58%, tr:  99.69%, tr_best:  99.69%, epoch time: 74.10 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6136%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6276%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 85657  11.075%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  3.191842/ 18.732178, val:  62.08%, val_best:  69.58%, tr:  99.69%, tr_best:  99.69%, epoch time: 74.55 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6650%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8439%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 86545  11.050%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  3.103107/ 17.470871, val:  67.08%, val_best:  69.58%, tr:  99.18%, tr_best:  99.69%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7341%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8121%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 87408  11.023%\n",
      "fc layer 1 self.abs_max_out: 4864.0\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  2.985173/ 30.575550, val:  45.42%, val_best:  69.58%, tr:  99.28%, tr_best:  99.69%, epoch time: 74.55 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8560%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6164%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 88248  10.993%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  3.355686/ 19.047729, val:  70.83%, val_best:  70.83%, tr:  98.67%, tr_best:  99.69%, epoch time: 74.73 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7740%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8212%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 89196  10.977%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  3.352646/ 22.588562, val:  50.83%, val_best:  70.83%, tr:  99.18%, tr_best:  99.69%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8345%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8671%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 90114  10.958%\n",
      "fc layer 1 self.abs_max_out: 4925.0\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  3.308185/ 19.688532, val:  49.17%, val_best:  70.83%, tr:  99.39%, tr_best:  99.69%, epoch time: 74.59 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6591%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9878%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 91039  10.940%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  3.226990/ 17.040110, val:  68.33%, val_best:  70.83%, tr:  99.69%, tr_best:  99.69%, epoch time: 74.95 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9840%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8785%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 91936  10.920%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  3.071528/ 22.864017, val:  60.42%, val_best:  70.83%, tr:  98.47%, tr_best:  99.69%, epoch time: 75.19 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7419%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0510%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 92813  10.897%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  3.301018/ 25.267698, val:  51.67%, val_best:  70.83%, tr:  98.88%, tr_best:  99.69%, epoch time: 75.14 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7103%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1376%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 93729  10.879%\n",
      "lif layer 1 self.abs_max_v: 8234.0\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  3.002440/ 23.899969, val:  57.08%, val_best:  70.83%, tr:  99.18%, tr_best:  99.69%, epoch time: 74.55 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5467%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9035%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 94594  10.857%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  3.213932/ 11.819122, val:  78.33%, val_best:  78.33%, tr:  98.26%, tr_best:  99.69%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7533%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9383%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 95476  10.836%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  3.304950/ 17.342773, val:  70.42%, val_best:  78.33%, tr:  98.98%, tr_best:  99.69%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6148%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1022%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 96395  10.820%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  3.020584/ 23.710686, val:  62.08%, val_best:  78.33%, tr:  99.18%, tr_best:  99.69%, epoch time: 74.93 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5846%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8026%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 97207  10.793%\n",
      "fc layer 1 self.abs_max_out: 4957.0\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  3.167286/ 17.301313, val:  68.33%, val_best:  78.33%, tr:  99.28%, tr_best:  99.69%, epoch time: 74.56 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6225%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9466%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 98085  10.773%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  3.112409/ 16.032188, val:  65.42%, val_best:  78.33%, tr:  99.59%, tr_best:  99.69%, epoch time: 74.63 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5012%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9873%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 98947  10.752%\n",
      "fc layer 1 self.abs_max_out: 5050.0\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  2.901106/ 22.130371, val:  57.50%, val_best:  78.33%, tr:  99.39%, tr_best:  99.69%, epoch time: 74.50 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3818%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4104%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 99758  10.726%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  3.167537/ 18.066610, val:  64.17%, val_best:  78.33%, tr:  99.39%, tr_best:  99.69%, epoch time: 74.95 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2355%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1664%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 100669  10.711%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  2.959175/ 24.693954, val:  57.08%, val_best:  78.33%, tr:  99.18%, tr_best:  99.69%, epoch time: 75.69 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4606%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0480%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 101515  10.690%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  2.847411/ 15.734138, val:  66.25%, val_best:  78.33%, tr:  99.18%, tr_best:  99.69%, epoch time: 74.81 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4867%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.3551%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 102305  10.663%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  2.947800/ 18.853252, val:  61.25%, val_best:  78.33%, tr:  98.77%, tr_best:  99.69%, epoch time: 74.83 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4522%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 103116  10.639%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  2.648138/ 16.787716, val:  68.33%, val_best:  78.33%, tr:  99.39%, tr_best:  99.69%, epoch time: 74.89 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2514%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.2980%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 103871  10.610%\n",
      "fc layer 1 self.abs_max_out: 5084.0\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  2.984938/ 17.860941, val:  67.08%, val_best:  78.33%, tr:  99.59%, tr_best:  99.69%, epoch time: 74.44 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4421%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.2977%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 104688  10.587%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  2.978701/ 15.947397, val:  59.17%, val_best:  78.33%, tr:  98.57%, tr_best:  99.69%, epoch time: 75.08 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3315%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.5536%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 105529  10.568%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  2.783859/ 16.849264, val:  58.33%, val_best:  78.33%, tr:  98.77%, tr_best:  99.69%, epoch time: 74.76 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4047%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6749%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 106354  10.547%\n",
      "lif layer 1 self.abs_max_v: 8395.5\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  2.830050/ 23.631626, val:  55.00%, val_best:  78.33%, tr:  99.08%, tr_best:  99.69%, epoch time: 74.86 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6070%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.2729%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 107172  10.526%\n",
      "fc layer 1 self.abs_max_out: 5255.0\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  2.946267/ 18.303394, val:  72.50%, val_best:  78.33%, tr:  99.08%, tr_best:  99.69%, epoch time: 74.52 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6419%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4635%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 107995  10.506%\n",
      "lif layer 1 self.abs_max_v: 8396.5\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  2.736050/ 23.593536, val:  62.50%, val_best:  78.33%, tr:  99.28%, tr_best:  99.69%, epoch time: 75.44 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3454%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1405%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 108759  10.480%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  2.717443/ 19.898491, val:  58.75%, val_best:  78.33%, tr:  99.49%, tr_best:  99.69%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6168%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0911%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 109512  10.454%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  3.078777/ 18.745235, val:  57.50%, val_best:  78.33%, tr:  99.18%, tr_best:  99.69%, epoch time: 74.81 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5639%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.3028%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 110360  10.438%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  2.922742/ 14.538293, val:  74.17%, val_best:  78.33%, tr:  99.39%, tr_best:  99.69%, epoch time: 73.86 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6554%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0450%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 111177  10.419%\n",
      "lif layer 1 self.abs_max_v: 8419.0\n",
      "lif layer 1 self.abs_max_v: 8443.0\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  2.838212/ 16.243799, val:  62.92%, val_best:  78.33%, tr:  98.98%, tr_best:  99.69%, epoch time: 74.54 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5935%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0525%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 111985  10.399%\n",
      "fc layer 1 self.abs_max_out: 5535.0\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  3.126089/ 15.291197, val:  70.42%, val_best:  78.33%, tr:  98.98%, tr_best:  99.69%, epoch time: 74.75 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7214%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9051%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 112834  10.383%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  2.947722/ 17.048416, val:  70.83%, val_best:  78.33%, tr:  99.69%, tr_best:  99.69%, epoch time: 74.82 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4341%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9482%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 113646  10.365%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  2.911711/ 20.242260, val:  65.00%, val_best:  78.33%, tr:  99.69%, tr_best:  99.69%, epoch time: 75.42 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4644%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4219%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 114460  10.346%\n",
      "lif layer 1 self.abs_max_v: 8500.5\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  2.916632/ 23.235174, val:  60.83%, val_best:  78.33%, tr:  99.49%, tr_best:  99.69%, epoch time: 75.01 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6982%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.3232%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 115268  10.328%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  3.074928/ 19.794460, val:  58.75%, val_best:  78.33%, tr:  98.88%, tr_best:  99.69%, epoch time: 74.45 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9584%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0640%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 116099  10.312%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  2.772365/ 21.573349, val:  63.75%, val_best:  78.33%, tr:  99.59%, tr_best:  99.69%, epoch time: 74.34 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7343%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0880%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 116874  10.291%\n",
      "fc layer 1 self.abs_max_out: 5715.0\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  3.020618/ 19.298056, val:  61.25%, val_best:  78.33%, tr:  99.49%, tr_best:  99.69%, epoch time: 74.42 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6597%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1893%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 117701  10.276%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  2.891482/ 15.665762, val:  61.67%, val_best:  78.33%, tr:  99.18%, tr_best:  99.69%, epoch time: 75.09 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5947%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.5613%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 118524  10.260%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  2.850646/ 26.573362, val:  55.00%, val_best:  78.33%, tr:  98.88%, tr_best:  99.69%, epoch time: 74.35 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5605%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.5916%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 119307  10.241%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  3.015796/ 18.835112, val:  57.08%, val_best:  78.33%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.22 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5817%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4144%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 120129  10.225%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  2.762174/ 17.637884, val:  74.17%, val_best:  78.33%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.32 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8170%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6685%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 120886  10.205%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  2.969768/ 25.692749, val:  51.67%, val_best:  78.33%, tr:  99.08%, tr_best:  99.69%, epoch time: 74.91 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6099%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4236%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 121716  10.191%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  2.892541/ 17.336971, val:  58.33%, val_best:  78.33%, tr:  99.18%, tr_best:  99.69%, epoch time: 74.60 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6323%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6404%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 122526  10.175%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  2.911363/ 13.310700, val:  74.17%, val_best:  78.33%, tr:  98.57%, tr_best:  99.69%, epoch time: 74.53 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7208%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4463%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 123350  10.161%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  2.810168/ 18.474001, val:  72.50%, val_best:  78.33%, tr:  99.39%, tr_best:  99.69%, epoch time: 73.87 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7194%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4347%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 124123  10.143%\n",
      "lif layer 1 self.abs_max_v: 8528.5\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  2.909641/ 18.322151, val:  66.25%, val_best:  78.33%, tr:  99.28%, tr_best:  99.69%, epoch time: 74.79 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6519%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6075%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 124946  10.129%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  3.017548/ 22.750307, val:  57.08%, val_best:  78.33%, tr:  99.08%, tr_best:  99.69%, epoch time: 73.91 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5782%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4256%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 125764  10.115%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  2.863732/ 26.527470, val:  42.50%, val_best:  78.33%, tr:  99.08%, tr_best:  99.69%, epoch time: 74.51 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7596%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.5603%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 126532  10.097%\n",
      "fc layer 1 self.abs_max_out: 5772.0\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  2.800002/ 21.951612, val:  60.00%, val_best:  78.33%, tr:  98.88%, tr_best:  99.69%, epoch time: 74.38 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6063%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.5039%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 127321  10.082%\n",
      "fc layer 1 self.abs_max_out: 5811.0\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  2.768853/ 21.079880, val:  57.50%, val_best:  78.33%, tr:  99.69%, tr_best:  99.69%, epoch time: 74.40 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.3722%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 128088  10.064%\n",
      "lif layer 1 self.abs_max_v: 8573.0\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  2.705410/ 22.081120, val:  68.33%, val_best:  78.33%, tr:  99.08%, tr_best:  99.69%, epoch time: 74.74 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5936%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4270%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 128851  10.047%\n",
      "lif layer 1 self.abs_max_v: 8587.0\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  2.955171/ 19.678011, val:  65.00%, val_best:  78.33%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.00 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6407%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.3981%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 129642  10.032%\n",
      "lif layer 1 self.abs_max_v: 8737.0\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  2.743867/ 20.460733, val:  63.75%, val_best:  78.33%, tr:  99.08%, tr_best:  99.69%, epoch time: 74.90 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4100%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4645%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 130384  10.014%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  2.857166/ 22.835110, val:  51.67%, val_best:  78.33%, tr:  99.49%, tr_best:  99.69%, epoch time: 74.75 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3933%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1914%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 131155   9.998%\n",
      "lif layer 1 self.abs_max_v: 8847.0\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  2.850038/ 14.543396, val:  75.00%, val_best:  78.33%, tr:  99.18%, tr_best:  99.69%, epoch time: 75.06 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7498%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4306%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 131926   9.982%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  2.869090/ 18.804466, val:  68.33%, val_best:  78.33%, tr:  99.18%, tr_best:  99.69%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1870%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6365%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 132713   9.968%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  2.585547/ 14.260493, val:  79.58%, val_best:  79.58%, tr:  99.69%, tr_best:  99.69%, epoch time: 74.68 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3458%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6877%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 133446   9.950%\n",
      "lif layer 1 self.abs_max_v: 8977.0\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  2.572032/ 13.322560, val:  70.83%, val_best:  79.58%, tr:  99.18%, tr_best:  99.69%, epoch time: 74.61 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6472%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7289%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 134148   9.929%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  2.547294/ 19.442957, val:  61.67%, val_best:  79.58%, tr:  98.67%, tr_best:  99.69%, epoch time: 74.52 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6551%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6311%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 134870   9.911%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  2.791659/ 17.776049, val:  70.00%, val_best:  79.58%, tr:  98.88%, tr_best:  99.69%, epoch time: 74.89 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6004%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6181%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 135652   9.897%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  2.639601/ 19.147772, val:  57.08%, val_best:  79.58%, tr:  99.08%, tr_best:  99.69%, epoch time: 74.86 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4801%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6282%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 136392   9.881%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  2.647367/ 19.682205, val:  52.50%, val_best:  79.58%, tr:  99.69%, tr_best:  99.69%, epoch time: 74.83 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8186%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.5507%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 137118   9.863%\n",
      "lif layer 1 self.abs_max_v: 9091.5\n",
      "lif layer 1 self.abs_max_v: 9236.5\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  2.965612/ 18.360275, val:  61.25%, val_best:  79.58%, tr:  99.18%, tr_best:  99.69%, epoch time: 75.91 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4512%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4308%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 137950   9.854%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  2.767903/ 20.278770, val:  68.33%, val_best:  79.58%, tr:  99.59%, tr_best:  99.69%, epoch time: 74.84 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8425%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6171%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 138694   9.838%\n",
      "fc layer 1 self.abs_max_out: 5979.0\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  2.615621/ 21.171133, val:  65.83%, val_best:  79.58%, tr:  99.18%, tr_best:  99.69%, epoch time: 74.80 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4562%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6588%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 139408   9.821%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  2.696657/ 21.905462, val:  53.75%, val_best:  79.58%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.37 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1957%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6330%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 140141   9.805%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  2.787252/ 21.738798, val:  64.17%, val_best:  79.58%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5748%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.5009%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 140890   9.790%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  2.542858/ 26.208834, val:  63.75%, val_best:  79.58%, tr:  99.39%, tr_best:  99.69%, epoch time: 74.67 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8056%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7703%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 141610   9.773%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  2.697389/ 13.965341, val:  74.58%, val_best:  79.58%, tr:  99.08%, tr_best:  99.69%, epoch time: 74.65 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7977%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9074%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 142330   9.757%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  2.565085/ 18.896641, val:  62.08%, val_best:  79.58%, tr:  99.28%, tr_best:  99.69%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9325%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9887%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 143071   9.743%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  2.663815/ 15.609071, val:  70.00%, val_best:  79.58%, tr:  99.49%, tr_best:  99.69%, epoch time: 75.16 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6615%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7274%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 143802   9.728%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  2.797317/ 18.674932, val:  71.25%, val_best:  79.58%, tr:  98.98%, tr_best:  99.69%, epoch time: 74.78 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5092%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 144608   9.718%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  2.620461/ 16.921066, val:  72.08%, val_best:  79.58%, tr:  98.88%, tr_best:  99.69%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6237%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7853%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 145348   9.704%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  2.634489/ 21.308197, val:  61.25%, val_best:  79.58%, tr:  99.28%, tr_best:  99.69%, epoch time: 75.00 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4439%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7277%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 146080   9.689%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  2.664822/ 15.467727, val:  71.25%, val_best:  79.58%, tr:  98.98%, tr_best:  99.69%, epoch time: 74.22 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3800%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7708%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 146826   9.676%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  2.581273/ 18.570848, val:  62.92%, val_best:  79.58%, tr:  99.28%, tr_best:  99.69%, epoch time: 74.46 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5614%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7288%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 147529   9.660%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  2.603309/ 21.311840, val:  55.83%, val_best:  79.58%, tr:  99.49%, tr_best:  99.69%, epoch time: 73.92 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5870%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.5698%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 148255   9.646%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  2.671288/ 18.021721, val:  70.83%, val_best:  79.58%, tr:  99.39%, tr_best:  99.69%, epoch time: 74.95 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7468%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.8461%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 148985   9.632%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  2.628915/ 17.141745, val:  71.25%, val_best:  79.58%, tr:  99.28%, tr_best:  99.69%, epoch time: 75.44 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4950%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0128%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 149709   9.618%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  2.789933/ 20.777140, val:  69.17%, val_best:  79.58%, tr:  99.39%, tr_best:  99.69%, epoch time: 74.73 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6903%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7236%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 150460   9.605%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  2.743755/ 18.061975, val:  62.08%, val_best:  79.58%, tr:  99.49%, tr_best:  99.69%, epoch time: 74.81 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5713%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7487%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 151197   9.593%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  2.955574/ 21.372059, val:  68.75%, val_best:  79.58%, tr:  99.18%, tr_best:  99.69%, epoch time: 74.95 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3827%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6797%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 151987   9.583%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  2.669071/ 22.630625, val:  66.25%, val_best:  79.58%, tr:  99.39%, tr_best:  99.69%, epoch time: 74.57 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3944%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9085%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 152698   9.569%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  2.639312/ 19.549284, val:  69.58%, val_best:  79.58%, tr:  99.59%, tr_best:  99.69%, epoch time: 75.02 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7338%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7557%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 153401   9.554%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  2.472528/ 18.163008, val:  70.42%, val_best:  79.58%, tr:  99.59%, tr_best:  99.69%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7016%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 154059   9.537%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  2.585253/ 16.851402, val:  69.58%, val_best:  79.58%, tr:  99.59%, tr_best:  99.69%, epoch time: 75.35 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7598%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.8135%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 154765   9.523%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  2.405440/ 13.506053, val:  78.75%, val_best:  79.58%, tr:  99.08%, tr_best:  99.69%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6361%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0864%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 155451   9.508%\n",
      "fc layer 1 self.abs_max_out: 6009.0\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  2.541250/ 22.966713, val:  63.75%, val_best:  79.58%, tr:  99.08%, tr_best:  99.69%, epoch time: 74.29 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6823%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9792%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 156177   9.496%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  2.567169/ 15.908347, val:  70.00%, val_best:  79.58%, tr:  99.69%, tr_best:  99.69%, epoch time: 74.94 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8074%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9538%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 156881   9.482%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  2.572637/ 20.522541, val:  67.50%, val_best:  79.58%, tr:  99.18%, tr_best:  99.69%, epoch time: 74.37 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9162%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9755%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 157599   9.469%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  2.608251/ 14.378575, val:  74.17%, val_best:  79.58%, tr:  99.28%, tr_best:  99.69%, epoch time: 74.91 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1141%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8068%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9883%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 158289   9.455%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  2.458462/ 19.025246, val:  60.83%, val_best:  79.58%, tr:  98.57%, tr_best:  99.69%, epoch time: 74.32 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8753%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.1214%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 158993   9.442%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  2.492603/ 20.125650, val:  69.17%, val_best:  79.58%, tr:  99.59%, tr_best:  99.69%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7051%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.8297%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 159676   9.428%\n",
      "fc layer 1 self.abs_max_out: 6061.0\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  2.520147/ 17.204803, val:  78.33%, val_best:  79.58%, tr:  99.59%, tr_best:  99.69%, epoch time: 75.15 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9218%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9278%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 160340   9.413%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  2.567867/ 18.123543, val:  68.75%, val_best:  79.58%, tr:  99.49%, tr_best:  99.69%, epoch time: 74.77 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7265%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9112%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 161026   9.399%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  2.593310/ 17.391815, val:  77.08%, val_best:  79.58%, tr:  99.59%, tr_best:  99.69%, epoch time: 74.32 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8119%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.8648%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 161746   9.387%\n",
      "fc layer 1 self.abs_max_out: 6071.0\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  2.683645/ 14.715231, val:  69.58%, val_best:  79.58%, tr:  99.08%, tr_best:  99.69%, epoch time: 74.35 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8820%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0307%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 162471   9.376%\n",
      "fc layer 1 self.abs_max_out: 6282.0\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  2.472803/ 16.830328, val:  69.17%, val_best:  79.58%, tr:  99.18%, tr_best:  99.69%, epoch time: 74.47 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0312%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9094%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.3482%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 163167   9.363%\n",
      "fc layer 1 self.abs_max_out: 6293.0\n",
      "lif layer 1 self.abs_max_v: 9282.5\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  2.489851/ 17.428806, val:  73.33%, val_best:  79.58%, tr:  99.28%, tr_best:  99.69%, epoch time: 74.80 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0376%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6470%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.2086%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 163845   9.350%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  2.722570/ 14.861818, val:  65.42%, val_best:  79.58%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.17 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5456%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0605%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 164605   9.341%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  2.573055/ 17.411211, val:  69.58%, val_best:  79.58%, tr:  99.39%, tr_best:  99.69%, epoch time: 74.90 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6432%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.8518%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 165299   9.328%\n",
      "fc layer 1 self.abs_max_out: 6434.0\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  2.538027/ 16.267462, val:  67.08%, val_best:  79.58%, tr:  98.88%, tr_best:  99.69%, epoch time: 74.91 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6461%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9567%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 165971   9.315%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  2.448265/ 15.480591, val:  76.67%, val_best:  79.58%, tr:  99.49%, tr_best:  99.69%, epoch time: 74.42 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6345%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.8833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 166634   9.301%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  2.392136/ 20.608599, val:  69.17%, val_best:  79.58%, tr:  99.18%, tr_best:  99.69%, epoch time: 74.04 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0426%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7941%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9234%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 167283   9.286%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  2.126816/ 16.886379, val:  72.08%, val_best:  79.58%, tr:  99.28%, tr_best:  99.69%, epoch time: 74.74 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6066%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.2763%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 167882   9.269%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  2.357722/ 17.489367, val:  68.75%, val_best:  79.58%, tr:  99.49%, tr_best:  99.69%, epoch time: 74.37 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6058%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9876%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 168544   9.256%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  2.476973/ 15.548601, val:  74.58%, val_best:  79.58%, tr:  99.18%, tr_best:  99.69%, epoch time: 74.34 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6003%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0081%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 169228   9.244%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  2.540998/ 14.185331, val:  81.25%, val_best:  81.25%, tr:  99.39%, tr_best:  99.69%, epoch time: 74.87 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7870%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0479%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 169928   9.233%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  2.393164/ 14.411002, val:  81.25%, val_best:  81.25%, tr:  99.39%, tr_best:  99.69%, epoch time: 74.18 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5029%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.2552%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 170599   9.220%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  2.559031/ 19.384398, val:  67.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.64 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6812%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.8875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 171294   9.209%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  2.518523/ 17.019501, val:  73.75%, val_best:  81.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 74.73 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7748%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0098%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 171957   9.196%\n",
      "lif layer 1 self.abs_max_v: 9570.0\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  2.497944/ 21.194130, val:  57.50%, val_best:  81.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.78 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7785%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0708%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 172644   9.185%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  2.622317/ 17.812504, val:  69.17%, val_best:  81.25%, tr:  99.08%, tr_best: 100.00%, epoch time: 74.95 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6005%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.8991%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 173337   9.174%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  2.525142/ 18.155596, val:  68.75%, val_best:  81.25%, tr:  99.28%, tr_best: 100.00%, epoch time: 74.51 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6495%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0343%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 174031   9.163%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  2.469800/ 21.782867, val:  63.33%, val_best:  81.25%, tr:  99.39%, tr_best: 100.00%, epoch time: 74.54 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6764%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9931%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 174701   9.151%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  2.438980/ 16.141537, val:  69.58%, val_best:  81.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7614%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.2247%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 175369   9.139%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  2.270267/ 17.132269, val:  73.33%, val_best:  81.25%, tr:  98.98%, tr_best: 100.00%, epoch time: 74.75 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5334%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0521%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 175989   9.125%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  2.459127/ 18.147461, val:  73.75%, val_best:  81.25%, tr:  99.18%, tr_best: 100.00%, epoch time: 75.17 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4572%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.1404%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 176676   9.114%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  2.462855/ 15.216922, val:  74.58%, val_best:  81.25%, tr:  99.39%, tr_best: 100.00%, epoch time: 74.01 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6324%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0452%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 177322   9.102%\n",
      "lif layer 1 self.abs_max_v: 9586.0\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  2.414760/ 20.351810, val:  72.92%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.73 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6715%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.8637%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f27bd8416044f81b81f42c1ca3faaca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÉ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÉ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÉ‚ñÖ‚ñá‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÅ‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÉ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99898</td></tr><tr><td>tr_epoch_loss</td><td>2.41476</td></tr><tr><td>val_acc_best</td><td>0.8125</td></tr><tr><td>val_acc_now</td><td>0.72917</td></tr><tr><td>val_loss</td><td>20.35181</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dry-sweep-6</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/w5ditk4p' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/w5ditk4p</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_052338-w5ditk4p/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 383nwo2z with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.046875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0234375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_093345-383nwo2z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/383nwo2z' target=\"_blank\">divine-sweep-11</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/383nwo2z' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/383nwo2z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': '20251214_093352_322', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 64, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 0.5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 32, 'lif_layer_v_threshold2': 256, 'init_scaling': [0.03125, 0.046875, 0.0234375], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 0.5, self.v_threshold 64\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 32, self.v_threshold 256\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=64, v_reset=10000, sg_width=0.5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=256, v_reset=10000, sg_width=32, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 46.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 59.5\n",
      "fc layer 1 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 69.0\n",
      "fc layer 2 self.abs_max_out: 6.0\n",
      "lif layer 2 self.abs_max_v: 6.0\n",
      "fc layer 1 self.abs_max_out: 62.0\n",
      "lif layer 1 self.abs_max_v: 90.0\n",
      "fc layer 2 self.abs_max_out: 29.0\n",
      "lif layer 2 self.abs_max_v: 29.0\n",
      "fc layer 1 self.abs_max_out: 101.0\n",
      "lif layer 1 self.abs_max_v: 138.0\n",
      "fc layer 2 self.abs_max_out: 39.0\n",
      "lif layer 2 self.abs_max_v: 43.5\n",
      "fc layer 1 self.abs_max_out: 147.0\n",
      "lif layer 1 self.abs_max_v: 210.0\n",
      "fc layer 2 self.abs_max_out: 49.0\n",
      "lif layer 2 self.abs_max_v: 64.5\n",
      "fc layer 1 self.abs_max_out: 200.0\n",
      "lif layer 1 self.abs_max_v: 270.0\n",
      "fc layer 2 self.abs_max_out: 54.0\n",
      "lif layer 2 self.abs_max_v: 86.0\n",
      "fc layer 1 self.abs_max_out: 225.0\n",
      "lif layer 1 self.abs_max_v: 360.0\n",
      "lif layer 2 self.abs_max_v: 90.0\n",
      "fc layer 1 self.abs_max_out: 277.0\n",
      "lif layer 1 self.abs_max_v: 424.0\n",
      "fc layer 2 self.abs_max_out: 64.0\n",
      "lif layer 2 self.abs_max_v: 109.0\n",
      "fc layer 1 self.abs_max_out: 338.0\n",
      "lif layer 1 self.abs_max_v: 436.0\n",
      "lif layer 1 self.abs_max_v: 484.0\n",
      "lif layer 1 self.abs_max_v: 549.5\n",
      "fc layer 1 self.abs_max_out: 388.0\n",
      "lif layer 1 self.abs_max_v: 650.0\n",
      "fc layer 1 self.abs_max_out: 638.0\n",
      "lif layer 1 self.abs_max_v: 963.0\n",
      "fc layer 1 self.abs_max_out: 820.0\n",
      "lif layer 1 self.abs_max_v: 1301.5\n",
      "lif layer 1 self.abs_max_v: 1400.0\n",
      "fc layer 1 self.abs_max_out: 841.0\n",
      "lif layer 1 self.abs_max_v: 1402.0\n",
      "lif layer 1 self.abs_max_v: 1512.5\n",
      "fc layer 1 self.abs_max_out: 1059.0\n",
      "lif layer 1 self.abs_max_v: 1783.0\n",
      "fc layer 1 self.abs_max_out: 1161.0\n",
      "lif layer 1 self.abs_max_v: 2039.5\n",
      "fc layer 1 self.abs_max_out: 1234.0\n",
      "lif layer 2 self.abs_max_v: 111.0\n",
      "fc layer 2 self.abs_max_out: 67.0\n",
      "fc layer 2 self.abs_max_out: 68.0\n",
      "lif layer 2 self.abs_max_v: 119.0\n",
      "lif layer 2 self.abs_max_v: 123.5\n",
      "lif layer 2 self.abs_max_v: 127.0\n",
      "fc layer 1 self.abs_max_out: 1644.0\n",
      "lif layer 1 self.abs_max_v: 2538.0\n",
      "fc layer 2 self.abs_max_out: 77.0\n",
      "lif layer 2 self.abs_max_v: 140.5\n",
      "fc layer 1 self.abs_max_out: 1701.0\n",
      "lif layer 1 self.abs_max_v: 2867.5\n",
      "fc layer 2 self.abs_max_out: 83.0\n",
      "fc layer 2 self.abs_max_out: 90.0\n",
      "lif layer 2 self.abs_max_v: 148.5\n",
      "lif layer 2 self.abs_max_v: 152.0\n",
      "lif layer 2 self.abs_max_v: 158.0\n",
      "lif layer 2 self.abs_max_v: 159.0\n",
      "fc layer 1 self.abs_max_out: 1722.0\n",
      "lif layer 2 self.abs_max_v: 165.5\n",
      "fc layer 1 self.abs_max_out: 1747.0\n",
      "fc layer 1 self.abs_max_out: 1818.0\n",
      "lif layer 1 self.abs_max_v: 2948.0\n",
      "fc layer 1 self.abs_max_out: 1955.0\n",
      "lif layer 1 self.abs_max_v: 3421.5\n",
      "lif layer 1 self.abs_max_v: 3467.0\n",
      "fc layer 1 self.abs_max_out: 2143.0\n",
      "lif layer 1 self.abs_max_v: 3865.5\n",
      "lif layer 1 self.abs_max_v: 4060.0\n",
      "fc layer 1 self.abs_max_out: 2384.0\n",
      "lif layer 1 self.abs_max_v: 4116.5\n",
      "fc layer 2 self.abs_max_out: 94.0\n",
      "lif layer 2 self.abs_max_v: 172.0\n",
      "lif layer 1 self.abs_max_v: 4135.5\n",
      "fc layer 1 self.abs_max_out: 2546.0\n",
      "lif layer 1 self.abs_max_v: 4285.0\n",
      "fc layer 1 self.abs_max_out: 3146.0\n",
      "lif layer 1 self.abs_max_v: 5288.5\n",
      "lif layer 1 self.abs_max_v: 5330.5\n",
      "fc layer 2 self.abs_max_out: 96.0\n",
      "fc layer 2 self.abs_max_out: 109.0\n",
      "lif layer 2 self.abs_max_v: 192.0\n",
      "lif layer 1 self.abs_max_v: 5627.0\n",
      "lif layer 2 self.abs_max_v: 196.5\n",
      "fc layer 2 self.abs_max_out: 114.0\n",
      "lif layer 2 self.abs_max_v: 212.5\n",
      "fc layer 2 self.abs_max_out: 116.0\n",
      "lif layer 2 self.abs_max_v: 222.5\n",
      "lif layer 2 self.abs_max_v: 225.5\n",
      "fc layer 2 self.abs_max_out: 117.0\n",
      "fc layer 2 self.abs_max_out: 119.0\n",
      "fc layer 2 self.abs_max_out: 120.0\n",
      "fc layer 1 self.abs_max_out: 3335.0\n",
      "lif layer 1 self.abs_max_v: 5840.5\n",
      "lif layer 1 self.abs_max_v: 6122.0\n",
      "lif layer 2 self.abs_max_v: 229.5\n",
      "fc layer 2 self.abs_max_out: 121.0\n",
      "fc layer 1 self.abs_max_out: 4382.0\n",
      "fc layer 1 self.abs_max_out: 5242.0\n",
      "lif layer 1 self.abs_max_v: 7433.0\n",
      "fc layer 1 self.abs_max_out: 5836.0\n",
      "lif layer 1 self.abs_max_v: 9552.5\n",
      "fc layer 2 self.abs_max_out: 128.0\n",
      "lif layer 1 self.abs_max_v: 10087.5\n",
      "fc layer 1 self.abs_max_out: 6014.0\n",
      "lif layer 1 self.abs_max_v: 11018.0\n",
      "lif layer 2 self.abs_max_v: 232.0\n",
      "fc layer 1 self.abs_max_out: 6358.0\n",
      "lif layer 2 self.abs_max_v: 233.5\n",
      "lif layer 2 self.abs_max_v: 237.5\n",
      "fc layer 2 self.abs_max_out: 131.0\n",
      "lif layer 2 self.abs_max_v: 249.5\n",
      "fc layer 2 self.abs_max_out: 132.0\n",
      "lif layer 2 self.abs_max_v: 256.0\n",
      "fc layer 3 self.abs_max_out: 3.0\n",
      "fc layer 2 self.abs_max_out: 133.0\n",
      "fc layer 2 self.abs_max_out: 137.0\n",
      "lif layer 2 self.abs_max_v: 262.5\n",
      "fc layer 2 self.abs_max_out: 140.0\n",
      "lif layer 1 self.abs_max_v: 11029.0\n",
      "fc layer 1 self.abs_max_out: 6973.0\n",
      "lif layer 1 self.abs_max_v: 12485.5\n",
      "fc layer 2 self.abs_max_out: 235.0\n",
      "lif layer 2 self.abs_max_v: 323.0\n",
      "fc layer 2 self.abs_max_out: 237.0\n",
      "lif layer 2 self.abs_max_v: 332.5\n",
      "fc layer 2 self.abs_max_out: 240.0\n",
      "lif layer 2 self.abs_max_v: 339.0\n",
      "fc layer 2 self.abs_max_out: 242.0\n",
      "lif layer 2 self.abs_max_v: 340.0\n",
      "lif layer 2 self.abs_max_v: 344.0\n",
      "lif layer 2 self.abs_max_v: 358.5\n",
      "lif layer 2 self.abs_max_v: 363.0\n",
      "fc layer 2 self.abs_max_out: 246.0\n",
      "lif layer 2 self.abs_max_v: 367.0\n",
      "fc layer 2 self.abs_max_out: 251.0\n",
      "lif layer 2 self.abs_max_v: 370.0\n",
      "fc layer 2 self.abs_max_out: 252.0\n",
      "lif layer 2 self.abs_max_v: 372.0\n",
      "fc layer 1 self.abs_max_out: 7135.0\n",
      "fc layer 2 self.abs_max_out: 406.0\n",
      "lif layer 2 self.abs_max_v: 406.0\n",
      "fc layer 2 self.abs_max_out: 409.0\n",
      "lif layer 2 self.abs_max_v: 409.0\n",
      "lif layer 2 self.abs_max_v: 425.5\n",
      "fc layer 1 self.abs_max_out: 7479.0\n",
      "lif layer 1 self.abs_max_v: 13337.0\n",
      "lif layer 1 self.abs_max_v: 13754.5\n",
      "lif layer 2 self.abs_max_v: 428.0\n",
      "fc layer 2 self.abs_max_out: 424.0\n",
      "fc layer 2 self.abs_max_out: 450.0\n",
      "lif layer 2 self.abs_max_v: 450.0\n",
      "fc layer 2 self.abs_max_out: 466.0\n",
      "lif layer 2 self.abs_max_v: 466.0\n",
      "lif layer 2 self.abs_max_v: 513.5\n",
      "fc layer 1 self.abs_max_out: 7667.0\n",
      "fc layer 1 self.abs_max_out: 9143.0\n",
      "lif layer 1 self.abs_max_v: 14778.0\n",
      "lif layer 1 self.abs_max_v: 16143.5\n",
      "lif layer 1 self.abs_max_v: 16381.5\n",
      "lif layer 1 self.abs_max_v: 16751.5\n",
      "fc layer 1 self.abs_max_out: 10112.0\n",
      "lif layer 1 self.abs_max_v: 17291.5\n",
      "fc layer 2 self.abs_max_out: 506.0\n",
      "fc layer 2 self.abs_max_out: 514.0\n",
      "lif layer 2 self.abs_max_v: 514.0\n",
      "fc layer 2 self.abs_max_out: 525.0\n",
      "lif layer 2 self.abs_max_v: 525.0\n",
      "fc layer 2 self.abs_max_out: 529.0\n",
      "lif layer 2 self.abs_max_v: 529.0\n",
      "fc layer 2 self.abs_max_out: 531.0\n",
      "lif layer 2 self.abs_max_v: 531.0\n",
      "lif layer 2 self.abs_max_v: 535.0\n",
      "lif layer 2 self.abs_max_v: 543.0\n",
      "lif layer 1 self.abs_max_v: 17421.0\n",
      "lif layer 1 self.abs_max_v: 17738.0\n",
      "fc layer 1 self.abs_max_out: 11155.0\n",
      "lif layer 1 self.abs_max_v: 19045.5\n",
      "lif layer 1 self.abs_max_v: 20631.5\n",
      "fc layer 1 self.abs_max_out: 12855.0\n",
      "lif layer 1 self.abs_max_v: 23151.0\n",
      "fc layer 2 self.abs_max_out: 545.0\n",
      "lif layer 2 self.abs_max_v: 545.0\n",
      "fc layer 2 self.abs_max_out: 567.0\n",
      "lif layer 2 self.abs_max_v: 567.0\n",
      "fc layer 2 self.abs_max_out: 583.0\n",
      "lif layer 2 self.abs_max_v: 583.0\n",
      "fc layer 2 self.abs_max_out: 666.0\n",
      "lif layer 2 self.abs_max_v: 666.0\n",
      "fc layer 1 self.abs_max_out: 13419.0\n",
      "fc layer 1 self.abs_max_out: 13529.0\n",
      "lif layer 1 self.abs_max_v: 24114.5\n",
      "lif layer 2 self.abs_max_v: 683.0\n",
      "lif layer 1 self.abs_max_v: 24124.5\n",
      "fc layer 1 self.abs_max_out: 13717.0\n",
      "fc layer 1 self.abs_max_out: 15372.0\n",
      "lif layer 1 self.abs_max_v: 26097.0\n",
      "fc layer 1 self.abs_max_out: 16575.0\n",
      "lif layer 1 self.abs_max_v: 29623.5\n",
      "lif layer 1 self.abs_max_v: 30301.0\n",
      "lif layer 1 self.abs_max_v: 31029.5\n",
      "fc layer 1 self.abs_max_out: 16683.0\n",
      "lif layer 1 self.abs_max_v: 32187.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss:  1.811687/  2.282784, val:  13.75%, val_best:  13.75%, tr:  58.94%, tr_best:  58.94%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 28.9116%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 99.6979%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 3566  36.425%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 718.0\n",
      "lif layer 2 self.abs_max_v: 718.0\n",
      "fc layer 2 self.abs_max_out: 719.0\n",
      "lif layer 2 self.abs_max_v: 719.0\n",
      "fc layer 2 self.abs_max_out: 802.0\n",
      "lif layer 2 self.abs_max_v: 802.0\n",
      "fc layer 2 self.abs_max_out: 815.0\n",
      "lif layer 2 self.abs_max_v: 815.0\n",
      "lif layer 2 self.abs_max_v: 816.5\n",
      "fc layer 2 self.abs_max_out: 873.0\n",
      "lif layer 2 self.abs_max_v: 873.0\n",
      "fc layer 1 self.abs_max_out: 17015.0\n",
      "fc layer 1 self.abs_max_out: 17765.0\n",
      "fc layer 2 self.abs_max_out: 875.0\n",
      "lif layer 2 self.abs_max_v: 875.0\n",
      "fc layer 2 self.abs_max_out: 901.0\n",
      "lif layer 2 self.abs_max_v: 901.0\n",
      "fc layer 2 self.abs_max_out: 942.0\n",
      "lif layer 2 self.abs_max_v: 942.0\n",
      "fc layer 1 self.abs_max_out: 18351.0\n",
      "lif layer 1 self.abs_max_v: 32616.5\n",
      "lif layer 1 self.abs_max_v: 33426.5\n",
      "lif layer 1 self.abs_max_v: 34303.5\n",
      "fc layer 1 self.abs_max_out: 18382.0\n",
      "lif layer 1 self.abs_max_v: 35534.0\n",
      "fc layer 2 self.abs_max_out: 946.0\n",
      "lif layer 2 self.abs_max_v: 946.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss:  1.847254/  2.359114, val:   5.00%, val_best:  13.75%, tr:  59.35%, tr_best:  59.35%, epoch time: 74.92 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 30.4985%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 99.6680%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3add0d16f1344174a3e8bd55f56013bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñà‚ñÅ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÅ‚ñà</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_now</td><td>‚ñà‚ñÅ</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>iter_acc</td><td>0.0</td></tr><tr><td>tr_acc</td><td>0.59346</td></tr><tr><td>tr_epoch_loss</td><td>1.84725</td></tr><tr><td>val_acc_best</td><td>0.1375</td></tr><tr><td>val_acc_now</td><td>0.05</td></tr><tr><td>val_loss</td><td>2.35911</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">divine-sweep-11</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/383nwo2z' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/383nwo2z</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_093345-383nwo2z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 383nwo2z errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31082/3004161549.py\", line 119, in hyper_iter\n",
      "    my_snn_system(\n",
      "  File \"/tmp/ipykernel_31082/3596498357.py\", line 929, in my_snn_system\n",
      "    assert val_acc_best > 0.2\n",
      "AssertionError\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 383nwo2z errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31082/3004161549.py\", line 119, in hyper_iter\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     my_snn_system(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31082/3596498357.py\", line 929, in my_snn_system\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     assert val_acc_best > 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m AssertionError\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: uvldmb1g with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.046875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0234375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_093643-uvldmb1g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/uvldmb1g' target=\"_blank\">crimson-sweep-12</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/uvldmb1g' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/uvldmb1g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': '20251214_093651_875', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 16, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 1, 'lif_layer_v_threshold2': 256, 'init_scaling': [0.03125, 0.046875, 0.0234375], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 16\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 1, self.v_threshold 256\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=16, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=256, v_reset=10000, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 46.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 41.0\n",
      "lif layer 2 self.abs_max_v: 41.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 51.0\n",
      "fc layer 2 self.abs_max_out: 80.0\n",
      "lif layer 2 self.abs_max_v: 94.5\n",
      "fc layer 1 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 69.0\n",
      "lif layer 2 self.abs_max_v: 107.5\n",
      "fc layer 1 self.abs_max_out: 62.0\n",
      "lif layer 1 self.abs_max_v: 90.0\n",
      "fc layer 2 self.abs_max_out: 82.0\n",
      "lif layer 2 self.abs_max_v: 121.5\n",
      "fc layer 1 self.abs_max_out: 101.0\n",
      "lif layer 1 self.abs_max_v: 138.0\n",
      "fc layer 2 self.abs_max_out: 87.0\n",
      "lif layer 2 self.abs_max_v: 142.5\n",
      "fc layer 1 self.abs_max_out: 147.0\n",
      "lif layer 1 self.abs_max_v: 210.0\n",
      "fc layer 2 self.abs_max_out: 126.0\n",
      "lif layer 2 self.abs_max_v: 194.5\n",
      "fc layer 1 self.abs_max_out: 200.0\n",
      "lif layer 1 self.abs_max_v: 270.0\n",
      "fc layer 2 self.abs_max_out: 195.0\n",
      "lif layer 2 self.abs_max_v: 292.5\n",
      "fc layer 1 self.abs_max_out: 225.0\n",
      "lif layer 1 self.abs_max_v: 360.0\n",
      "fc layer 2 self.abs_max_out: 219.0\n",
      "lif layer 2 self.abs_max_v: 365.5\n",
      "fc layer 3 self.abs_max_out: 12.0\n",
      "fc layer 1 self.abs_max_out: 277.0\n",
      "lif layer 1 self.abs_max_v: 405.0\n",
      "fc layer 2 self.abs_max_out: 228.0\n",
      "lif layer 2 self.abs_max_v: 393.0\n",
      "lif layer 2 self.abs_max_v: 409.5\n",
      "fc layer 1 self.abs_max_out: 297.0\n",
      "fc layer 2 self.abs_max_out: 271.0\n",
      "fc layer 2 self.abs_max_out: 294.0\n",
      "fc layer 3 self.abs_max_out: 30.0\n",
      "fc layer 1 self.abs_max_out: 392.0\n",
      "fc layer 1 self.abs_max_out: 481.0\n",
      "lif layer 1 self.abs_max_v: 511.0\n",
      "fc layer 2 self.abs_max_out: 321.0\n",
      "lif layer 1 self.abs_max_v: 518.5\n",
      "lif layer 2 self.abs_max_v: 417.5\n",
      "lif layer 2 self.abs_max_v: 418.0\n",
      "fc layer 3 self.abs_max_out: 31.0\n",
      "lif layer 2 self.abs_max_v: 425.5\n",
      "fc layer 2 self.abs_max_out: 327.0\n",
      "lif layer 2 self.abs_max_v: 440.0\n",
      "lif layer 1 self.abs_max_v: 532.5\n",
      "lif layer 2 self.abs_max_v: 450.0\n",
      "lif layer 2 self.abs_max_v: 458.0\n",
      "lif layer 2 self.abs_max_v: 470.0\n",
      "lif layer 2 self.abs_max_v: 496.0\n",
      "lif layer 2 self.abs_max_v: 522.0\n",
      "lif layer 2 self.abs_max_v: 523.0\n",
      "lif layer 2 self.abs_max_v: 568.5\n",
      "lif layer 2 self.abs_max_v: 587.5\n",
      "fc layer 1 self.abs_max_out: 485.0\n",
      "lif layer 2 self.abs_max_v: 600.5\n",
      "lif layer 2 self.abs_max_v: 610.0\n",
      "lif layer 2 self.abs_max_v: 624.0\n",
      "fc layer 1 self.abs_max_out: 550.0\n",
      "lif layer 1 self.abs_max_v: 550.0\n",
      "fc layer 2 self.abs_max_out: 336.0\n",
      "fc layer 3 self.abs_max_out: 36.0\n",
      "fc layer 2 self.abs_max_out: 363.0\n",
      "lif layer 2 self.abs_max_v: 674.5\n",
      "lif layer 2 self.abs_max_v: 696.5\n",
      "fc layer 2 self.abs_max_out: 396.0\n",
      "lif layer 2 self.abs_max_v: 731.0\n",
      "fc layer 3 self.abs_max_out: 47.0\n",
      "fc layer 2 self.abs_max_out: 407.0\n",
      "lif layer 2 self.abs_max_v: 772.5\n",
      "fc layer 2 self.abs_max_out: 410.0\n",
      "lif layer 2 self.abs_max_v: 780.5\n",
      "lif layer 2 self.abs_max_v: 799.5\n",
      "fc layer 2 self.abs_max_out: 432.0\n",
      "lif layer 2 self.abs_max_v: 816.0\n",
      "fc layer 1 self.abs_max_out: 561.0\n",
      "lif layer 1 self.abs_max_v: 561.0\n",
      "lif layer 2 self.abs_max_v: 821.0\n",
      "fc layer 3 self.abs_max_out: 52.0\n",
      "fc layer 2 self.abs_max_out: 493.0\n",
      "lif layer 2 self.abs_max_v: 848.0\n",
      "lif layer 2 self.abs_max_v: 868.0\n",
      "lif layer 2 self.abs_max_v: 913.0\n",
      "fc layer 2 self.abs_max_out: 532.0\n",
      "lif layer 2 self.abs_max_v: 932.0\n",
      "lif layer 2 self.abs_max_v: 945.0\n",
      "lif layer 2 self.abs_max_v: 951.5\n",
      "lif layer 2 self.abs_max_v: 980.0\n",
      "fc layer 2 self.abs_max_out: 592.0\n",
      "lif layer 2 self.abs_max_v: 996.5\n",
      "lif layer 2 self.abs_max_v: 1044.5\n",
      "lif layer 2 self.abs_max_v: 1059.5\n",
      "lif layer 2 self.abs_max_v: 1073.5\n",
      "lif layer 2 self.abs_max_v: 1112.0\n",
      "lif layer 2 self.abs_max_v: 1116.0\n",
      "lif layer 2 self.abs_max_v: 1129.0\n",
      "lif layer 2 self.abs_max_v: 1142.5\n",
      "fc layer 2 self.abs_max_out: 751.0\n",
      "lif layer 1 self.abs_max_v: 567.5\n",
      "lif layer 2 self.abs_max_v: 1161.5\n",
      "lif layer 2 self.abs_max_v: 1235.0\n",
      "lif layer 2 self.abs_max_v: 1326.5\n",
      "lif layer 2 self.abs_max_v: 1330.5\n",
      "fc layer 1 self.abs_max_out: 586.0\n",
      "lif layer 1 self.abs_max_v: 586.0\n",
      "fc layer 1 self.abs_max_out: 706.0\n",
      "lif layer 1 self.abs_max_v: 706.0\n",
      "fc layer 1 self.abs_max_out: 775.0\n",
      "lif layer 1 self.abs_max_v: 775.0\n",
      "lif layer 1 self.abs_max_v: 830.5\n",
      "lif layer 1 self.abs_max_v: 910.5\n",
      "fc layer 2 self.abs_max_out: 753.0\n",
      "lif layer 2 self.abs_max_v: 1403.0\n",
      "fc layer 2 self.abs_max_out: 758.0\n",
      "lif layer 2 self.abs_max_v: 1459.5\n",
      "fc layer 3 self.abs_max_out: 67.0\n",
      "fc layer 2 self.abs_max_out: 796.0\n",
      "fc layer 1 self.abs_max_out: 794.0\n",
      "fc layer 3 self.abs_max_out: 68.0\n",
      "fc layer 3 self.abs_max_out: 74.0\n",
      "fc layer 3 self.abs_max_out: 81.0\n",
      "fc layer 3 self.abs_max_out: 85.0\n",
      "lif layer 1 self.abs_max_v: 947.5\n",
      "fc layer 3 self.abs_max_out: 92.0\n",
      "fc layer 2 self.abs_max_out: 816.0\n",
      "fc layer 3 self.abs_max_out: 101.0\n",
      "fc layer 3 self.abs_max_out: 106.0\n",
      "fc layer 3 self.abs_max_out: 125.0\n",
      "lif layer 1 self.abs_max_v: 962.0\n",
      "fc layer 2 self.abs_max_out: 817.0\n",
      "fc layer 2 self.abs_max_out: 819.0\n",
      "lif layer 1 self.abs_max_v: 1000.5\n",
      "fc layer 1 self.abs_max_out: 800.0\n",
      "lif layer 1 self.abs_max_v: 1055.5\n",
      "lif layer 1 self.abs_max_v: 1187.0\n",
      "fc layer 1 self.abs_max_out: 802.0\n",
      "fc layer 1 self.abs_max_out: 841.0\n",
      "lif layer 2 self.abs_max_v: 1472.5\n",
      "lif layer 2 self.abs_max_v: 1476.0\n",
      "lif layer 2 self.abs_max_v: 1503.0\n",
      "fc layer 3 self.abs_max_out: 126.0\n",
      "fc layer 3 self.abs_max_out: 134.0\n",
      "fc layer 3 self.abs_max_out: 136.0\n",
      "fc layer 2 self.abs_max_out: 874.0\n",
      "lif layer 1 self.abs_max_v: 1189.0\n",
      "lif layer 1 self.abs_max_v: 1249.0\n",
      "fc layer 2 self.abs_max_out: 886.0\n",
      "lif layer 2 self.abs_max_v: 1521.5\n",
      "lif layer 2 self.abs_max_v: 1539.5\n",
      "fc layer 2 self.abs_max_out: 959.0\n",
      "lif layer 2 self.abs_max_v: 1584.5\n",
      "lif layer 2 self.abs_max_v: 1637.0\n",
      "lif layer 1 self.abs_max_v: 1267.5\n",
      "lif layer 1 self.abs_max_v: 1308.0\n",
      "fc layer 1 self.abs_max_out: 958.0\n",
      "lif layer 1 self.abs_max_v: 1462.0\n",
      "lif layer 2 self.abs_max_v: 1673.0\n",
      "lif layer 2 self.abs_max_v: 1688.5\n",
      "fc layer 3 self.abs_max_out: 146.0\n",
      "fc layer 3 self.abs_max_out: 150.0\n",
      "fc layer 3 self.abs_max_out: 163.0\n",
      "lif layer 1 self.abs_max_v: 1509.0\n",
      "lif layer 1 self.abs_max_v: 1549.0\n",
      "fc layer 2 self.abs_max_out: 986.0\n",
      "fc layer 2 self.abs_max_out: 1024.0\n",
      "fc layer 3 self.abs_max_out: 176.0\n",
      "fc layer 1 self.abs_max_out: 1001.0\n",
      "lif layer 1 self.abs_max_v: 1614.5\n",
      "fc layer 1 self.abs_max_out: 1051.0\n",
      "lif layer 1 self.abs_max_v: 1852.5\n",
      "lif layer 1 self.abs_max_v: 1884.5\n",
      "lif layer 1 self.abs_max_v: 1948.5\n",
      "lif layer 2 self.abs_max_v: 1709.5\n",
      "fc layer 3 self.abs_max_out: 184.0\n",
      "fc layer 2 self.abs_max_out: 1075.0\n",
      "fc layer 3 self.abs_max_out: 189.0\n",
      "fc layer 2 self.abs_max_out: 1193.0\n",
      "fc layer 3 self.abs_max_out: 199.0\n",
      "lif layer 2 self.abs_max_v: 1715.5\n",
      "lif layer 2 self.abs_max_v: 1816.0\n",
      "fc layer 3 self.abs_max_out: 204.0\n",
      "fc layer 1 self.abs_max_out: 1079.0\n",
      "fc layer 2 self.abs_max_out: 1262.0\n",
      "lif layer 2 self.abs_max_v: 1904.5\n",
      "lif layer 2 self.abs_max_v: 1930.5\n",
      "fc layer 1 self.abs_max_out: 1085.0\n",
      "fc layer 3 self.abs_max_out: 205.0\n",
      "fc layer 2 self.abs_max_out: 1296.0\n",
      "lif layer 1 self.abs_max_v: 2014.0\n",
      "fc layer 1 self.abs_max_out: 1088.0\n",
      "lif layer 1 self.abs_max_v: 2095.0\n",
      "lif layer 1 self.abs_max_v: 2117.5\n",
      "fc layer 1 self.abs_max_out: 1152.0\n",
      "fc layer 2 self.abs_max_out: 1300.0\n",
      "fc layer 2 self.abs_max_out: 1336.0\n",
      "lif layer 2 self.abs_max_v: 1956.0\n",
      "lif layer 2 self.abs_max_v: 1974.0\n",
      "lif layer 2 self.abs_max_v: 2024.0\n",
      "fc layer 1 self.abs_max_out: 1158.0\n",
      "lif layer 1 self.abs_max_v: 2164.5\n",
      "fc layer 1 self.abs_max_out: 1179.0\n",
      "fc layer 1 self.abs_max_out: 1270.0\n",
      "fc layer 1 self.abs_max_out: 1312.0\n",
      "lif layer 1 self.abs_max_v: 2241.0\n",
      "lif layer 1 self.abs_max_v: 2395.5\n",
      "lif layer 1 self.abs_max_v: 2458.0\n",
      "lif layer 1 self.abs_max_v: 2461.0\n",
      "fc layer 1 self.abs_max_out: 1340.0\n",
      "lif layer 2 self.abs_max_v: 2037.0\n",
      "fc layer 2 self.abs_max_out: 1342.0\n",
      "fc layer 2 self.abs_max_out: 1344.0\n",
      "fc layer 2 self.abs_max_out: 1352.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss:  6.965924/ 49.080685, val:  28.33%, val_best:  28.33%, tr:  98.26%, tr_best:  98.26%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1884%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.8637%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 1721  17.579%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1380.0\n",
      "fc layer 2 self.abs_max_out: 1382.0\n",
      "fc layer 2 self.abs_max_out: 1476.0\n",
      "fc layer 3 self.abs_max_out: 216.0\n",
      "fc layer 2 self.abs_max_out: 1518.0\n",
      "fc layer 2 self.abs_max_out: 1523.0\n",
      "fc layer 2 self.abs_max_out: 1525.0\n",
      "fc layer 2 self.abs_max_out: 1526.0\n",
      "fc layer 3 self.abs_max_out: 243.0\n",
      "fc layer 2 self.abs_max_out: 1584.0\n",
      "fc layer 2 self.abs_max_out: 1596.0\n",
      "fc layer 2 self.abs_max_out: 1622.0\n",
      "fc layer 2 self.abs_max_out: 1666.0\n",
      "fc layer 2 self.abs_max_out: 1671.0\n",
      "fc layer 2 self.abs_max_out: 1738.0\n",
      "lif layer 2 self.abs_max_v: 2101.5\n",
      "lif layer 2 self.abs_max_v: 2130.5\n",
      "lif layer 2 self.abs_max_v: 2172.0\n",
      "lif layer 2 self.abs_max_v: 2183.5\n",
      "fc layer 2 self.abs_max_out: 1781.0\n",
      "fc layer 1 self.abs_max_out: 1361.0\n",
      "lif layer 2 self.abs_max_v: 2199.5\n",
      "lif layer 2 self.abs_max_v: 2277.5\n",
      "fc layer 3 self.abs_max_out: 257.0\n",
      "lif layer 2 self.abs_max_v: 2368.5\n",
      "lif layer 2 self.abs_max_v: 2389.5\n",
      "lif layer 2 self.abs_max_v: 2395.0\n",
      "fc layer 1 self.abs_max_out: 1435.0\n",
      "lif layer 1 self.abs_max_v: 2464.0\n",
      "fc layer 3 self.abs_max_out: 266.0\n",
      "fc layer 3 self.abs_max_out: 269.0\n",
      "lif layer 2 self.abs_max_v: 2409.5\n",
      "lif layer 2 self.abs_max_v: 2464.0\n",
      "fc layer 2 self.abs_max_out: 1799.0\n",
      "fc layer 2 self.abs_max_out: 1811.0\n",
      "fc layer 2 self.abs_max_out: 1852.0\n",
      "fc layer 2 self.abs_max_out: 1874.0\n",
      "fc layer 2 self.abs_max_out: 1877.0\n",
      "fc layer 2 self.abs_max_out: 1884.0\n",
      "fc layer 2 self.abs_max_out: 1914.0\n",
      "fc layer 2 self.abs_max_out: 1950.0\n",
      "lif layer 2 self.abs_max_v: 2485.5\n",
      "lif layer 2 self.abs_max_v: 2649.0\n",
      "lif layer 2 self.abs_max_v: 2675.0\n",
      "lif layer 2 self.abs_max_v: 2736.5\n",
      "fc layer 2 self.abs_max_out: 1952.0\n",
      "fc layer 3 self.abs_max_out: 276.0\n",
      "fc layer 2 self.abs_max_out: 1981.0\n",
      "fc layer 2 self.abs_max_out: 2039.0\n",
      "fc layer 2 self.abs_max_out: 2066.0\n",
      "fc layer 2 self.abs_max_out: 2067.0\n",
      "fc layer 2 self.abs_max_out: 2103.0\n",
      "fc layer 2 self.abs_max_out: 2129.0\n",
      "fc layer 2 self.abs_max_out: 2141.0\n",
      "fc layer 2 self.abs_max_out: 2180.0\n",
      "lif layer 2 self.abs_max_v: 2777.0\n",
      "fc layer 2 self.abs_max_out: 2197.0\n",
      "fc layer 1 self.abs_max_out: 1470.0\n",
      "fc layer 1 self.abs_max_out: 1479.0\n",
      "lif layer 1 self.abs_max_v: 2528.5\n",
      "lif layer 1 self.abs_max_v: 2706.5\n",
      "fc layer 1 self.abs_max_out: 1575.0\n",
      "lif layer 1 self.abs_max_v: 2823.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss:  8.218301/ 38.810112, val:  41.67%, val_best:  41.67%, tr:  98.88%, tr_best:  98.88%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0584%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.6623%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3316  16.936%\n",
      "fc layer 3 self.abs_max_out: 287.0\n",
      "lif layer 2 self.abs_max_v: 2798.0\n",
      "fc layer 2 self.abs_max_out: 2232.0\n",
      "lif layer 2 self.abs_max_v: 2842.5\n",
      "fc layer 2 self.abs_max_out: 2248.0\n",
      "fc layer 3 self.abs_max_out: 308.0\n",
      "lif layer 2 self.abs_max_v: 2897.0\n",
      "lif layer 2 self.abs_max_v: 3046.5\n",
      "fc layer 3 self.abs_max_out: 310.0\n",
      "fc layer 2 self.abs_max_out: 2261.0\n",
      "fc layer 2 self.abs_max_out: 2262.0\n",
      "fc layer 2 self.abs_max_out: 2304.0\n",
      "fc layer 2 self.abs_max_out: 2343.0\n",
      "fc layer 2 self.abs_max_out: 2354.0\n",
      "lif layer 2 self.abs_max_v: 3119.0\n",
      "fc layer 2 self.abs_max_out: 2365.0\n",
      "lif layer 2 self.abs_max_v: 3216.0\n",
      "lif layer 2 self.abs_max_v: 3264.0\n",
      "lif layer 2 self.abs_max_v: 3268.0\n",
      "lif layer 2 self.abs_max_v: 3305.0\n",
      "lif layer 2 self.abs_max_v: 3372.0\n",
      "lif layer 2 self.abs_max_v: 3519.5\n",
      "fc layer 2 self.abs_max_out: 2378.0\n",
      "fc layer 2 self.abs_max_out: 2424.0\n",
      "fc layer 2 self.abs_max_out: 2442.0\n",
      "fc layer 1 self.abs_max_out: 1581.0\n",
      "fc layer 2 self.abs_max_out: 2468.0\n",
      "lif layer 2 self.abs_max_v: 3652.0\n",
      "lif layer 2 self.abs_max_v: 3710.5\n",
      "fc layer 2 self.abs_max_out: 2513.0\n",
      "fc layer 2 self.abs_max_out: 2576.0\n",
      "lif layer 2 self.abs_max_v: 3762.5\n",
      "lif layer 2 self.abs_max_v: 3853.5\n",
      "lif layer 2 self.abs_max_v: 3891.0\n",
      "fc layer 2 self.abs_max_out: 2594.0\n",
      "fc layer 2 self.abs_max_out: 2598.0\n",
      "fc layer 2 self.abs_max_out: 2654.0\n",
      "fc layer 1 self.abs_max_out: 1612.0\n",
      "fc layer 1 self.abs_max_out: 1695.0\n",
      "lif layer 1 self.abs_max_v: 2838.5\n",
      "fc layer 1 self.abs_max_out: 1757.0\n",
      "lif layer 1 self.abs_max_v: 3176.5\n",
      "lif layer 2 self.abs_max_v: 3987.5\n",
      "lif layer 2 self.abs_max_v: 4056.5\n",
      "lif layer 2 self.abs_max_v: 4078.5\n",
      "lif layer 2 self.abs_max_v: 4178.5\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss:  8.260167/ 57.048294, val:  38.75%, val_best:  41.67%, tr:  98.57%, tr_best:  98.88%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.2721%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.1963%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 4743  16.149%\n",
      "fc layer 1 self.abs_max_out: 1785.0\n",
      "lif layer 1 self.abs_max_v: 3229.0\n",
      "lif layer 2 self.abs_max_v: 4254.5\n",
      "fc layer 3 self.abs_max_out: 338.0\n",
      "lif layer 2 self.abs_max_v: 4368.0\n",
      "lif layer 2 self.abs_max_v: 4552.0\n",
      "lif layer 2 self.abs_max_v: 4610.5\n",
      "lif layer 2 self.abs_max_v: 4655.0\n",
      "fc layer 1 self.abs_max_out: 1882.0\n",
      "fc layer 1 self.abs_max_out: 1909.0\n",
      "lif layer 1 self.abs_max_v: 3229.5\n",
      "fc layer 1 self.abs_max_out: 1927.0\n",
      "lif layer 1 self.abs_max_v: 3533.0\n",
      "fc layer 1 self.abs_max_out: 1975.0\n",
      "lif layer 1 self.abs_max_v: 3618.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  8.094145/ 59.559101, val:  28.33%, val_best:  41.67%, tr:  98.37%, tr_best:  98.88%, epoch time: 75.68 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2751%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.1411%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6117  15.621%\n",
      "fc layer 2 self.abs_max_out: 2671.0\n",
      "fc layer 2 self.abs_max_out: 2688.0\n",
      "fc layer 2 self.abs_max_out: 2803.0\n",
      "lif layer 2 self.abs_max_v: 4720.0\n",
      "fc layer 1 self.abs_max_out: 2062.0\n",
      "lif layer 1 self.abs_max_v: 3751.5\n",
      "fc layer 1 self.abs_max_out: 2140.0\n",
      "lif layer 1 self.abs_max_v: 3776.0\n",
      "fc layer 1 self.abs_max_out: 2253.0\n",
      "lif layer 1 self.abs_max_v: 4141.0\n",
      "lif layer 2 self.abs_max_v: 4862.5\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  7.642323/ 57.603603, val:  32.50%, val_best:  41.67%, tr:  98.88%, tr_best:  98.88%, epoch time: 75.21 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7208%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.9217%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 7477  15.275%\n",
      "fc layer 1 self.abs_max_out: 2319.0\n",
      "lif layer 2 self.abs_max_v: 4902.5\n",
      "lif layer 2 self.abs_max_v: 4937.5\n",
      "fc layer 2 self.abs_max_out: 3075.0\n",
      "lif layer 2 self.abs_max_v: 5037.5\n",
      "lif layer 2 self.abs_max_v: 5215.0\n",
      "fc layer 1 self.abs_max_out: 2385.0\n",
      "fc layer 1 self.abs_max_out: 2410.0\n",
      "lif layer 1 self.abs_max_v: 4432.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  7.538672/ 39.904667, val:  39.58%, val_best:  41.67%, tr:  98.57%, tr_best:  98.88%, epoch time: 75.30 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5173%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.0771%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 8847  15.061%\n",
      "lif layer 2 self.abs_max_v: 5269.0\n",
      "lif layer 2 self.abs_max_v: 5423.5\n",
      "lif layer 2 self.abs_max_v: 5571.0\n",
      "fc layer 2 self.abs_max_out: 3077.0\n",
      "lif layer 2 self.abs_max_v: 5829.0\n",
      "fc layer 1 self.abs_max_out: 2417.0\n",
      "lif layer 1 self.abs_max_v: 4446.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  6.637997/ 58.432556, val:  39.17%, val_best:  41.67%, tr:  98.37%, tr_best:  98.88%, epoch time: 75.22 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9159%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.4424%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10187  14.865%\n",
      "fc layer 2 self.abs_max_out: 3226.0\n",
      "lif layer 2 self.abs_max_v: 5839.5\n",
      "fc layer 2 self.abs_max_out: 3264.0\n",
      "lif layer 2 self.abs_max_v: 6039.0\n",
      "lif layer 2 self.abs_max_v: 6113.5\n",
      "fc layer 2 self.abs_max_out: 3303.0\n",
      "fc layer 2 self.abs_max_out: 3336.0\n",
      "fc layer 1 self.abs_max_out: 2652.0\n",
      "fc layer 1 self.abs_max_out: 2791.0\n",
      "lif layer 1 self.abs_max_v: 4685.5\n",
      "lif layer 1 self.abs_max_v: 5064.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  6.856523/ 45.967438, val:  41.67%, val_best:  41.67%, tr:  98.06%, tr_best:  98.88%, epoch time: 75.80 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5185%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.9281%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 11530  14.722%\n",
      "fc layer 2 self.abs_max_out: 3452.0\n",
      "lif layer 2 self.abs_max_v: 6274.0\n",
      "lif layer 2 self.abs_max_v: 6455.0\n",
      "lif layer 2 self.abs_max_v: 6562.5\n",
      "lif layer 2 self.abs_max_v: 6573.5\n",
      "lif layer 2 self.abs_max_v: 6600.0\n",
      "lif layer 1 self.abs_max_v: 5070.5\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  6.501037/ 33.811302, val:  43.33%, val_best:  43.33%, tr:  99.08%, tr_best:  99.08%, epoch time: 75.59 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0760%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.1579%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 12894  14.634%\n",
      "fc layer 2 self.abs_max_out: 3479.0\n",
      "fc layer 2 self.abs_max_out: 3484.0\n",
      "lif layer 2 self.abs_max_v: 6604.5\n",
      "fc layer 2 self.abs_max_out: 3643.0\n",
      "fc layer 2 self.abs_max_out: 3685.0\n",
      "fc layer 2 self.abs_max_out: 3699.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  6.020388/ 32.520302, val:  48.33%, val_best:  48.33%, tr:  98.67%, tr_best:  99.08%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0934%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.3476%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 14195  14.499%\n",
      "fc layer 2 self.abs_max_out: 3770.0\n",
      "lif layer 2 self.abs_max_v: 6708.0\n",
      "lif layer 2 self.abs_max_v: 6945.0\n",
      "lif layer 2 self.abs_max_v: 7074.5\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  5.795158/ 37.025291, val:  44.58%, val_best:  48.33%, tr:  98.37%, tr_best:  99.08%, epoch time: 74.78 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.2438%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.2123%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 15511  14.403%\n",
      "fc layer 1 self.abs_max_out: 2829.0\n",
      "fc layer 2 self.abs_max_out: 3791.0\n",
      "fc layer 2 self.abs_max_out: 3918.0\n",
      "fc layer 1 self.abs_max_out: 2862.0\n",
      "fc layer 1 self.abs_max_out: 3033.0\n",
      "lif layer 1 self.abs_max_v: 5085.0\n",
      "lif layer 1 self.abs_max_v: 5495.5\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  5.548516/ 28.624247, val:  54.17%, val_best:  54.17%, tr:  99.08%, tr_best:  99.08%, epoch time: 74.92 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0095%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.7097%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 16780  14.283%\n",
      "fc layer 2 self.abs_max_out: 4000.0\n",
      "fc layer 2 self.abs_max_out: 4162.0\n",
      "fc layer 1 self.abs_max_out: 3066.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  5.178112/ 33.205166, val:  42.50%, val_best:  54.17%, tr:  98.77%, tr_best:  99.08%, epoch time: 74.96 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0483%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.1867%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 18012  14.153%\n",
      "lif layer 2 self.abs_max_v: 7089.5\n",
      "fc layer 1 self.abs_max_out: 3113.0\n",
      "lif layer 1 self.abs_max_v: 5663.0\n",
      "fc layer 2 self.abs_max_out: 4165.0\n",
      "fc layer 2 self.abs_max_out: 4234.0\n",
      "lif layer 2 self.abs_max_v: 7503.5\n",
      "lif layer 2 self.abs_max_v: 7514.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  5.131362/ 35.435867, val:  43.75%, val_best:  54.17%, tr:  98.16%, tr_best:  99.08%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.2619%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.5502%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 19254  14.048%\n",
      "fc layer 2 self.abs_max_out: 4414.0\n",
      "fc layer 1 self.abs_max_out: 3160.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  5.184893/ 29.482996, val:  45.42%, val_best:  54.17%, tr:  98.67%, tr_best:  99.08%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6139%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.7083%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 20546  13.991%\n",
      "fc layer 2 self.abs_max_out: 4417.0\n",
      "lif layer 2 self.abs_max_v: 7569.5\n",
      "lif layer 2 self.abs_max_v: 7775.0\n",
      "lif layer 2 self.abs_max_v: 7963.0\n",
      "fc layer 1 self.abs_max_out: 3249.0\n",
      "fc layer 1 self.abs_max_out: 3250.0\n",
      "lif layer 1 self.abs_max_v: 5916.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  4.864833/ 19.592583, val:  53.33%, val_best:  54.17%, tr:  98.47%, tr_best:  99.08%, epoch time: 74.55 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5459%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.0326%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 21748  13.884%\n",
      "fc layer 1 self.abs_max_out: 3303.0\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  4.641924/ 32.516914, val:  50.42%, val_best:  54.17%, tr:  98.88%, tr_best:  99.08%, epoch time: 74.50 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7731%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.5373%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 22905  13.763%\n",
      "lif layer 2 self.abs_max_v: 8156.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  4.557277/ 21.640379, val:  57.08%, val_best:  57.08%, tr:  98.57%, tr_best:  99.08%, epoch time: 75.64 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.1977%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.7795%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 24088  13.669%\n",
      "fc layer 2 self.abs_max_out: 4548.0\n",
      "lif layer 2 self.abs_max_v: 8184.5\n",
      "lif layer 2 self.abs_max_v: 8241.5\n",
      "fc layer 1 self.abs_max_out: 3416.0\n",
      "fc layer 1 self.abs_max_out: 3437.0\n",
      "lif layer 1 self.abs_max_v: 6258.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  4.642508/ 26.836388, val:  45.83%, val_best:  57.08%, tr:  99.18%, tr_best:  99.18%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3664%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.7137%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 25312  13.608%\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  4.388690/ 24.542500, val:  44.17%, val_best:  57.08%, tr:  98.77%, tr_best:  99.18%, epoch time: 74.84 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3628%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.8747%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 26455  13.511%\n",
      "fc layer 2 self.abs_max_out: 4622.0\n",
      "lif layer 2 self.abs_max_v: 8245.5\n",
      "lif layer 2 self.abs_max_v: 8463.0\n",
      "fc layer 1 self.abs_max_out: 3539.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  4.308469/ 27.226566, val:  51.67%, val_best:  57.08%, tr:  98.57%, tr_best:  99.18%, epoch time: 75.53 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8065%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.8089%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 27578  13.414%\n",
      "fc layer 2 self.abs_max_out: 4631.0\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  4.449396/ 18.476889, val:  58.75%, val_best:  58.75%, tr:  98.47%, tr_best:  99.18%, epoch time: 75.46 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8522%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.1155%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 28761  13.354%\n",
      "lif layer 2 self.abs_max_v: 8489.5\n",
      "lif layer 2 self.abs_max_v: 8531.0\n",
      "fc layer 1 self.abs_max_out: 3584.0\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  4.425652/ 17.166084, val:  62.92%, val_best:  62.92%, tr:  98.26%, tr_best:  99.18%, epoch time: 74.75 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7336%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.9483%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 29911  13.284%\n",
      "lif layer 2 self.abs_max_v: 8580.0\n",
      "lif layer 2 self.abs_max_v: 8634.5\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  4.402909/ 23.602581, val:  52.50%, val_best:  62.92%, tr:  98.98%, tr_best:  99.18%, epoch time: 75.17 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0713%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.1233%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 31045  13.213%\n",
      "fc layer 2 self.abs_max_out: 4691.0\n",
      "fc layer 1 self.abs_max_out: 3807.0\n",
      "lif layer 1 self.abs_max_v: 6332.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  4.449677/ 25.923918, val:  47.92%, val_best:  62.92%, tr:  98.98%, tr_best:  99.18%, epoch time: 74.62 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7848%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.2304%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 32192  13.153%\n",
      "lif layer 2 self.abs_max_v: 8640.5\n",
      "fc layer 2 self.abs_max_out: 4719.0\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  4.585789/ 22.762154, val:  55.00%, val_best:  62.92%, tr:  98.47%, tr_best:  99.18%, epoch time: 71.09 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8386%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.2495%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 33409  13.125%\n",
      "fc layer 1 self.abs_max_out: 3875.0\n",
      "lif layer 1 self.abs_max_v: 6534.5\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  4.227386/ 23.849228, val:  58.33%, val_best:  62.92%, tr:  98.67%, tr_best:  99.18%, epoch time: 74.40 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6042%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.2401%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 34472  13.041%\n",
      "lif layer 2 self.abs_max_v: 8701.5\n",
      "lif layer 2 self.abs_max_v: 8705.5\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  4.247909/ 23.786709, val:  59.17%, val_best:  62.92%, tr:  98.77%, tr_best:  99.18%, epoch time: 74.82 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2509%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.5758%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 35596  12.986%\n",
      "fc layer 2 self.abs_max_out: 4730.0\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  4.159892/ 18.742487, val:  63.33%, val_best:  63.33%, tr:  99.18%, tr_best:  99.18%, epoch time: 75.41 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9452%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.7276%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 36694  12.925%\n",
      "fc layer 1 self.abs_max_out: 3876.0\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  4.075986/ 32.617603, val:  52.92%, val_best:  63.33%, tr:  98.98%, tr_best:  99.18%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9128%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.7400%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 37791  12.867%\n",
      "fc layer 1 self.abs_max_out: 3909.0\n",
      "lif layer 1 self.abs_max_v: 6598.0\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  4.014257/ 23.487812, val:  60.00%, val_best:  63.33%, tr:  99.08%, tr_best:  99.18%, epoch time: 74.42 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0128%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8829%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 38839  12.797%\n",
      "fc layer 1 self.abs_max_out: 3949.0\n",
      "lif layer 1 self.abs_max_v: 6632.0\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  4.005191/ 22.927637, val:  54.58%, val_best:  63.33%, tr:  99.28%, tr_best:  99.28%, epoch time: 74.10 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6025%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0895%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 39924  12.744%\n",
      "fc layer 1 self.abs_max_out: 4021.0\n",
      "lif layer 1 self.abs_max_v: 6827.5\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  3.848790/ 28.814156, val:  46.25%, val_best:  63.33%, tr:  98.98%, tr_best:  99.28%, epoch time: 74.90 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2792%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2900%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 40981  12.685%\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  4.202685/ 24.862587, val:  52.50%, val_best:  63.33%, tr:  98.57%, tr_best:  99.28%, epoch time: 74.31 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2911%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9550%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 42109  12.651%\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  3.905316/ 28.814066, val:  50.83%, val_best:  63.33%, tr:  98.88%, tr_best:  99.28%, epoch time: 74.57 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8509%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3121%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 43154  12.594%\n",
      "fc layer 1 self.abs_max_out: 4081.0\n",
      "lif layer 1 self.abs_max_v: 6924.0\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  3.807679/ 23.792355, val:  41.25%, val_best:  63.33%, tr:  98.88%, tr_best:  99.28%, epoch time: 74.86 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.1206%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6222%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 44207  12.543%\n",
      "fc layer 1 self.abs_max_out: 4112.0\n",
      "lif layer 1 self.abs_max_v: 6962.0\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  3.721684/ 21.013954, val:  63.33%, val_best:  63.33%, tr:  98.77%, tr_best:  99.28%, epoch time: 74.01 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6198%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5640%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 45239  12.489%\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  3.598440/ 15.905412, val:  65.00%, val_best:  65.00%, tr:  98.98%, tr_best:  99.28%, epoch time: 75.16 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1107%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4619%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 46221  12.424%\n",
      "fc layer 2 self.abs_max_out: 4807.0\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  3.854264/ 25.056892, val:  56.25%, val_best:  65.00%, tr:  99.08%, tr_best:  99.28%, epoch time: 74.24 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2811%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4006%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 47259  12.378%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  3.574104/ 25.907396, val:  46.25%, val_best:  65.00%, tr:  99.39%, tr_best:  99.39%, epoch time: 74.77 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3164%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5136%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 48229  12.316%\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  3.662677/ 15.840083, val:  56.67%, val_best:  65.00%, tr:  99.28%, tr_best:  99.39%, epoch time: 74.58 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8374%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3838%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 49225  12.264%\n",
      "fc layer 1 self.abs_max_out: 4139.0\n",
      "lif layer 1 self.abs_max_v: 6987.5\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  3.933856/ 23.628988, val:  52.50%, val_best:  65.00%, tr:  99.28%, tr_best:  99.39%, epoch time: 74.16 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5213%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0227%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 50258  12.223%\n",
      "fc layer 2 self.abs_max_out: 4835.0\n",
      "fc layer 1 self.abs_max_out: 4154.0\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  3.841076/ 24.458361, val:  50.42%, val_best:  65.00%, tr:  99.28%, tr_best:  99.39%, epoch time: 74.03 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6225%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5224%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 51261  12.177%\n",
      "fc layer 1 self.abs_max_out: 4246.0\n",
      "lif layer 1 self.abs_max_v: 7196.0\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  3.612981/ 20.722315, val:  57.50%, val_best:  65.00%, tr:  99.18%, tr_best:  99.39%, epoch time: 74.96 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5914%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9088%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 52268  12.134%\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  3.632451/ 22.598852, val:  60.42%, val_best:  65.00%, tr:  98.98%, tr_best:  99.39%, epoch time: 74.65 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1213%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9947%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 53291  12.096%\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  3.531158/ 26.843563, val:  50.83%, val_best:  65.00%, tr:  99.28%, tr_best:  99.39%, epoch time: 74.26 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7142%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2189%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 54302  12.058%\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  3.614990/ 20.102428, val:  60.83%, val_best:  65.00%, tr:  99.08%, tr_best:  99.39%, epoch time: 74.65 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2849%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7595%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 55298  12.018%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  3.687632/ 29.679403, val:  49.17%, val_best:  65.00%, tr:  99.18%, tr_best:  99.39%, epoch time: 74.49 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3444%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8383%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 56323  11.986%\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  3.293340/ 21.726908, val:  57.92%, val_best:  65.00%, tr:  99.49%, tr_best:  99.49%, epoch time: 74.57 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6369%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3913%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 57281  11.941%\n",
      "fc layer 2 self.abs_max_out: 4989.0\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  3.495325/ 18.438522, val:  66.67%, val_best:  66.67%, tr:  98.67%, tr_best:  99.49%, epoch time: 75.38 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2946%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3303%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 58287  11.907%\n",
      "fc layer 1 self.abs_max_out: 4360.0\n",
      "lif layer 1 self.abs_max_v: 7449.5\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  3.594155/ 18.281477, val:  57.08%, val_best:  66.67%, tr:  98.88%, tr_best:  99.49%, epoch time: 75.19 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5842%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5160%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 59292  11.875%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  3.615306/ 22.490038, val:  50.83%, val_best:  66.67%, tr:  99.49%, tr_best:  99.49%, epoch time: 74.34 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7626%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0062%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 60299  11.845%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  3.588657/ 16.122152, val:  57.08%, val_best:  66.67%, tr:  99.39%, tr_best:  99.49%, epoch time: 74.51 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6532%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3104%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 61308  11.816%\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  3.415055/ 20.216803, val:  57.50%, val_best:  66.67%, tr:  98.77%, tr_best:  99.49%, epoch time: 74.74 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4657%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6320%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 62297  11.784%\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  3.434166/ 24.513285, val:  53.75%, val_best:  66.67%, tr:  98.98%, tr_best:  99.49%, epoch time: 74.98 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2028%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4400%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 63279  11.752%\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  3.771423/ 16.050232, val:  66.25%, val_best:  66.67%, tr:  98.98%, tr_best:  99.49%, epoch time: 74.58 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0601%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1151%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 64293  11.727%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  3.490534/ 20.686142, val:  57.92%, val_best:  66.67%, tr:  99.59%, tr_best:  99.59%, epoch time: 75.02 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1689%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0694%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 65273  11.697%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  3.400648/ 23.359474, val:  57.92%, val_best:  66.67%, tr:  98.98%, tr_best:  99.59%, epoch time: 74.80 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9766%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2526%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 66242  11.666%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  3.501972/ 18.231678, val:  67.08%, val_best:  67.08%, tr:  98.77%, tr_best:  99.59%, epoch time: 74.52 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0959%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0115%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 67204  11.635%\n",
      "fc layer 1 self.abs_max_out: 4530.0\n",
      "lif layer 1 self.abs_max_v: 7821.0\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  3.542205/ 27.248207, val:  48.75%, val_best:  67.08%, tr:  99.39%, tr_best:  99.59%, epoch time: 74.95 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0799%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 68186  11.608%\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  3.488564/ 28.954987, val:  49.17%, val_best:  67.08%, tr:  98.77%, tr_best:  99.59%, epoch time: 74.19 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3624%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2338%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 69118  11.574%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  3.482769/ 17.411860, val:  69.58%, val_best:  69.58%, tr:  99.39%, tr_best:  99.59%, epoch time: 74.93 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2025%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0843%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 70078  11.545%\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  3.395637/ 23.129152, val:  60.83%, val_best:  69.58%, tr:  99.08%, tr_best:  99.59%, epoch time: 75.22 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4505%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6652%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 71047  11.519%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  3.173119/ 18.754475, val:  59.58%, val_best:  69.58%, tr:  99.39%, tr_best:  99.59%, epoch time: 74.23 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0827%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6468%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 71951  11.483%\n",
      "fc layer 1 self.abs_max_out: 4609.0\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  3.651934/ 27.985325, val:  40.83%, val_best:  69.58%, tr:  99.08%, tr_best:  99.59%, epoch time: 74.65 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4384%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2258%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 72930  11.461%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  3.356709/ 15.204924, val:  65.83%, val_best:  69.58%, tr:  98.98%, tr_best:  99.59%, epoch time: 74.81 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6388%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3630%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 73860  11.431%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  3.435344/ 23.567507, val:  59.17%, val_best:  69.58%, tr:  98.77%, tr_best:  99.59%, epoch time: 74.68 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6411%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3939%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 74814  11.406%\n",
      "fc layer 1 self.abs_max_out: 4654.0\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  3.344776/ 21.062731, val:  61.25%, val_best:  69.58%, tr:  99.59%, tr_best:  99.59%, epoch time: 74.66 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3109%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4744%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 75736  11.377%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  3.378099/ 15.916752, val:  62.50%, val_best:  69.58%, tr:  99.08%, tr_best:  99.59%, epoch time: 74.69 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4756%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4377%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 76705  11.355%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  2.996153/ 16.081667, val:  57.08%, val_best:  69.58%, tr:  99.18%, tr_best:  99.59%, epoch time: 74.67 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4538%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8131%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 77557  11.317%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  3.350399/ 14.811749, val:  66.67%, val_best:  69.58%, tr:  99.08%, tr_best:  99.59%, epoch time: 74.76 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5236%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8257%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 78506  11.294%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  3.369477/ 21.414753, val:  53.75%, val_best:  69.58%, tr:  98.98%, tr_best:  99.59%, epoch time: 74.86 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6311%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9128%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 79452  11.272%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  3.062429/ 20.076960, val:  66.25%, val_best:  69.58%, tr:  99.08%, tr_best:  99.59%, epoch time: 74.55 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6736%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6963%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 80305  11.237%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  3.071279/ 20.483593, val:  61.25%, val_best:  69.58%, tr:  99.08%, tr_best:  99.59%, epoch time: 74.77 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5588%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.7747%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 81179  11.205%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  3.239206/ 20.828085, val:  63.33%, val_best:  69.58%, tr:  99.39%, tr_best:  99.59%, epoch time: 74.20 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8793%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5617%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 82074  11.178%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  3.225450/ 18.640059, val:  68.33%, val_best:  69.58%, tr:  98.98%, tr_best:  99.59%, epoch time: 74.92 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7468%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4947%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 82963  11.150%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  3.444096/ 18.830564, val:  58.75%, val_best:  69.58%, tr:  99.08%, tr_best:  99.59%, epoch time: 74.06 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6453%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5598%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 83901  11.130%\n",
      "fc layer 1 self.abs_max_out: 4680.0\n",
      "lif layer 1 self.abs_max_v: 8111.5\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  3.185546/ 25.530457, val:  52.92%, val_best:  69.58%, tr:  98.98%, tr_best:  99.59%, epoch time: 74.32 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3728%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6452%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 84787  11.103%\n",
      "fc layer 1 self.abs_max_out: 4699.0\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  3.146338/ 23.473206, val:  59.17%, val_best:  69.58%, tr:  99.69%, tr_best:  99.69%, epoch time: 74.55 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6136%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6276%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 85657  11.075%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  3.191842/ 18.732178, val:  62.08%, val_best:  69.58%, tr:  99.69%, tr_best:  99.69%, epoch time: 74.49 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6650%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8439%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 86545  11.050%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  3.103107/ 17.470871, val:  67.08%, val_best:  69.58%, tr:  99.18%, tr_best:  99.69%, epoch time: 74.93 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7341%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8121%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 87408  11.023%\n",
      "fc layer 1 self.abs_max_out: 4864.0\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  2.985173/ 30.575550, val:  45.42%, val_best:  69.58%, tr:  99.28%, tr_best:  99.69%, epoch time: 74.74 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8560%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6164%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 88248  10.993%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  3.355686/ 19.047729, val:  70.83%, val_best:  70.83%, tr:  98.67%, tr_best:  99.69%, epoch time: 75.17 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7740%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8212%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 89196  10.977%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  3.352646/ 22.588562, val:  50.83%, val_best:  70.83%, tr:  99.18%, tr_best:  99.69%, epoch time: 74.81 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8345%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8671%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 90114  10.958%\n",
      "fc layer 1 self.abs_max_out: 4925.0\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  3.308185/ 19.688532, val:  49.17%, val_best:  70.83%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6591%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9878%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 91039  10.940%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  3.226990/ 17.040110, val:  68.33%, val_best:  70.83%, tr:  99.69%, tr_best:  99.69%, epoch time: 74.68 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9840%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8785%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 91936  10.920%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  3.071528/ 22.864017, val:  60.42%, val_best:  70.83%, tr:  98.47%, tr_best:  99.69%, epoch time: 74.26 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7419%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0510%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 92813  10.897%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  3.301018/ 25.267698, val:  51.67%, val_best:  70.83%, tr:  98.88%, tr_best:  99.69%, epoch time: 74.46 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7103%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1376%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 93729  10.879%\n",
      "lif layer 1 self.abs_max_v: 8234.0\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  3.002440/ 23.899969, val:  57.08%, val_best:  70.83%, tr:  99.18%, tr_best:  99.69%, epoch time: 75.20 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5467%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9035%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 94594  10.857%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  3.213932/ 11.819122, val:  78.33%, val_best:  78.33%, tr:  98.26%, tr_best:  99.69%, epoch time: 74.85 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7533%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9383%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 95476  10.836%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  3.304950/ 17.342773, val:  70.42%, val_best:  78.33%, tr:  98.98%, tr_best:  99.69%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6148%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1022%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 96395  10.820%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  3.020584/ 23.710686, val:  62.08%, val_best:  78.33%, tr:  99.18%, tr_best:  99.69%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5846%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8026%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 97207  10.793%\n",
      "fc layer 1 self.abs_max_out: 4957.0\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  3.167286/ 17.301313, val:  68.33%, val_best:  78.33%, tr:  99.28%, tr_best:  99.69%, epoch time: 74.58 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6225%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9466%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 98085  10.773%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  3.112409/ 16.032188, val:  65.42%, val_best:  78.33%, tr:  99.59%, tr_best:  99.69%, epoch time: 74.79 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5012%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9873%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 98947  10.752%\n",
      "fc layer 1 self.abs_max_out: 5050.0\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  2.901106/ 22.130371, val:  57.50%, val_best:  78.33%, tr:  99.39%, tr_best:  99.69%, epoch time: 74.85 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3818%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4104%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 99758  10.726%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  3.167537/ 18.066610, val:  64.17%, val_best:  78.33%, tr:  99.39%, tr_best:  99.69%, epoch time: 74.66 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2355%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1664%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 100669  10.711%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  2.959175/ 24.693954, val:  57.08%, val_best:  78.33%, tr:  99.18%, tr_best:  99.69%, epoch time: 74.89 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4606%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0480%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 101515  10.690%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  2.847411/ 15.734138, val:  66.25%, val_best:  78.33%, tr:  99.18%, tr_best:  99.69%, epoch time: 75.77 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4867%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.3551%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 102305  10.663%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  2.947800/ 18.853252, val:  61.25%, val_best:  78.33%, tr:  98.77%, tr_best:  99.69%, epoch time: 75.12 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4522%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 103116  10.639%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  2.648138/ 16.787716, val:  68.33%, val_best:  78.33%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2514%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.2980%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 103871  10.610%\n",
      "fc layer 1 self.abs_max_out: 5084.0\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  2.984938/ 17.860941, val:  67.08%, val_best:  78.33%, tr:  99.59%, tr_best:  99.69%, epoch time: 74.72 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4421%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.2977%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 104688  10.587%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  2.978701/ 15.947397, val:  59.17%, val_best:  78.33%, tr:  98.57%, tr_best:  99.69%, epoch time: 74.27 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3315%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.5536%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 105529  10.568%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  2.783859/ 16.849264, val:  58.33%, val_best:  78.33%, tr:  98.77%, tr_best:  99.69%, epoch time: 74.95 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4047%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6749%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 106354  10.547%\n",
      "lif layer 1 self.abs_max_v: 8395.5\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  2.830050/ 23.631626, val:  55.00%, val_best:  78.33%, tr:  99.08%, tr_best:  99.69%, epoch time: 74.74 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6070%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.2729%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 107172  10.526%\n",
      "fc layer 1 self.abs_max_out: 5255.0\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  2.946267/ 18.303394, val:  72.50%, val_best:  78.33%, tr:  99.08%, tr_best:  99.69%, epoch time: 74.38 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6419%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4635%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 107995  10.506%\n",
      "lif layer 1 self.abs_max_v: 8396.5\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  2.736050/ 23.593536, val:  62.50%, val_best:  78.33%, tr:  99.28%, tr_best:  99.69%, epoch time: 74.61 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3454%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1405%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 108759  10.480%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  2.717443/ 19.898491, val:  58.75%, val_best:  78.33%, tr:  99.49%, tr_best:  99.69%, epoch time: 74.75 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6168%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0911%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 109512  10.454%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  3.078777/ 18.745235, val:  57.50%, val_best:  78.33%, tr:  99.18%, tr_best:  99.69%, epoch time: 74.43 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5639%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.3028%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 110360  10.438%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  2.922742/ 14.538293, val:  74.17%, val_best:  78.33%, tr:  99.39%, tr_best:  99.69%, epoch time: 74.42 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6554%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0450%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 111177  10.419%\n",
      "lif layer 1 self.abs_max_v: 8419.0\n",
      "lif layer 1 self.abs_max_v: 8443.0\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  2.838212/ 16.243799, val:  62.92%, val_best:  78.33%, tr:  98.98%, tr_best:  99.69%, epoch time: 74.68 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5935%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0525%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 111985  10.399%\n",
      "fc layer 1 self.abs_max_out: 5535.0\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  3.126089/ 15.291197, val:  70.42%, val_best:  78.33%, tr:  98.98%, tr_best:  99.69%, epoch time: 75.05 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7214%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9051%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 112834  10.383%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  2.947722/ 17.048416, val:  70.83%, val_best:  78.33%, tr:  99.69%, tr_best:  99.69%, epoch time: 74.54 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4341%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9482%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 113646  10.365%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  2.911711/ 20.242260, val:  65.00%, val_best:  78.33%, tr:  99.69%, tr_best:  99.69%, epoch time: 74.55 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4644%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4219%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 114460  10.346%\n",
      "lif layer 1 self.abs_max_v: 8500.5\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  2.916632/ 23.235174, val:  60.83%, val_best:  78.33%, tr:  99.49%, tr_best:  99.69%, epoch time: 74.68 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6982%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.3232%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 115268  10.328%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  3.074928/ 19.794460, val:  58.75%, val_best:  78.33%, tr:  98.88%, tr_best:  99.69%, epoch time: 75.01 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9584%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0640%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 116099  10.312%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  2.772365/ 21.573349, val:  63.75%, val_best:  78.33%, tr:  99.59%, tr_best:  99.69%, epoch time: 74.37 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7343%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0880%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 116874  10.291%\n",
      "fc layer 1 self.abs_max_out: 5715.0\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  3.020618/ 19.298056, val:  61.25%, val_best:  78.33%, tr:  99.49%, tr_best:  99.69%, epoch time: 74.08 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6597%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1893%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 117701  10.276%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  2.891482/ 15.665762, val:  61.67%, val_best:  78.33%, tr:  99.18%, tr_best:  99.69%, epoch time: 75.05 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5947%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.5613%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 118524  10.260%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  2.850646/ 26.573362, val:  55.00%, val_best:  78.33%, tr:  98.88%, tr_best:  99.69%, epoch time: 74.61 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5605%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.5916%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 119307  10.241%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  3.015796/ 18.835112, val:  57.08%, val_best:  78.33%, tr:  99.39%, tr_best:  99.69%, epoch time: 74.45 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5817%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4144%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 120129  10.225%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  2.762174/ 17.637884, val:  74.17%, val_best:  78.33%, tr:  99.39%, tr_best:  99.69%, epoch time: 74.24 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8170%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6685%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 120886  10.205%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  2.969768/ 25.692749, val:  51.67%, val_best:  78.33%, tr:  99.08%, tr_best:  99.69%, epoch time: 74.37 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6099%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4236%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 121716  10.191%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  2.892541/ 17.336971, val:  58.33%, val_best:  78.33%, tr:  99.18%, tr_best:  99.69%, epoch time: 74.89 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6323%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6404%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 122526  10.175%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  2.911363/ 13.310700, val:  74.17%, val_best:  78.33%, tr:  98.57%, tr_best:  99.69%, epoch time: 74.74 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7208%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4463%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 123350  10.161%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  2.810168/ 18.474001, val:  72.50%, val_best:  78.33%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.03 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7194%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4347%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 124123  10.143%\n",
      "lif layer 1 self.abs_max_v: 8528.5\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  2.909641/ 18.322151, val:  66.25%, val_best:  78.33%, tr:  99.28%, tr_best:  99.69%, epoch time: 74.37 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6519%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6075%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 124946  10.129%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  3.017548/ 22.750307, val:  57.08%, val_best:  78.33%, tr:  99.08%, tr_best:  99.69%, epoch time: 74.36 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5782%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4256%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 125764  10.115%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  2.863732/ 26.527470, val:  42.50%, val_best:  78.33%, tr:  99.08%, tr_best:  99.69%, epoch time: 75.09 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7596%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.5603%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 126532  10.097%\n",
      "fc layer 1 self.abs_max_out: 5772.0\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  2.800002/ 21.951612, val:  60.00%, val_best:  78.33%, tr:  98.88%, tr_best:  99.69%, epoch time: 74.64 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6063%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.5039%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 127321  10.082%\n",
      "fc layer 1 self.abs_max_out: 5811.0\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  2.768853/ 21.079880, val:  57.50%, val_best:  78.33%, tr:  99.69%, tr_best:  99.69%, epoch time: 74.33 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.3722%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 128088  10.064%\n",
      "lif layer 1 self.abs_max_v: 8573.0\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  2.705410/ 22.081120, val:  68.33%, val_best:  78.33%, tr:  99.08%, tr_best:  99.69%, epoch time: 74.50 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5936%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4270%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 128851  10.047%\n",
      "lif layer 1 self.abs_max_v: 8587.0\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  2.955171/ 19.678011, val:  65.00%, val_best:  78.33%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.21 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6407%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.3981%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 129642  10.032%\n",
      "lif layer 1 self.abs_max_v: 8737.0\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  2.743867/ 20.460733, val:  63.75%, val_best:  78.33%, tr:  99.08%, tr_best:  99.69%, epoch time: 74.84 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4100%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4645%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 130384  10.014%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  2.857166/ 22.835110, val:  51.67%, val_best:  78.33%, tr:  99.49%, tr_best:  99.69%, epoch time: 74.80 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3933%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1914%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 131155   9.998%\n",
      "lif layer 1 self.abs_max_v: 8847.0\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  2.850038/ 14.543396, val:  75.00%, val_best:  78.33%, tr:  99.18%, tr_best:  99.69%, epoch time: 74.56 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7498%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4306%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 131926   9.982%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  2.869090/ 18.804466, val:  68.33%, val_best:  78.33%, tr:  99.18%, tr_best:  99.69%, epoch time: 74.35 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1870%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6365%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 132713   9.968%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  2.585547/ 14.260493, val:  79.58%, val_best:  79.58%, tr:  99.69%, tr_best:  99.69%, epoch time: 74.50 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3458%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6877%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 133446   9.950%\n",
      "lif layer 1 self.abs_max_v: 8977.0\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  2.572032/ 13.322560, val:  70.83%, val_best:  79.58%, tr:  99.18%, tr_best:  99.69%, epoch time: 75.02 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6472%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7289%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 134148   9.929%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  2.547294/ 19.442957, val:  61.67%, val_best:  79.58%, tr:  98.67%, tr_best:  99.69%, epoch time: 74.68 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6551%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6311%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 134870   9.911%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  2.791659/ 17.776049, val:  70.00%, val_best:  79.58%, tr:  98.88%, tr_best:  99.69%, epoch time: 74.17 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6004%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6181%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 135652   9.897%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  2.639601/ 19.147772, val:  57.08%, val_best:  79.58%, tr:  99.08%, tr_best:  99.69%, epoch time: 74.38 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4801%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6282%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 136392   9.881%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  2.647367/ 19.682205, val:  52.50%, val_best:  79.58%, tr:  99.69%, tr_best:  99.69%, epoch time: 74.98 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8186%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.5507%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 137118   9.863%\n",
      "lif layer 1 self.abs_max_v: 9091.5\n",
      "lif layer 1 self.abs_max_v: 9236.5\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  2.965612/ 18.360275, val:  61.25%, val_best:  79.58%, tr:  99.18%, tr_best:  99.69%, epoch time: 74.81 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4512%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4308%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 137950   9.854%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  2.767903/ 20.278770, val:  68.33%, val_best:  79.58%, tr:  99.59%, tr_best:  99.69%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8425%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6171%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 138694   9.838%\n",
      "fc layer 1 self.abs_max_out: 5979.0\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  2.615621/ 21.171133, val:  65.83%, val_best:  79.58%, tr:  99.18%, tr_best:  99.69%, epoch time: 74.94 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4562%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6588%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 139408   9.821%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  2.696657/ 21.905462, val:  53.75%, val_best:  79.58%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1957%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6330%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 140141   9.805%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  2.787252/ 21.738798, val:  64.17%, val_best:  79.58%, tr:  99.39%, tr_best:  99.69%, epoch time: 74.53 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5748%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.5009%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 140890   9.790%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  2.542858/ 26.208834, val:  63.75%, val_best:  79.58%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.39 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8056%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7703%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 141610   9.773%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  2.697389/ 13.965341, val:  74.58%, val_best:  79.58%, tr:  99.08%, tr_best:  99.69%, epoch time: 75.32 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7977%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9074%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 142330   9.757%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  2.565085/ 18.896641, val:  62.08%, val_best:  79.58%, tr:  99.28%, tr_best:  99.69%, epoch time: 74.81 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9325%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9887%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 143071   9.743%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  2.663815/ 15.609071, val:  70.00%, val_best:  79.58%, tr:  99.49%, tr_best:  99.69%, epoch time: 74.76 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6615%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7274%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 143802   9.728%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  2.797317/ 18.674932, val:  71.25%, val_best:  79.58%, tr:  98.98%, tr_best:  99.69%, epoch time: 74.82 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5092%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 144608   9.718%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  2.620461/ 16.921066, val:  72.08%, val_best:  79.58%, tr:  98.88%, tr_best:  99.69%, epoch time: 75.27 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6237%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7853%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 145348   9.704%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  2.634489/ 21.308197, val:  61.25%, val_best:  79.58%, tr:  99.28%, tr_best:  99.69%, epoch time: 74.82 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4439%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7277%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 146080   9.689%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  2.664822/ 15.467727, val:  71.25%, val_best:  79.58%, tr:  98.98%, tr_best:  99.69%, epoch time: 74.38 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3800%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7708%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 146826   9.676%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  2.581273/ 18.570848, val:  62.92%, val_best:  79.58%, tr:  99.28%, tr_best:  99.69%, epoch time: 75.21 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5614%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7288%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 147529   9.660%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  2.603309/ 21.311840, val:  55.83%, val_best:  79.58%, tr:  99.49%, tr_best:  99.69%, epoch time: 75.72 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5870%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.5698%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 148255   9.646%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  2.671288/ 18.021721, val:  70.83%, val_best:  79.58%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.08 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7468%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.8461%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 148985   9.632%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  2.628915/ 17.141745, val:  71.25%, val_best:  79.58%, tr:  99.28%, tr_best:  99.69%, epoch time: 74.91 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4950%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0128%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 149709   9.618%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  2.789933/ 20.777140, val:  69.17%, val_best:  79.58%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.14 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6903%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7236%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 150460   9.605%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  2.743755/ 18.061975, val:  62.08%, val_best:  79.58%, tr:  99.49%, tr_best:  99.69%, epoch time: 75.02 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5713%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7487%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 151197   9.593%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  2.955574/ 21.372059, val:  68.75%, val_best:  79.58%, tr:  99.18%, tr_best:  99.69%, epoch time: 75.66 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3827%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6797%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 151987   9.583%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  2.669071/ 22.630625, val:  66.25%, val_best:  79.58%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.69 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3944%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9085%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 152698   9.569%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  2.639312/ 19.549284, val:  69.58%, val_best:  79.58%, tr:  99.59%, tr_best:  99.69%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7338%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7557%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 153401   9.554%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  2.472528/ 18.163008, val:  70.42%, val_best:  79.58%, tr:  99.59%, tr_best:  99.69%, epoch time: 74.69 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7016%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 154059   9.537%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  2.585253/ 16.851402, val:  69.58%, val_best:  79.58%, tr:  99.59%, tr_best:  99.69%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7598%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.8135%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 154765   9.523%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  2.405440/ 13.506053, val:  78.75%, val_best:  79.58%, tr:  99.08%, tr_best:  99.69%, epoch time: 74.91 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6361%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0864%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 155451   9.508%\n",
      "fc layer 1 self.abs_max_out: 6009.0\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  2.541250/ 22.966713, val:  63.75%, val_best:  79.58%, tr:  99.08%, tr_best:  99.69%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6823%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9792%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 156177   9.496%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  2.567169/ 15.908347, val:  70.00%, val_best:  79.58%, tr:  99.69%, tr_best:  99.69%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8074%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9538%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 156881   9.482%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  2.572637/ 20.522541, val:  67.50%, val_best:  79.58%, tr:  99.18%, tr_best:  99.69%, epoch time: 75.59 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9162%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9755%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 157599   9.469%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  2.608251/ 14.378575, val:  74.17%, val_best:  79.58%, tr:  99.28%, tr_best:  99.69%, epoch time: 75.31 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1141%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8068%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9883%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 158289   9.455%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  2.458462/ 19.025246, val:  60.83%, val_best:  79.58%, tr:  98.57%, tr_best:  99.69%, epoch time: 75.05 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8753%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.1214%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 158993   9.442%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  2.492603/ 20.125650, val:  69.17%, val_best:  79.58%, tr:  99.59%, tr_best:  99.69%, epoch time: 74.81 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7051%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.8297%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 159676   9.428%\n",
      "fc layer 1 self.abs_max_out: 6061.0\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  2.520147/ 17.204803, val:  78.33%, val_best:  79.58%, tr:  99.59%, tr_best:  99.69%, epoch time: 74.70 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9218%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9278%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 160340   9.413%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  2.567867/ 18.123543, val:  68.75%, val_best:  79.58%, tr:  99.49%, tr_best:  99.69%, epoch time: 75.44 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7265%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9112%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 161026   9.399%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  2.593310/ 17.391815, val:  77.08%, val_best:  79.58%, tr:  99.59%, tr_best:  99.69%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8119%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.8648%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 161746   9.387%\n",
      "fc layer 1 self.abs_max_out: 6071.0\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  2.683645/ 14.715231, val:  69.58%, val_best:  79.58%, tr:  99.08%, tr_best:  99.69%, epoch time: 75.16 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8820%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0307%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 162471   9.376%\n",
      "fc layer 1 self.abs_max_out: 6282.0\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  2.472803/ 16.830328, val:  69.17%, val_best:  79.58%, tr:  99.18%, tr_best:  99.69%, epoch time: 75.47 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0312%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9094%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.3482%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 163167   9.363%\n",
      "fc layer 1 self.abs_max_out: 6293.0\n",
      "lif layer 1 self.abs_max_v: 9282.5\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  2.489851/ 17.428806, val:  73.33%, val_best:  79.58%, tr:  99.28%, tr_best:  99.69%, epoch time: 75.00 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0376%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6470%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.2086%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 163845   9.350%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  2.722570/ 14.861818, val:  65.42%, val_best:  79.58%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5456%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0605%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 164605   9.341%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  2.573055/ 17.411211, val:  69.58%, val_best:  79.58%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.02 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6432%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.8518%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 165299   9.328%\n",
      "fc layer 1 self.abs_max_out: 6434.0\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  2.538027/ 16.267462, val:  67.08%, val_best:  79.58%, tr:  98.88%, tr_best:  99.69%, epoch time: 75.49 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6461%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9567%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 165971   9.315%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  2.448265/ 15.480591, val:  76.67%, val_best:  79.58%, tr:  99.49%, tr_best:  99.69%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6345%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.8833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 166634   9.301%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  2.392136/ 20.608599, val:  69.17%, val_best:  79.58%, tr:  99.18%, tr_best:  99.69%, epoch time: 75.12 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0426%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7941%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9234%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 167283   9.286%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  2.126816/ 16.886379, val:  72.08%, val_best:  79.58%, tr:  99.28%, tr_best:  99.69%, epoch time: 75.81 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6066%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.2763%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 167882   9.269%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  2.357722/ 17.489367, val:  68.75%, val_best:  79.58%, tr:  99.49%, tr_best:  99.69%, epoch time: 75.80 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6058%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9876%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 168544   9.256%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  2.476973/ 15.548601, val:  74.58%, val_best:  79.58%, tr:  99.18%, tr_best:  99.69%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6003%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0081%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 169228   9.244%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  2.540998/ 14.185331, val:  81.25%, val_best:  81.25%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7870%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0479%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 169928   9.233%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  2.393164/ 14.411002, val:  81.25%, val_best:  81.25%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.41 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5029%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.2552%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 170599   9.220%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  2.559031/ 19.384398, val:  67.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.74 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6812%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.8875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 171294   9.209%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  2.518523/ 17.019501, val:  73.75%, val_best:  81.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.61 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7748%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0098%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 171957   9.196%\n",
      "lif layer 1 self.abs_max_v: 9570.0\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  2.497944/ 21.194130, val:  57.50%, val_best:  81.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.16 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7785%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0708%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 172644   9.185%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  2.622317/ 17.812504, val:  69.17%, val_best:  81.25%, tr:  99.08%, tr_best: 100.00%, epoch time: 74.99 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6005%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.8991%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 173337   9.174%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  2.525142/ 18.155596, val:  68.75%, val_best:  81.25%, tr:  99.28%, tr_best: 100.00%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6495%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0343%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 174031   9.163%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  2.469800/ 21.782867, val:  63.33%, val_best:  81.25%, tr:  99.39%, tr_best: 100.00%, epoch time: 74.81 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6764%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9931%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 174701   9.151%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  2.438980/ 16.141537, val:  69.58%, val_best:  81.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.47 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7614%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.2247%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 175369   9.139%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  2.270267/ 17.132269, val:  73.33%, val_best:  81.25%, tr:  98.98%, tr_best: 100.00%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5334%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0521%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 175989   9.125%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  2.459127/ 18.147461, val:  73.75%, val_best:  81.25%, tr:  99.18%, tr_best: 100.00%, epoch time: 75.59 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4572%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.1404%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 176676   9.114%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  2.462855/ 15.216922, val:  74.58%, val_best:  81.25%, tr:  99.39%, tr_best: 100.00%, epoch time: 69.74 seconds, 1.16 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6324%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0452%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 177322   9.102%\n",
      "lif layer 1 self.abs_max_v: 9586.0\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  2.414760/ 20.351810, val:  72.92%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.91 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6715%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.8637%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b3ed3431e7a41f5b288f84d2466545d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÉ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÉ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÉ‚ñÖ‚ñá‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÅ‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÉ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99898</td></tr><tr><td>tr_epoch_loss</td><td>2.41476</td></tr><tr><td>val_acc_best</td><td>0.8125</td></tr><tr><td>val_acc_now</td><td>0.72917</td></tr><tr><td>val_loss</td><td>20.35181</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">crimson-sweep-12</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/uvldmb1g' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/uvldmb1g</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_093643-uvldmb1g/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: p75ln5fu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.046875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0234375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_134701-p75ln5fu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/p75ln5fu' target=\"_blank\">magic-sweep-18</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/p75ln5fu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/p75ln5fu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': '20251214_134709_529', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 256, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 16, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 64, 'lif_layer_v_threshold2': 128, 'init_scaling': [0.03125, 0.046875, 0.0234375], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 16, self.v_threshold 256\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 64, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=256, v_reset=10000, sg_width=16, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=64, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 46.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 59.5\n",
      "fc layer 1 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 69.0\n",
      "fc layer 1 self.abs_max_out: 55.0\n",
      "lif layer 1 self.abs_max_v: 75.0\n",
      "lif layer 1 self.abs_max_v: 91.5\n",
      "fc layer 1 self.abs_max_out: 63.0\n",
      "lif layer 1 self.abs_max_v: 101.5\n",
      "fc layer 1 self.abs_max_out: 86.0\n",
      "lif layer 1 self.abs_max_v: 114.0\n",
      "lif layer 1 self.abs_max_v: 121.0\n",
      "fc layer 1 self.abs_max_out: 94.0\n",
      "lif layer 1 self.abs_max_v: 129.0\n",
      "lif layer 1 self.abs_max_v: 132.5\n",
      "lif layer 1 self.abs_max_v: 133.0\n",
      "fc layer 1 self.abs_max_out: 98.0\n",
      "lif layer 1 self.abs_max_v: 133.5\n",
      "fc layer 1 self.abs_max_out: 114.0\n",
      "lif layer 1 self.abs_max_v: 173.0\n",
      "fc layer 1 self.abs_max_out: 121.0\n",
      "lif layer 1 self.abs_max_v: 177.0\n",
      "fc layer 1 self.abs_max_out: 130.0\n",
      "lif layer 1 self.abs_max_v: 194.5\n",
      "lif layer 1 self.abs_max_v: 195.0\n",
      "lif layer 1 self.abs_max_v: 217.0\n",
      "fc layer 1 self.abs_max_out: 160.0\n",
      "lif layer 1 self.abs_max_v: 219.5\n",
      "fc layer 1 self.abs_max_out: 188.0\n",
      "lif layer 1 self.abs_max_v: 297.5\n",
      "fc layer 2 self.abs_max_out: 6.0\n",
      "lif layer 2 self.abs_max_v: 6.0\n",
      "fc layer 1 self.abs_max_out: 220.0\n",
      "lif layer 1 self.abs_max_v: 316.0\n",
      "lif layer 2 self.abs_max_v: 7.0\n",
      "fc layer 1 self.abs_max_out: 238.0\n",
      "lif layer 1 self.abs_max_v: 340.0\n",
      "lif layer 2 self.abs_max_v: 8.0\n",
      "fc layer 1 self.abs_max_out: 256.0\n",
      "lif layer 1 self.abs_max_v: 364.5\n",
      "lif layer 1 self.abs_max_v: 369.0\n",
      "fc layer 1 self.abs_max_out: 276.0\n",
      "lif layer 1 self.abs_max_v: 390.0\n",
      "fc layer 1 self.abs_max_out: 294.0\n",
      "lif layer 2 self.abs_max_v: 10.0\n",
      "fc layer 1 self.abs_max_out: 332.0\n",
      "fc layer 1 self.abs_max_out: 395.0\n",
      "lif layer 1 self.abs_max_v: 395.0\n",
      "lif layer 2 self.abs_max_v: 10.5\n",
      "lif layer 2 self.abs_max_v: 11.5\n",
      "lif layer 2 self.abs_max_v: 12.0\n",
      "fc layer 1 self.abs_max_out: 500.0\n",
      "lif layer 1 self.abs_max_v: 500.0\n",
      "fc layer 1 self.abs_max_out: 583.0\n",
      "lif layer 1 self.abs_max_v: 583.0\n",
      "fc layer 1 self.abs_max_out: 766.0\n",
      "lif layer 1 self.abs_max_v: 766.0\n",
      "fc layer 1 self.abs_max_out: 816.0\n",
      "lif layer 1 self.abs_max_v: 816.0\n",
      "fc layer 1 self.abs_max_out: 836.0\n",
      "lif layer 1 self.abs_max_v: 836.0\n",
      "fc layer 1 self.abs_max_out: 876.0\n",
      "lif layer 1 self.abs_max_v: 876.0\n",
      "fc layer 1 self.abs_max_out: 960.0\n",
      "lif layer 1 self.abs_max_v: 960.0\n",
      "fc layer 1 self.abs_max_out: 962.0\n",
      "lif layer 1 self.abs_max_v: 962.0\n",
      "fc layer 1 self.abs_max_out: 1092.0\n",
      "lif layer 1 self.abs_max_v: 1092.0\n",
      "fc layer 1 self.abs_max_out: 1105.0\n",
      "lif layer 1 self.abs_max_v: 1105.0\n",
      "fc layer 1 self.abs_max_out: 1146.0\n",
      "lif layer 1 self.abs_max_v: 1146.0\n",
      "fc layer 1 self.abs_max_out: 1183.0\n",
      "lif layer 1 self.abs_max_v: 1183.0\n",
      "fc layer 1 self.abs_max_out: 1232.0\n",
      "lif layer 1 self.abs_max_v: 1232.0\n",
      "fc layer 1 self.abs_max_out: 1301.0\n",
      "lif layer 1 self.abs_max_v: 1301.0\n",
      "fc layer 1 self.abs_max_out: 1356.0\n",
      "lif layer 1 self.abs_max_v: 1356.0\n",
      "fc layer 1 self.abs_max_out: 1436.0\n",
      "lif layer 1 self.abs_max_v: 1436.0\n",
      "fc layer 2 self.abs_max_out: 11.0\n",
      "lif layer 2 self.abs_max_v: 14.0\n",
      "lif layer 2 self.abs_max_v: 17.5\n",
      "lif layer 2 self.abs_max_v: 18.0\n",
      "lif layer 2 self.abs_max_v: 18.5\n",
      "fc layer 2 self.abs_max_out: 12.0\n",
      "lif layer 2 self.abs_max_v: 21.0\n",
      "fc layer 2 self.abs_max_out: 17.0\n",
      "lif layer 2 self.abs_max_v: 23.5\n",
      "lif layer 2 self.abs_max_v: 25.0\n",
      "lif layer 2 self.abs_max_v: 28.5\n",
      "lif layer 2 self.abs_max_v: 29.0\n",
      "lif layer 2 self.abs_max_v: 29.5\n",
      "lif layer 2 self.abs_max_v: 31.5\n",
      "lif layer 2 self.abs_max_v: 32.0\n",
      "lif layer 2 self.abs_max_v: 33.0\n",
      "lif layer 2 self.abs_max_v: 33.5\n",
      "fc layer 2 self.abs_max_out: 18.0\n",
      "lif layer 2 self.abs_max_v: 34.5\n",
      "lif layer 2 self.abs_max_v: 35.0\n",
      "lif layer 2 self.abs_max_v: 35.5\n",
      "fc layer 2 self.abs_max_out: 21.0\n",
      "lif layer 2 self.abs_max_v: 36.0\n",
      "lif layer 2 self.abs_max_v: 37.5\n",
      "lif layer 2 self.abs_max_v: 38.0\n",
      "lif layer 2 self.abs_max_v: 39.5\n",
      "lif layer 2 self.abs_max_v: 41.0\n",
      "lif layer 2 self.abs_max_v: 41.5\n",
      "lif layer 2 self.abs_max_v: 42.0\n",
      "fc layer 1 self.abs_max_out: 1598.0\n",
      "lif layer 1 self.abs_max_v: 1598.0\n",
      "fc layer 1 self.abs_max_out: 1658.0\n",
      "lif layer 1 self.abs_max_v: 1658.0\n",
      "fc layer 1 self.abs_max_out: 1664.0\n",
      "lif layer 1 self.abs_max_v: 1664.0\n",
      "fc layer 2 self.abs_max_out: 24.0\n",
      "lif layer 2 self.abs_max_v: 42.5\n",
      "lif layer 2 self.abs_max_v: 45.5\n",
      "lif layer 2 self.abs_max_v: 46.0\n",
      "fc layer 2 self.abs_max_out: 25.0\n",
      "lif layer 2 self.abs_max_v: 47.0\n",
      "lif layer 2 self.abs_max_v: 47.5\n",
      "lif layer 2 self.abs_max_v: 48.0\n",
      "fc layer 1 self.abs_max_out: 1677.0\n",
      "lif layer 1 self.abs_max_v: 1677.0\n",
      "fc layer 2 self.abs_max_out: 29.0\n",
      "lif layer 2 self.abs_max_v: 52.0\n",
      "lif layer 2 self.abs_max_v: 52.5\n",
      "lif layer 2 self.abs_max_v: 53.5\n",
      "lif layer 2 self.abs_max_v: 54.5\n",
      "lif layer 2 self.abs_max_v: 56.0\n",
      "lif layer 2 self.abs_max_v: 57.0\n",
      "lif layer 2 self.abs_max_v: 57.5\n",
      "fc layer 1 self.abs_max_out: 1710.0\n",
      "lif layer 1 self.abs_max_v: 1710.0\n",
      "fc layer 1 self.abs_max_out: 1852.0\n",
      "lif layer 1 self.abs_max_v: 1852.0\n",
      "fc layer 1 self.abs_max_out: 1869.0\n",
      "lif layer 1 self.abs_max_v: 1869.0\n",
      "fc layer 1 self.abs_max_out: 1905.0\n",
      "lif layer 1 self.abs_max_v: 1905.0\n",
      "fc layer 1 self.abs_max_out: 1918.0\n",
      "lif layer 1 self.abs_max_v: 1918.0\n",
      "fc layer 1 self.abs_max_out: 1977.0\n",
      "lif layer 1 self.abs_max_v: 1977.0\n",
      "fc layer 1 self.abs_max_out: 2165.0\n",
      "lif layer 1 self.abs_max_v: 2165.0\n",
      "fc layer 1 self.abs_max_out: 2179.0\n",
      "lif layer 1 self.abs_max_v: 2179.0\n",
      "fc layer 1 self.abs_max_out: 2229.0\n",
      "lif layer 1 self.abs_max_v: 2229.0\n",
      "fc layer 1 self.abs_max_out: 2230.0\n",
      "lif layer 1 self.abs_max_v: 2230.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss:  2.302601/  2.302555, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 72.84 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.3167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 100.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 8820  90.092%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 2232.0\n",
      "lif layer 1 self.abs_max_v: 2232.0\n",
      "fc layer 1 self.abs_max_out: 2243.0\n",
      "lif layer 1 self.abs_max_v: 2243.0\n",
      "fc layer 1 self.abs_max_out: 2340.0\n",
      "lif layer 1 self.abs_max_v: 2340.0\n",
      "fc layer 1 self.abs_max_out: 2350.0\n",
      "lif layer 1 self.abs_max_v: 2350.0\n",
      "fc layer 2 self.abs_max_out: 31.0\n",
      "fc layer 2 self.abs_max_out: 32.0\n",
      "lif layer 2 self.abs_max_v: 59.0\n",
      "lif layer 2 self.abs_max_v: 61.0\n",
      "lif layer 2 self.abs_max_v: 62.5\n",
      "lif layer 2 self.abs_max_v: 63.0\n",
      "lif layer 2 self.abs_max_v: 63.5\n",
      "fc layer 1 self.abs_max_out: 2371.0\n",
      "lif layer 1 self.abs_max_v: 2371.0\n",
      "fc layer 1 self.abs_max_out: 2510.0\n",
      "lif layer 1 self.abs_max_v: 2510.0\n",
      "fc layer 1 self.abs_max_out: 2513.0\n",
      "lif layer 1 self.abs_max_v: 2513.0\n",
      "fc layer 1 self.abs_max_out: 2803.0\n",
      "lif layer 1 self.abs_max_v: 2803.0\n",
      "fc layer 1 self.abs_max_out: 2820.0\n",
      "lif layer 1 self.abs_max_v: 2820.0\n",
      "fc layer 1 self.abs_max_out: 2895.0\n",
      "lif layer 1 self.abs_max_v: 2895.0\n",
      "fc layer 2 self.abs_max_out: 33.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss:  2.302601/  2.302555, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 74.89 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.8417%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 100.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9fa4402185d4a5eb0750de989b59aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>iter_acc</td><td>0.0</td></tr><tr><td>tr_acc</td><td>0.09908</td></tr><tr><td>tr_epoch_loss</td><td>2.3026</td></tr><tr><td>val_acc_best</td><td>0.1</td></tr><tr><td>val_acc_now</td><td>0.1</td></tr><tr><td>val_loss</td><td>2.30256</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">magic-sweep-18</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/p75ln5fu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/p75ln5fu</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_134701-p75ln5fu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run p75ln5fu errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31082/3004161549.py\", line 119, in hyper_iter\n",
      "    my_snn_system(\n",
      "  File \"/tmp/ipykernel_31082/3596498357.py\", line 929, in my_snn_system\n",
      "    assert val_acc_best > 0.2\n",
      "AssertionError\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run p75ln5fu errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31082/3004161549.py\", line 119, in hyper_iter\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     my_snn_system(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31082/3596498357.py\", line 929, in my_snn_system\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     assert val_acc_best > 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m AssertionError\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: m8yjxhgd with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.046875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0234375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_135004-m8yjxhgd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m8yjxhgd' target=\"_blank\">confused-sweep-19</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m8yjxhgd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m8yjxhgd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': '20251214_135014_074', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 64, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 64, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 2, 'lif_layer_v_threshold2': 32, 'init_scaling': [0.03125, 0.046875, 0.0234375], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 64, self.v_threshold 64\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 2, self.v_threshold 32\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=64, v_reset=10000, sg_width=64, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=32, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 46.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 59.5\n",
      "fc layer 1 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 69.0\n",
      "fc layer 2 self.abs_max_out: 6.0\n",
      "lif layer 2 self.abs_max_v: 6.0\n",
      "fc layer 1 self.abs_max_out: 55.0\n",
      "lif layer 1 self.abs_max_v: 75.0\n",
      "fc layer 2 self.abs_max_out: 15.0\n",
      "lif layer 2 self.abs_max_v: 16.0\n",
      "fc layer 1 self.abs_max_out: 66.0\n",
      "lif layer 1 self.abs_max_v: 96.5\n",
      "fc layer 2 self.abs_max_out: 16.0\n",
      "lif layer 2 self.abs_max_v: 18.0\n",
      "fc layer 1 self.abs_max_out: 122.0\n",
      "lif layer 1 self.abs_max_v: 122.0\n",
      "fc layer 2 self.abs_max_out: 25.0\n",
      "lif layer 2 self.abs_max_v: 29.0\n",
      "fc layer 1 self.abs_max_out: 176.0\n",
      "lif layer 1 self.abs_max_v: 176.0\n",
      "fc layer 2 self.abs_max_out: 27.0\n",
      "lif layer 2 self.abs_max_v: 32.5\n",
      "fc layer 3 self.abs_max_out: 2.0\n",
      "fc layer 2 self.abs_max_out: 36.0\n",
      "lif layer 2 self.abs_max_v: 51.5\n",
      "fc layer 3 self.abs_max_out: 5.0\n",
      "fc layer 1 self.abs_max_out: 208.0\n",
      "lif layer 1 self.abs_max_v: 208.0\n",
      "fc layer 2 self.abs_max_out: 37.0\n",
      "lif layer 2 self.abs_max_v: 62.0\n",
      "fc layer 3 self.abs_max_out: 6.0\n",
      "fc layer 3 self.abs_max_out: 10.0\n",
      "fc layer 2 self.abs_max_out: 44.0\n",
      "fc layer 2 self.abs_max_out: 51.0\n",
      "fc layer 3 self.abs_max_out: 20.0\n",
      "fc layer 2 self.abs_max_out: 60.0\n",
      "fc layer 3 self.abs_max_out: 21.0\n",
      "lif layer 2 self.abs_max_v: 65.0\n",
      "lif layer 2 self.abs_max_v: 72.5\n",
      "fc layer 1 self.abs_max_out: 238.0\n",
      "lif layer 1 self.abs_max_v: 238.0\n",
      "fc layer 2 self.abs_max_out: 72.0\n",
      "lif layer 2 self.abs_max_v: 79.5\n",
      "lif layer 2 self.abs_max_v: 81.0\n",
      "fc layer 3 self.abs_max_out: 22.0\n",
      "fc layer 1 self.abs_max_out: 240.0\n",
      "lif layer 1 self.abs_max_v: 240.0\n",
      "fc layer 3 self.abs_max_out: 28.0\n",
      "lif layer 2 self.abs_max_v: 84.5\n",
      "fc layer 1 self.abs_max_out: 256.0\n",
      "lif layer 1 self.abs_max_v: 256.0\n",
      "lif layer 2 self.abs_max_v: 93.0\n",
      "fc layer 1 self.abs_max_out: 308.0\n",
      "lif layer 1 self.abs_max_v: 308.0\n",
      "fc layer 3 self.abs_max_out: 31.0\n",
      "fc layer 2 self.abs_max_out: 85.0\n",
      "fc layer 3 self.abs_max_out: 33.0\n",
      "fc layer 2 self.abs_max_out: 91.0\n",
      "fc layer 3 self.abs_max_out: 36.0\n",
      "fc layer 3 self.abs_max_out: 39.0\n",
      "lif layer 2 self.abs_max_v: 98.5\n",
      "fc layer 2 self.abs_max_out: 95.0\n",
      "fc layer 2 self.abs_max_out: 102.0\n",
      "lif layer 2 self.abs_max_v: 102.0\n",
      "fc layer 2 self.abs_max_out: 104.0\n",
      "lif layer 2 self.abs_max_v: 104.0\n",
      "fc layer 2 self.abs_max_out: 111.0\n",
      "lif layer 2 self.abs_max_v: 114.0\n",
      "lif layer 1 self.abs_max_v: 323.5\n",
      "fc layer 3 self.abs_max_out: 43.0\n",
      "fc layer 1 self.abs_max_out: 348.0\n",
      "lif layer 1 self.abs_max_v: 348.0\n",
      "lif layer 2 self.abs_max_v: 118.0\n",
      "fc layer 3 self.abs_max_out: 46.0\n",
      "lif layer 2 self.abs_max_v: 120.5\n",
      "fc layer 1 self.abs_max_out: 357.0\n",
      "lif layer 1 self.abs_max_v: 357.0\n",
      "fc layer 3 self.abs_max_out: 52.0\n",
      "lif layer 2 self.abs_max_v: 134.0\n",
      "lif layer 2 self.abs_max_v: 134.5\n",
      "lif layer 2 self.abs_max_v: 138.5\n",
      "fc layer 3 self.abs_max_out: 67.0\n",
      "fc layer 1 self.abs_max_out: 365.0\n",
      "lif layer 1 self.abs_max_v: 365.0\n",
      "lif layer 2 self.abs_max_v: 150.5\n",
      "lif layer 2 self.abs_max_v: 159.0\n",
      "fc layer 3 self.abs_max_out: 70.0\n",
      "fc layer 1 self.abs_max_out: 383.0\n",
      "lif layer 1 self.abs_max_v: 383.0\n",
      "lif layer 2 self.abs_max_v: 177.0\n",
      "lif layer 2 self.abs_max_v: 178.0\n",
      "fc layer 1 self.abs_max_out: 422.0\n",
      "lif layer 1 self.abs_max_v: 422.0\n",
      "fc layer 1 self.abs_max_out: 472.0\n",
      "lif layer 1 self.abs_max_v: 472.0\n",
      "lif layer 2 self.abs_max_v: 180.5\n",
      "fc layer 2 self.abs_max_out: 114.0\n",
      "lif layer 2 self.abs_max_v: 181.0\n",
      "lif layer 2 self.abs_max_v: 184.0\n",
      "fc layer 2 self.abs_max_out: 115.0\n",
      "fc layer 2 self.abs_max_out: 120.0\n",
      "fc layer 2 self.abs_max_out: 130.0\n",
      "lif layer 2 self.abs_max_v: 187.5\n",
      "lif layer 2 self.abs_max_v: 196.0\n",
      "lif layer 2 self.abs_max_v: 202.0\n",
      "lif layer 2 self.abs_max_v: 218.0\n",
      "lif layer 2 self.abs_max_v: 225.5\n",
      "lif layer 2 self.abs_max_v: 226.0\n",
      "fc layer 3 self.abs_max_out: 71.0\n",
      "fc layer 3 self.abs_max_out: 77.0\n",
      "fc layer 3 self.abs_max_out: 78.0\n",
      "fc layer 2 self.abs_max_out: 142.0\n",
      "fc layer 3 self.abs_max_out: 86.0\n",
      "fc layer 3 self.abs_max_out: 98.0\n",
      "fc layer 3 self.abs_max_out: 102.0\n",
      "fc layer 2 self.abs_max_out: 150.0\n",
      "lif layer 2 self.abs_max_v: 244.0\n",
      "lif layer 2 self.abs_max_v: 254.0\n",
      "lif layer 2 self.abs_max_v: 264.0\n",
      "fc layer 2 self.abs_max_out: 157.0\n",
      "lif layer 2 self.abs_max_v: 268.0\n",
      "fc layer 3 self.abs_max_out: 103.0\n",
      "fc layer 2 self.abs_max_out: 167.0\n",
      "fc layer 3 self.abs_max_out: 111.0\n",
      "lif layer 2 self.abs_max_v: 279.5\n",
      "fc layer 3 self.abs_max_out: 113.0\n",
      "fc layer 3 self.abs_max_out: 122.0\n",
      "fc layer 3 self.abs_max_out: 133.0\n",
      "fc layer 3 self.abs_max_out: 134.0\n",
      "fc layer 2 self.abs_max_out: 170.0\n",
      "lif layer 2 self.abs_max_v: 285.0\n",
      "fc layer 2 self.abs_max_out: 183.0\n",
      "lif layer 2 self.abs_max_v: 325.5\n",
      "fc layer 3 self.abs_max_out: 139.0\n",
      "fc layer 3 self.abs_max_out: 142.0\n",
      "fc layer 3 self.abs_max_out: 169.0\n",
      "lif layer 2 self.abs_max_v: 328.5\n",
      "lif layer 2 self.abs_max_v: 329.0\n",
      "fc layer 2 self.abs_max_out: 187.0\n",
      "fc layer 2 self.abs_max_out: 202.0\n",
      "fc layer 1 self.abs_max_out: 498.0\n",
      "lif layer 1 self.abs_max_v: 498.0\n",
      "fc layer 2 self.abs_max_out: 214.0\n",
      "fc layer 3 self.abs_max_out: 193.0\n",
      "fc layer 2 self.abs_max_out: 225.0\n",
      "fc layer 2 self.abs_max_out: 227.0\n",
      "fc layer 2 self.abs_max_out: 260.0\n",
      "lif layer 1 self.abs_max_v: 528.0\n",
      "fc layer 3 self.abs_max_out: 230.0\n",
      "lif layer 2 self.abs_max_v: 343.0\n",
      "lif layer 2 self.abs_max_v: 364.0\n",
      "fc layer 2 self.abs_max_out: 284.0\n",
      "fc layer 1 self.abs_max_out: 505.0\n",
      "lif layer 2 self.abs_max_v: 376.0\n",
      "fc layer 1 self.abs_max_out: 528.0\n",
      "fc layer 1 self.abs_max_out: 536.0\n",
      "lif layer 1 self.abs_max_v: 536.0\n",
      "lif layer 1 self.abs_max_v: 564.0\n",
      "lif layer 1 self.abs_max_v: 572.0\n",
      "lif layer 1 self.abs_max_v: 598.5\n",
      "lif layer 2 self.abs_max_v: 382.0\n",
      "lif layer 2 self.abs_max_v: 390.0\n",
      "fc layer 3 self.abs_max_out: 248.0\n",
      "fc layer 3 self.abs_max_out: 310.0\n",
      "fc layer 2 self.abs_max_out: 287.0\n",
      "lif layer 1 self.abs_max_v: 625.0\n",
      "lif layer 1 self.abs_max_v: 638.0\n",
      "fc layer 2 self.abs_max_out: 290.0\n",
      "fc layer 2 self.abs_max_out: 295.0\n",
      "lif layer 2 self.abs_max_v: 395.5\n",
      "fc layer 1 self.abs_max_out: 573.0\n",
      "fc layer 1 self.abs_max_out: 596.0\n",
      "fc layer 2 self.abs_max_out: 298.0\n",
      "fc layer 2 self.abs_max_out: 306.0\n",
      "lif layer 2 self.abs_max_v: 412.0\n",
      "lif layer 2 self.abs_max_v: 419.0\n",
      "fc layer 2 self.abs_max_out: 308.0\n",
      "fc layer 2 self.abs_max_out: 313.0\n",
      "fc layer 2 self.abs_max_out: 323.0\n",
      "lif layer 2 self.abs_max_v: 422.0\n",
      "lif layer 2 self.abs_max_v: 429.5\n",
      "lif layer 2 self.abs_max_v: 430.5\n",
      "lif layer 2 self.abs_max_v: 450.0\n",
      "lif layer 2 self.abs_max_v: 474.0\n",
      "fc layer 3 self.abs_max_out: 320.0\n",
      "lif layer 2 self.abs_max_v: 477.5\n",
      "lif layer 2 self.abs_max_v: 492.5\n",
      "lif layer 2 self.abs_max_v: 493.5\n",
      "lif layer 2 self.abs_max_v: 513.0\n",
      "fc layer 2 self.abs_max_out: 327.0\n",
      "fc layer 2 self.abs_max_out: 349.0\n",
      "lif layer 1 self.abs_max_v: 653.0\n",
      "lif layer 1 self.abs_max_v: 697.5\n",
      "fc layer 2 self.abs_max_out: 352.0\n",
      "fc layer 2 self.abs_max_out: 363.0\n",
      "fc layer 2 self.abs_max_out: 383.0\n",
      "lif layer 2 self.abs_max_v: 525.0\n",
      "lif layer 2 self.abs_max_v: 528.5\n",
      "lif layer 2 self.abs_max_v: 544.0\n",
      "fc layer 2 self.abs_max_out: 389.0\n",
      "fc layer 2 self.abs_max_out: 405.0\n",
      "fc layer 2 self.abs_max_out: 414.0\n",
      "fc layer 2 self.abs_max_out: 441.0\n",
      "lif layer 2 self.abs_max_v: 544.5\n",
      "lif layer 2 self.abs_max_v: 555.5\n",
      "lif layer 2 self.abs_max_v: 580.5\n",
      "lif layer 2 self.abs_max_v: 596.5\n",
      "lif layer 2 self.abs_max_v: 597.5\n",
      "fc layer 3 self.abs_max_out: 330.0\n",
      "fc layer 1 self.abs_max_out: 642.0\n",
      "lif layer 1 self.abs_max_v: 713.5\n",
      "lif layer 2 self.abs_max_v: 613.5\n",
      "lif layer 2 self.abs_max_v: 617.0\n",
      "lif layer 1 self.abs_max_v: 772.5\n",
      "lif layer 1 self.abs_max_v: 866.5\n",
      "lif layer 1 self.abs_max_v: 892.5\n",
      "lif layer 1 self.abs_max_v: 902.0\n",
      "lif layer 1 self.abs_max_v: 927.0\n",
      "fc layer 2 self.abs_max_out: 447.0\n",
      "fc layer 2 self.abs_max_out: 470.0\n",
      "fc layer 1 self.abs_max_out: 647.0\n",
      "fc layer 2 self.abs_max_out: 474.0\n",
      "fc layer 1 self.abs_max_out: 653.0\n",
      "fc layer 1 self.abs_max_out: 680.0\n",
      "fc layer 1 self.abs_max_out: 706.0\n",
      "fc layer 2 self.abs_max_out: 486.0\n",
      "fc layer 2 self.abs_max_out: 489.0\n",
      "lif layer 2 self.abs_max_v: 623.0\n",
      "lif layer 2 self.abs_max_v: 636.5\n",
      "fc layer 2 self.abs_max_out: 495.0\n",
      "fc layer 2 self.abs_max_out: 508.0\n",
      "lif layer 2 self.abs_max_v: 663.5\n",
      "lif layer 2 self.abs_max_v: 697.0\n",
      "lif layer 1 self.abs_max_v: 947.5\n",
      "lif layer 1 self.abs_max_v: 956.0\n",
      "lif layer 1 self.abs_max_v: 983.5\n",
      "lif layer 1 self.abs_max_v: 1079.0\n",
      "lif layer 1 self.abs_max_v: 1114.5\n",
      "lif layer 1 self.abs_max_v: 1150.5\n",
      "lif layer 1 self.abs_max_v: 1160.5\n",
      "lif layer 2 self.abs_max_v: 747.5\n",
      "lif layer 2 self.abs_max_v: 748.5\n",
      "lif layer 2 self.abs_max_v: 756.5\n",
      "lif layer 2 self.abs_max_v: 760.5\n",
      "lif layer 2 self.abs_max_v: 775.5\n",
      "lif layer 2 self.abs_max_v: 820.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss:  9.137783/ 62.993061, val:  34.58%, val_best:  34.58%, tr:  98.47%, tr_best:  98.47%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.0197%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4735%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 1739  17.763%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 345.0\n",
      "fc layer 3 self.abs_max_out: 364.0\n",
      "fc layer 2 self.abs_max_out: 536.0\n",
      "fc layer 2 self.abs_max_out: 552.0\n",
      "fc layer 2 self.abs_max_out: 585.0\n",
      "fc layer 2 self.abs_max_out: 592.0\n",
      "lif layer 2 self.abs_max_v: 831.5\n",
      "fc layer 2 self.abs_max_out: 615.0\n",
      "fc layer 2 self.abs_max_out: 626.0\n",
      "fc layer 2 self.abs_max_out: 639.0\n",
      "fc layer 2 self.abs_max_out: 644.0\n",
      "fc layer 2 self.abs_max_out: 649.0\n",
      "fc layer 2 self.abs_max_out: 682.0\n",
      "fc layer 2 self.abs_max_out: 687.0\n",
      "fc layer 2 self.abs_max_out: 691.0\n",
      "fc layer 2 self.abs_max_out: 693.0\n",
      "fc layer 2 self.abs_max_out: 698.0\n",
      "fc layer 2 self.abs_max_out: 722.0\n",
      "fc layer 2 self.abs_max_out: 733.0\n",
      "fc layer 2 self.abs_max_out: 742.0\n",
      "fc layer 2 self.abs_max_out: 756.0\n",
      "fc layer 2 self.abs_max_out: 771.0\n",
      "lif layer 2 self.abs_max_v: 833.5\n",
      "lif layer 2 self.abs_max_v: 851.0\n",
      "fc layer 2 self.abs_max_out: 793.0\n",
      "fc layer 1 self.abs_max_out: 720.0\n",
      "fc layer 3 self.abs_max_out: 434.0\n",
      "lif layer 2 self.abs_max_v: 1007.5\n",
      "fc layer 2 self.abs_max_out: 823.0\n",
      "fc layer 2 self.abs_max_out: 854.0\n",
      "lif layer 2 self.abs_max_v: 1022.0\n",
      "lif layer 2 self.abs_max_v: 1132.5\n",
      "fc layer 1 self.abs_max_out: 725.0\n",
      "fc layer 2 self.abs_max_out: 869.0\n",
      "fc layer 1 self.abs_max_out: 748.0\n",
      "fc layer 1 self.abs_max_out: 822.0\n",
      "lif layer 1 self.abs_max_v: 1250.5\n",
      "lif layer 1 self.abs_max_v: 1296.5\n",
      "lif layer 1 self.abs_max_v: 1311.5\n",
      "lif layer 1 self.abs_max_v: 1343.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 10.642908/ 66.506195, val:  38.75%, val_best:  38.75%, tr:  99.18%, tr_best:  99.18%, epoch time: 75.66 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.9669%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.1079%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3219  16.440%\n",
      "fc layer 2 self.abs_max_out: 872.0\n",
      "fc layer 2 self.abs_max_out: 875.0\n",
      "fc layer 2 self.abs_max_out: 881.0\n",
      "fc layer 2 self.abs_max_out: 905.0\n",
      "lif layer 2 self.abs_max_v: 1151.0\n",
      "lif layer 2 self.abs_max_v: 1189.5\n",
      "fc layer 2 self.abs_max_out: 930.0\n",
      "fc layer 2 self.abs_max_out: 982.0\n",
      "fc layer 2 self.abs_max_out: 991.0\n",
      "lif layer 2 self.abs_max_v: 1193.5\n",
      "lif layer 2 self.abs_max_v: 1234.0\n",
      "fc layer 3 self.abs_max_out: 437.0\n",
      "lif layer 2 self.abs_max_v: 1254.5\n",
      "fc layer 2 self.abs_max_out: 993.0\n",
      "fc layer 2 self.abs_max_out: 1011.0\n",
      "fc layer 2 self.abs_max_out: 1016.0\n",
      "fc layer 2 self.abs_max_out: 1076.0\n",
      "fc layer 1 self.abs_max_out: 877.0\n",
      "fc layer 2 self.abs_max_out: 1107.0\n",
      "lif layer 1 self.abs_max_v: 1441.0\n",
      "lif layer 2 self.abs_max_v: 1295.5\n",
      "fc layer 2 self.abs_max_out: 1128.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 11.316422/ 88.923073, val:  31.25%, val_best:  38.75%, tr:  98.57%, tr_best:  99.18%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.4510%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.9743%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 4704  16.016%\n",
      "fc layer 3 self.abs_max_out: 487.0\n",
      "fc layer 2 self.abs_max_out: 1139.0\n",
      "fc layer 2 self.abs_max_out: 1151.0\n",
      "fc layer 2 self.abs_max_out: 1208.0\n",
      "lif layer 2 self.abs_max_v: 1352.0\n",
      "lif layer 2 self.abs_max_v: 1374.5\n",
      "lif layer 2 self.abs_max_v: 1411.5\n",
      "lif layer 2 self.abs_max_v: 1446.0\n",
      "lif layer 2 self.abs_max_v: 1455.0\n",
      "lif layer 2 self.abs_max_v: 1462.0\n",
      "lif layer 2 self.abs_max_v: 1473.5\n",
      "fc layer 2 self.abs_max_out: 1253.0\n",
      "fc layer 1 self.abs_max_out: 894.0\n",
      "fc layer 2 self.abs_max_out: 1257.0\n",
      "fc layer 2 self.abs_max_out: 1307.0\n",
      "fc layer 2 self.abs_max_out: 1363.0\n",
      "fc layer 2 self.abs_max_out: 1366.0\n",
      "fc layer 1 self.abs_max_out: 917.0\n",
      "lif layer 1 self.abs_max_v: 1444.5\n",
      "lif layer 2 self.abs_max_v: 1503.5\n",
      "fc layer 1 self.abs_max_out: 972.0\n",
      "lif layer 1 self.abs_max_v: 1565.0\n",
      "lif layer 1 self.abs_max_v: 1570.5\n",
      "lif layer 1 self.abs_max_v: 1661.5\n",
      "lif layer 1 self.abs_max_v: 1741.0\n",
      "lif layer 1 self.abs_max_v: 1784.5\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss: 10.772183/ 76.721840, val:  31.67%, val_best:  38.75%, tr:  99.08%, tr_best:  99.18%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.7217%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0640%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6171  15.758%\n",
      "fc layer 2 self.abs_max_out: 1372.0\n",
      "fc layer 2 self.abs_max_out: 1375.0\n",
      "fc layer 2 self.abs_max_out: 1425.0\n",
      "lif layer 2 self.abs_max_v: 1616.0\n",
      "fc layer 2 self.abs_max_out: 1456.0\n",
      "lif layer 2 self.abs_max_v: 1642.5\n",
      "lif layer 2 self.abs_max_v: 1658.0\n",
      "fc layer 2 self.abs_max_out: 1473.0\n",
      "lif layer 2 self.abs_max_v: 1692.0\n",
      "lif layer 2 self.abs_max_v: 1801.0\n",
      "lif layer 2 self.abs_max_v: 1814.0\n",
      "lif layer 2 self.abs_max_v: 1835.0\n",
      "lif layer 2 self.abs_max_v: 1898.5\n",
      "fc layer 2 self.abs_max_out: 1522.0\n",
      "fc layer 1 self.abs_max_out: 1009.0\n",
      "fc layer 1 self.abs_max_out: 1086.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  9.755836/ 70.859337, val:  26.25%, val_best:  38.75%, tr:  98.98%, tr_best:  99.18%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.9777%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8806%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 7577  15.479%\n",
      "lif layer 2 self.abs_max_v: 1987.0\n",
      "lif layer 2 self.abs_max_v: 2010.5\n",
      "lif layer 2 self.abs_max_v: 2135.5\n",
      "lif layer 2 self.abs_max_v: 2149.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss: 10.066415/ 77.743538, val:  26.67%, val_best:  38.75%, tr:  98.26%, tr_best:  99.18%, epoch time: 75.39 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.7570%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5978%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9014  15.346%\n",
      "lif layer 2 self.abs_max_v: 2169.0\n",
      "fc layer 2 self.abs_max_out: 1633.0\n",
      "lif layer 2 self.abs_max_v: 2199.0\n",
      "fc layer 1 self.abs_max_out: 1105.0\n",
      "lif layer 1 self.abs_max_v: 1795.5\n",
      "fc layer 1 self.abs_max_out: 1149.0\n",
      "lif layer 1 self.abs_max_v: 1991.5\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  9.697610/ 54.047501, val:  41.25%, val_best:  41.25%, tr:  97.96%, tr_best:  99.18%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.9357%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6512%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10475  15.285%\n",
      "lif layer 2 self.abs_max_v: 2255.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  9.909885/ 72.182861, val:  43.33%, val_best:  43.33%, tr:  98.67%, tr_best:  99.18%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 90.4898%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7102%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 11952  15.260%\n",
      "lif layer 2 self.abs_max_v: 2288.0\n",
      "lif layer 2 self.abs_max_v: 2314.0\n",
      "lif layer 2 self.abs_max_v: 2325.0\n",
      "lif layer 1 self.abs_max_v: 1993.0\n",
      "lif layer 1 self.abs_max_v: 2034.5\n",
      "fc layer 1 self.abs_max_out: 1225.0\n",
      "lif layer 1 self.abs_max_v: 2097.0\n",
      "lif layer 1 self.abs_max_v: 2214.5\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  9.380307/ 57.900883, val:  35.83%, val_best:  43.33%, tr:  98.26%, tr_best:  99.18%, epoch time: 75.16 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 90.2082%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.9548%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 13468  15.285%\n",
      "lif layer 2 self.abs_max_v: 2348.5\n",
      "fc layer 1 self.abs_max_out: 1247.0\n",
      "fc layer 1 self.abs_max_out: 1299.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  8.306089/ 57.639549, val:  41.67%, val_best:  43.33%, tr:  98.67%, tr_best:  99.18%, epoch time: 75.22 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 90.0269%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.3906%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 14862  15.181%\n",
      "lif layer 2 self.abs_max_v: 2563.0\n",
      "lif layer 2 self.abs_max_v: 2706.5\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  8.295499/ 56.988297, val:  33.33%, val_best:  43.33%, tr:  98.57%, tr_best:  99.18%, epoch time: 75.68 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 90.1468%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.2716%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 16254  15.093%\n",
      "lif layer 2 self.abs_max_v: 2783.0\n",
      "lif layer 2 self.abs_max_v: 2801.0\n",
      "lif layer 2 self.abs_max_v: 2902.5\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  8.129796/ 44.910297, val:  40.00%, val_best:  43.33%, tr:  98.88%, tr_best:  99.18%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.5051%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.7033%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 17692  15.060%\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  7.973060/ 43.778744, val:  44.17%, val_best:  44.17%, tr:  99.18%, tr_best:  99.18%, epoch time: 75.38 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.1377%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.4013%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 19088  14.998%\n",
      "lif layer 1 self.abs_max_v: 2233.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  7.455963/ 44.236092, val:  33.75%, val_best:  44.17%, tr:  98.98%, tr_best:  99.18%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.4097%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.7130%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 20467  14.933%\n",
      "lif layer 1 self.abs_max_v: 2265.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  6.932026/ 45.257027, val:  36.25%, val_best:  44.17%, tr:  98.88%, tr_best:  99.18%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.6311%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.5420%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 21822  14.860%\n",
      "fc layer 1 self.abs_max_out: 1366.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  6.855704/ 34.761311, val:  40.42%, val_best:  44.17%, tr:  98.06%, tr_best:  99.18%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.6988%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.2489%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 23183  14.800%\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  6.610311/ 35.788082, val:  46.67%, val_best:  46.67%, tr:  98.67%, tr_best:  99.18%, epoch time: 74.93 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2877%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.6748%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 24581  14.770%\n",
      "lif layer 1 self.abs_max_v: 2352.5\n",
      "fc layer 1 self.abs_max_out: 1409.0\n",
      "lif layer 1 self.abs_max_v: 2413.5\n",
      "lif layer 1 self.abs_max_v: 2536.0\n",
      "fc layer 1 self.abs_max_out: 1520.0\n",
      "fc layer 1 self.abs_max_out: 1550.0\n",
      "fc layer 1 self.abs_max_out: 1656.0\n",
      "lif layer 1 self.abs_max_v: 2735.5\n",
      "lif layer 1 self.abs_max_v: 2784.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  6.780602/ 40.848259, val:  50.00%, val_best:  50.00%, tr:  98.67%, tr_best:  99.18%, epoch time: 75.13 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2279%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.1684%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 26044  14.779%\n",
      "fc layer 2 self.abs_max_out: 1698.0\n",
      "lif layer 2 self.abs_max_v: 3132.0\n",
      "lif layer 2 self.abs_max_v: 3212.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  6.906251/ 31.785789, val:  47.08%, val_best:  50.00%, tr:  98.98%, tr_best:  99.18%, epoch time: 75.40 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.3075%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.3032%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 27490  14.779%\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  6.235066/ 45.374615, val:  40.83%, val_best:  50.00%, tr:  99.18%, tr_best:  99.18%, epoch time: 75.09 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2022%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.4223%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 28787  14.702%\n",
      "fc layer 2 self.abs_max_out: 1747.0\n",
      "fc layer 2 self.abs_max_out: 1833.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  6.426085/ 32.245342, val:  45.83%, val_best:  50.00%, tr:  99.18%, tr_best:  99.18%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.1755%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.7689%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 30112  14.647%\n",
      "lif layer 2 self.abs_max_v: 3230.5\n",
      "fc layer 2 self.abs_max_out: 1912.0\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  6.572363/ 26.910917, val:  41.67%, val_best:  50.00%, tr:  98.16%, tr_best:  99.18%, epoch time: 75.89 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.5998%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.7407%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 31542  14.645%\n",
      "lif layer 2 self.abs_max_v: 3253.0\n",
      "fc layer 1 self.abs_max_out: 1727.0\n",
      "lif layer 1 self.abs_max_v: 2843.5\n",
      "lif layer 1 self.abs_max_v: 2845.0\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  5.981676/ 26.533314, val:  51.67%, val_best:  51.67%, tr:  98.57%, tr_best:  99.18%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2953%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.9227%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 32914  14.617%\n",
      "lif layer 2 self.abs_max_v: 3301.5\n",
      "lif layer 2 self.abs_max_v: 3361.5\n",
      "fc layer 2 self.abs_max_out: 1930.0\n",
      "lif layer 2 self.abs_max_v: 3492.0\n",
      "lif layer 2 self.abs_max_v: 3505.0\n",
      "fc layer 1 self.abs_max_out: 1744.0\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  6.192448/ 37.303558, val:  46.67%, val_best:  51.67%, tr:  98.57%, tr_best:  99.18%, epoch time: 75.59 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.1223%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.3514%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 34233  14.570%\n",
      "fc layer 2 self.abs_max_out: 1941.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  5.750741/ 26.602242, val:  47.50%, val_best:  51.67%, tr:  98.16%, tr_best:  99.18%, epoch time: 75.12 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.7928%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.4311%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 35529  14.516%\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  5.837725/ 29.855894, val:  47.08%, val_best:  51.67%, tr:  98.06%, tr_best:  99.18%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 87.9930%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.9153%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 36913  14.502%\n",
      "fc layer 2 self.abs_max_out: 1961.0\n",
      "fc layer 1 self.abs_max_out: 1826.0\n",
      "lif layer 1 self.abs_max_v: 2975.5\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  5.865220/ 25.681381, val:  55.83%, val_best:  55.83%, tr:  98.37%, tr_best:  99.18%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.1172%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.2269%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 38253  14.472%\n",
      "fc layer 1 self.abs_max_out: 1836.0\n",
      "fc layer 1 self.abs_max_out: 1927.0\n",
      "lif layer 1 self.abs_max_v: 3182.0\n",
      "lif layer 1 self.abs_max_v: 3199.0\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  6.208470/ 30.182678, val:  50.00%, val_best:  55.83%, tr:  98.57%, tr_best:  99.18%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.0579%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.0170%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 39639  14.460%\n",
      "fc layer 2 self.abs_max_out: 2002.0\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  6.088283/ 38.205917, val:  49.17%, val_best:  55.83%, tr:  98.37%, tr_best:  99.18%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.5627%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.1965%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 40988  14.437%\n",
      "lif layer 2 self.abs_max_v: 3515.0\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  6.065549/ 35.403522, val:  48.75%, val_best:  55.83%, tr:  98.88%, tr_best:  99.18%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.4808%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.0128%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 42315  14.408%\n",
      "fc layer 2 self.abs_max_out: 2060.0\n",
      "lif layer 2 self.abs_max_v: 3548.0\n",
      "lif layer 2 self.abs_max_v: 3573.5\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  6.145657/ 24.983133, val:  40.42%, val_best:  55.83%, tr:  98.26%, tr_best:  99.18%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.4520%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.4883%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 43669  14.389%\n",
      "fc layer 1 self.abs_max_out: 1947.0\n",
      "lif layer 1 self.abs_max_v: 3203.0\n",
      "lif layer 1 self.abs_max_v: 3216.5\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  5.670421/ 35.481625, val:  39.17%, val_best:  55.83%, tr:  97.65%, tr_best:  99.18%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.3947%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.3180%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 44988  14.360%\n",
      "fc layer 2 self.abs_max_out: 2081.0\n",
      "lif layer 2 self.abs_max_v: 3635.5\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  5.317534/ 29.836470, val:  46.67%, val_best:  55.83%, tr:  98.26%, tr_best:  99.18%, epoch time: 75.32 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.3952%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.7974%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 46262  14.319%\n",
      "fc layer 2 self.abs_max_out: 2095.0\n",
      "lif layer 2 self.abs_max_v: 3695.5\n",
      "lif layer 2 self.abs_max_v: 3894.0\n",
      "fc layer 1 self.abs_max_out: 1994.0\n",
      "lif layer 1 self.abs_max_v: 3275.5\n",
      "lif layer 1 self.abs_max_v: 3314.0\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  5.547883/ 37.945499, val:  37.92%, val_best:  55.83%, tr:  98.47%, tr_best:  99.18%, epoch time: 75.61 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.9363%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.6054%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 47617  14.305%\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  5.453166/ 33.391842, val:  45.42%, val_best:  55.83%, tr:  98.67%, tr_best:  99.18%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.1278%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.6106%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 48911  14.274%\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  5.390006/ 27.707146, val:  47.50%, val_best:  55.83%, tr:  99.08%, tr_best:  99.18%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.8540%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.7802%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 50206  14.245%\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  5.252475/ 27.513319, val:  50.42%, val_best:  55.83%, tr:  98.98%, tr_best:  99.18%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.7550%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.6956%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 51480  14.212%\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  5.433792/ 42.711384, val:  45.83%, val_best:  55.83%, tr:  98.37%, tr_best:  99.18%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2355%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.0066%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 52735  14.175%\n",
      "fc layer 1 self.abs_max_out: 2018.0\n",
      "lif layer 1 self.abs_max_v: 3318.5\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  5.965192/ 34.470940, val:  41.25%, val_best:  55.83%, tr:  98.57%, tr_best:  99.18%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.5278%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.4926%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 54099  14.169%\n",
      "lif layer 1 self.abs_max_v: 3338.0\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  5.237942/ 27.849054, val:  40.83%, val_best:  55.83%, tr:  97.96%, tr_best:  99.18%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.9348%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.6705%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 55439  14.157%\n",
      "fc layer 1 self.abs_max_out: 2070.0\n",
      "lif layer 1 self.abs_max_v: 3349.0\n",
      "lif layer 1 self.abs_max_v: 3475.5\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  5.050130/ 24.182257, val:  47.08%, val_best:  55.83%, tr:  98.77%, tr_best:  99.18%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.9340%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.0602%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 56672  14.119%\n",
      "fc layer 1 self.abs_max_out: 2074.0\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  5.451276/ 29.027929, val:  45.00%, val_best:  55.83%, tr:  98.37%, tr_best:  99.18%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2623%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.6825%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 57936  14.090%\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  5.364388/ 35.333290, val:  40.00%, val_best:  55.83%, tr:  99.08%, tr_best:  99.18%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2525%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.6612%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 59208  14.065%\n",
      "lif layer 1 self.abs_max_v: 3636.5\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  5.969753/ 27.090984, val:  52.08%, val_best:  55.83%, tr:  99.08%, tr_best:  99.18%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.1573%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.5331%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 60562  14.059%\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  5.205904/ 30.054535, val:  49.17%, val_best:  55.83%, tr:  98.88%, tr_best:  99.18%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2755%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.0936%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 61827  14.034%\n",
      "lif layer 1 self.abs_max_v: 3646.0\n",
      "lif layer 1 self.abs_max_v: 3815.0\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  5.281168/ 25.317366, val:  52.50%, val_best:  55.83%, tr:  98.57%, tr_best:  99.18%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2602%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.0705%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 63094  14.010%\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  5.231783/ 23.538391, val:  55.42%, val_best:  55.83%, tr:  98.57%, tr_best:  99.18%, epoch time: 75.76 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.7670%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.3416%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 64401  13.996%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  5.613005/ 44.410801, val:  34.17%, val_best:  55.83%, tr:  98.98%, tr_best:  99.18%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.7614%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.4587%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 65682  13.977%\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  5.231762/ 26.930382, val:  48.75%, val_best:  55.83%, tr:  98.57%, tr_best:  99.18%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.6983%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.6840%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 66927  13.952%\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  5.448158/ 44.373940, val:  49.58%, val_best:  55.83%, tr:  99.28%, tr_best:  99.28%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.9333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.5054%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 68212  13.935%\n",
      "fc layer 1 self.abs_max_out: 2081.0\n",
      "lif layer 1 self.abs_max_v: 3860.0\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  5.488659/ 25.842548, val:  51.25%, val_best:  55.83%, tr:  98.88%, tr_best:  99.28%, epoch time: 75.85 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2749%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.0840%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 69466  13.913%\n",
      "fc layer 2 self.abs_max_out: 2374.0\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  4.926024/ 29.635378, val:  43.75%, val_best:  55.83%, tr:  98.57%, tr_best:  99.28%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.1492%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.5951%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 70669  13.882%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  4.894290/ 28.153698, val:  58.33%, val_best:  58.33%, tr:  98.47%, tr_best:  99.28%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.9702%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.7148%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 71924  13.862%\n",
      "fc layer 1 self.abs_max_out: 2143.0\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  5.016753/ 23.669609, val:  54.17%, val_best:  58.33%, tr:  99.08%, tr_best:  99.28%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.8155%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.5198%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 73171  13.841%\n",
      "fc layer 1 self.abs_max_out: 2177.0\n",
      "lif layer 1 self.abs_max_v: 3951.0\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  5.174947/ 21.510588, val:  52.92%, val_best:  58.33%, tr:  98.88%, tr_best:  99.28%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.8268%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.2693%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 74425  13.822%\n",
      "fc layer 1 self.abs_max_out: 2262.0\n",
      "lif layer 1 self.abs_max_v: 4009.5\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  4.892106/ 32.536823, val:  45.42%, val_best:  58.33%, tr:  98.88%, tr_best:  99.28%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.8220%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.4179%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 75609  13.791%\n",
      "lif layer 1 self.abs_max_v: 4171.0\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  5.313897/ 34.459999, val:  43.33%, val_best:  58.33%, tr:  99.18%, tr_best:  99.28%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.7559%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.0592%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 76861  13.774%\n",
      "fc layer 1 self.abs_max_out: 2298.0\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  4.992945/ 42.352394, val:  45.00%, val_best:  58.33%, tr:  99.08%, tr_best:  99.28%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.7859%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.9360%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 78043  13.744%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  5.139474/ 29.797424, val:  42.92%, val_best:  58.33%, tr:  98.16%, tr_best:  99.28%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.8545%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.6908%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 79241  13.719%\n",
      "lif layer 2 self.abs_max_v: 4065.5\n",
      "fc layer 1 self.abs_max_out: 2352.0\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  4.828249/ 43.046417, val:  38.33%, val_best:  58.33%, tr:  98.77%, tr_best:  99.28%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.9575%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.2906%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 80508  13.706%\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  5.073310/ 47.656898, val:  35.83%, val_best:  58.33%, tr:  98.98%, tr_best:  99.28%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2148%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.0927%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 81748  13.689%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  5.016469/ 20.562784, val:  57.50%, val_best:  58.33%, tr:  98.67%, tr_best:  99.28%, epoch time: 75.87 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.1140%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.0743%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 83013  13.676%\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  4.891492/ 27.577620, val:  49.17%, val_best:  58.33%, tr:  98.67%, tr_best:  99.28%, epoch time: 75.75 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.7674%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.3364%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 84219  13.655%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  4.692694/ 28.980610, val:  45.83%, val_best:  58.33%, tr:  98.98%, tr_best:  99.28%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.5360%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.0527%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 85424  13.634%\n",
      "fc layer 1 self.abs_max_out: 2361.0\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  4.683717/ 28.931574, val:  48.33%, val_best:  58.33%, tr:  98.16%, tr_best:  99.28%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.8632%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.3087%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 86616  13.611%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  4.350023/ 26.310139, val:  51.25%, val_best:  58.33%, tr:  98.77%, tr_best:  99.28%, epoch time: 75.76 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.8211%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.4654%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 87774  13.584%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  4.906103/ 21.630070, val:  52.50%, val_best:  58.33%, tr:  98.77%, tr_best:  99.28%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.9713%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.7131%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 88986  13.566%\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  5.083106/ 26.570524, val:  45.83%, val_best:  58.33%, tr:  98.57%, tr_best:  99.28%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.9191%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.4719%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 90216  13.552%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  5.131986/ 25.973413, val:  60.83%, val_best:  60.83%, tr:  98.77%, tr_best:  99.28%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2309%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.3882%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 91421  13.534%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  4.466227/ 36.032887, val:  41.67%, val_best:  60.83%, tr:  98.16%, tr_best:  99.28%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.5821%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.2247%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 92528  13.502%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  4.739703/ 28.549255, val:  46.67%, val_best:  60.83%, tr:  99.08%, tr_best:  99.28%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.3147%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.4447%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 93735  13.485%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  4.675422/ 44.062183, val:  42.08%, val_best:  60.83%, tr:  98.57%, tr_best:  99.28%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.1130%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.7580%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 94974  13.474%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  4.541172/ 23.016901, val:  62.50%, val_best:  62.50%, tr:  97.75%, tr_best:  99.28%, epoch time: 75.80 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.0566%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8340%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 96183  13.458%\n",
      "lif layer 2 self.abs_max_v: 4085.0\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  4.409084/ 40.553627, val:  50.42%, val_best:  62.50%, tr:  98.77%, tr_best:  99.28%, epoch time: 75.90 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.9696%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.3554%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 97343  13.437%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  4.897827/ 29.872393, val:  48.33%, val_best:  62.50%, tr:  98.98%, tr_best:  99.28%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.1931%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.0610%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 98575  13.425%\n",
      "fc layer 1 self.abs_max_out: 2410.0\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  4.839056/ 26.046452, val:  53.75%, val_best:  62.50%, tr:  98.67%, tr_best:  99.28%, epoch time: 75.59 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2626%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.0757%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 99756  13.407%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  4.679412/ 22.877600, val:  58.33%, val_best:  62.50%, tr:  98.37%, tr_best:  99.28%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2807%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.6192%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 100986  13.396%\n",
      "fc layer 1 self.abs_max_out: 2414.0\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  4.553899/ 37.608673, val:  39.17%, val_best:  62.50%, tr:  98.67%, tr_best:  99.28%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.1864%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.3862%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 102141  13.376%\n",
      "lif layer 2 self.abs_max_v: 4194.0\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  4.433211/ 35.638409, val:  47.08%, val_best:  62.50%, tr:  99.08%, tr_best:  99.28%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.8629%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.4506%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 103296  13.356%\n",
      "fc layer 1 self.abs_max_out: 2463.0\n",
      "lif layer 1 self.abs_max_v: 4184.0\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  4.571073/ 20.509495, val:  60.83%, val_best:  62.50%, tr:  98.88%, tr_best:  99.28%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.8100%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.3696%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 104460  13.338%\n",
      "fc layer 1 self.abs_max_out: 2494.0\n",
      "lif layer 1 self.abs_max_v: 4309.5\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  4.689428/ 28.348684, val:  50.42%, val_best:  62.50%, tr:  98.88%, tr_best:  99.28%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.8605%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.4742%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 105689  13.328%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  4.353976/ 25.732615, val:  44.58%, val_best:  62.50%, tr:  99.08%, tr_best:  99.28%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.0931%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.4021%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 106793  13.303%\n",
      "fc layer 1 self.abs_max_out: 2505.0\n",
      "lif layer 1 self.abs_max_v: 4327.0\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  4.649777/ 23.375956, val:  53.33%, val_best:  62.50%, tr:  98.16%, tr_best:  99.28%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.0514%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.6906%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 108007  13.292%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  4.564271/ 25.132170, val:  49.17%, val_best:  62.50%, tr:  98.06%, tr_best:  99.28%, epoch time: 75.60 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.9276%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.6951%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 109223  13.282%\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  4.633492/ 29.819550, val:  47.08%, val_best:  62.50%, tr:  98.47%, tr_best:  99.28%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.0504%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8734%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 110465  13.275%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  4.478965/ 24.846552, val:  52.50%, val_best:  62.50%, tr:  98.26%, tr_best:  99.28%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.0331%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8795%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 111669  13.263%\n",
      "fc layer 1 self.abs_max_out: 2508.0\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  4.622527/ 31.935692, val:  47.92%, val_best:  62.50%, tr:  98.67%, tr_best:  99.28%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.0276%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.5266%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 112834  13.248%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  4.446198/ 29.217831, val:  61.67%, val_best:  62.50%, tr:  98.16%, tr_best:  99.28%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.3074%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8819%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 114001  13.233%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  4.416342/ 29.673817, val:  47.92%, val_best:  62.50%, tr:  98.47%, tr_best:  99.28%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2688%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.6883%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 115144  13.215%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  4.669464/ 24.422934, val:  45.42%, val_best:  62.50%, tr:  98.57%, tr_best:  99.28%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.9771%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.4952%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 116344  13.204%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  4.563233/ 28.594770, val:  48.75%, val_best:  62.50%, tr:  98.37%, tr_best:  99.28%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.8758%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.6499%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 117541  13.194%\n",
      "fc layer 1 self.abs_max_out: 2523.0\n",
      "lif layer 1 self.abs_max_v: 4335.5\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  4.661780/ 18.901834, val:  48.33%, val_best:  62.50%, tr:  98.77%, tr_best:  99.28%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2037%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.4442%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 118702  13.179%\n",
      "fc layer 1 self.abs_max_out: 2547.0\n",
      "lif layer 1 self.abs_max_v: 4336.5\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  4.330162/ 34.754616, val:  38.33%, val_best:  62.50%, tr:  98.06%, tr_best:  99.28%, epoch time: 76.22 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.1518%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8077%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 119840  13.162%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  4.644673/ 24.705551, val:  59.58%, val_best:  62.50%, tr:  98.26%, tr_best:  99.28%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2804%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.5141%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 121020  13.151%\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  4.377019/ 26.341530, val:  49.58%, val_best:  62.50%, tr:  99.18%, tr_best:  99.28%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.0210%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8409%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 122142  13.133%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  4.530829/ 20.884790, val:  58.33%, val_best:  62.50%, tr:  98.06%, tr_best:  99.28%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.6915%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1068%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 123358  13.125%\n",
      "fc layer 1 self.abs_max_out: 2640.0\n",
      "lif layer 1 self.abs_max_v: 4506.5\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  4.255537/ 32.604267, val:  50.42%, val_best:  62.50%, tr:  98.67%, tr_best:  99.28%, epoch time: 76.01 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.6466%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1899%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 124500  13.110%\n",
      "fc layer 2 self.abs_max_out: 2433.0\n",
      "fc layer 1 self.abs_max_out: 2663.0\n",
      "lif layer 1 self.abs_max_v: 4563.0\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  4.209741/ 19.183092, val:  59.58%, val_best:  62.50%, tr:  98.67%, tr_best:  99.28%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.9621%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8827%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 125628  13.094%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  4.517055/ 35.615520, val:  35.00%, val_best:  62.50%, tr:  98.98%, tr_best:  99.28%, epoch time: 75.70 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.0780%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.3454%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 126724  13.075%\n",
      "lif layer 1 self.abs_max_v: 4577.0\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  4.356914/ 22.698769, val:  61.25%, val_best:  62.50%, tr:  98.67%, tr_best:  99.28%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.0286%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1733%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 127890  13.063%\n",
      "fc layer 1 self.abs_max_out: 2715.0\n",
      "lif layer 1 self.abs_max_v: 4623.5\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  4.067505/ 27.482679, val:  42.92%, val_best:  62.50%, tr:  99.08%, tr_best:  99.28%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.9151%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1068%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 128978  13.044%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  4.346481/ 18.325861, val:  60.83%, val_best:  62.50%, tr:  98.47%, tr_best:  99.28%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.1311%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4223%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 130137  13.032%\n",
      "fc layer 1 self.abs_max_out: 2739.0\n",
      "lif layer 1 self.abs_max_v: 4703.0\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  4.015566/ 27.511507, val:  52.08%, val_best:  62.50%, tr:  99.18%, tr_best:  99.28%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.5082%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8260%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 131229  13.014%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  4.521392/ 25.489540, val:  52.92%, val_best:  62.50%, tr:  98.26%, tr_best:  99.28%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.4034%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.6570%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 132358  13.000%\n",
      "fc layer 2 self.abs_max_out: 2478.0\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  4.346004/ 20.787277, val:  62.08%, val_best:  62.50%, tr:  98.98%, tr_best:  99.28%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2232%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1293%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 133546  12.991%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  4.565280/ 24.454609, val:  53.33%, val_best:  62.50%, tr:  98.47%, tr_best:  99.28%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1615%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 134753  12.985%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  4.217210/ 29.458252, val:  39.58%, val_best:  62.50%, tr:  98.57%, tr_best:  99.28%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.1868%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.7809%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 135848  12.968%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  4.385974/ 36.015743, val:  49.17%, val_best:  62.50%, tr:  98.88%, tr_best:  99.28%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.6249%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.2942%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 136975  12.955%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  4.854517/ 26.714153, val:  45.83%, val_best:  62.50%, tr:  98.67%, tr_best:  99.28%, epoch time: 75.62 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.5129%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.1583%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 138156  12.947%\n",
      "lif layer 2 self.abs_max_v: 4343.0\n",
      "lif layer 1 self.abs_max_v: 4778.5\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  4.491322/ 28.864885, val:  49.58%, val_best:  62.50%, tr:  98.88%, tr_best:  99.28%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.3286%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.5107%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 139333  12.938%\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  4.383586/ 21.348370, val:  51.67%, val_best:  62.50%, tr:  98.26%, tr_best:  99.28%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.5265%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.4344%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 140438  12.923%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  4.048101/ 19.725613, val:  65.42%, val_best:  65.42%, tr:  98.57%, tr_best:  99.28%, epoch time: 75.62 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2788%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5909%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 141524  12.907%\n",
      "fc layer 1 self.abs_max_out: 2778.0\n",
      "lif layer 1 self.abs_max_v: 4785.0\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  4.216314/ 16.049957, val:  53.75%, val_best:  65.42%, tr:  98.57%, tr_best:  99.28%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.3054%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4707%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 142696  12.899%\n",
      "fc layer 2 self.abs_max_out: 2504.0\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  4.204057/ 26.186762, val:  61.25%, val_best:  65.42%, tr:  98.57%, tr_best:  99.28%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.5114%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0354%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 143801  12.885%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  4.261593/ 32.822109, val:  49.58%, val_best:  65.42%, tr:  98.26%, tr_best:  99.28%, epoch time: 75.75 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.5919%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9249%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 144915  12.872%\n",
      "fc layer 2 self.abs_max_out: 2628.0\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  4.198232/ 24.158188, val:  55.83%, val_best:  65.42%, tr:  98.47%, tr_best:  99.28%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.5975%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2920%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 146026  12.858%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  4.160617/ 23.848663, val:  57.92%, val_best:  65.42%, tr:  98.67%, tr_best:  99.28%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.3444%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3806%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 147170  12.848%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  4.287203/ 27.779699, val:  56.67%, val_best:  65.42%, tr:  98.77%, tr_best:  99.28%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.1581%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.6698%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 148288  12.836%\n",
      "fc layer 1 self.abs_max_out: 2855.0\n",
      "lif layer 1 self.abs_max_v: 4983.0\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  4.545341/ 25.347431, val:  47.50%, val_best:  65.42%, tr:  98.77%, tr_best:  99.28%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.3579%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0502%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 149451  12.828%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  4.419508/ 22.527372, val:  56.67%, val_best:  65.42%, tr:  98.47%, tr_best:  99.28%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.3355%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9104%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 150589  12.818%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  4.027641/ 25.349854, val:  59.17%, val_best:  65.42%, tr:  98.67%, tr_best:  99.28%, epoch time: 76.01 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.3247%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4037%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 151693  12.806%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  4.387303/ 21.042351, val:  53.75%, val_best:  65.42%, tr:  98.37%, tr_best:  99.28%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2938%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1645%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 152822  12.795%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  4.084034/ 22.103157, val:  63.33%, val_best:  65.42%, tr:  98.88%, tr_best:  99.28%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.4806%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3071%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 153907  12.781%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  4.208011/ 17.035713, val:  65.00%, val_best:  65.42%, tr:  98.88%, tr_best:  99.28%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.3178%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3776%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 155027  12.770%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  4.496841/ 26.492668, val:  54.17%, val_best:  65.42%, tr:  98.06%, tr_best:  99.28%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2984%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8027%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 156203  12.764%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  4.122358/ 24.301407, val:  61.67%, val_best:  65.42%, tr:  98.47%, tr_best:  99.28%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.3439%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5248%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 157330  12.754%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  4.269266/ 28.914299, val:  53.75%, val_best:  65.42%, tr:  99.08%, tr_best:  99.28%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.3788%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8632%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 158449  12.744%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  4.266133/ 26.735346, val:  47.92%, val_best:  65.42%, tr:  98.67%, tr_best:  99.28%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.4533%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9973%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 159521  12.730%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  4.067431/ 19.156536, val:  44.58%, val_best:  65.42%, tr:  98.57%, tr_best:  99.28%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.3856%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4388%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 160618  12.718%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  3.898173/ 13.884948, val:  62.92%, val_best:  65.42%, tr:  99.18%, tr_best:  99.28%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.3348%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7592%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 161692  12.705%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  4.023030/ 35.009972, val:  49.58%, val_best:  65.42%, tr:  98.67%, tr_best:  99.28%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.3131%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9772%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 162805  12.694%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  4.512446/ 26.044298, val:  58.75%, val_best:  65.42%, tr:  97.96%, tr_best:  99.28%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.1922%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0258%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 163976  12.689%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  3.857506/ 21.574045, val:  58.75%, val_best:  65.42%, tr:  98.67%, tr_best:  99.28%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.3166%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2310%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 164973  12.670%\n",
      "lif layer 1 self.abs_max_v: 5023.0\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  4.229027/ 20.879293, val:  56.25%, val_best:  65.42%, tr:  98.67%, tr_best:  99.28%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.3796%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2978%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 166101  12.661%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  3.717351/ 22.555107, val:  52.50%, val_best:  65.42%, tr:  99.28%, tr_best:  99.28%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.3749%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4610%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 167101  12.643%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  4.082810/ 23.949068, val:  58.75%, val_best:  65.42%, tr:  98.57%, tr_best:  99.28%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2034%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3038%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 168172  12.631%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  4.021602/ 22.902662, val:  50.42%, val_best:  65.42%, tr:  98.98%, tr_best:  99.28%, epoch time: 75.85 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2976%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2823%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 169217  12.617%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  4.114644/ 16.912069, val:  62.50%, val_best:  65.42%, tr:  98.16%, tr_best:  99.28%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2320%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4519%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 170321  12.607%\n",
      "lif layer 2 self.abs_max_v: 4421.5\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  4.039690/ 15.852922, val:  67.92%, val_best:  67.92%, tr:  98.26%, tr_best:  99.28%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2394%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3008%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 171394  12.595%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  4.003974/ 22.898075, val:  55.42%, val_best:  67.92%, tr:  98.57%, tr_best:  99.28%, epoch time: 75.60 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.3511%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7971%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 172473  12.584%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  4.058800/ 24.782190, val:  52.92%, val_best:  67.92%, tr:  98.37%, tr_best:  99.28%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.1786%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4434%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 173588  12.575%\n",
      "lif layer 2 self.abs_max_v: 4481.5\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  4.236059/ 25.750896, val:  49.17%, val_best:  67.92%, tr:  98.88%, tr_best:  99.28%, epoch time: 75.68 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2509%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2984%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 174677  12.565%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  4.343203/ 24.476801, val:  61.67%, val_best:  67.92%, tr:  98.57%, tr_best:  99.28%, epoch time: 75.81 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.4674%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2294%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 175821  12.559%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  4.202182/ 22.511772, val:  56.25%, val_best:  67.92%, tr:  98.16%, tr_best:  99.28%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.7349%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1658%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 176903  12.548%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  4.302866/ 21.556309, val:  56.67%, val_best:  67.92%, tr:  98.77%, tr_best:  99.28%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.7061%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1805%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 178020  12.541%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  4.024199/ 17.747971, val:  54.58%, val_best:  67.92%, tr:  98.88%, tr_best:  99.28%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.6541%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3078%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 179069  12.528%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  4.107222/ 19.607183, val:  59.58%, val_best:  67.92%, tr:  99.08%, tr_best:  99.28%, epoch time: 75.81 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.5386%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1989%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 180142  12.517%\n",
      "fc layer 2 self.abs_max_out: 2735.0\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  4.076381/ 29.696461, val:  42.92%, val_best:  67.92%, tr:  99.08%, tr_best:  99.28%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.6912%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3911%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 181197  12.506%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  4.185087/ 20.238169, val:  67.92%, val_best:  67.92%, tr:  98.88%, tr_best:  99.28%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.6575%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5259%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 182332  12.500%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  3.926920/ 32.207771, val:  50.83%, val_best:  67.92%, tr:  98.77%, tr_best:  99.28%, epoch time: 75.90 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.5797%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5090%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 183390  12.488%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  3.943430/ 16.878698, val:  70.00%, val_best:  70.00%, tr:  98.88%, tr_best:  99.28%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.3891%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7164%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 184443  12.477%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  3.711248/ 33.862396, val:  45.00%, val_best:  70.00%, tr:  98.47%, tr_best:  99.28%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2356%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8137%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 185483  12.465%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  4.035879/ 37.619442, val:  54.17%, val_best:  70.00%, tr:  98.98%, tr_best:  99.28%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.1646%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4898%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 186562  12.455%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  4.276646/ 25.512627, val:  53.75%, val_best:  70.00%, tr:  97.75%, tr_best:  99.28%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2444%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1112%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 187660  12.447%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  4.069112/ 19.323454, val:  57.92%, val_best:  70.00%, tr:  97.96%, tr_best:  99.28%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.5609%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6018%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 188711  12.436%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  4.001638/ 23.461119, val:  52.92%, val_best:  70.00%, tr:  98.67%, tr_best:  99.28%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.6927%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3642%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 189768  12.426%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  4.094936/ 28.521208, val:  53.33%, val_best:  70.00%, tr:  98.57%, tr_best:  99.28%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.6256%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3486%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 190859  12.417%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  4.137728/ 24.755610, val:  59.58%, val_best:  70.00%, tr:  99.28%, tr_best:  99.28%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.4669%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3724%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 191901  12.406%\n",
      "fc layer 1 self.abs_max_out: 2888.0\n",
      "lif layer 1 self.abs_max_v: 5100.5\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  3.897683/ 15.497193, val:  64.58%, val_best:  70.00%, tr:  97.75%, tr_best:  99.28%, epoch time: 75.62 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.5062%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5444%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 192939  12.395%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  3.975894/ 20.833975, val:  61.67%, val_best:  70.00%, tr:  98.67%, tr_best:  99.28%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.5890%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5183%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 194008  12.386%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  3.946560/ 29.477896, val:  45.83%, val_best:  70.00%, tr:  99.08%, tr_best:  99.28%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.7236%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1207%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 195040  12.374%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  4.390299/ 23.230698, val:  50.42%, val_best:  70.00%, tr:  98.57%, tr_best:  99.28%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.5698%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4688%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 196186  12.370%\n",
      "fc layer 1 self.abs_max_out: 2925.0\n",
      "lif layer 1 self.abs_max_v: 5151.5\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  3.860976/ 29.057732, val:  50.42%, val_best:  70.00%, tr:  98.26%, tr_best:  99.28%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.6324%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7970%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 197243  12.360%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  4.126394/ 23.857700, val:  51.67%, val_best:  70.00%, tr:  98.47%, tr_best:  99.28%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.5924%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3235%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 198332  12.353%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  4.112046/ 25.254013, val:  58.33%, val_best:  70.00%, tr:  97.96%, tr_best:  99.28%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.5361%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6613%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 199430  12.346%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  4.069703/ 21.559620, val:  65.00%, val_best:  70.00%, tr:  98.47%, tr_best:  99.28%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.5665%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4945%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 200542  12.340%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  4.164420/ 22.707129, val:  52.08%, val_best:  70.00%, tr:  98.67%, tr_best:  99.28%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.4950%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5104%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 201603  12.331%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  4.069861/ 26.396637, val:  53.33%, val_best:  70.00%, tr:  98.77%, tr_best:  99.28%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.4106%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4700%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 202687  12.323%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  3.927817/ 31.079702, val:  49.17%, val_best:  70.00%, tr:  97.96%, tr_best:  99.28%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.6843%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4069%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 203718  12.313%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  3.943417/ 30.008402, val:  52.92%, val_best:  70.00%, tr:  98.37%, tr_best:  99.28%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.7680%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5682%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 204719  12.301%\n",
      "fc layer 1 self.abs_max_out: 2946.0\n",
      "lif layer 1 self.abs_max_v: 5186.5\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  4.088807/ 31.303602, val:  46.25%, val_best:  70.00%, tr:  98.57%, tr_best:  99.28%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1141%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.7039%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5048%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 205787  12.292%\n",
      "fc layer 1 self.abs_max_out: 3004.0\n",
      "lif layer 1 self.abs_max_v: 5342.0\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  4.064922/ 24.276762, val:  57.08%, val_best:  70.00%, tr:  98.47%, tr_best:  99.28%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.5995%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6506%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 206845  12.284%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  3.978790/ 22.468506, val:  58.75%, val_best:  70.00%, tr:  98.88%, tr_best:  99.28%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.4756%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0593%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 207912  12.276%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  3.895986/ 26.776091, val:  50.83%, val_best:  70.00%, tr:  98.47%, tr_best:  99.28%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2495%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9003%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 208934  12.265%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  4.121237/ 23.838963, val:  54.58%, val_best:  70.00%, tr:  98.06%, tr_best:  99.28%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.2362%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4143%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 209993  12.257%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  4.300382/ 22.570541, val:  58.33%, val_best:  70.00%, tr:  98.47%, tr_best:  99.28%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.1716%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2589%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 211066  12.250%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  3.922716/ 19.160891, val:  52.92%, val_best:  70.00%, tr:  98.98%, tr_best:  99.28%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.4521%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9983%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 212117  12.241%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  3.662827/ 14.151776, val:  71.25%, val_best:  71.25%, tr:  98.88%, tr_best:  99.28%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0312%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.4321%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9793%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 213138  12.231%\n",
      "lif layer 2 self.abs_max_v: 4625.5\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  4.044812/ 22.305563, val:  66.67%, val_best:  71.25%, tr:  98.67%, tr_best:  99.28%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0376%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.6042%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2085%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 214167  12.221%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  3.880920/ 24.149866, val:  59.58%, val_best:  71.25%, tr:  98.47%, tr_best:  99.28%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.6152%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7513%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 215180  12.211%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  3.871168/ 17.420174, val:  68.33%, val_best:  71.25%, tr:  98.98%, tr_best:  99.28%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.5840%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8117%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 216203  12.201%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  3.751275/ 29.583048, val:  50.00%, val_best:  71.25%, tr:  98.47%, tr_best:  99.28%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.5308%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8249%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 217195  12.190%\n",
      "lif layer 1 self.abs_max_v: 5371.5\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  3.800924/ 23.938623, val:  63.75%, val_best:  71.25%, tr:  98.57%, tr_best:  99.28%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.4546%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2929%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 218235  12.181%\n",
      "fc layer 1 self.abs_max_out: 3022.0\n",
      "lif layer 1 self.abs_max_v: 5387.5\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  3.717531/ 32.916553, val:  46.67%, val_best:  71.25%, tr:  98.06%, tr_best:  99.28%, epoch time: 75.68 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0426%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.4829%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2315%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 219265  12.172%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  3.820018/ 32.088940, val:  42.92%, val_best:  71.25%, tr:  98.16%, tr_best:  99.28%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.6275%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2215%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 220289  12.163%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  3.937801/ 28.814671, val:  47.92%, val_best:  71.25%, tr:  98.57%, tr_best:  99.28%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.5353%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2312%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 221378  12.157%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  4.045330/ 16.853497, val:  73.33%, val_best:  73.33%, tr:  98.47%, tr_best:  99.28%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.5544%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8087%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 222444  12.151%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  4.171552/ 24.555140, val:  48.33%, val_best:  73.33%, tr:  98.98%, tr_best:  99.28%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.7814%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7713%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 223504  12.144%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  3.861843/ 19.103262, val:  62.92%, val_best:  73.33%, tr:  98.06%, tr_best:  99.28%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.8100%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0598%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 224535  12.135%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  3.740734/ 23.400982, val:  57.50%, val_best:  73.33%, tr:  98.37%, tr_best:  99.28%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.7115%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0354%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 225556  12.126%\n",
      "fc layer 1 self.abs_max_out: 3061.0\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  3.847916/ 23.550577, val:  57.92%, val_best:  73.33%, tr:  99.08%, tr_best:  99.28%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.7299%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0852%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 226562  12.116%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  3.883689/ 28.105417, val:  52.08%, val_best:  73.33%, tr:  98.77%, tr_best:  99.28%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.7321%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4144%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 227599  12.108%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  3.879672/ 28.556196, val:  51.25%, val_best:  73.33%, tr:  98.57%, tr_best:  99.28%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.7514%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8440%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 228620  12.100%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  3.853256/ 28.247133, val:  60.00%, val_best:  73.33%, tr:  98.26%, tr_best:  99.28%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.6145%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7776%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 229648  12.091%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  3.917210/ 28.377443, val:  44.17%, val_best:  73.33%, tr:  98.47%, tr_best:  99.28%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.5603%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9566%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 230666  12.083%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  3.990818/ 26.862055, val:  57.92%, val_best:  73.33%, tr:  98.88%, tr_best:  99.28%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.4405%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0943%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 231743  12.077%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  4.147486/ 25.862776, val:  56.25%, val_best:  73.33%, tr:  98.47%, tr_best:  99.28%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.4300%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7943%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 232797  12.071%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  3.952520/ 30.584143, val:  48.75%, val_best:  73.33%, tr:  97.45%, tr_best:  99.28%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.4503%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0733%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 233876  12.065%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  3.905365/ 24.069172, val:  55.00%, val_best:  73.33%, tr:  98.47%, tr_best:  99.28%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.6635%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7562%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 234895  12.057%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  3.891446/ 24.624512, val:  65.00%, val_best:  73.33%, tr:  98.88%, tr_best:  99.28%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 89.6419%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5595%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b6cd3c40ef45ecb59e0dfa176546cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñÑ‚ñÑ‚ñÜ‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñà‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñá‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÉ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñá‚ñÅ‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÉ‚ñá‚ñÑ‚ñÜ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñÑ‚ñÑ‚ñÜ‚ñá</td></tr><tr><td>val_loss</td><td>‚ñá‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.98876</td></tr><tr><td>tr_epoch_loss</td><td>3.89145</td></tr><tr><td>val_acc_best</td><td>0.73333</td></tr><tr><td>val_acc_now</td><td>0.65</td></tr><tr><td>val_loss</td><td>24.62451</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">confused-sweep-19</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m8yjxhgd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m8yjxhgd</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_135004-m8yjxhgd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 831kno65 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.046875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0234375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_180450-831kno65</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/831kno65' target=\"_blank\">dutiful-sweep-26</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/831kno65' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/831kno65</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': '20251214_180459_905', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 16, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 0.5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 0.5, 'lif_layer_v_threshold2': 128, 'init_scaling': [0.03125, 0.046875, 0.0234375], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 0.5, self.v_threshold 16\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 0.5, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=16, v_reset=10000, sg_width=0.5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=0.5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 46.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 41.0\n",
      "lif layer 2 self.abs_max_v: 41.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 51.0\n",
      "fc layer 2 self.abs_max_out: 80.0\n",
      "lif layer 2 self.abs_max_v: 94.5\n",
      "fc layer 1 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 69.0\n",
      "lif layer 2 self.abs_max_v: 107.5\n",
      "fc layer 1 self.abs_max_out: 62.0\n",
      "lif layer 1 self.abs_max_v: 90.0\n",
      "fc layer 2 self.abs_max_out: 82.0\n",
      "lif layer 2 self.abs_max_v: 121.5\n",
      "fc layer 1 self.abs_max_out: 101.0\n",
      "lif layer 1 self.abs_max_v: 138.0\n",
      "fc layer 2 self.abs_max_out: 87.0\n",
      "lif layer 2 self.abs_max_v: 142.5\n",
      "fc layer 1 self.abs_max_out: 147.0\n",
      "lif layer 1 self.abs_max_v: 210.0\n",
      "fc layer 2 self.abs_max_out: 126.0\n",
      "lif layer 2 self.abs_max_v: 194.5\n",
      "fc layer 3 self.abs_max_out: 10.0\n",
      "fc layer 1 self.abs_max_out: 200.0\n",
      "lif layer 1 self.abs_max_v: 270.0\n",
      "fc layer 2 self.abs_max_out: 195.0\n",
      "lif layer 2 self.abs_max_v: 292.5\n",
      "fc layer 3 self.abs_max_out: 16.0\n",
      "fc layer 1 self.abs_max_out: 225.0\n",
      "lif layer 1 self.abs_max_v: 360.0\n",
      "lif layer 2 self.abs_max_v: 326.5\n",
      "fc layer 3 self.abs_max_out: 17.0\n",
      "fc layer 1 self.abs_max_out: 234.0\n",
      "lif layer 1 self.abs_max_v: 381.0\n",
      "lif layer 2 self.abs_max_v: 338.5\n",
      "fc layer 3 self.abs_max_out: 25.0\n",
      "lif layer 2 self.abs_max_v: 345.5\n",
      "fc layer 1 self.abs_max_out: 264.0\n",
      "fc layer 2 self.abs_max_out: 229.0\n",
      "lif layer 2 self.abs_max_v: 348.0\n",
      "lif layer 2 self.abs_max_v: 355.0\n",
      "lif layer 2 self.abs_max_v: 356.0\n",
      "fc layer 3 self.abs_max_out: 26.0\n",
      "lif layer 2 self.abs_max_v: 381.0\n",
      "fc layer 1 self.abs_max_out: 315.0\n",
      "lif layer 1 self.abs_max_v: 504.0\n",
      "fc layer 2 self.abs_max_out: 232.0\n",
      "lif layer 2 self.abs_max_v: 407.5\n",
      "lif layer 1 self.abs_max_v: 514.0\n",
      "fc layer 2 self.abs_max_out: 243.0\n",
      "lif layer 2 self.abs_max_v: 437.0\n",
      "lif layer 1 self.abs_max_v: 522.0\n",
      "fc layer 1 self.abs_max_out: 424.0\n",
      "fc layer 2 self.abs_max_out: 255.0\n",
      "fc layer 3 self.abs_max_out: 35.0\n",
      "lif layer 1 self.abs_max_v: 591.0\n",
      "lif layer 2 self.abs_max_v: 453.5\n",
      "fc layer 3 self.abs_max_out: 37.0\n",
      "lif layer 1 self.abs_max_v: 592.5\n",
      "lif layer 2 self.abs_max_v: 459.0\n",
      "fc layer 2 self.abs_max_out: 262.0\n",
      "fc layer 2 self.abs_max_out: 290.0\n",
      "fc layer 3 self.abs_max_out: 62.0\n",
      "fc layer 2 self.abs_max_out: 313.0\n",
      "fc layer 2 self.abs_max_out: 333.0\n",
      "fc layer 2 self.abs_max_out: 375.0\n",
      "lif layer 1 self.abs_max_v: 693.5\n",
      "lif layer 1 self.abs_max_v: 696.5\n",
      "lif layer 2 self.abs_max_v: 508.5\n",
      "lif layer 2 self.abs_max_v: 512.5\n",
      "lif layer 2 self.abs_max_v: 538.5\n",
      "lif layer 2 self.abs_max_v: 588.0\n",
      "lif layer 2 self.abs_max_v: 623.5\n",
      "fc layer 1 self.abs_max_out: 444.0\n",
      "fc layer 1 self.abs_max_out: 532.0\n",
      "lif layer 1 self.abs_max_v: 834.0\n",
      "fc layer 2 self.abs_max_out: 444.0\n",
      "lif layer 1 self.abs_max_v: 923.0\n",
      "lif layer 2 self.abs_max_v: 624.0\n",
      "lif layer 1 self.abs_max_v: 971.0\n",
      "fc layer 1 self.abs_max_out: 699.0\n",
      "fc layer 1 self.abs_max_out: 755.0\n",
      "fc layer 2 self.abs_max_out: 478.0\n",
      "fc layer 3 self.abs_max_out: 64.0\n",
      "fc layer 3 self.abs_max_out: 65.0\n",
      "lif layer 2 self.abs_max_v: 658.5\n",
      "lif layer 2 self.abs_max_v: 682.0\n",
      "lif layer 2 self.abs_max_v: 685.0\n",
      "fc layer 3 self.abs_max_out: 67.0\n",
      "fc layer 3 self.abs_max_out: 68.0\n",
      "fc layer 3 self.abs_max_out: 77.0\n",
      "lif layer 1 self.abs_max_v: 1018.0\n",
      "fc layer 3 self.abs_max_out: 89.0\n",
      "lif layer 1 self.abs_max_v: 1026.0\n",
      "fc layer 2 self.abs_max_out: 547.0\n",
      "fc layer 3 self.abs_max_out: 90.0\n",
      "fc layer 3 self.abs_max_out: 103.0\n",
      "fc layer 1 self.abs_max_out: 795.0\n",
      "lif layer 1 self.abs_max_v: 1131.0\n",
      "lif layer 1 self.abs_max_v: 1187.0\n",
      "lif layer 2 self.abs_max_v: 714.5\n",
      "fc layer 3 self.abs_max_out: 130.0\n",
      "lif layer 2 self.abs_max_v: 726.5\n",
      "lif layer 2 self.abs_max_v: 728.5\n",
      "lif layer 2 self.abs_max_v: 733.5\n",
      "lif layer 2 self.abs_max_v: 735.0\n",
      "fc layer 2 self.abs_max_out: 657.0\n",
      "lif layer 2 self.abs_max_v: 735.5\n",
      "lif layer 2 self.abs_max_v: 758.0\n",
      "lif layer 2 self.abs_max_v: 764.0\n",
      "lif layer 2 self.abs_max_v: 772.0\n",
      "lif layer 2 self.abs_max_v: 778.5\n",
      "fc layer 2 self.abs_max_out: 729.0\n",
      "lif layer 2 self.abs_max_v: 871.5\n",
      "lif layer 1 self.abs_max_v: 1218.0\n",
      "lif layer 2 self.abs_max_v: 894.0\n",
      "lif layer 2 self.abs_max_v: 894.5\n",
      "lif layer 2 self.abs_max_v: 916.0\n",
      "fc layer 3 self.abs_max_out: 136.0\n",
      "fc layer 3 self.abs_max_out: 164.0\n",
      "lif layer 2 self.abs_max_v: 922.5\n",
      "lif layer 2 self.abs_max_v: 939.0\n",
      "fc layer 1 self.abs_max_out: 806.0\n",
      "fc layer 1 self.abs_max_out: 828.0\n",
      "lif layer 1 self.abs_max_v: 1226.0\n",
      "fc layer 1 self.abs_max_out: 944.0\n",
      "lif layer 1 self.abs_max_v: 1557.0\n",
      "fc layer 1 self.abs_max_out: 1145.0\n",
      "lif layer 1 self.abs_max_v: 1602.5\n",
      "lif layer 2 self.abs_max_v: 941.0\n",
      "lif layer 2 self.abs_max_v: 953.5\n",
      "lif layer 2 self.abs_max_v: 958.0\n",
      "lif layer 2 self.abs_max_v: 969.0\n",
      "lif layer 2 self.abs_max_v: 980.0\n",
      "fc layer 2 self.abs_max_out: 751.0\n",
      "fc layer 2 self.abs_max_out: 754.0\n",
      "fc layer 2 self.abs_max_out: 799.0\n",
      "fc layer 3 self.abs_max_out: 180.0\n",
      "fc layer 3 self.abs_max_out: 193.0\n",
      "fc layer 3 self.abs_max_out: 225.0\n",
      "fc layer 3 self.abs_max_out: 229.0\n",
      "lif layer 2 self.abs_max_v: 988.0\n",
      "lif layer 1 self.abs_max_v: 1625.5\n",
      "lif layer 2 self.abs_max_v: 1021.0\n",
      "fc layer 1 self.abs_max_out: 1148.0\n",
      "fc layer 1 self.abs_max_out: 1287.0\n",
      "fc layer 1 self.abs_max_out: 1321.0\n",
      "lif layer 2 self.abs_max_v: 1028.5\n",
      "lif layer 2 self.abs_max_v: 1052.0\n",
      "lif layer 2 self.abs_max_v: 1073.0\n",
      "lif layer 2 self.abs_max_v: 1080.5\n",
      "fc layer 2 self.abs_max_out: 816.0\n",
      "fc layer 1 self.abs_max_out: 1392.0\n",
      "lif layer 1 self.abs_max_v: 2027.5\n",
      "lif layer 1 self.abs_max_v: 2339.0\n",
      "fc layer 1 self.abs_max_out: 1426.0\n",
      "fc layer 1 self.abs_max_out: 1457.0\n",
      "fc layer 1 self.abs_max_out: 1497.0\n",
      "fc layer 1 self.abs_max_out: 1500.0\n",
      "lif layer 2 self.abs_max_v: 1084.0\n",
      "fc layer 2 self.abs_max_out: 836.0\n",
      "fc layer 2 self.abs_max_out: 851.0\n",
      "lif layer 2 self.abs_max_v: 1117.0\n",
      "fc layer 1 self.abs_max_out: 1502.0\n",
      "fc layer 1 self.abs_max_out: 1527.0\n",
      "fc layer 1 self.abs_max_out: 1569.0\n",
      "lif layer 2 self.abs_max_v: 1135.0\n",
      "lif layer 2 self.abs_max_v: 1193.5\n",
      "lif layer 2 self.abs_max_v: 1227.5\n",
      "lif layer 2 self.abs_max_v: 1232.5\n",
      "lif layer 2 self.abs_max_v: 1242.5\n",
      "fc layer 1 self.abs_max_out: 1788.0\n",
      "fc layer 1 self.abs_max_out: 1851.0\n",
      "fc layer 1 self.abs_max_out: 1903.0\n",
      "fc layer 1 self.abs_max_out: 1932.0\n",
      "fc layer 2 self.abs_max_out: 928.0\n",
      "fc layer 2 self.abs_max_out: 960.0\n",
      "fc layer 3 self.abs_max_out: 230.0\n",
      "fc layer 1 self.abs_max_out: 1997.0\n",
      "fc layer 1 self.abs_max_out: 2134.0\n",
      "lif layer 2 self.abs_max_v: 1282.5\n",
      "lif layer 2 self.abs_max_v: 1331.0\n",
      "fc layer 1 self.abs_max_out: 2360.0\n",
      "lif layer 1 self.abs_max_v: 2360.0\n",
      "fc layer 1 self.abs_max_out: 2401.0\n",
      "lif layer 1 self.abs_max_v: 2401.0\n",
      "fc layer 1 self.abs_max_out: 2418.0\n",
      "lif layer 1 self.abs_max_v: 2418.0\n",
      "lif layer 1 self.abs_max_v: 2429.5\n",
      "lif layer 2 self.abs_max_v: 1338.5\n",
      "fc layer 2 self.abs_max_out: 996.0\n",
      "lif layer 2 self.abs_max_v: 1359.0\n",
      "lif layer 2 self.abs_max_v: 1409.5\n",
      "lif layer 1 self.abs_max_v: 2735.5\n",
      "lif layer 1 self.abs_max_v: 2838.5\n",
      "lif layer 2 self.abs_max_v: 1412.0\n",
      "lif layer 2 self.abs_max_v: 1438.0\n",
      "lif layer 2 self.abs_max_v: 1452.5\n",
      "fc layer 3 self.abs_max_out: 232.0\n",
      "lif layer 2 self.abs_max_v: 1612.0\n",
      "lif layer 1 self.abs_max_v: 2892.5\n",
      "fc layer 2 self.abs_max_out: 1105.0\n",
      "lif layer 2 self.abs_max_v: 1626.0\n",
      "lif layer 2 self.abs_max_v: 1745.0\n",
      "fc layer 1 self.abs_max_out: 2471.0\n",
      "fc layer 1 self.abs_max_out: 2517.0\n",
      "fc layer 3 self.abs_max_out: 249.0\n",
      "fc layer 3 self.abs_max_out: 251.0\n",
      "fc layer 2 self.abs_max_out: 1118.0\n",
      "fc layer 2 self.abs_max_out: 1138.0\n",
      "fc layer 2 self.abs_max_out: 1166.0\n",
      "lif layer 1 self.abs_max_v: 2949.5\n",
      "fc layer 2 self.abs_max_out: 1170.0\n",
      "fc layer 1 self.abs_max_out: 2525.0\n",
      "fc layer 1 self.abs_max_out: 2631.0\n",
      "fc layer 1 self.abs_max_out: 2686.0\n",
      "fc layer 1 self.abs_max_out: 2708.0\n",
      "fc layer 1 self.abs_max_out: 2774.0\n",
      "fc layer 3 self.abs_max_out: 261.0\n",
      "fc layer 2 self.abs_max_out: 1187.0\n",
      "fc layer 3 self.abs_max_out: 274.0\n",
      "fc layer 2 self.abs_max_out: 1199.0\n",
      "fc layer 2 self.abs_max_out: 1230.0\n",
      "fc layer 2 self.abs_max_out: 1276.0\n",
      "fc layer 2 self.abs_max_out: 1414.0\n",
      "lif layer 1 self.abs_max_v: 3472.5\n",
      "fc layer 1 self.abs_max_out: 3281.0\n",
      "lif layer 1 self.abs_max_v: 4061.5\n",
      "lif layer 1 self.abs_max_v: 4286.0\n",
      "lif layer 1 self.abs_max_v: 4354.0\n",
      "lif layer 2 self.abs_max_v: 1788.0\n",
      "lif layer 2 self.abs_max_v: 1879.0\n",
      "lif layer 2 self.abs_max_v: 1924.5\n",
      "fc layer 2 self.abs_max_out: 1565.0\n",
      "fc layer 3 self.abs_max_out: 302.0\n",
      "fc layer 2 self.abs_max_out: 1570.0\n",
      "lif layer 2 self.abs_max_v: 1926.0\n",
      "lif layer 2 self.abs_max_v: 1930.0\n",
      "lif layer 2 self.abs_max_v: 1942.5\n",
      "lif layer 2 self.abs_max_v: 2028.5\n",
      "lif layer 2 self.abs_max_v: 2071.5\n",
      "lif layer 2 self.abs_max_v: 2093.0\n",
      "lif layer 2 self.abs_max_v: 2103.5\n",
      "lif layer 1 self.abs_max_v: 4361.5\n",
      "lif layer 1 self.abs_max_v: 4377.0\n",
      "lif layer 1 self.abs_max_v: 4448.5\n",
      "lif layer 1 self.abs_max_v: 4483.0\n",
      "lif layer 1 self.abs_max_v: 4568.5\n",
      "lif layer 1 self.abs_max_v: 4830.5\n",
      "fc layer 3 self.abs_max_out: 316.0\n",
      "fc layer 3 self.abs_max_out: 322.0\n",
      "lif layer 2 self.abs_max_v: 2156.5\n",
      "lif layer 1 self.abs_max_v: 4917.0\n",
      "fc layer 1 self.abs_max_out: 3624.0\n",
      "lif layer 1 self.abs_max_v: 5075.5\n",
      "lif layer 1 self.abs_max_v: 5279.0\n",
      "lif layer 2 self.abs_max_v: 2193.0\n",
      "lif layer 2 self.abs_max_v: 2375.0\n",
      "fc layer 2 self.abs_max_out: 1674.0\n",
      "fc layer 1 self.abs_max_out: 3713.0\n",
      "fc layer 1 self.abs_max_out: 3923.0\n",
      "lif layer 2 self.abs_max_v: 2529.5\n",
      "lif layer 2 self.abs_max_v: 2652.0\n",
      "lif layer 2 self.abs_max_v: 2713.0\n",
      "lif layer 2 self.abs_max_v: 2743.5\n",
      "lif layer 2 self.abs_max_v: 2751.5\n",
      "lif layer 1 self.abs_max_v: 5350.0\n",
      "fc layer 2 self.abs_max_out: 1818.0\n",
      "lif layer 1 self.abs_max_v: 5398.0\n",
      "fc layer 3 self.abs_max_out: 337.0\n",
      "fc layer 1 self.abs_max_out: 4035.0\n",
      "lif layer 1 self.abs_max_v: 5418.0\n",
      "lif layer 1 self.abs_max_v: 5896.0\n",
      "lif layer 1 self.abs_max_v: 6460.0\n",
      "fc layer 1 self.abs_max_out: 4133.0\n",
      "lif layer 1 self.abs_max_v: 6643.5\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss:  9.948283/ 84.247757, val:  26.25%, val_best:  26.25%, tr:  98.37%, tr_best:  98.37%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.5670%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.4950%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 1539  15.720%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 366.0\n",
      "fc layer 2 self.abs_max_out: 1908.0\n",
      "fc layer 2 self.abs_max_out: 2050.0\n",
      "lif layer 1 self.abs_max_v: 6753.5\n",
      "lif layer 1 self.abs_max_v: 6911.5\n",
      "lif layer 1 self.abs_max_v: 7284.5\n",
      "lif layer 1 self.abs_max_v: 7423.0\n",
      "fc layer 1 self.abs_max_out: 4259.0\n",
      "fc layer 1 self.abs_max_out: 4452.0\n",
      "fc layer 2 self.abs_max_out: 2107.0\n",
      "fc layer 2 self.abs_max_out: 2165.0\n",
      "fc layer 1 self.abs_max_out: 4461.0\n",
      "fc layer 2 self.abs_max_out: 2196.0\n",
      "fc layer 2 self.abs_max_out: 2223.0\n",
      "lif layer 2 self.abs_max_v: 2904.5\n",
      "fc layer 2 self.abs_max_out: 2411.0\n",
      "lif layer 2 self.abs_max_v: 2923.0\n",
      "lif layer 2 self.abs_max_v: 3118.5\n",
      "lif layer 2 self.abs_max_v: 3216.5\n",
      "fc layer 1 self.abs_max_out: 4845.0\n",
      "fc layer 1 self.abs_max_out: 5009.0\n",
      "fc layer 2 self.abs_max_out: 2461.0\n",
      "fc layer 2 self.abs_max_out: 2504.0\n",
      "fc layer 2 self.abs_max_out: 2536.0\n",
      "fc layer 2 self.abs_max_out: 2576.0\n",
      "lif layer 2 self.abs_max_v: 3248.5\n",
      "lif layer 2 self.abs_max_v: 3282.5\n",
      "lif layer 2 self.abs_max_v: 3299.5\n",
      "lif layer 2 self.abs_max_v: 3308.0\n",
      "lif layer 2 self.abs_max_v: 3312.0\n",
      "fc layer 2 self.abs_max_out: 2606.0\n",
      "fc layer 1 self.abs_max_out: 5318.0\n",
      "fc layer 2 self.abs_max_out: 2708.0\n",
      "fc layer 1 self.abs_max_out: 5710.0\n",
      "lif layer 1 self.abs_max_v: 7527.5\n",
      "lif layer 1 self.abs_max_v: 7809.5\n",
      "lif layer 1 self.abs_max_v: 8207.0\n",
      "fc layer 3 self.abs_max_out: 370.0\n",
      "fc layer 2 self.abs_max_out: 2743.0\n",
      "fc layer 2 self.abs_max_out: 2755.0\n",
      "fc layer 2 self.abs_max_out: 2845.0\n",
      "fc layer 2 self.abs_max_out: 2958.0\n",
      "fc layer 3 self.abs_max_out: 380.0\n",
      "fc layer 2 self.abs_max_out: 2966.0\n",
      "lif layer 2 self.abs_max_v: 3583.0\n",
      "lif layer 2 self.abs_max_v: 3719.5\n",
      "lif layer 2 self.abs_max_v: 3788.0\n",
      "fc layer 2 self.abs_max_out: 3104.0\n",
      "fc layer 3 self.abs_max_out: 412.0\n",
      "fc layer 1 self.abs_max_out: 5772.0\n",
      "fc layer 2 self.abs_max_out: 3121.0\n",
      "lif layer 1 self.abs_max_v: 8557.5\n",
      "lif layer 1 self.abs_max_v: 8573.5\n",
      "fc layer 1 self.abs_max_out: 5974.0\n",
      "lif layer 1 self.abs_max_v: 9283.0\n",
      "lif layer 1 self.abs_max_v: 10014.5\n",
      "lif layer 2 self.abs_max_v: 3805.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 12.691770/ 94.726105, val:  27.50%, val_best:  27.50%, tr:  98.57%, tr_best:  98.57%, epoch time: 73.71 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.7223%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.8308%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3100  15.832%\n",
      "fc layer 1 self.abs_max_out: 6287.0\n",
      "lif layer 1 self.abs_max_v: 10652.0\n",
      "lif layer 2 self.abs_max_v: 3850.5\n",
      "lif layer 2 self.abs_max_v: 4035.0\n",
      "lif layer 2 self.abs_max_v: 4128.5\n",
      "lif layer 2 self.abs_max_v: 4175.5\n",
      "fc layer 2 self.abs_max_out: 3168.0\n",
      "fc layer 2 self.abs_max_out: 3237.0\n",
      "fc layer 2 self.abs_max_out: 3317.0\n",
      "lif layer 2 self.abs_max_v: 4307.0\n",
      "fc layer 2 self.abs_max_out: 3357.0\n",
      "fc layer 2 self.abs_max_out: 3366.0\n",
      "lif layer 2 self.abs_max_v: 4352.5\n",
      "fc layer 2 self.abs_max_out: 3440.0\n",
      "lif layer 2 self.abs_max_v: 4398.5\n",
      "lif layer 2 self.abs_max_v: 4695.0\n",
      "fc layer 2 self.abs_max_out: 3487.0\n",
      "lif layer 2 self.abs_max_v: 4852.5\n",
      "fc layer 2 self.abs_max_out: 3534.0\n",
      "fc layer 2 self.abs_max_out: 3644.0\n",
      "fc layer 2 self.abs_max_out: 3745.0\n",
      "fc layer 1 self.abs_max_out: 6535.0\n",
      "lif layer 1 self.abs_max_v: 11063.5\n",
      "lif layer 1 self.abs_max_v: 11610.0\n",
      "fc layer 1 self.abs_max_out: 7035.0\n",
      "fc layer 2 self.abs_max_out: 3911.0\n",
      "fc layer 2 self.abs_max_out: 3912.0\n",
      "fc layer 2 self.abs_max_out: 3983.0\n",
      "fc layer 2 self.abs_max_out: 4090.0\n",
      "lif layer 2 self.abs_max_v: 4927.0\n",
      "lif layer 2 self.abs_max_v: 4976.5\n",
      "lif layer 2 self.abs_max_v: 5053.0\n",
      "lif layer 2 self.abs_max_v: 5072.5\n",
      "lif layer 2 self.abs_max_v: 5075.0\n",
      "lif layer 2 self.abs_max_v: 5082.0\n",
      "fc layer 1 self.abs_max_out: 7167.0\n",
      "lif layer 1 self.abs_max_v: 11672.5\n",
      "lif layer 1 self.abs_max_v: 12610.5\n",
      "lif layer 1 self.abs_max_v: 12963.5\n",
      "fc layer 1 self.abs_max_out: 7188.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 13.099066/ 82.003754, val:  29.17%, val_best:  29.17%, tr:  98.67%, tr_best:  98.67%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.1182%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.5142%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 4716  16.057%\n",
      "lif layer 2 self.abs_max_v: 5136.5\n",
      "lif layer 2 self.abs_max_v: 5217.5\n",
      "lif layer 2 self.abs_max_v: 5251.5\n",
      "lif layer 1 self.abs_max_v: 13319.0\n",
      "fc layer 1 self.abs_max_out: 7482.0\n",
      "lif layer 2 self.abs_max_v: 5353.0\n",
      "lif layer 2 self.abs_max_v: 5425.5\n",
      "lif layer 2 self.abs_max_v: 5439.0\n",
      "lif layer 2 self.abs_max_v: 5552.5\n",
      "lif layer 2 self.abs_max_v: 5703.5\n",
      "lif layer 2 self.abs_max_v: 5794.0\n",
      "lif layer 2 self.abs_max_v: 5897.0\n",
      "lif layer 2 self.abs_max_v: 5948.5\n",
      "lif layer 2 self.abs_max_v: 5974.5\n",
      "lif layer 2 self.abs_max_v: 5987.5\n",
      "lif layer 2 self.abs_max_v: 5994.0\n",
      "lif layer 2 self.abs_max_v: 6137.5\n",
      "lif layer 2 self.abs_max_v: 6382.0\n",
      "lif layer 2 self.abs_max_v: 6504.0\n",
      "lif layer 2 self.abs_max_v: 6565.0\n",
      "fc layer 1 self.abs_max_out: 7601.0\n",
      "fc layer 1 self.abs_max_out: 8001.0\n",
      "lif layer 1 self.abs_max_v: 13454.5\n",
      "lif layer 1 self.abs_max_v: 14647.5\n",
      "fc layer 1 self.abs_max_out: 8199.0\n",
      "fc layer 1 self.abs_max_out: 8580.0\n",
      "lif layer 1 self.abs_max_v: 15578.5\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss: 12.124243/ 78.418549, val:  22.50%, val_best:  29.17%, tr:  98.37%, tr_best:  98.67%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.2655%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1763%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6349  16.213%\n",
      "fc layer 1 self.abs_max_out: 8596.0\n",
      "lif layer 2 self.abs_max_v: 6579.5\n",
      "lif layer 2 self.abs_max_v: 6644.0\n",
      "lif layer 2 self.abs_max_v: 6676.0\n",
      "lif layer 2 self.abs_max_v: 6772.5\n",
      "lif layer 2 self.abs_max_v: 6826.5\n",
      "fc layer 1 self.abs_max_out: 8723.0\n",
      "lif layer 1 self.abs_max_v: 15882.0\n",
      "fc layer 1 self.abs_max_out: 9102.0\n",
      "lif layer 1 self.abs_max_v: 16319.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss: 11.525258/ 67.272202, val:  34.58%, val_best:  34.58%, tr:  98.26%, tr_best:  98.67%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2943%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3368%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 7938  16.217%\n",
      "fc layer 2 self.abs_max_out: 4173.0\n",
      "lif layer 2 self.abs_max_v: 6854.5\n",
      "fc layer 2 self.abs_max_out: 4244.0\n",
      "fc layer 1 self.abs_max_out: 9951.0\n",
      "lif layer 1 self.abs_max_v: 17344.0\n",
      "lif layer 2 self.abs_max_v: 6924.0\n",
      "lif layer 2 self.abs_max_v: 7070.5\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss: 11.038365/ 51.975327, val:  32.92%, val_best:  34.58%, tr:  97.34%, tr_best:  98.67%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0213%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6689%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9424  16.044%\n",
      "lif layer 2 self.abs_max_v: 7218.0\n",
      "lif layer 2 self.abs_max_v: 7262.0\n",
      "lif layer 2 self.abs_max_v: 7357.0\n",
      "lif layer 1 self.abs_max_v: 17628.5\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss: 10.616577/ 69.367645, val:  37.92%, val_best:  37.92%, tr:  97.75%, tr_best:  98.67%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1661%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1530%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10918  15.932%\n",
      "lif layer 2 self.abs_max_v: 7420.5\n",
      "lif layer 2 self.abs_max_v: 7479.5\n",
      "lif layer 2 self.abs_max_v: 7509.0\n",
      "fc layer 2 self.abs_max_out: 4317.0\n",
      "fc layer 2 self.abs_max_out: 4322.0\n",
      "fc layer 2 self.abs_max_out: 4622.0\n",
      "lif layer 2 self.abs_max_v: 7741.5\n",
      "fc layer 1 self.abs_max_out: 10156.0\n",
      "lif layer 1 self.abs_max_v: 18525.5\n",
      "fc layer 1 self.abs_max_out: 10237.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss: 10.197514/ 89.594650, val:  29.58%, val_best:  37.92%, tr:  97.85%, tr_best:  98.67%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2108%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6811%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 12392  15.822%\n",
      "lif layer 2 self.abs_max_v: 7765.0\n",
      "lif layer 2 self.abs_max_v: 7937.0\n",
      "lif layer 2 self.abs_max_v: 8013.5\n",
      "lif layer 2 self.abs_max_v: 8092.0\n",
      "fc layer 1 self.abs_max_out: 10626.0\n",
      "lif layer 1 self.abs_max_v: 19538.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss: 11.109296/ 64.259903, val:  31.25%, val_best:  37.92%, tr:  98.67%, tr_best:  98.67%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9874%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8385%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 13992  15.880%\n",
      "lif layer 2 self.abs_max_v: 8143.5\n",
      "fc layer 2 self.abs_max_out: 5279.0\n",
      "lif layer 2 self.abs_max_v: 9238.5\n",
      "fc layer 1 self.abs_max_out: 11166.0\n",
      "fc layer 1 self.abs_max_out: 11168.0\n",
      "lif layer 1 self.abs_max_v: 20474.0\n",
      "fc layer 1 self.abs_max_out: 11240.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss: 10.513868/ 52.177135, val:  40.83%, val_best:  40.83%, tr:  98.57%, tr_best:  98.67%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6863%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1528%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 15525  15.858%\n",
      "lif layer 2 self.abs_max_v: 9281.5\n",
      "fc layer 1 self.abs_max_out: 11530.0\n",
      "lif layer 1 self.abs_max_v: 21189.5\n",
      "fc layer 1 self.abs_max_out: 11841.0\n",
      "lif layer 2 self.abs_max_v: 9287.5\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss: 10.619453/ 63.689079, val:  35.00%, val_best:  40.83%, tr:  97.04%, tr_best:  98.67%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0899%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2475%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 17084  15.864%\n",
      "lif layer 2 self.abs_max_v: 9302.5\n",
      "lif layer 2 self.abs_max_v: 9513.5\n",
      "lif layer 2 self.abs_max_v: 9538.5\n",
      "lif layer 2 self.abs_max_v: 9623.5\n",
      "lif layer 2 self.abs_max_v: 9714.0\n",
      "lif layer 2 self.abs_max_v: 9874.0\n",
      "lif layer 2 self.abs_max_v: 9961.0\n",
      "fc layer 1 self.abs_max_out: 11932.0\n",
      "lif layer 2 self.abs_max_v: 9994.5\n",
      "fc layer 2 self.abs_max_out: 5283.0\n",
      "fc layer 2 self.abs_max_out: 5423.0\n",
      "lif layer 2 self.abs_max_v: 10105.5\n",
      "lif layer 2 self.abs_max_v: 10120.5\n",
      "lif layer 2 self.abs_max_v: 10203.5\n",
      "lif layer 2 self.abs_max_v: 10245.0\n",
      "fc layer 2 self.abs_max_out: 5477.0\n",
      "fc layer 2 self.abs_max_out: 5752.0\n",
      "fc layer 2 self.abs_max_out: 6048.0\n",
      "lif layer 2 self.abs_max_v: 10276.0\n",
      "lif layer 1 self.abs_max_v: 21725.0\n",
      "fc layer 1 self.abs_max_out: 11976.0\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss: 10.404176/ 87.623222, val:  22.92%, val_best:  40.83%, tr:  97.85%, tr_best:  98.67%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0289%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1892%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 18614  15.844%\n",
      "lif layer 2 self.abs_max_v: 10283.0\n",
      "lif layer 2 self.abs_max_v: 10309.5\n",
      "lif layer 2 self.abs_max_v: 10323.0\n",
      "fc layer 1 self.abs_max_out: 12081.0\n",
      "lif layer 1 self.abs_max_v: 22152.5\n",
      "fc layer 1 self.abs_max_out: 12818.0\n",
      "lif layer 1 self.abs_max_v: 22470.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss: 10.068796/ 57.764011, val:  32.50%, val_best:  40.83%, tr:  98.67%, tr_best:  98.67%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4214%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9209%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 20137  15.822%\n",
      "lif layer 2 self.abs_max_v: 10434.5\n",
      "lif layer 2 self.abs_max_v: 10542.0\n",
      "lif layer 2 self.abs_max_v: 10664.0\n",
      "lif layer 2 self.abs_max_v: 10725.0\n",
      "lif layer 2 self.abs_max_v: 10755.5\n",
      "lif layer 2 self.abs_max_v: 10771.0\n",
      "lif layer 1 self.abs_max_v: 22696.5\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss: 10.160027/ 84.559380, val:  20.00%, val_best:  40.83%, tr:  98.16%, tr_best:  98.67%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6410%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5011%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 21685  15.822%\n",
      "lif layer 2 self.abs_max_v: 10866.5\n",
      "lif layer 2 self.abs_max_v: 10994.5\n",
      "lif layer 2 self.abs_max_v: 11058.5\n",
      "lif layer 2 self.abs_max_v: 11090.5\n",
      "lif layer 2 self.abs_max_v: 11106.5\n",
      "lif layer 2 self.abs_max_v: 11111.5\n",
      "fc layer 2 self.abs_max_out: 6262.0\n",
      "fc layer 1 self.abs_max_out: 13098.0\n",
      "lif layer 1 self.abs_max_v: 22820.5\n",
      "lif layer 2 self.abs_max_v: 11220.0\n",
      "lif layer 2 self.abs_max_v: 11449.0\n",
      "lif layer 2 self.abs_max_v: 11563.5\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  9.803204/ 93.091125, val:  15.83%, val_best:  40.83%, tr:  98.47%, tr_best:  98.67%, epoch time: 75.76 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9183%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0769%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 23191  15.792%\n",
      "lif layer 2 self.abs_max_v: 11729.5\n",
      "lif layer 2 self.abs_max_v: 11866.0\n",
      "lif layer 2 self.abs_max_v: 11934.0\n",
      "lif layer 2 self.abs_max_v: 11968.0\n",
      "lif layer 2 self.abs_max_v: 11985.0\n",
      "fc layer 2 self.abs_max_out: 7418.0\n",
      "fc layer 1 self.abs_max_out: 13177.0\n",
      "lif layer 1 self.abs_max_v: 22918.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  9.798949/ 43.785301, val:  40.42%, val_best:  40.83%, tr:  97.85%, tr_best:  98.67%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0740%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0411%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 24750  15.801%\n",
      "lif layer 2 self.abs_max_v: 12060.5\n",
      "lif layer 2 self.abs_max_v: 12191.5\n",
      "lif layer 2 self.abs_max_v: 12257.0\n",
      "lif layer 2 self.abs_max_v: 12380.0\n",
      "lif layer 2 self.abs_max_v: 12593.0\n",
      "lif layer 2 self.abs_max_v: 12699.5\n",
      "lif layer 2 self.abs_max_v: 12753.0\n",
      "fc layer 2 self.abs_max_out: 7801.0\n",
      "lif layer 1 self.abs_max_v: 23498.0\n",
      "fc layer 1 self.abs_max_out: 13425.0\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  9.532226/ 49.532452, val:  34.58%, val_best:  40.83%, tr:  98.06%, tr_best:  98.67%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7672%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9333%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 26260  15.778%\n",
      "lif layer 2 self.abs_max_v: 12838.0\n",
      "lif layer 2 self.abs_max_v: 12920.0\n",
      "lif layer 2 self.abs_max_v: 13077.0\n",
      "lif layer 2 self.abs_max_v: 13163.5\n",
      "lif layer 2 self.abs_max_v: 13207.0\n",
      "lif layer 2 self.abs_max_v: 13287.5\n",
      "lif layer 1 self.abs_max_v: 24071.0\n",
      "fc layer 1 self.abs_max_out: 13632.0\n",
      "lif layer 2 self.abs_max_v: 13412.5\n",
      "lif layer 2 self.abs_max_v: 13579.5\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  9.880565/ 50.266006, val:  41.67%, val_best:  41.67%, tr:  98.16%, tr_best:  98.67%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9640%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5653%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 27792  15.771%\n",
      "fc layer 2 self.abs_max_out: 8122.0\n",
      "lif layer 2 self.abs_max_v: 13609.0\n",
      "lif layer 2 self.abs_max_v: 13709.5\n",
      "lif layer 2 self.abs_max_v: 13760.0\n",
      "fc layer 1 self.abs_max_out: 14028.0\n",
      "lif layer 1 self.abs_max_v: 24668.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  9.773576/ 42.346195, val:  41.25%, val_best:  41.67%, tr:  98.67%, tr_best:  98.67%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9501%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5098%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 29322  15.764%\n",
      "fc layer 1 self.abs_max_out: 14249.0\n",
      "lif layer 1 self.abs_max_v: 25218.0\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  8.998860/ 73.515137, val:  32.50%, val_best:  41.67%, tr:  98.57%, tr_best:  98.67%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2775%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3369%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 30762  15.711%\n",
      "fc layer 2 self.abs_max_out: 8647.0\n",
      "fc layer 1 self.abs_max_out: 14279.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  9.313806/ 64.153244, val:  35.83%, val_best:  41.67%, tr:  97.75%, tr_best:  98.67%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9165%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4404%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 32238  15.681%\n",
      "fc layer 1 self.abs_max_out: 14579.0\n",
      "lif layer 1 self.abs_max_v: 25764.5\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  9.798827/ 62.040184, val:  22.08%, val_best:  41.67%, tr:  97.24%, tr_best:  98.67%, epoch time: 75.70 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8861%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3746%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 33771  15.680%\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  9.776359/ 47.147377, val:  43.33%, val_best:  43.33%, tr:  97.55%, tr_best:  98.67%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7844%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4510%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 35307  15.680%\n",
      "lif layer 2 self.abs_max_v: 13774.5\n",
      "lif layer 2 self.abs_max_v: 13837.5\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  9.582169/ 45.974331, val:  45.42%, val_best:  45.42%, tr:  97.96%, tr_best:  98.67%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1937%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7812%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 36858  15.687%\n",
      "lif layer 2 self.abs_max_v: 13846.0\n",
      "lif layer 2 self.abs_max_v: 13953.0\n",
      "lif layer 2 self.abs_max_v: 14006.5\n",
      "fc layer 2 self.abs_max_out: 9017.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  9.626369/ 65.124771, val:  39.17%, val_best:  45.42%, tr:  98.06%, tr_best:  98.67%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3767%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7621%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 38398  15.689%\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  9.898371/ 40.385803, val:  46.25%, val_best:  46.25%, tr:  97.85%, tr_best:  98.67%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7927%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6988%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 39987  15.710%\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  9.358539/ 49.722687, val:  42.92%, val_best:  46.25%, tr:  98.47%, tr_best:  98.67%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3627%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1706%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 41467  15.688%\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss: 10.058658/ 53.133152, val:  41.25%, val_best:  46.25%, tr:  98.47%, tr_best:  98.67%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6526%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6098%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 42982  15.680%\n",
      "fc layer 1 self.abs_max_out: 15072.0\n",
      "lif layer 1 self.abs_max_v: 26696.5\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  9.587718/ 56.787312, val:  42.50%, val_best:  46.25%, tr:  98.57%, tr_best:  98.67%, epoch time: 75.45 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8225%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1364%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 44445  15.655%\n",
      "lif layer 2 self.abs_max_v: 14123.0\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  9.756484/102.866455, val:  32.08%, val_best:  46.25%, tr:  98.26%, tr_best:  98.67%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5961%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3368%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 45884  15.623%\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  9.970169/ 49.125927, val:  31.67%, val_best:  46.25%, tr:  98.77%, tr_best:  98.77%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9181%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1216%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 47389  15.615%\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  9.934389/ 63.016991, val:  39.58%, val_best:  46.25%, tr:  98.06%, tr_best:  98.77%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6227%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9128%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 48896  15.608%\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss: 10.445348/ 83.955833, val:  34.17%, val_best:  46.25%, tr:  98.57%, tr_best:  98.77%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7763%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3364%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 50441  15.613%\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss: 10.391955/ 58.245255, val:  38.75%, val_best:  46.25%, tr:  98.77%, tr_best:  98.77%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2120%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8308%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 51954  15.608%\n",
      "fc layer 1 self.abs_max_out: 15583.0\n",
      "lif layer 1 self.abs_max_v: 27644.0\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss: 10.039891/ 58.364063, val:  40.00%, val_best:  46.25%, tr:  98.47%, tr_best:  98.77%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1023%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4792%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 53502  15.614%\n",
      "lif layer 2 self.abs_max_v: 14930.0\n",
      "lif layer 2 self.abs_max_v: 15314.0\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  9.987070/ 65.898590, val:  39.17%, val_best:  46.25%, tr:  98.98%, tr_best:  98.98%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1155%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8898%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 55060  15.623%\n",
      "fc layer 3 self.abs_max_out: 422.0\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss: 10.257764/ 57.117744, val:  48.33%, val_best:  48.33%, tr:  98.88%, tr_best:  98.98%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2968%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9847%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 56624  15.632%\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  9.930306/ 66.839470, val:  37.92%, val_best:  48.33%, tr:  99.08%, tr_best:  99.08%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9810%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0685%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 58114  15.621%\n",
      "lif layer 2 self.abs_max_v: 15393.5\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss: 10.728839/ 58.282677, val:  38.33%, val_best:  48.33%, tr:  98.77%, tr_best:  99.08%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9623%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4181%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 59665  15.627%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss: 10.358134/ 58.308754, val:  39.17%, val_best:  48.33%, tr:  98.37%, tr_best:  99.08%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8459%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6057%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 61202  15.629%\n",
      "lif layer 2 self.abs_max_v: 15576.0\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss: 10.301828/ 50.192738, val:  43.75%, val_best:  48.33%, tr:  98.67%, tr_best:  99.08%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8967%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8818%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 62752  15.634%\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss: 10.379766/ 69.350754, val:  29.58%, val_best:  48.33%, tr:  98.77%, tr_best:  99.08%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7579%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3853%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 64308  15.640%\n",
      "lif layer 2 self.abs_max_v: 16065.0\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss: 10.262651/ 44.923264, val:  42.08%, val_best:  48.33%, tr:  98.77%, tr_best:  99.08%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6751%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4777%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 65834  15.639%\n",
      "lif layer 2 self.abs_max_v: 16229.5\n",
      "lif layer 2 self.abs_max_v: 16868.5\n",
      "fc layer 3 self.abs_max_out: 432.0\n",
      "fc layer 3 self.abs_max_out: 458.0\n",
      "fc layer 1 self.abs_max_out: 15836.0\n",
      "lif layer 1 self.abs_max_v: 28142.5\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss: 10.631557/ 72.205719, val:  37.08%, val_best:  48.33%, tr:  98.57%, tr_best:  99.08%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8049%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5611%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 67385  15.643%\n",
      "fc layer 3 self.abs_max_out: 508.0\n",
      "fc layer 3 self.abs_max_out: 519.0\n",
      "fc layer 1 self.abs_max_out: 16034.0\n",
      "lif layer 1 self.abs_max_v: 28513.5\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss: 11.037960/ 84.245361, val:  25.83%, val_best:  48.33%, tr:  99.18%, tr_best:  99.18%, epoch time: 75.93 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8886%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0503%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 68944  15.650%\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss: 10.768578/ 74.155037, val:  34.58%, val_best:  48.33%, tr:  99.08%, tr_best:  99.18%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6746%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4155%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 70458  15.646%\n",
      "fc layer 1 self.abs_max_out: 16355.0\n",
      "lif layer 1 self.abs_max_v: 29174.5\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss: 10.508565/ 72.775642, val:  26.25%, val_best:  48.33%, tr:  98.77%, tr_best:  99.18%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9335%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1999%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 71946  15.636%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss: 10.519740/ 78.777596, val:  28.33%, val_best:  48.33%, tr:  98.26%, tr_best:  99.18%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3293%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9194%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 73469  15.634%\n",
      "fc layer 1 self.abs_max_out: 16382.0\n",
      "lif layer 1 self.abs_max_v: 29331.0\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss: 10.847753/ 53.618080, val:  39.17%, val_best:  48.33%, tr:  98.57%, tr_best:  99.18%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7983%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1938%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 75022  15.639%\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss: 10.743538/ 59.136395, val:  35.42%, val_best:  48.33%, tr:  98.47%, tr_best:  99.18%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1429%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2150%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 76589  15.646%\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss: 10.720232/ 72.550156, val:  39.58%, val_best:  48.33%, tr:  98.88%, tr_best:  99.18%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0484%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7661%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 78136  15.649%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss: 10.301829/ 52.015717, val:  40.83%, val_best:  48.33%, tr:  98.57%, tr_best:  99.18%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2785%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0515%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 79643  15.644%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss: 11.069118/ 61.680706, val:  41.67%, val_best:  48.33%, tr:  99.28%, tr_best:  99.28%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3685%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4013%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 81192  15.648%\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss: 11.158908/ 87.691628, val:  26.25%, val_best:  48.33%, tr:  98.67%, tr_best:  99.28%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7541%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1176%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 82724  15.648%\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss: 11.560731/ 53.654968, val:  47.08%, val_best:  48.33%, tr:  98.57%, tr_best:  99.28%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6969%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0981%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 84239  15.645%\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss: 11.546338/ 63.298626, val:  43.75%, val_best:  48.33%, tr:  98.57%, tr_best:  99.28%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5149%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8599%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 85766  15.644%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss: 11.327365/ 92.874359, val:  20.00%, val_best:  48.33%, tr:  98.88%, tr_best:  99.28%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3849%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1620%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 87230  15.632%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss: 10.577043/ 57.086090, val:  33.75%, val_best:  48.33%, tr:  98.47%, tr_best:  99.28%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8108%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0772%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 88708  15.623%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss: 10.380605/ 57.951962, val:  28.75%, val_best:  48.33%, tr:  98.16%, tr_best:  99.28%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4342%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8918%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 90152  15.608%\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss: 10.603226/ 85.000572, val:  27.50%, val_best:  48.33%, tr:  98.88%, tr_best:  99.28%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1109%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0789%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 91659  15.604%\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss: 11.098606/ 78.059296, val:  31.67%, val_best:  48.33%, tr:  98.67%, tr_best:  99.28%, epoch time: 75.76 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1472%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2685%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 93206  15.607%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss: 10.395761/ 76.981140, val:  32.50%, val_best:  48.33%, tr:  98.98%, tr_best:  99.28%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4763%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2934%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 94713  15.604%\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss: 10.728790/ 71.985367, val:  37.08%, val_best:  48.33%, tr:  98.57%, tr_best:  99.28%, epoch time: 76.01 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2405%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7078%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 96201  15.598%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss: 10.083113/ 69.622513, val:  42.08%, val_best:  48.33%, tr:  98.47%, tr_best:  99.28%, epoch time: 76.22 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6244%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9784%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 97652  15.585%\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss: 10.481062/ 63.048370, val:  45.42%, val_best:  48.33%, tr:  98.37%, tr_best:  99.28%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9189%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2877%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 99184  15.586%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss: 10.646552/ 55.356720, val:  42.50%, val_best:  48.33%, tr:  98.37%, tr_best:  99.28%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7457%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0341%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 100673  15.581%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss: 10.759144/ 73.944847, val:  30.00%, val_best:  48.33%, tr:  98.16%, tr_best:  99.28%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6997%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1457%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 102191  15.580%\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss: 10.668078/ 74.207741, val:  33.33%, val_best:  48.33%, tr:  98.57%, tr_best:  99.28%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4386%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3357%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 103677  15.574%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss: 10.948681/ 43.872070, val:  48.33%, val_best:  48.33%, tr:  99.18%, tr_best:  99.28%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5029%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1595%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 105204  15.574%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss: 10.466593/ 69.177582, val:  36.67%, val_best:  48.33%, tr:  99.28%, tr_best:  99.28%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9626%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7052%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 106715  15.572%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss: 10.570361/ 66.409683, val:  36.25%, val_best:  48.33%, tr:  98.57%, tr_best:  99.28%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8879%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3312%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 108226  15.570%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss: 10.380864/ 74.828293, val:  40.42%, val_best:  48.33%, tr:  98.98%, tr_best:  99.28%, epoch time: 75.74 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8923%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8703%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 109762  15.572%\n",
      "fc layer 2 self.abs_max_out: 9196.0\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss: 10.736985/ 52.040302, val:  41.25%, val_best:  48.33%, tr:  99.18%, tr_best:  99.28%, epoch time: 75.69 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5960%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3032%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 111293  15.573%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss: 10.658841/113.223846, val:  36.25%, val_best:  48.33%, tr:  98.47%, tr_best:  99.28%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6999%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8089%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 112855  15.578%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss: 11.168515/ 46.657330, val:  45.42%, val_best:  48.33%, tr:  98.98%, tr_best:  99.28%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5805%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9074%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 114419  15.583%\n",
      "fc layer 2 self.abs_max_out: 9879.0\n",
      "lif layer 2 self.abs_max_v: 16972.5\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss: 10.674214/ 46.514095, val:  44.17%, val_best:  48.33%, tr:  99.08%, tr_best:  99.28%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7888%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8549%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 115929  15.581%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss: 11.112809/ 48.576229, val:  45.42%, val_best:  48.33%, tr:  98.67%, tr_best:  99.28%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4748%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7092%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 117459  15.582%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss: 11.007986/ 95.445656, val:  31.67%, val_best:  48.33%, tr:  99.18%, tr_best:  99.28%, epoch time: 75.95 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2387%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1038%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 118983  15.581%\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss: 11.272335/ 68.824867, val:  37.50%, val_best:  48.33%, tr:  98.67%, tr_best:  99.28%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0665%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8882%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 120492  15.579%\n",
      "lif layer 2 self.abs_max_v: 17388.5\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss: 11.235392/ 47.798462, val:  44.58%, val_best:  48.33%, tr:  97.96%, tr_best:  99.28%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1355%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9687%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 121982  15.575%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss: 10.702722/ 59.819412, val:  39.17%, val_best:  48.33%, tr:  98.88%, tr_best:  99.28%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1063%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8227%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 123456  15.568%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss: 10.778674/ 94.695061, val:  27.92%, val_best:  48.33%, tr:  98.16%, tr_best:  99.28%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1550%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1748%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 124941  15.564%\n",
      "fc layer 2 self.abs_max_out: 10082.0\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss: 11.024235/ 56.002007, val:  44.58%, val_best:  48.33%, tr:  98.57%, tr_best:  99.28%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2540%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9465%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 126458  15.563%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss: 11.130232/ 77.868935, val:  34.17%, val_best:  48.33%, tr:  99.08%, tr_best:  99.28%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1842%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2058%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 127977  15.562%\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss: 11.285246/ 61.587727, val:  31.67%, val_best:  48.33%, tr:  99.08%, tr_best:  99.28%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4211%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4604%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 129479  15.560%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss: 10.928169/ 68.614792, val:  32.92%, val_best:  48.33%, tr:  98.47%, tr_best:  99.28%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6732%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6442%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 130977  15.557%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss: 10.733008/ 45.709576, val:  47.08%, val_best:  48.33%, tr:  98.57%, tr_best:  99.28%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7270%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9979%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 132474  15.554%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss: 11.126554/ 89.026497, val:  37.92%, val_best:  48.33%, tr:  98.88%, tr_best:  99.28%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7755%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5563%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 134012  15.555%\n",
      "fc layer 2 self.abs_max_out: 10390.0\n",
      "lif layer 2 self.abs_max_v: 17467.0\n",
      "lif layer 2 self.abs_max_v: 18201.5\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss: 11.335488/ 59.588619, val:  37.50%, val_best:  48.33%, tr:  98.88%, tr_best:  99.28%, epoch time: 76.01 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6203%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9776%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 135489  15.550%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss: 11.131344/ 55.244801, val:  47.08%, val_best:  48.33%, tr:  98.98%, tr_best:  99.28%, epoch time: 75.95 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3480%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6538%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 136999  15.549%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss: 11.420320/ 62.786480, val:  37.92%, val_best:  48.33%, tr:  98.06%, tr_best:  99.28%, epoch time: 75.95 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8083%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6938%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 138538  15.551%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss: 11.037123/ 64.648102, val:  44.17%, val_best:  48.33%, tr:  98.77%, tr_best:  99.28%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3801%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2394%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 139962  15.540%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss: 11.455452/ 74.340782, val:  33.75%, val_best:  48.33%, tr:  99.28%, tr_best:  99.28%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2838%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1306%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 141446  15.535%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss: 10.984363/ 66.182884, val:  34.58%, val_best:  48.33%, tr:  98.77%, tr_best:  99.28%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1351%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9082%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 142911  15.529%\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss: 11.100261/ 71.918571, val:  38.75%, val_best:  48.33%, tr:  99.08%, tr_best:  99.28%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6158%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2912%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 144362  15.522%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss: 11.343654/ 68.047150, val:  41.67%, val_best:  48.33%, tr:  98.37%, tr_best:  99.28%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5584%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0637%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 145835  15.517%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss: 11.263759/ 71.222382, val:  45.00%, val_best:  48.33%, tr:  98.88%, tr_best:  99.28%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5468%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4221%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 147332  15.515%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss: 10.929145/ 70.654167, val:  32.92%, val_best:  48.33%, tr:  99.18%, tr_best:  99.28%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5745%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2121%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 148740  15.503%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss: 11.066249/ 74.865387, val:  34.58%, val_best:  48.33%, tr:  98.67%, tr_best:  99.28%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5456%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1047%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 150176  15.495%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss: 10.700221/ 58.615459, val:  49.58%, val_best:  49.58%, tr:  99.08%, tr_best:  99.28%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0299%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7940%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 151627  15.488%\n",
      "fc layer 3 self.abs_max_out: 548.0\n",
      "fc layer 3 self.abs_max_out: 592.0\n",
      "fc layer 3 self.abs_max_out: 609.0\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss: 11.532678/ 68.112022, val:  38.75%, val_best:  49.58%, tr:  99.18%, tr_best:  99.28%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6380%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8836%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 153067  15.480%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss: 11.648747/ 67.005669, val:  40.42%, val_best:  49.58%, tr:  99.49%, tr_best:  99.49%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5903%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7747%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 154551  15.477%\n",
      "lif layer 2 self.abs_max_v: 18290.0\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss: 11.114626/ 75.304192, val:  40.42%, val_best:  49.58%, tr:  99.28%, tr_best:  99.49%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5529%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5270%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 155985  15.469%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss: 11.474463/ 66.877098, val:  33.33%, val_best:  49.58%, tr:  98.47%, tr_best:  99.49%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3073%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2091%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 157487  15.468%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss: 11.797513/ 59.551456, val:  40.83%, val_best:  49.58%, tr:  98.88%, tr_best:  99.49%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8301%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 158958  15.464%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss: 11.790724/ 51.598801, val:  33.33%, val_best:  49.58%, tr:  98.77%, tr_best:  99.49%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0479%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8416%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 160452  15.462%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss: 11.718954/103.779945, val:  23.33%, val_best:  49.58%, tr:  98.88%, tr_best:  99.49%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1188%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5178%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 161933  15.459%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss: 11.629221/ 75.642082, val:  41.25%, val_best:  49.58%, tr:  98.77%, tr_best:  99.49%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4036%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0929%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 163427  15.457%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss: 11.805482/ 65.289604, val:  38.33%, val_best:  49.58%, tr:  98.26%, tr_best:  99.49%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3374%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3070%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 164987  15.461%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss: 10.858589/ 61.183517, val:  36.67%, val_best:  49.58%, tr:  98.57%, tr_best:  99.49%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3100%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6440%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 166469  15.458%\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss: 11.301830/ 57.496445, val:  38.75%, val_best:  49.58%, tr:  98.57%, tr_best:  99.49%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1837%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8096%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 167953  15.455%\n",
      "lif layer 2 self.abs_max_v: 18590.0\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss: 11.668015/ 54.817966, val:  46.25%, val_best:  49.58%, tr:  99.08%, tr_best:  99.49%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1880%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5421%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 169458  15.455%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss: 11.449996/ 43.248100, val:  50.83%, val_best:  50.83%, tr:  98.77%, tr_best:  99.49%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0943%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7876%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 170941  15.452%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss: 11.355537/ 78.431297, val:  31.25%, val_best:  50.83%, tr:  98.77%, tr_best:  99.49%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7093%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3211%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 172478  15.454%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss: 11.159077/ 95.564629, val:  31.25%, val_best:  50.83%, tr:  98.77%, tr_best:  99.49%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7867%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5598%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 173986  15.454%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss: 11.309233/ 97.867073, val:  22.08%, val_best:  50.83%, tr:  98.67%, tr_best:  99.49%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9260%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6643%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 175504  15.454%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss: 11.061539/ 74.460663, val:  36.25%, val_best:  50.83%, tr:  98.88%, tr_best:  99.49%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8695%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0082%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 177039  15.456%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss: 11.220269/ 65.197456, val:  45.00%, val_best:  50.83%, tr:  98.47%, tr_best:  99.49%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1183%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7650%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 178588  15.459%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss: 11.218319/ 55.732258, val:  39.58%, val_best:  50.83%, tr:  99.18%, tr_best:  99.49%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6909%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6898%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 180125  15.461%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss: 10.605597/ 55.440594, val:  50.83%, val_best:  50.83%, tr:  98.88%, tr_best:  99.49%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7686%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3231%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 181567  15.455%\n",
      "fc layer 2 self.abs_max_out: 10579.0\n",
      "lif layer 2 self.abs_max_v: 19437.0\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss: 10.531936/ 58.028683, val:  48.75%, val_best:  50.83%, tr:  98.88%, tr_best:  99.49%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1338%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4806%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 182990  15.448%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss: 11.128102/ 65.602013, val:  38.75%, val_best:  50.83%, tr:  98.67%, tr_best:  99.49%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9234%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5695%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 184511  15.448%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss: 11.620976/ 62.265804, val:  33.75%, val_best:  50.83%, tr:  98.98%, tr_best:  99.49%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6382%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6665%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 186027  15.449%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss: 11.563087/ 78.218689, val:  37.92%, val_best:  50.83%, tr:  98.67%, tr_best:  99.49%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4973%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5466%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 187563  15.451%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss: 11.866807/ 67.180801, val:  47.50%, val_best:  50.83%, tr:  98.88%, tr_best:  99.49%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3047%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.4666%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 189075  15.450%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss: 11.659975/ 65.848366, val:  32.92%, val_best:  50.83%, tr:  98.98%, tr_best:  99.49%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1824%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.1616%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 190561  15.448%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss: 11.476975/ 82.719025, val:  28.33%, val_best:  50.83%, tr:  98.88%, tr_best:  99.49%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8057%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5874%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 192024  15.444%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss: 10.832190/ 54.848293, val:  37.50%, val_best:  50.83%, tr:  99.08%, tr_best:  99.49%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9599%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7974%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 193441  15.437%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss: 11.614041/ 57.928062, val:  37.08%, val_best:  50.83%, tr:  98.77%, tr_best:  99.49%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8032%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2899%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 194991  15.440%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss: 11.475831/ 56.200405, val:  40.42%, val_best:  50.83%, tr:  98.57%, tr_best:  99.49%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8046%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3062%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 196536  15.442%\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        # \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [256.0,128.0,64.0,32.0,16.0]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "\n",
    "        \"learning_rate\": {\"values\": [1.0]}, \n",
    "        # \"lr_factor\": {\"values\": [-6, -7, -8, -9]}, \n",
    "        \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [14]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [-1]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [0]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        \"scale_exp_2w\": {\"values\": [0]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        \"scale_exp_3w\": {\"values\": [0]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "        \"lif_layer_sg_width2\": {\"values\": [0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0]},\n",
    "        \"lif_layer_v_threshold2\": {\"values\": [256.0,128.0,64.0,32.0,16.0]},\n",
    "        \"learning_rate2\": {\"values\": [1.0]},\n",
    "        \"init_scaling_0\": {\"values\": [4/128]},\n",
    "        \"init_scaling_1\": {\"values\": [6/128]},\n",
    "        \"init_scaling_2\": {\"values\": [3/128]},\n",
    "        \n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"1\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_2w,wandb.config.scale_exp_2w],[wandb.config.scale_exp_3w,wandb.config.scale_exp_3w]],\n",
    "        lif_layer_sg_width2  =  wandb.config.lif_layer_sg_width2,\n",
    "        lif_layer_v_threshold2  =  wandb.config.lif_layer_v_threshold2,\n",
    "        learning_rate2  =  wandb.config.learning_rate2,\n",
    "        init_scaling = [wandb.config.init_scaling_0,wandb.config.init_scaling_1,wandb.config.init_scaling_2],\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'e1m59f1o'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
