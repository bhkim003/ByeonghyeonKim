{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31401/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    lif_layer_sg_width2 = None,\n",
    "                    lif_layer_v_threshold2 = None,\n",
    "                    learning_rate2 = None,\n",
    "                    init_scaling = None,\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp,\n",
    "                    ANPI_MODE=False,\n",
    "                    lif_layer_sg_width2=lif_layer_sg_width2,\n",
    "                    lif_layer_v_threshold2=lif_layer_v_threshold2,\n",
    "                    init_scaling=init_scaling).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                # lr = group['lr']\n",
    "\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        lr = learning_rate\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        lr = learning_rate2\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        lr = 1.0\n",
    "\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "                now_T = inputs.shape[1]\n",
    "                now_time_steps = temporal_filter*TIME\n",
    "                # start_idx = random.randint(0, now_T - now_time_steps)\n",
    "                start_idx = random.choice(range(0, now_T - now_time_steps + 1, now_time_steps))\n",
    "                # start_idx = random.choice([i for i in range(0, now_T - now_time_steps + 1, now_time_steps)])\n",
    "                inputs = inputs[:, start_idx : start_idx + now_time_steps]\n",
    "                if dvs_clipping != 0:\n",
    "                    inputs[inputs<dvs_clipping] = 0.0\n",
    "                    inputs[inputs>=dvs_clipping] = 1.0\n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            now_T = inputs_val.shape[1]\n",
    "                            now_time_steps = temporal_filter*TIME\n",
    "                            start_idx = 0\n",
    "                            inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if dvs_clipping != 0:\n",
    "                                inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "\n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    assert val_acc_best > 0.2\n",
    "                elif epoch > 10:\n",
    "                    assert val_acc_best > 0.4\n",
    "                elif epoch > 30:\n",
    "                    assert val_acc_best > 0.5\n",
    "                elif epoch > 100:\n",
    "                    assert val_acc_best > 0.6\n",
    "                    \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"5\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 2871,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 8, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 200,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = -1, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[0,0],[0,0],[0,0]], \n",
    "#                 lif_layer_sg_width2 = 4.0,\n",
    "#                 lif_layer_v_threshold2 = 8,\n",
    "#                 learning_rate2 = 8,\n",
    "#                 init_scaling = [1/2,1/2,1/2],\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7caqf936 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.046875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0234375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_010846-7caqf936</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7caqf936' target=\"_blank\">electric-sweep-3</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7caqf936' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7caqf936</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251214_010853_947', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 128, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 16, 'lif_layer_v_threshold2': 32, 'init_scaling': [0.03125, 0.046875, 0.0234375], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 2, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 16, self.v_threshold 32\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=32, v_reset=10000, sg_width=16, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 46.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 59.5\n",
      "fc layer 1 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 69.0\n",
      "fc layer 1 self.abs_max_out: 62.0\n",
      "lif layer 1 self.abs_max_v: 90.0\n",
      "fc layer 1 self.abs_max_out: 101.0\n",
      "lif layer 1 self.abs_max_v: 138.0\n",
      "fc layer 2 self.abs_max_out: 16.0\n",
      "lif layer 2 self.abs_max_v: 16.0\n",
      "fc layer 1 self.abs_max_out: 147.0\n",
      "lif layer 1 self.abs_max_v: 210.0\n",
      "fc layer 2 self.abs_max_out: 37.0\n",
      "lif layer 2 self.abs_max_v: 42.5\n",
      "fc layer 3 self.abs_max_out: 6.0\n",
      "fc layer 1 self.abs_max_out: 200.0\n",
      "lif layer 1 self.abs_max_v: 270.0\n",
      "fc layer 2 self.abs_max_out: 51.0\n",
      "lif layer 2 self.abs_max_v: 65.5\n",
      "fc layer 3 self.abs_max_out: 14.0\n",
      "fc layer 1 self.abs_max_out: 225.0\n",
      "lif layer 1 self.abs_max_v: 360.0\n",
      "fc layer 2 self.abs_max_out: 52.0\n",
      "lif layer 2 self.abs_max_v: 84.0\n",
      "fc layer 3 self.abs_max_out: 18.0\n",
      "fc layer 1 self.abs_max_out: 277.0\n",
      "lif layer 1 self.abs_max_v: 381.0\n",
      "fc layer 2 self.abs_max_out: 55.0\n",
      "lif layer 2 self.abs_max_v: 93.0\n",
      "fc layer 3 self.abs_max_out: 23.0\n",
      "lif layer 2 self.abs_max_v: 98.5\n",
      "fc layer 3 self.abs_max_out: 25.0\n",
      "fc layer 1 self.abs_max_out: 297.0\n",
      "fc layer 3 self.abs_max_out: 30.0\n",
      "fc layer 2 self.abs_max_out: 59.0\n",
      "lif layer 2 self.abs_max_v: 101.0\n",
      "fc layer 1 self.abs_max_out: 318.0\n",
      "lif layer 1 self.abs_max_v: 413.5\n",
      "fc layer 2 self.abs_max_out: 69.0\n",
      "fc layer 1 self.abs_max_out: 423.0\n",
      "lif layer 1 self.abs_max_v: 559.0\n",
      "fc layer 2 self.abs_max_out: 73.0\n",
      "lif layer 1 self.abs_max_v: 576.5\n",
      "fc layer 3 self.abs_max_out: 32.0\n",
      "fc layer 3 self.abs_max_out: 34.0\n",
      "lif layer 2 self.abs_max_v: 102.5\n",
      "lif layer 2 self.abs_max_v: 103.5\n",
      "lif layer 2 self.abs_max_v: 104.0\n",
      "fc layer 3 self.abs_max_out: 37.0\n",
      "fc layer 1 self.abs_max_out: 452.0\n",
      "lif layer 1 self.abs_max_v: 665.0\n",
      "fc layer 2 self.abs_max_out: 93.0\n",
      "lif layer 2 self.abs_max_v: 108.5\n",
      "lif layer 1 self.abs_max_v: 676.5\n",
      "lif layer 2 self.abs_max_v: 129.5\n",
      "fc layer 2 self.abs_max_out: 95.0\n",
      "fc layer 2 self.abs_max_out: 99.0\n",
      "lif layer 2 self.abs_max_v: 131.5\n",
      "lif layer 2 self.abs_max_v: 142.0\n",
      "lif layer 1 self.abs_max_v: 736.5\n",
      "fc layer 2 self.abs_max_out: 134.0\n",
      "lif layer 2 self.abs_max_v: 168.0\n",
      "lif layer 1 self.abs_max_v: 750.5\n",
      "lif layer 1 self.abs_max_v: 758.5\n",
      "lif layer 2 self.abs_max_v: 173.0\n",
      "fc layer 1 self.abs_max_out: 457.0\n",
      "lif layer 1 self.abs_max_v: 775.5\n",
      "fc layer 1 self.abs_max_out: 535.0\n",
      "lif layer 1 self.abs_max_v: 873.0\n",
      "fc layer 2 self.abs_max_out: 153.0\n",
      "lif layer 2 self.abs_max_v: 188.5\n",
      "fc layer 3 self.abs_max_out: 43.0\n",
      "lif layer 1 self.abs_max_v: 939.5\n",
      "lif layer 2 self.abs_max_v: 189.5\n",
      "fc layer 3 self.abs_max_out: 50.0\n",
      "fc layer 3 self.abs_max_out: 51.0\n",
      "fc layer 2 self.abs_max_out: 163.0\n",
      "fc layer 3 self.abs_max_out: 55.0\n",
      "lif layer 2 self.abs_max_v: 190.5\n",
      "lif layer 2 self.abs_max_v: 204.5\n",
      "fc layer 3 self.abs_max_out: 63.0\n",
      "fc layer 3 self.abs_max_out: 65.0\n",
      "lif layer 1 self.abs_max_v: 942.5\n",
      "lif layer 2 self.abs_max_v: 217.5\n",
      "lif layer 1 self.abs_max_v: 985.5\n",
      "fc layer 2 self.abs_max_out: 164.0\n",
      "lif layer 2 self.abs_max_v: 264.0\n",
      "lif layer 2 self.abs_max_v: 265.0\n",
      "fc layer 1 self.abs_max_out: 771.0\n",
      "fc layer 2 self.abs_max_out: 176.0\n",
      "lif layer 1 self.abs_max_v: 1086.5\n",
      "fc layer 1 self.abs_max_out: 904.0\n",
      "lif layer 1 self.abs_max_v: 1447.5\n",
      "lif layer 2 self.abs_max_v: 282.5\n",
      "lif layer 1 self.abs_max_v: 1549.0\n",
      "fc layer 2 self.abs_max_out: 180.0\n",
      "lif layer 2 self.abs_max_v: 305.0\n",
      "lif layer 1 self.abs_max_v: 1575.0\n",
      "fc layer 3 self.abs_max_out: 70.0\n",
      "fc layer 1 self.abs_max_out: 1146.0\n",
      "lif layer 1 self.abs_max_v: 1914.5\n",
      "lif layer 2 self.abs_max_v: 310.0\n",
      "lif layer 1 self.abs_max_v: 2027.5\n",
      "lif layer 2 self.abs_max_v: 313.0\n",
      "fc layer 2 self.abs_max_out: 183.0\n",
      "fc layer 2 self.abs_max_out: 184.0\n",
      "fc layer 3 self.abs_max_out: 80.0\n",
      "fc layer 3 self.abs_max_out: 83.0\n",
      "fc layer 2 self.abs_max_out: 199.0\n",
      "fc layer 2 self.abs_max_out: 201.0\n",
      "fc layer 2 self.abs_max_out: 218.0\n",
      "fc layer 3 self.abs_max_out: 90.0\n",
      "fc layer 2 self.abs_max_out: 225.0\n",
      "fc layer 2 self.abs_max_out: 255.0\n",
      "fc layer 2 self.abs_max_out: 267.0\n",
      "lif layer 2 self.abs_max_v: 316.5\n",
      "lif layer 2 self.abs_max_v: 331.5\n",
      "fc layer 3 self.abs_max_out: 99.0\n",
      "fc layer 3 self.abs_max_out: 104.0\n",
      "fc layer 2 self.abs_max_out: 273.0\n",
      "fc layer 2 self.abs_max_out: 326.0\n",
      "fc layer 2 self.abs_max_out: 361.0\n",
      "lif layer 2 self.abs_max_v: 365.5\n",
      "lif layer 2 self.abs_max_v: 376.0\n",
      "lif layer 2 self.abs_max_v: 419.5\n",
      "fc layer 3 self.abs_max_out: 117.0\n",
      "fc layer 3 self.abs_max_out: 123.0\n",
      "fc layer 3 self.abs_max_out: 125.0\n",
      "fc layer 3 self.abs_max_out: 136.0\n",
      "fc layer 3 self.abs_max_out: 143.0\n",
      "fc layer 1 self.abs_max_out: 1152.0\n",
      "lif layer 1 self.abs_max_v: 2150.5\n",
      "fc layer 3 self.abs_max_out: 152.0\n",
      "fc layer 3 self.abs_max_out: 154.0\n",
      "fc layer 3 self.abs_max_out: 164.0\n",
      "lif layer 2 self.abs_max_v: 421.0\n",
      "lif layer 2 self.abs_max_v: 453.5\n",
      "lif layer 2 self.abs_max_v: 482.0\n",
      "fc layer 3 self.abs_max_out: 171.0\n",
      "fc layer 3 self.abs_max_out: 196.0\n",
      "fc layer 2 self.abs_max_out: 370.0\n",
      "fc layer 2 self.abs_max_out: 372.0\n",
      "fc layer 1 self.abs_max_out: 1496.0\n",
      "lif layer 1 self.abs_max_v: 2302.5\n",
      "lif layer 2 self.abs_max_v: 516.5\n",
      "lif layer 2 self.abs_max_v: 569.5\n",
      "lif layer 1 self.abs_max_v: 2307.0\n",
      "fc layer 2 self.abs_max_out: 375.0\n",
      "fc layer 3 self.abs_max_out: 203.0\n",
      "fc layer 3 self.abs_max_out: 206.0\n",
      "fc layer 2 self.abs_max_out: 381.0\n",
      "fc layer 3 self.abs_max_out: 217.0\n",
      "fc layer 3 self.abs_max_out: 225.0\n",
      "fc layer 3 self.abs_max_out: 233.0\n",
      "fc layer 2 self.abs_max_out: 435.0\n",
      "lif layer 1 self.abs_max_v: 2391.5\n",
      "lif layer 2 self.abs_max_v: 570.5\n",
      "lif layer 2 self.abs_max_v: 582.5\n",
      "fc layer 2 self.abs_max_out: 455.0\n",
      "lif layer 2 self.abs_max_v: 606.5\n",
      "lif layer 2 self.abs_max_v: 621.0\n",
      "lif layer 2 self.abs_max_v: 635.0\n",
      "lif layer 2 self.abs_max_v: 662.5\n",
      "fc layer 2 self.abs_max_out: 463.0\n",
      "fc layer 2 self.abs_max_out: 485.0\n",
      "fc layer 2 self.abs_max_out: 550.0\n",
      "lif layer 2 self.abs_max_v: 663.5\n",
      "lif layer 2 self.abs_max_v: 694.0\n",
      "fc layer 3 self.abs_max_out: 242.0\n",
      "lif layer 1 self.abs_max_v: 2429.5\n",
      "lif layer 2 self.abs_max_v: 721.0\n",
      "lif layer 2 self.abs_max_v: 722.5\n",
      "lif layer 2 self.abs_max_v: 746.5\n",
      "lif layer 2 self.abs_max_v: 755.0\n",
      "fc layer 3 self.abs_max_out: 243.0\n",
      "fc layer 3 self.abs_max_out: 263.0\n",
      "fc layer 1 self.abs_max_out: 1528.0\n",
      "lif layer 1 self.abs_max_v: 2514.5\n",
      "lif layer 1 self.abs_max_v: 2536.5\n",
      "lif layer 2 self.abs_max_v: 756.0\n",
      "lif layer 2 self.abs_max_v: 765.0\n",
      "lif layer 2 self.abs_max_v: 782.5\n",
      "lif layer 2 self.abs_max_v: 792.5\n",
      "lif layer 2 self.abs_max_v: 852.5\n",
      "lif layer 2 self.abs_max_v: 911.5\n",
      "fc layer 3 self.abs_max_out: 308.0\n",
      "fc layer 3 self.abs_max_out: 311.0\n",
      "fc layer 1 self.abs_max_out: 1587.0\n",
      "lif layer 1 self.abs_max_v: 2552.0\n",
      "lif layer 1 self.abs_max_v: 2746.0\n",
      "fc layer 2 self.abs_max_out: 565.0\n",
      "fc layer 2 self.abs_max_out: 580.0\n",
      "fc layer 2 self.abs_max_out: 585.0\n",
      "fc layer 2 self.abs_max_out: 588.0\n",
      "fc layer 2 self.abs_max_out: 594.0\n",
      "fc layer 2 self.abs_max_out: 596.0\n",
      "lif layer 2 self.abs_max_v: 949.5\n",
      "lif layer 2 self.abs_max_v: 1036.0\n",
      "fc layer 1 self.abs_max_out: 1618.0\n",
      "fc layer 1 self.abs_max_out: 1679.0\n",
      "fc layer 1 self.abs_max_out: 1973.0\n",
      "lif layer 1 self.abs_max_v: 3121.0\n",
      "lif layer 1 self.abs_max_v: 3388.5\n",
      "lif layer 1 self.abs_max_v: 3575.0\n",
      "lif layer 2 self.abs_max_v: 1039.5\n",
      "fc layer 2 self.abs_max_out: 609.0\n",
      "lif layer 2 self.abs_max_v: 1066.0\n",
      "fc layer 2 self.abs_max_out: 612.0\n",
      "lif layer 2 self.abs_max_v: 1068.0\n",
      "fc layer 2 self.abs_max_out: 631.0\n",
      "lif layer 2 self.abs_max_v: 1077.5\n",
      "lif layer 2 self.abs_max_v: 1083.0\n",
      "lif layer 2 self.abs_max_v: 1094.5\n",
      "lif layer 2 self.abs_max_v: 1128.5\n",
      "fc layer 2 self.abs_max_out: 646.0\n",
      "lif layer 2 self.abs_max_v: 1136.5\n",
      "lif layer 2 self.abs_max_v: 1159.0\n",
      "lif layer 2 self.abs_max_v: 1197.5\n",
      "lif layer 2 self.abs_max_v: 1200.5\n",
      "lif layer 2 self.abs_max_v: 1218.5\n",
      "fc layer 2 self.abs_max_out: 693.0\n",
      "fc layer 2 self.abs_max_out: 704.0\n",
      "fc layer 2 self.abs_max_out: 705.0\n",
      "fc layer 2 self.abs_max_out: 751.0\n",
      "lif layer 2 self.abs_max_v: 1245.5\n",
      "lif layer 2 self.abs_max_v: 1265.5\n",
      "fc layer 1 self.abs_max_out: 2001.0\n",
      "fc layer 2 self.abs_max_out: 763.0\n",
      "fc layer 1 self.abs_max_out: 2031.0\n",
      "fc layer 3 self.abs_max_out: 344.0\n",
      "fc layer 1 self.abs_max_out: 2159.0\n",
      "lif layer 2 self.abs_max_v: 1282.5\n",
      "lif layer 2 self.abs_max_v: 1300.5\n",
      "lif layer 2 self.abs_max_v: 1302.0\n",
      "lif layer 2 self.abs_max_v: 1323.0\n",
      "lif layer 2 self.abs_max_v: 1333.5\n",
      "lif layer 2 self.abs_max_v: 1336.5\n",
      "fc layer 3 self.abs_max_out: 348.0\n",
      "fc layer 3 self.abs_max_out: 352.0\n",
      "fc layer 1 self.abs_max_out: 2273.0\n",
      "fc layer 1 self.abs_max_out: 2279.0\n",
      "fc layer 1 self.abs_max_out: 2354.0\n",
      "lif layer 1 self.abs_max_v: 3988.0\n",
      "lif layer 1 self.abs_max_v: 4304.0\n",
      "lif layer 2 self.abs_max_v: 1339.0\n",
      "lif layer 2 self.abs_max_v: 1368.5\n",
      "lif layer 2 self.abs_max_v: 1382.5\n",
      "lif layer 2 self.abs_max_v: 1405.5\n",
      "lif layer 2 self.abs_max_v: 1417.0\n",
      "lif layer 2 self.abs_max_v: 1422.5\n",
      "lif layer 2 self.abs_max_v: 1440.0\n",
      "lif layer 2 self.abs_max_v: 1458.5\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 10.086633/102.144775, val:  25.83%, val_best:  25.83%, tr:  99.18%, tr_best:  99.18%, epoch time: 81.69 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.8292%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.7470%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 1629  16.639%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 771.0\n",
      "fc layer 2 self.abs_max_out: 784.0\n",
      "fc layer 3 self.abs_max_out: 362.0\n",
      "fc layer 2 self.abs_max_out: 785.0\n",
      "fc layer 2 self.abs_max_out: 851.0\n",
      "fc layer 2 self.abs_max_out: 860.0\n",
      "lif layer 2 self.abs_max_v: 1478.0\n",
      "fc layer 3 self.abs_max_out: 369.0\n",
      "lif layer 2 self.abs_max_v: 1500.0\n",
      "fc layer 2 self.abs_max_out: 896.0\n",
      "lif layer 2 self.abs_max_v: 1552.5\n",
      "lif layer 2 self.abs_max_v: 1561.5\n",
      "lif layer 2 self.abs_max_v: 1562.0\n",
      "fc layer 1 self.abs_max_out: 2401.0\n",
      "lif layer 2 self.abs_max_v: 1565.0\n",
      "fc layer 1 self.abs_max_out: 2529.0\n",
      "lif layer 2 self.abs_max_v: 1587.5\n",
      "fc layer 1 self.abs_max_out: 2615.0\n",
      "fc layer 2 self.abs_max_out: 966.0\n",
      "fc layer 1 self.abs_max_out: 2958.0\n",
      "lif layer 1 self.abs_max_v: 4886.0\n",
      "lif layer 1 self.abs_max_v: 5355.0\n",
      "lif layer 2 self.abs_max_v: 1609.0\n",
      "lif layer 2 self.abs_max_v: 1616.5\n",
      "lif layer 2 self.abs_max_v: 1671.5\n",
      "lif layer 2 self.abs_max_v: 1683.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 10.100355/ 78.712662, val:  31.25%, val_best:  31.25%, tr:  99.49%, tr_best:  99.49%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4217%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.3466%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3008  15.363%\n",
      "lif layer 2 self.abs_max_v: 1687.5\n",
      "fc layer 3 self.abs_max_out: 396.0\n",
      "fc layer 2 self.abs_max_out: 976.0\n",
      "fc layer 2 self.abs_max_out: 992.0\n",
      "lif layer 2 self.abs_max_v: 1766.0\n",
      "fc layer 3 self.abs_max_out: 399.0\n",
      "fc layer 3 self.abs_max_out: 413.0\n",
      "fc layer 3 self.abs_max_out: 420.0\n",
      "fc layer 2 self.abs_max_out: 1010.0\n",
      "lif layer 2 self.abs_max_v: 1783.5\n",
      "lif layer 2 self.abs_max_v: 1786.5\n",
      "lif layer 2 self.abs_max_v: 1789.0\n",
      "lif layer 2 self.abs_max_v: 1825.0\n",
      "fc layer 2 self.abs_max_out: 1016.0\n",
      "fc layer 2 self.abs_max_out: 1025.0\n",
      "lif layer 2 self.abs_max_v: 1825.5\n",
      "lif layer 2 self.abs_max_v: 1874.0\n",
      "lif layer 2 self.abs_max_v: 1955.5\n",
      "fc layer 2 self.abs_max_out: 1045.0\n",
      "fc layer 2 self.abs_max_out: 1059.0\n",
      "fc layer 2 self.abs_max_out: 1109.0\n",
      "lif layer 2 self.abs_max_v: 2023.5\n",
      "fc layer 1 self.abs_max_out: 3077.0\n",
      "lif layer 1 self.abs_max_v: 5476.0\n",
      "fc layer 2 self.abs_max_out: 1129.0\n",
      "lif layer 2 self.abs_max_v: 2037.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 10.317189/ 68.308800, val:  36.25%, val_best:  36.25%, tr:  99.08%, tr_best:  99.49%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4436%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.6253%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 4383  14.923%\n",
      "fc layer 2 self.abs_max_out: 1155.0\n",
      "fc layer 3 self.abs_max_out: 428.0\n",
      "fc layer 3 self.abs_max_out: 465.0\n",
      "fc layer 1 self.abs_max_out: 3520.0\n",
      "lif layer 1 self.abs_max_v: 5899.5\n",
      "lif layer 1 self.abs_max_v: 6430.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss: 10.770625/ 76.395332, val:  37.50%, val_best:  37.50%, tr:  99.59%, tr_best:  99.59%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4912%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.8427%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 5800  14.811%\n",
      "lif layer 2 self.abs_max_v: 2041.0\n",
      "lif layer 2 self.abs_max_v: 2055.5\n",
      "lif layer 2 self.abs_max_v: 2063.0\n",
      "lif layer 2 self.abs_max_v: 2108.5\n",
      "lif layer 2 self.abs_max_v: 2194.5\n",
      "lif layer 2 self.abs_max_v: 2232.0\n",
      "lif layer 2 self.abs_max_v: 2256.0\n",
      "lif layer 2 self.abs_max_v: 2268.0\n",
      "lif layer 2 self.abs_max_v: 2274.0\n",
      "fc layer 2 self.abs_max_out: 1157.0\n",
      "lif layer 1 self.abs_max_v: 6445.5\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss: 10.520677/ 90.665421, val:  33.75%, val_best:  37.50%, tr:  99.18%, tr_best:  99.59%, epoch time: 79.96 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3991%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.3703%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 7150  14.607%\n",
      "fc layer 2 self.abs_max_out: 1162.0\n",
      "fc layer 2 self.abs_max_out: 1166.0\n",
      "lif layer 2 self.abs_max_v: 2282.5\n",
      "fc layer 2 self.abs_max_out: 1174.0\n",
      "fc layer 2 self.abs_max_out: 1178.0\n",
      "fc layer 2 self.abs_max_out: 1223.0\n",
      "fc layer 2 self.abs_max_out: 1262.0\n",
      "fc layer 1 self.abs_max_out: 3623.0\n",
      "fc layer 1 self.abs_max_out: 3971.0\n",
      "fc layer 1 self.abs_max_out: 3991.0\n",
      "lif layer 1 self.abs_max_v: 6824.0\n",
      "fc layer 1 self.abs_max_out: 4037.0\n",
      "lif layer 1 self.abs_max_v: 7449.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss: 10.909410/ 84.477913, val:  31.67%, val_best:  37.50%, tr:  99.18%, tr_best:  99.59%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4342%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.9253%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 8459  14.401%\n",
      "lif layer 2 self.abs_max_v: 2307.0\n",
      "lif layer 2 self.abs_max_v: 2351.5\n",
      "fc layer 2 self.abs_max_out: 1291.0\n",
      "lif layer 2 self.abs_max_v: 2467.0\n",
      "fc layer 2 self.abs_max_out: 1319.0\n",
      "fc layer 2 self.abs_max_out: 1321.0\n",
      "fc layer 2 self.abs_max_out: 1368.0\n",
      "lif layer 2 self.abs_max_v: 2492.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss: 10.996172/ 72.125816, val:  43.33%, val_best:  43.33%, tr:  99.08%, tr_best:  99.59%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4362%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.6398%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 9773  14.261%\n",
      "lif layer 2 self.abs_max_v: 2502.5\n",
      "lif layer 2 self.abs_max_v: 2516.0\n",
      "fc layer 3 self.abs_max_out: 469.0\n",
      "fc layer 3 self.abs_max_out: 482.0\n",
      "fc layer 3 self.abs_max_out: 491.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss: 10.530889/ 62.479607, val:  45.00%, val_best:  45.00%, tr:  99.59%, tr_best:  99.59%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0490%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1737%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 11038  14.093%\n",
      "fc layer 3 self.abs_max_out: 495.0\n",
      "fc layer 3 self.abs_max_out: 502.0\n",
      "fc layer 2 self.abs_max_out: 1375.0\n",
      "fc layer 3 self.abs_max_out: 504.0\n",
      "fc layer 2 self.abs_max_out: 1383.0\n",
      "lif layer 2 self.abs_max_v: 2538.0\n",
      "lif layer 2 self.abs_max_v: 2652.0\n",
      "lif layer 2 self.abs_max_v: 2709.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss: 11.648293/ 88.026253, val:  34.58%, val_best:  45.00%, tr:  99.49%, tr_best:  99.59%, epoch time: 79.57 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9382%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8621%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 12354  14.021%\n",
      "lif layer 2 self.abs_max_v: 2722.5\n",
      "lif layer 2 self.abs_max_v: 2724.0\n",
      "fc layer 3 self.abs_max_out: 511.0\n",
      "fc layer 2 self.abs_max_out: 1392.0\n",
      "fc layer 3 self.abs_max_out: 516.0\n",
      "fc layer 3 self.abs_max_out: 522.0\n",
      "fc layer 2 self.abs_max_out: 1410.0\n",
      "fc layer 2 self.abs_max_out: 1432.0\n",
      "fc layer 1 self.abs_max_out: 4171.0\n",
      "fc layer 1 self.abs_max_out: 4194.0\n",
      "fc layer 1 self.abs_max_out: 4219.0\n",
      "lif layer 1 self.abs_max_v: 7786.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss: 11.391353/ 81.602837, val:  40.00%, val_best:  45.00%, tr:  99.69%, tr_best:  99.69%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9349%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.9804%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 13636  13.928%\n",
      "fc layer 2 self.abs_max_out: 1468.0\n",
      "fc layer 3 self.abs_max_out: 528.0\n",
      "fc layer 2 self.abs_max_out: 1485.0\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss: 11.173196/101.863380, val:  35.00%, val_best:  45.00%, tr:  99.49%, tr_best:  99.69%, epoch time: 79.64 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5416%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.9465%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 14880  13.817%\n",
      "fc layer 2 self.abs_max_out: 1494.0\n",
      "fc layer 2 self.abs_max_out: 1500.0\n",
      "fc layer 1 self.abs_max_out: 4368.0\n",
      "lif layer 1 self.abs_max_v: 7915.0\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss: 11.585760/ 90.190613, val:  40.42%, val_best:  45.00%, tr:  99.39%, tr_best:  99.69%, epoch time: 79.69 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0782%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.5906%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 16182  13.774%\n",
      "fc layer 3 self.abs_max_out: 559.0\n",
      "fc layer 3 self.abs_max_out: 575.0\n",
      "fc layer 3 self.abs_max_out: 628.0\n",
      "fc layer 2 self.abs_max_out: 1505.0\n",
      "fc layer 1 self.abs_max_out: 4515.0\n",
      "lif layer 1 self.abs_max_v: 8269.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss: 11.603766/ 57.525105, val:  47.50%, val_best:  47.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4906%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.3445%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 17421  13.688%\n",
      "fc layer 3 self.abs_max_out: 629.0\n",
      "fc layer 2 self.abs_max_out: 1518.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss: 11.440879/114.221298, val:  37.92%, val_best:  47.50%, tr:  99.28%, tr_best:  99.90%, epoch time: 79.99 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9920%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.9717%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 18637  13.598%\n",
      "fc layer 2 self.abs_max_out: 1531.0\n",
      "fc layer 3 self.abs_max_out: 634.0\n",
      "lif layer 2 self.abs_max_v: 2730.0\n",
      "fc layer 1 self.abs_max_out: 4556.0\n",
      "lif layer 1 self.abs_max_v: 8273.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss: 11.709437/107.146637, val:  37.50%, val_best:  47.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9439%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.7266%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 19854  13.520%\n",
      "lif layer 2 self.abs_max_v: 2743.0\n",
      "fc layer 2 self.abs_max_out: 1536.0\n",
      "fc layer 2 self.abs_max_out: 1543.0\n",
      "fc layer 2 self.abs_max_out: 1546.0\n",
      "lif layer 2 self.abs_max_v: 2759.5\n",
      "lif layer 2 self.abs_max_v: 2791.0\n",
      "lif layer 2 self.abs_max_v: 2806.5\n",
      "fc layer 1 self.abs_max_out: 4651.0\n",
      "fc layer 1 self.abs_max_out: 4683.0\n",
      "lif layer 1 self.abs_max_v: 8552.5\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss: 12.300892/ 84.474991, val:  43.75%, val_best:  47.50%, tr:  99.39%, tr_best:  99.90%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2018%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.7799%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 21097  13.468%\n",
      "fc layer 2 self.abs_max_out: 1569.0\n",
      "fc layer 2 self.abs_max_out: 1578.0\n",
      "fc layer 2 self.abs_max_out: 1633.0\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss: 11.672431/ 61.159229, val:  44.17%, val_best:  47.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3960%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.4287%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 22317  13.409%\n",
      "fc layer 2 self.abs_max_out: 1638.0\n",
      "lif layer 2 self.abs_max_v: 2842.5\n",
      "fc layer 1 self.abs_max_out: 4918.0\n",
      "fc layer 1 self.abs_max_out: 4931.0\n",
      "lif layer 1 self.abs_max_v: 8968.5\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss: 12.243118/ 63.016941, val:  52.08%, val_best:  52.08%, tr:  99.28%, tr_best:  99.90%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9223%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.9356%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 23589  13.386%\n",
      "lif layer 2 self.abs_max_v: 2874.0\n",
      "fc layer 2 self.abs_max_out: 1648.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss: 12.554381/117.348297, val:  35.83%, val_best:  52.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5605%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.5500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 24852  13.361%\n",
      "lif layer 2 self.abs_max_v: 2897.0\n",
      "lif layer 2 self.abs_max_v: 2952.5\n",
      "fc layer 2 self.abs_max_out: 1682.0\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss: 11.076736/101.239128, val:  42.50%, val_best:  52.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2842%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.4642%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 26004  13.281%\n",
      "lif layer 2 self.abs_max_v: 3000.5\n",
      "lif layer 2 self.abs_max_v: 3006.0\n",
      "fc layer 3 self.abs_max_out: 649.0\n",
      "fc layer 3 self.abs_max_out: 665.0\n",
      "fc layer 1 self.abs_max_out: 5008.0\n",
      "lif layer 1 self.abs_max_v: 9008.5\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss: 11.627289/115.117447, val:  37.08%, val_best:  52.08%, tr:  99.39%, tr_best:  99.90%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7205%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.8715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 27212  13.236%\n",
      "fc layer 2 self.abs_max_out: 1685.0\n",
      "fc layer 2 self.abs_max_out: 1702.0\n",
      "fc layer 1 self.abs_max_out: 5103.0\n",
      "lif layer 1 self.abs_max_v: 9150.0\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss: 12.600412/ 88.391289, val:  45.00%, val_best:  52.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.2214%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.6675%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 28475  13.221%\n",
      "lif layer 2 self.abs_max_v: 3018.0\n",
      "fc layer 1 self.abs_max_out: 5107.0\n",
      "fc layer 1 self.abs_max_out: 5227.0\n",
      "lif layer 1 self.abs_max_v: 9445.5\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss: 12.116652/ 74.491234, val:  51.67%, val_best:  52.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6185%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.4647%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 29694  13.187%\n",
      "fc layer 2 self.abs_max_out: 1705.0\n",
      "fc layer 2 self.abs_max_out: 1713.0\n",
      "lif layer 2 self.abs_max_v: 3042.5\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss: 11.474799/ 84.605858, val:  43.75%, val_best:  52.08%, tr:  99.49%, tr_best:  99.90%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6984%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.7804%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 30861  13.135%\n",
      "fc layer 1 self.abs_max_out: 5339.0\n",
      "lif layer 1 self.abs_max_v: 9762.0\n",
      "fc layer 2 self.abs_max_out: 1733.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss: 11.503366/ 99.838280, val:  38.75%, val_best:  52.08%, tr:  99.39%, tr_best:  99.90%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6100%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.1055%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 32058  13.098%\n",
      "lif layer 2 self.abs_max_v: 3062.0\n",
      "fc layer 1 self.abs_max_out: 5340.0\n",
      "fc layer 1 self.abs_max_out: 5654.0\n",
      "lif layer 1 self.abs_max_v: 10289.5\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss: 12.705419/ 99.301521, val:  48.33%, val_best:  52.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3405%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.8501%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 33331  13.095%\n",
      "fc layer 2 self.abs_max_out: 1780.0\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss: 11.549777/ 74.402397, val:  50.42%, val_best:  52.08%, tr:  99.39%, tr_best:  99.90%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7634%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.0234%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 34523  13.061%\n",
      "fc layer 3 self.abs_max_out: 669.0\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss: 12.246638/ 60.153194, val:  53.75%, val_best:  53.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3858%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.3152%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 35727  13.033%\n",
      "lif layer 2 self.abs_max_v: 3063.5\n",
      "lif layer 2 self.abs_max_v: 3080.5\n",
      "lif layer 2 self.abs_max_v: 3134.5\n",
      "fc layer 3 self.abs_max_out: 705.0\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss: 11.810010/112.779205, val:  42.92%, val_best:  53.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3277%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.5755%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 36917  13.003%\n",
      "lif layer 2 self.abs_max_v: 3147.5\n",
      "fc layer 2 self.abs_max_out: 1928.0\n",
      "fc layer 1 self.abs_max_out: 5723.0\n",
      "lif layer 1 self.abs_max_v: 10345.5\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss: 11.909079/113.415749, val:  37.08%, val_best:  53.75%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8063%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.3577%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 38105  12.974%\n",
      "fc layer 3 self.abs_max_out: 714.0\n",
      "lif layer 2 self.abs_max_v: 3158.0\n",
      "lif layer 2 self.abs_max_v: 3323.5\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss: 11.873374/ 72.048386, val:  51.25%, val_best:  53.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0154%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.6838%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 39285  12.944%\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss: 11.971802/107.671814, val:  41.67%, val_best:  53.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7062%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.5047%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 40485  12.923%\n",
      "fc layer 1 self.abs_max_out: 5771.0\n",
      "lif layer 1 self.abs_max_v: 10507.0\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss: 11.909170/120.430641, val:  30.83%, val_best:  53.75%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.9886%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 41642  12.889%\n",
      "fc layer 1 self.abs_max_out: 5781.0\n",
      "lif layer 1 self.abs_max_v: 10568.0\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss: 12.425956/ 99.865768, val:  45.83%, val_best:  53.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0361%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.7743%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 42862  12.877%\n",
      "fc layer 1 self.abs_max_out: 5946.0\n",
      "fc layer 1 self.abs_max_out: 5958.0\n",
      "lif layer 1 self.abs_max_v: 10916.0\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss: 11.385635/ 78.457153, val:  56.25%, val_best:  56.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9098%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.0250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 44011  12.844%\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss: 11.894789/ 77.040298, val:  49.17%, val_best:  56.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2138%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.0092%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 45171  12.817%\n",
      "fc layer 3 self.abs_max_out: 730.0\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss: 11.974015/ 57.588234, val:  50.83%, val_best:  56.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1703%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.1911%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 46337  12.792%\n",
      "lif layer 2 self.abs_max_v: 3348.5\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss: 10.933677/ 71.277885, val:  50.00%, val_best:  56.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5293%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.3577%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 47433  12.750%\n",
      "fc layer 1 self.abs_max_out: 6101.0\n",
      "fc layer 1 self.abs_max_out: 6172.0\n",
      "lif layer 1 self.abs_max_v: 11248.5\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss: 11.609613/ 69.228073, val:  54.17%, val_best:  56.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2767%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.9789%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 48583  12.724%\n",
      "fc layer 2 self.abs_max_out: 1983.0\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss: 11.945998/119.817543, val:  39.17%, val_best:  56.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7394%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.4171%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 49729  12.699%\n",
      "fc layer 3 self.abs_max_out: 733.0\n",
      "fc layer 2 self.abs_max_out: 2006.0\n",
      "fc layer 2 self.abs_max_out: 2016.0\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss: 11.372594/ 81.329163, val:  56.67%, val_best:  56.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6860%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.0139%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 50869  12.673%\n",
      "fc layer 2 self.abs_max_out: 2120.0\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss: 11.976527/ 71.089272, val:  56.67%, val_best:  56.67%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8196%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.7475%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 52001  12.647%\n",
      "fc layer 2 self.abs_max_out: 2153.0\n",
      "lif layer 2 self.abs_max_v: 3395.0\n",
      "fc layer 2 self.abs_max_out: 2262.0\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss: 12.179090/ 58.712292, val:  56.25%, val_best:  56.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4986%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.9112%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 53165  12.629%\n",
      "fc layer 2 self.abs_max_out: 2275.0\n",
      "lif layer 2 self.abs_max_v: 3468.0\n",
      "lif layer 2 self.abs_max_v: 3494.0\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss: 11.468670/116.382912, val:  45.00%, val_best:  56.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9030%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.8603%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 54290  12.603%\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss: 11.807762/103.935188, val:  39.17%, val_best:  56.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7350%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.4989%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 55405  12.576%\n",
      "lif layer 2 self.abs_max_v: 3497.5\n",
      "lif layer 2 self.abs_max_v: 3679.0\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss: 11.281468/ 91.240631, val:  50.42%, val_best:  56.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4235%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.7704%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 56533  12.553%\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss: 11.911533/ 60.745533, val:  57.08%, val_best:  57.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7426%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.6387%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 57693  12.538%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss: 11.769610/ 80.179916, val:  45.83%, val_best:  57.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4433%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.4936%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 58829  12.519%\n",
      "fc layer 3 self.abs_max_out: 734.0\n",
      "fc layer 3 self.abs_max_out: 750.0\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss: 11.736680/ 90.897591, val:  47.08%, val_best:  57.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1690%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.0567%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 59954  12.498%\n",
      "fc layer 3 self.abs_max_out: 752.0\n",
      "fc layer 1 self.abs_max_out: 6199.0\n",
      "fc layer 1 self.abs_max_out: 6278.0\n",
      "lif layer 1 self.abs_max_v: 11412.5\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss: 11.219775/117.938614, val:  46.25%, val_best:  57.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3034%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.9088%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 61040  12.470%\n",
      "fc layer 3 self.abs_max_out: 755.0\n",
      "fc layer 3 self.abs_max_out: 778.0\n",
      "fc layer 2 self.abs_max_out: 2398.0\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss: 12.430851/ 89.826180, val:  51.67%, val_best:  57.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6752%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.6235%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 62201  12.458%\n",
      "fc layer 2 self.abs_max_out: 2450.0\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss: 11.540368/ 57.471527, val:  56.25%, val_best:  57.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0404%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.1730%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 63329  12.440%\n",
      "lif layer 2 self.abs_max_v: 3711.5\n",
      "fc layer 3 self.abs_max_out: 809.0\n",
      "fc layer 1 self.abs_max_out: 6291.0\n",
      "lif layer 1 self.abs_max_v: 11430.0\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss: 11.565056/ 72.509216, val:  59.17%, val_best:  59.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9872%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.1050%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 64448  12.421%\n",
      "fc layer 2 self.abs_max_out: 2496.0\n",
      "fc layer 1 self.abs_max_out: 6367.0\n",
      "lif layer 1 self.abs_max_v: 11627.0\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss: 10.848420/ 86.238609, val:  52.50%, val_best:  59.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6051%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.8999%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 65554  12.400%\n",
      "fc layer 3 self.abs_max_out: 824.0\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss: 11.981350/106.909958, val:  42.50%, val_best:  59.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8527%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.5662%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 66691  12.386%\n",
      "fc layer 1 self.abs_max_out: 6394.0\n",
      "fc layer 1 self.abs_max_out: 6472.0\n",
      "lif layer 1 self.abs_max_v: 11815.5\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss: 12.060579/ 97.897774, val:  44.58%, val_best:  59.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2038%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.7958%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 67821  12.371%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss: 11.758713/ 70.263542, val:  53.75%, val_best:  59.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1773%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.0270%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 68949  12.356%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss: 11.472421/ 89.958366, val:  50.42%, val_best:  59.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.1776%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.9382%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 70070  12.340%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss: 11.639389/ 73.899734, val:  56.25%, val_best:  59.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0271%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.6024%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 71159  12.320%\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss: 11.439485/ 99.795975, val:  46.25%, val_best:  59.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4739%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.1071%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 72255  12.301%\n",
      "lif layer 2 self.abs_max_v: 3715.5\n",
      "fc layer 3 self.abs_max_out: 837.0\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss: 11.618465/111.369637, val:  40.00%, val_best:  59.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1993%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.1523%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 73368  12.286%\n",
      "fc layer 3 self.abs_max_out: 843.0\n",
      "lif layer 2 self.abs_max_v: 3723.0\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss: 11.921415/ 82.474510, val:  52.08%, val_best:  59.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6473%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.7087%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 74462  12.268%\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss: 12.602225/ 85.633255, val:  51.25%, val_best:  59.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7751%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.0013%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 75620  12.261%\n",
      "fc layer 1 self.abs_max_out: 6484.0\n",
      "fc layer 1 self.abs_max_out: 6538.0\n",
      "lif layer 1 self.abs_max_v: 11919.0\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss: 11.511244/ 98.650291, val:  51.67%, val_best:  59.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5647%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.4397%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 76697  12.241%\n",
      "fc layer 3 self.abs_max_out: 848.0\n",
      "fc layer 3 self.abs_max_out: 870.0\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss: 11.290228/ 62.489380, val:  61.25%, val_best:  61.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5427%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.9368%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 77791  12.225%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss: 11.124582/ 85.078827, val:  47.50%, val_best:  61.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6467%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.3703%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 78865  12.206%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss: 11.077971/ 79.613564, val:  47.92%, val_best:  61.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1215%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.9365%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 79924  12.185%\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss: 10.719494/ 96.499954, val:  53.33%, val_best:  61.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7790%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.8192%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 80998  12.167%\n",
      "lif layer 2 self.abs_max_v: 3791.5\n",
      "lif layer 2 self.abs_max_v: 3824.0\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss: 10.823066/ 73.475624, val:  52.50%, val_best:  61.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4288%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.5761%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 82065  12.149%\n",
      "fc layer 1 self.abs_max_out: 6665.0\n",
      "lif layer 1 self.abs_max_v: 12087.0\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss: 10.697829/123.681953, val:  45.00%, val_best:  61.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1122%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.5832%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 83097  12.126%\n",
      "lif layer 2 self.abs_max_v: 3903.0\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss: 11.668308/ 63.203133, val:  60.00%, val_best:  61.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3957%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.6698%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 84187  12.112%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss: 10.954633/ 72.210197, val:  55.00%, val_best:  61.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0744%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.5964%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 85244  12.093%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss: 11.300124/ 56.976181, val:  63.75%, val_best:  63.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9474%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.4694%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 86309  12.077%\n",
      "lif layer 2 self.abs_max_v: 3917.0\n",
      "lif layer 2 self.abs_max_v: 4029.5\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss: 10.528449/103.876274, val:  55.42%, val_best:  63.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5391%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.4246%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 87353  12.058%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss: 10.691998/ 68.416550, val:  62.92%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0336%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.6864%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 88419  12.042%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss: 11.016234/ 57.472443, val:  61.25%, val_best:  63.75%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.2980%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.4879%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 89492  12.028%\n",
      "fc layer 3 self.abs_max_out: 902.0\n",
      "lif layer 1 self.abs_max_v: 12093.0\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss: 11.629111/ 74.558723, val:  52.08%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7610%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.1452%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 90625  12.022%\n",
      "fc layer 1 self.abs_max_out: 6784.0\n",
      "fc layer 3 self.abs_max_out: 986.0\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss: 10.513853/125.827652, val:  45.83%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6190%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.2674%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 91683  12.006%\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss: 10.953245/ 96.655899, val:  48.33%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9144%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.8264%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 92755  11.993%\n",
      "lif layer 1 self.abs_max_v: 12162.5\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss: 10.660995/ 71.975769, val:  58.75%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3313%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.7811%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 93801  11.977%\n",
      "lif layer 2 self.abs_max_v: 4147.0\n",
      "lif layer 2 self.abs_max_v: 4236.0\n",
      "lif layer 2 self.abs_max_v: 4345.0\n",
      "lif layer 1 self.abs_max_v: 12225.5\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  9.941494/ 87.242455, val:  50.42%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7694%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.2183%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 94799  11.955%\n",
      "lif layer 1 self.abs_max_v: 12254.5\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss: 10.027564/120.021080, val:  42.08%, val_best:  63.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.1100%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.5662%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 95837  11.938%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss: 10.668953/ 83.434052, val:  48.33%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.4811%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.2634%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 96905  11.926%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss: 10.526994/116.203392, val:  45.00%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3282%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.2829%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 97965  11.913%\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss: 10.918754/ 75.199722, val:  56.25%, val_best:  63.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.1860%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.8275%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 99067  11.905%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss: 10.103505/ 98.699303, val:  54.17%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.4105%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.2025%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 100122  11.892%\n",
      "fc layer 2 self.abs_max_out: 2638.0\n",
      "lif layer 2 self.abs_max_v: 4381.5\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss: 10.539499/ 55.523323, val:  67.92%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9133%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.9686%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 101185  11.880%\n",
      "lif layer 1 self.abs_max_v: 12258.0\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss: 10.040421/ 96.579208, val:  52.92%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4379%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.4574%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 102230  11.866%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss: 10.584518/ 76.737770, val:  61.25%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1366%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.2276%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 103290  11.855%\n",
      "lif layer 2 self.abs_max_v: 4646.0\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss: 11.097934/ 60.223774, val:  67.08%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3729%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.1655%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 104391  11.848%\n",
      "fc layer 1 self.abs_max_out: 6817.0\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss: 10.920052/ 54.840984, val:  69.17%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4726%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.3256%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 105451  11.837%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  9.956003/ 60.440639, val:  65.42%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1604%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.6872%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 106457  11.820%\n",
      "fc layer 2 self.abs_max_out: 2774.0\n",
      "fc layer 1 self.abs_max_out: 6920.0\n",
      "lif layer 1 self.abs_max_v: 12455.0\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss: 10.418842/ 84.588760, val:  57.92%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6897%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.0645%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 107493  11.806%\n",
      "lif layer 2 self.abs_max_v: 4678.0\n",
      "lif layer 2 self.abs_max_v: 4919.0\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  9.564528/ 92.959175, val:  53.75%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0351%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.9773%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 108529  11.793%\n",
      "fc layer 1 self.abs_max_out: 6997.0\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  9.810339/ 51.742775, val:  70.42%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9237%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.8480%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 109542  11.778%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss: 10.258403/ 60.646107, val:  70.42%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5978%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.1964%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 110627  11.771%\n",
      "fc layer 2 self.abs_max_out: 2880.0\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss: 10.428952/ 59.871685, val:  72.92%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3346%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.1757%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 111668  11.759%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  8.945644/ 75.870712, val:  57.50%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.3767%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 112630  11.739%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  9.971141/116.198074, val:  47.92%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3952%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.5702%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 113619  11.723%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  9.844079/ 72.097542, val:  60.00%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2883%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.3012%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 114609  11.707%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  9.899108/ 68.527901, val:  65.83%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8358%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.3098%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 115635  11.695%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss: 10.039442/ 67.744293, val:  62.50%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.53 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9687%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.9772%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 116681  11.685%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  9.510991/ 87.215401, val:  51.67%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3210%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.3327%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 117678  11.670%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  9.639143/ 63.876770, val:  67.50%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0083%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.0682%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 118669  11.655%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  9.500347/ 70.435959, val:  60.00%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7379%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.0705%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 119629  11.638%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  9.819832/101.718544, val:  53.75%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6099%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.9457%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 120650  11.626%\n",
      "fc layer 3 self.abs_max_out: 1010.0\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  9.599060/ 70.936356, val:  62.50%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0093%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.5577%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 121647  11.613%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  9.370860/ 85.095688, val:  55.42%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2918%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.7735%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 122650  11.600%\n",
      "fc layer 2 self.abs_max_out: 2918.0\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  9.529881/ 72.948730, val:  57.50%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0041%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.9307%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 123637  11.586%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  9.333786/ 76.308380, val:  59.58%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0266%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.0609%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 124630  11.573%\n",
      "fc layer 1 self.abs_max_out: 7064.0\n",
      "fc layer 2 self.abs_max_out: 2929.0\n",
      "fc layer 1 self.abs_max_out: 7547.0\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  9.160506/ 53.977825, val:  72.08%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9756%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.4041%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 125603  11.558%\n",
      "lif layer 2 self.abs_max_v: 4972.5\n",
      "fc layer 2 self.abs_max_out: 3053.0\n",
      "fc layer 3 self.abs_max_out: 1027.0\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  9.094722/ 73.857956, val:  61.67%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7010%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 126553  11.542%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  9.158602/ 67.206215, val:  65.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7706%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.2030%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 127494  11.525%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  9.375585/ 91.491241, val:  61.25%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2088%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.5265%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 128470  11.511%\n",
      "fc layer 2 self.abs_max_out: 3104.0\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  9.541351/ 93.052490, val:  51.67%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0161%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.5317%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 129437  11.497%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  9.572694/ 53.354969, val:  76.25%, val_best:  76.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0795%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.0177%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 130427  11.485%\n",
      "lif layer 2 self.abs_max_v: 4999.5\n",
      "lif layer 2 self.abs_max_v: 5028.0\n",
      "lif layer 2 self.abs_max_v: 5038.5\n",
      "fc layer 1 self.abs_max_out: 7589.0\n",
      "lif layer 1 self.abs_max_v: 12685.0\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  8.835767/ 66.115303, val:  66.25%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7845%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.0894%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 131376  11.470%\n",
      "lif layer 2 self.abs_max_v: 5084.5\n",
      "fc layer 2 self.abs_max_out: 3205.0\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  9.515080/ 50.763752, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3950%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.8201%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 132321  11.454%\n",
      "fc layer 3 self.abs_max_out: 1067.0\n",
      "lif layer 1 self.abs_max_v: 12975.5\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  8.767784/ 61.106537, val:  70.83%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6528%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.0934%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 133251  11.438%\n",
      "lif layer 2 self.abs_max_v: 5369.5\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  8.644443/ 66.663155, val:  62.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8365%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.2536%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 134177  11.421%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  8.730642/ 93.852570, val:  60.42%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6281%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.2049%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 135082  11.403%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  8.794548/ 67.426216, val:  63.75%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3907%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.1935%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 136007  11.387%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  8.374233/ 64.525276, val:  66.25%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3001%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.6278%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 136953  11.373%\n",
      "fc layer 1 self.abs_max_out: 7733.0\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  8.817852/ 51.821503, val:  72.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0613%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.5964%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 137870  11.357%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  8.553591/ 45.053062, val:  81.25%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4073%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1652%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 138820  11.344%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  8.382044/ 59.050682, val:  74.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5120%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.8657%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 139742  11.329%\n",
      "fc layer 3 self.abs_max_out: 1118.0\n",
      "fc layer 2 self.abs_max_out: 3269.0\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  8.459715/ 72.700836, val:  65.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2621%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.9674%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 140655  11.313%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  8.501335/ 70.834938, val:  64.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0374%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0117%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 141549  11.296%\n",
      "fc layer 1 self.abs_max_out: 7798.0\n",
      "lif layer 2 self.abs_max_v: 5375.5\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  8.376880/ 63.095570, val:  70.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9530%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.2573%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 142469  11.281%\n",
      "fc layer 2 self.abs_max_out: 3270.0\n",
      "fc layer 1 self.abs_max_out: 7906.0\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  7.845742/ 57.990250, val:  71.67%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6022%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0023%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 143362  11.264%\n",
      "lif layer 2 self.abs_max_v: 5407.0\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  8.356805/ 97.963936, val:  53.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9358%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.4524%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 144278  11.250%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  8.824519/ 51.270309, val:  75.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9251%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.4589%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 145220  11.238%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  7.258826/ 81.518333, val:  67.92%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0159%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0374%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 146063  11.218%\n",
      "fc layer 3 self.abs_max_out: 1178.0\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  8.173024/ 71.124496, val:  69.17%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5276%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.2697%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 146945  11.201%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  8.175364/ 78.051033, val:  60.42%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3669%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1568%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 147843  11.186%\n",
      "fc layer 3 self.abs_max_out: 1184.0\n",
      "fc layer 3 self.abs_max_out: 1202.0\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  7.924522/ 57.847290, val:  73.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5507%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.2688%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 148772  11.174%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  8.051208/ 53.050411, val:  77.08%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3435%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3792%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 149679  11.160%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  7.937819/ 64.065109, val:  70.42%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9083%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.9914%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 150556  11.144%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  8.179570/ 55.007744, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0276%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3637%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 151466  11.131%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  8.365985/ 73.124222, val:  70.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4764%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.9410%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 152393  11.119%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  7.874501/ 64.483658, val:  75.00%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2134%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.6930%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 153298  11.105%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  7.947974/ 60.363480, val:  70.00%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5655%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3001%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 154144  11.088%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  8.539016/ 60.126251, val:  70.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6792%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.2929%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 155086  11.078%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  7.537855/ 53.050186, val:  76.67%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2362%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.2994%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 155935  11.061%\n",
      "fc layer 1 self.abs_max_out: 8374.0\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  8.013921/ 76.582336, val:  60.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1944%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0208%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 156842  11.049%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  7.640950/ 67.655006, val:  69.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5425%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3944%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 157699  11.033%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  7.900446/ 55.677174, val:  73.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5105%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.2945%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 158571  11.019%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  7.344107/ 80.763687, val:  62.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0949%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0696%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 159399  11.001%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  8.015670/ 90.330467, val:  61.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8300%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.9074%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 160256  10.986%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  7.591771/ 72.382462, val:  67.92%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4237%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.9762%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 161088  10.970%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  8.665228/ 66.710381, val:  72.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1447%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1779%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 161953  10.955%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  8.206458/ 56.302757, val:  75.00%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4468%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.4171%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 162833  10.942%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  7.513960/ 50.103119, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9149%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.7442%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 163641  10.925%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  7.979346/ 58.598549, val:  69.17%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9911%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3612%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 164513  10.912%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  7.169386/ 51.422310, val:  77.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0797%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.2538%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 165306  10.894%\n",
      "lif layer 2 self.abs_max_v: 5528.5\n",
      "lif layer 2 self.abs_max_v: 5533.0\n",
      "lif layer 2 self.abs_max_v: 5756.0\n",
      "fc layer 2 self.abs_max_out: 3368.0\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  8.080045/ 84.387398, val:  65.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6146%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.6469%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 166193  10.882%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  7.406755/ 45.194675, val:  79.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8393%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.5875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 167024  10.867%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  7.009183/ 41.040977, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.72 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8574%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.7704%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 167832  10.850%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  6.936798/ 52.742416, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9127%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.3037%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 168650  10.834%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  7.169501/ 55.456207, val:  77.50%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8445%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0437%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 169469  10.819%\n",
      "fc layer 2 self.abs_max_out: 3389.0\n",
      "fc layer 2 self.abs_max_out: 3429.0\n",
      "fc layer 3 self.abs_max_out: 1226.0\n",
      "lif layer 2 self.abs_max_v: 5949.5\n",
      "fc layer 2 self.abs_max_out: 3451.0\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  7.410532/ 58.918968, val:  74.17%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.66 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7248%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8381%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 170274  10.803%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  7.609729/ 53.825432, val:  75.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.01 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0002%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.8042%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 171126  10.790%\n",
      "lif layer 2 self.abs_max_v: 6005.0\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  7.754644/ 76.770538, val:  64.58%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8135%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.6640%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 171939  10.775%\n",
      "fc layer 2 self.abs_max_out: 3485.0\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  7.363733/ 45.963760, val:  82.50%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8852%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2818%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 172751  10.760%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  6.547061/ 70.783669, val:  62.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8314%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.3676%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 173510  10.741%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  7.334567/ 58.096600, val:  75.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.22 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8770%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.8340%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 174338  10.728%\n",
      "lif layer 1 self.abs_max_v: 13095.5\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  7.079530/ 66.900436, val:  72.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7813%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.5944%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 175119  10.711%\n",
      "lif layer 1 self.abs_max_v: 13203.5\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  7.027506/ 78.310829, val:  61.67%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4836%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.5070%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 175931  10.697%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  6.590056/ 58.356491, val:  76.67%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2341%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.4676%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 176706  10.680%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  6.907237/ 52.490829, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1570%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.5473%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 177505  10.665%\n",
      "fc layer 3 self.abs_max_out: 1250.0\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  6.499783/ 71.307922, val:  67.92%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1141%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7057%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.8944%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 178282  10.649%\n",
      "fc layer 2 self.abs_max_out: 3508.0\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  6.935890/ 66.080597, val:  63.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1436%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.2614%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 179074  10.635%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  7.576630/ 55.061840, val:  74.17%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4479%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.7228%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 179920  10.623%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  6.946947/ 57.264587, val:  77.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.95 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2563%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.6205%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 180690  10.607%\n",
      "fc layer 2 self.abs_max_out: 3514.0\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  6.486782/ 51.361759, val:  80.00%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3740%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.1425%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 181454  10.591%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  6.330164/ 47.736187, val:  80.42%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1599%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.7511%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 182193  10.574%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  6.965523/ 51.471027, val:  75.42%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3353%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.4228%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 182981  10.560%\n",
      "fc layer 1 self.abs_max_out: 8500.0\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  6.560757/ 51.329220, val:  80.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0312%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4418%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.1814%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 183748  10.544%\n",
      "fc layer 1 self.abs_max_out: 8614.0\n",
      "lif layer 1 self.abs_max_v: 13516.5\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  6.781197/ 76.922043, val:  66.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0376%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2569%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.5650%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 184495  10.528%\n",
      "lif layer 1 self.abs_max_v: 13529.5\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  6.785260/ 59.666569, val:  70.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2060%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.7614%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 185261  10.513%\n",
      "lif layer 1 self.abs_max_v: 13910.0\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  7.114538/ 59.422447, val:  69.17%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.81 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9319%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.3890%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 186068  10.501%\n",
      "fc layer 1 self.abs_max_out: 8651.0\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  6.518507/ 64.377930, val:  70.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7805%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.1785%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 186828  10.485%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  6.716774/ 54.888092, val:  80.42%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0710%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.9717%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 187584  10.470%\n",
      "lif layer 1 self.abs_max_v: 13971.0\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  6.193417/ 78.792763, val:  57.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0426%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0811%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.3572%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 188319  10.454%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  6.171015/ 75.630028, val:  66.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0388%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.6959%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 189053  10.438%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  6.269593/ 58.471775, val:  70.00%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7643%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.6569%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 189793  10.423%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  6.458948/ 54.514503, val:  76.25%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2108%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.6180%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 190536  10.408%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  6.728222/ 61.274521, val:  75.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5217%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.3468%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 191300  10.394%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  6.306905/ 60.190823, val:  75.42%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3737%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.1232%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 192015  10.377%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  6.519876/ 46.721512, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5028%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.3172%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 192743  10.362%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  6.201875/ 65.782974, val:  65.00%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4830%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.7596%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 193471  10.347%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  6.043852/ 72.492218, val:  61.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9079%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.2610%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 194205  10.332%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  5.980974/ 48.874039, val:  78.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7713%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.1993%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 194923  10.316%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  5.778100/ 64.701927, val:  68.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0729%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.8594%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 195642  10.301%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  6.375787/ 57.555843, val:  68.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3342%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.1801%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 196357  10.286%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  5.896362/ 66.637131, val:  70.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5352%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.2037%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 197075  10.271%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  5.953043/ 48.618103, val:  77.50%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4045%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.2982%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 197809  10.256%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  6.374366/ 53.679424, val:  74.17%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.29 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4124%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.6648%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 198566  10.244%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  5.816638/ 47.469933, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.72 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2077%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.8034%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 199280  10.229%\n",
      "lif layer 1 self.abs_max_v: 13984.5\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  5.445691/ 48.167057, val:  79.58%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.49 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1587%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.9889%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "482f7a6d08204c8eab84b4d5ff957059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñà‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñà‚ñà‚ñÖ‚ñá‚ñà‚ñÖ‚ñá‚ñÜ‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñà‚ñÑ‚ñÜ‚ñà‚ñÖ‚ñÖ‚ñá‚ñà‚ñà‚ñÖ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñà‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñà‚ñà‚ñÖ‚ñá‚ñà‚ñÖ‚ñá‚ñÜ‚ñà</td></tr><tr><td>val_loss</td><td>‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÑ‚ñÉ‚ñÖ‚ñÇ‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñà‚ñà‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>5.44569</td></tr><tr><td>val_acc_best</td><td>0.84167</td></tr><tr><td>val_acc_now</td><td>0.79583</td></tr><tr><td>val_loss</td><td>48.16706</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">electric-sweep-3</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7caqf936' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7caqf936</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_010846-7caqf936/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cj2qvsbd with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.046875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0234375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_052828-cj2qvsbd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cj2qvsbd' target=\"_blank\">splendid-sweep-7</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cj2qvsbd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cj2qvsbd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251214_052836_492', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 256, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 64, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 8, 'lif_layer_v_threshold2': 16, 'init_scaling': [0.03125, 0.046875, 0.0234375], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 64, self.v_threshold 256\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 8, self.v_threshold 16\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=256, v_reset=10000, sg_width=64, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=16, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 46.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 59.5\n",
      "fc layer 1 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 69.0\n",
      "fc layer 1 self.abs_max_out: 55.0\n",
      "lif layer 1 self.abs_max_v: 75.0\n",
      "lif layer 1 self.abs_max_v: 91.5\n",
      "fc layer 1 self.abs_max_out: 63.0\n",
      "lif layer 1 self.abs_max_v: 101.5\n",
      "fc layer 1 self.abs_max_out: 86.0\n",
      "lif layer 1 self.abs_max_v: 114.0\n",
      "lif layer 1 self.abs_max_v: 121.0\n",
      "fc layer 1 self.abs_max_out: 94.0\n",
      "lif layer 1 self.abs_max_v: 129.0\n",
      "lif layer 1 self.abs_max_v: 132.5\n",
      "lif layer 1 self.abs_max_v: 133.0\n",
      "fc layer 1 self.abs_max_out: 98.0\n",
      "lif layer 1 self.abs_max_v: 133.5\n",
      "fc layer 1 self.abs_max_out: 114.0\n",
      "lif layer 1 self.abs_max_v: 173.0\n",
      "fc layer 1 self.abs_max_out: 121.0\n",
      "lif layer 1 self.abs_max_v: 177.0\n",
      "fc layer 1 self.abs_max_out: 130.0\n",
      "lif layer 1 self.abs_max_v: 194.5\n",
      "lif layer 1 self.abs_max_v: 195.0\n",
      "lif layer 1 self.abs_max_v: 204.5\n",
      "fc layer 1 self.abs_max_out: 140.0\n",
      "lif layer 1 self.abs_max_v: 205.5\n",
      "fc layer 1 self.abs_max_out: 143.0\n",
      "lif layer 1 self.abs_max_v: 230.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss:  2.302601/  2.302555, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 100.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 100.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 8820  90.092%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 1 self.abs_max_v: 234.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss:  2.302601/  2.302555, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 100.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 100.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c1e211a5ce431b8e37f69a7d374f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>iter_acc</td><td>0.0</td></tr><tr><td>tr_acc</td><td>0.09908</td></tr><tr><td>tr_epoch_loss</td><td>2.3026</td></tr><tr><td>val_acc_best</td><td>0.1</td></tr><tr><td>val_acc_now</td><td>0.1</td></tr><tr><td>val_loss</td><td>2.30256</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">splendid-sweep-7</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cj2qvsbd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cj2qvsbd</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_052828-cj2qvsbd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run cj2qvsbd errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31401/1959304274.py\", line 119, in hyper_iter\n",
      "    my_snn_system(\n",
      "  File \"/tmp/ipykernel_31401/3596498357.py\", line 929, in my_snn_system\n",
      "    assert val_acc_best > 0.2\n",
      "AssertionError\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run cj2qvsbd errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31401/1959304274.py\", line 119, in hyper_iter\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     my_snn_system(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31401/3596498357.py\", line 929, in my_snn_system\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     assert val_acc_best > 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m AssertionError\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 20t1cjpr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.046875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0234375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_053126-20t1cjpr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/20t1cjpr' target=\"_blank\">young-sweep-8</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/20t1cjpr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/20t1cjpr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251214_053134_226', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 64, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 0.5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 8, 'lif_layer_v_threshold2': 16, 'init_scaling': [0.03125, 0.046875, 0.0234375], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 0.5, self.v_threshold 64\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 8, self.v_threshold 16\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=64, v_reset=10000, sg_width=0.5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=16, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 46.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 59.5\n",
      "fc layer 1 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 69.0\n",
      "fc layer 2 self.abs_max_out: 6.0\n",
      "lif layer 2 self.abs_max_v: 6.0\n",
      "fc layer 1 self.abs_max_out: 62.0\n",
      "lif layer 1 self.abs_max_v: 90.0\n",
      "fc layer 2 self.abs_max_out: 29.0\n",
      "lif layer 2 self.abs_max_v: 29.0\n",
      "fc layer 3 self.abs_max_out: 7.0\n",
      "fc layer 1 self.abs_max_out: 101.0\n",
      "lif layer 1 self.abs_max_v: 138.0\n",
      "fc layer 2 self.abs_max_out: 39.0\n",
      "lif layer 2 self.abs_max_v: 43.5\n",
      "fc layer 3 self.abs_max_out: 14.0\n",
      "fc layer 1 self.abs_max_out: 147.0\n",
      "lif layer 1 self.abs_max_v: 210.0\n",
      "fc layer 2 self.abs_max_out: 49.0\n",
      "lif layer 2 self.abs_max_v: 59.5\n",
      "fc layer 3 self.abs_max_out: 33.0\n",
      "fc layer 1 self.abs_max_out: 200.0\n",
      "lif layer 1 self.abs_max_v: 235.0\n",
      "fc layer 2 self.abs_max_out: 53.0\n",
      "lif layer 2 self.abs_max_v: 80.0\n",
      "fc layer 1 self.abs_max_out: 212.0\n",
      "lif layer 1 self.abs_max_v: 301.5\n",
      "lif layer 2 self.abs_max_v: 88.0\n",
      "fc layer 1 self.abs_max_out: 234.0\n",
      "lif layer 1 self.abs_max_v: 310.0\n",
      "fc layer 2 self.abs_max_out: 64.0\n",
      "lif layer 2 self.abs_max_v: 108.0\n",
      "fc layer 3 self.abs_max_out: 39.0\n",
      "fc layer 1 self.abs_max_out: 255.0\n",
      "lif layer 1 self.abs_max_v: 357.0\n",
      "fc layer 3 self.abs_max_out: 42.0\n",
      "lif layer 1 self.abs_max_v: 363.5\n",
      "fc layer 1 self.abs_max_out: 264.0\n",
      "fc layer 3 self.abs_max_out: 47.0\n",
      "fc layer 2 self.abs_max_out: 74.0\n",
      "lif layer 2 self.abs_max_v: 111.5\n",
      "fc layer 3 self.abs_max_out: 51.0\n",
      "fc layer 2 self.abs_max_out: 82.0\n",
      "fc layer 2 self.abs_max_out: 93.0\n",
      "fc layer 3 self.abs_max_out: 54.0\n",
      "lif layer 2 self.abs_max_v: 128.5\n",
      "fc layer 3 self.abs_max_out: 71.0\n",
      "lif layer 2 self.abs_max_v: 129.0\n",
      "lif layer 2 self.abs_max_v: 131.5\n",
      "lif layer 1 self.abs_max_v: 401.0\n",
      "lif layer 2 self.abs_max_v: 145.0\n",
      "lif layer 1 self.abs_max_v: 456.5\n",
      "fc layer 2 self.abs_max_out: 94.0\n",
      "lif layer 2 self.abs_max_v: 146.5\n",
      "fc layer 2 self.abs_max_out: 95.0\n",
      "lif layer 2 self.abs_max_v: 152.5\n",
      "lif layer 2 self.abs_max_v: 156.0\n",
      "fc layer 2 self.abs_max_out: 108.0\n",
      "fc layer 1 self.abs_max_out: 269.0\n",
      "fc layer 3 self.abs_max_out: 86.0\n",
      "fc layer 1 self.abs_max_out: 280.0\n",
      "fc layer 1 self.abs_max_out: 426.0\n",
      "fc layer 2 self.abs_max_out: 109.0\n",
      "lif layer 2 self.abs_max_v: 168.5\n",
      "fc layer 2 self.abs_max_out: 117.0\n",
      "lif layer 2 self.abs_max_v: 177.0\n",
      "lif layer 2 self.abs_max_v: 189.0\n",
      "fc layer 2 self.abs_max_out: 140.0\n",
      "lif layer 2 self.abs_max_v: 204.5\n",
      "lif layer 2 self.abs_max_v: 206.0\n",
      "fc layer 1 self.abs_max_out: 459.0\n",
      "lif layer 1 self.abs_max_v: 459.0\n",
      "fc layer 3 self.abs_max_out: 90.0\n",
      "fc layer 3 self.abs_max_out: 102.0\n",
      "fc layer 3 self.abs_max_out: 109.0\n",
      "fc layer 2 self.abs_max_out: 172.0\n",
      "lif layer 2 self.abs_max_v: 212.5\n",
      "fc layer 2 self.abs_max_out: 179.0\n",
      "lif layer 1 self.abs_max_v: 493.0\n",
      "lif layer 1 self.abs_max_v: 495.5\n",
      "lif layer 1 self.abs_max_v: 573.5\n",
      "lif layer 2 self.abs_max_v: 219.5\n",
      "lif layer 2 self.abs_max_v: 227.5\n",
      "lif layer 2 self.abs_max_v: 255.0\n",
      "fc layer 3 self.abs_max_out: 127.0\n",
      "lif layer 2 self.abs_max_v: 255.5\n",
      "fc layer 3 self.abs_max_out: 130.0\n",
      "lif layer 2 self.abs_max_v: 261.5\n",
      "lif layer 2 self.abs_max_v: 276.0\n",
      "fc layer 2 self.abs_max_out: 199.0\n",
      "fc layer 2 self.abs_max_out: 203.0\n",
      "fc layer 2 self.abs_max_out: 205.0\n",
      "fc layer 2 self.abs_max_out: 210.0\n",
      "lif layer 2 self.abs_max_v: 299.5\n",
      "lif layer 1 self.abs_max_v: 644.5\n",
      "lif layer 2 self.abs_max_v: 320.0\n",
      "fc layer 2 self.abs_max_out: 232.0\n",
      "fc layer 2 self.abs_max_out: 244.0\n",
      "lif layer 2 self.abs_max_v: 397.0\n",
      "lif layer 2 self.abs_max_v: 401.5\n",
      "lif layer 2 self.abs_max_v: 431.5\n",
      "lif layer 1 self.abs_max_v: 726.5\n",
      "fc layer 2 self.abs_max_out: 245.0\n",
      "lif layer 2 self.abs_max_v: 461.0\n",
      "fc layer 2 self.abs_max_out: 251.0\n",
      "lif layer 2 self.abs_max_v: 461.5\n",
      "fc layer 2 self.abs_max_out: 261.0\n",
      "fc layer 3 self.abs_max_out: 137.0\n",
      "fc layer 2 self.abs_max_out: 263.0\n",
      "fc layer 3 self.abs_max_out: 140.0\n",
      "lif layer 1 self.abs_max_v: 750.5\n",
      "fc layer 2 self.abs_max_out: 274.0\n",
      "fc layer 3 self.abs_max_out: 201.0\n",
      "fc layer 2 self.abs_max_out: 277.0\n",
      "fc layer 3 self.abs_max_out: 207.0\n",
      "fc layer 2 self.abs_max_out: 287.0\n",
      "fc layer 1 self.abs_max_out: 464.0\n",
      "fc layer 2 self.abs_max_out: 304.0\n",
      "lif layer 2 self.abs_max_v: 473.0\n",
      "lif layer 2 self.abs_max_v: 498.5\n",
      "fc layer 2 self.abs_max_out: 315.0\n",
      "lif layer 2 self.abs_max_v: 511.0\n",
      "lif layer 2 self.abs_max_v: 515.5\n",
      "fc layer 2 self.abs_max_out: 368.0\n",
      "lif layer 2 self.abs_max_v: 518.0\n",
      "lif layer 2 self.abs_max_v: 522.0\n",
      "fc layer 2 self.abs_max_out: 379.0\n",
      "lif layer 2 self.abs_max_v: 597.0\n",
      "lif layer 2 self.abs_max_v: 600.5\n",
      "lif layer 1 self.abs_max_v: 773.0\n",
      "lif layer 2 self.abs_max_v: 655.5\n",
      "fc layer 1 self.abs_max_out: 516.0\n",
      "fc layer 1 self.abs_max_out: 531.0\n",
      "lif layer 1 self.abs_max_v: 912.0\n",
      "fc layer 2 self.abs_max_out: 390.0\n",
      "lif layer 2 self.abs_max_v: 659.0\n",
      "fc layer 1 self.abs_max_out: 611.0\n",
      "lif layer 1 self.abs_max_v: 943.0\n",
      "lif layer 2 self.abs_max_v: 684.0\n",
      "lif layer 2 self.abs_max_v: 686.5\n",
      "lif layer 2 self.abs_max_v: 697.0\n",
      "lif layer 1 self.abs_max_v: 947.5\n",
      "fc layer 2 self.abs_max_out: 393.0\n",
      "fc layer 2 self.abs_max_out: 420.0\n",
      "fc layer 1 self.abs_max_out: 738.0\n",
      "lif layer 1 self.abs_max_v: 1184.0\n",
      "fc layer 2 self.abs_max_out: 434.0\n",
      "lif layer 1 self.abs_max_v: 1236.5\n",
      "fc layer 2 self.abs_max_out: 448.0\n",
      "fc layer 3 self.abs_max_out: 215.0\n",
      "fc layer 3 self.abs_max_out: 216.0\n",
      "lif layer 2 self.abs_max_v: 711.5\n",
      "lif layer 2 self.abs_max_v: 746.0\n",
      "lif layer 2 self.abs_max_v: 751.5\n",
      "fc layer 2 self.abs_max_out: 453.0\n",
      "fc layer 2 self.abs_max_out: 464.0\n",
      "lif layer 2 self.abs_max_v: 767.5\n",
      "fc layer 2 self.abs_max_out: 468.0\n",
      "fc layer 2 self.abs_max_out: 482.0\n",
      "fc layer 3 self.abs_max_out: 241.0\n",
      "fc layer 3 self.abs_max_out: 244.0\n",
      "fc layer 3 self.abs_max_out: 254.0\n",
      "fc layer 1 self.abs_max_out: 755.0\n",
      "fc layer 1 self.abs_max_out: 773.0\n",
      "fc layer 3 self.abs_max_out: 268.0\n",
      "fc layer 1 self.abs_max_out: 865.0\n",
      "fc layer 1 self.abs_max_out: 972.0\n",
      "fc layer 1 self.abs_max_out: 985.0\n",
      "fc layer 1 self.abs_max_out: 1006.0\n",
      "lif layer 1 self.abs_max_v: 1429.0\n",
      "fc layer 2 self.abs_max_out: 506.0\n",
      "lif layer 1 self.abs_max_v: 1630.5\n",
      "fc layer 2 self.abs_max_out: 531.0\n",
      "fc layer 1 self.abs_max_out: 1072.0\n",
      "fc layer 1 self.abs_max_out: 1125.0\n",
      "fc layer 1 self.abs_max_out: 1200.0\n",
      "fc layer 1 self.abs_max_out: 1246.0\n",
      "fc layer 2 self.abs_max_out: 585.0\n",
      "fc layer 2 self.abs_max_out: 591.0\n",
      "fc layer 2 self.abs_max_out: 592.0\n",
      "lif layer 2 self.abs_max_v: 775.5\n",
      "lif layer 2 self.abs_max_v: 787.5\n",
      "lif layer 2 self.abs_max_v: 829.5\n",
      "lif layer 2 self.abs_max_v: 847.5\n",
      "lif layer 2 self.abs_max_v: 866.0\n",
      "fc layer 1 self.abs_max_out: 1260.0\n",
      "fc layer 1 self.abs_max_out: 1314.0\n",
      "lif layer 2 self.abs_max_v: 874.0\n",
      "lif layer 2 self.abs_max_v: 879.0\n",
      "lif layer 2 self.abs_max_v: 881.5\n",
      "lif layer 2 self.abs_max_v: 883.0\n",
      "lif layer 2 self.abs_max_v: 883.5\n",
      "lif layer 1 self.abs_max_v: 1739.5\n",
      "fc layer 1 self.abs_max_out: 1505.0\n",
      "fc layer 1 self.abs_max_out: 1566.0\n",
      "fc layer 1 self.abs_max_out: 1615.0\n",
      "fc layer 1 self.abs_max_out: 1632.0\n",
      "lif layer 2 self.abs_max_v: 901.5\n",
      "lif layer 1 self.abs_max_v: 1892.0\n",
      "lif layer 1 self.abs_max_v: 1940.0\n",
      "fc layer 2 self.abs_max_out: 609.0\n",
      "fc layer 2 self.abs_max_out: 654.0\n",
      "fc layer 1 self.abs_max_out: 1668.0\n",
      "fc layer 1 self.abs_max_out: 1679.0\n",
      "fc layer 1 self.abs_max_out: 1696.0\n",
      "fc layer 1 self.abs_max_out: 1781.0\n",
      "fc layer 1 self.abs_max_out: 1840.0\n",
      "lif layer 2 self.abs_max_v: 919.5\n",
      "lif layer 2 self.abs_max_v: 966.0\n",
      "fc layer 2 self.abs_max_out: 663.0\n",
      "lif layer 1 self.abs_max_v: 1966.0\n",
      "fc layer 2 self.abs_max_out: 683.0\n",
      "fc layer 2 self.abs_max_out: 698.0\n",
      "fc layer 1 self.abs_max_out: 2097.0\n",
      "lif layer 1 self.abs_max_v: 2097.0\n",
      "fc layer 1 self.abs_max_out: 2147.0\n",
      "lif layer 1 self.abs_max_v: 2147.0\n",
      "lif layer 1 self.abs_max_v: 2153.5\n",
      "fc layer 2 self.abs_max_out: 738.0\n",
      "lif layer 1 self.abs_max_v: 2295.5\n",
      "lif layer 1 self.abs_max_v: 2380.5\n",
      "lif layer 2 self.abs_max_v: 1010.5\n",
      "fc layer 1 self.abs_max_out: 2178.0\n",
      "fc layer 1 self.abs_max_out: 2215.0\n",
      "fc layer 3 self.abs_max_out: 270.0\n",
      "fc layer 2 self.abs_max_out: 758.0\n",
      "fc layer 3 self.abs_max_out: 276.0\n",
      "fc layer 1 self.abs_max_out: 2223.0\n",
      "fc layer 1 self.abs_max_out: 2324.0\n",
      "fc layer 1 self.abs_max_out: 2372.0\n",
      "fc layer 1 self.abs_max_out: 2465.0\n",
      "lif layer 1 self.abs_max_v: 2465.0\n",
      "fc layer 1 self.abs_max_out: 2532.0\n",
      "lif layer 1 self.abs_max_v: 2532.0\n",
      "fc layer 1 self.abs_max_out: 2553.0\n",
      "lif layer 1 self.abs_max_v: 2569.5\n",
      "lif layer 2 self.abs_max_v: 1047.0\n",
      "lif layer 2 self.abs_max_v: 1086.0\n",
      "lif layer 2 self.abs_max_v: 1092.0\n",
      "fc layer 3 self.abs_max_out: 286.0\n",
      "fc layer 3 self.abs_max_out: 291.0\n",
      "lif layer 2 self.abs_max_v: 1103.0\n",
      "lif layer 2 self.abs_max_v: 1120.5\n",
      "fc layer 1 self.abs_max_out: 2577.0\n",
      "lif layer 1 self.abs_max_v: 2577.0\n",
      "fc layer 1 self.abs_max_out: 2609.0\n",
      "lif layer 1 self.abs_max_v: 2609.0\n",
      "fc layer 1 self.abs_max_out: 2689.0\n",
      "lif layer 1 self.abs_max_v: 2689.0\n",
      "lif layer 1 self.abs_max_v: 3081.5\n",
      "lif layer 1 self.abs_max_v: 3705.0\n",
      "lif layer 1 self.abs_max_v: 3784.5\n",
      "lif layer 2 self.abs_max_v: 1228.0\n",
      "lif layer 2 self.abs_max_v: 1306.0\n",
      "lif layer 2 self.abs_max_v: 1361.0\n",
      "lif layer 2 self.abs_max_v: 1405.0\n",
      "fc layer 1 self.abs_max_out: 2692.0\n",
      "lif layer 1 self.abs_max_v: 3885.5\n",
      "lif layer 1 self.abs_max_v: 4265.5\n",
      "lif layer 1 self.abs_max_v: 4301.5\n",
      "fc layer 1 self.abs_max_out: 2721.0\n",
      "fc layer 1 self.abs_max_out: 2806.0\n",
      "fc layer 1 self.abs_max_out: 2871.0\n",
      "fc layer 3 self.abs_max_out: 297.0\n",
      "fc layer 3 self.abs_max_out: 308.0\n",
      "fc layer 1 self.abs_max_out: 3035.0\n",
      "fc layer 1 self.abs_max_out: 3101.0\n",
      "fc layer 1 self.abs_max_out: 3267.0\n",
      "fc layer 2 self.abs_max_out: 790.0\n",
      "lif layer 2 self.abs_max_v: 1425.5\n",
      "lif layer 1 self.abs_max_v: 4354.0\n",
      "lif layer 1 self.abs_max_v: 4454.0\n",
      "fc layer 3 self.abs_max_out: 315.0\n",
      "fc layer 1 self.abs_max_out: 3286.0\n",
      "fc layer 1 self.abs_max_out: 3318.0\n",
      "lif layer 2 self.abs_max_v: 1434.5\n",
      "lif layer 2 self.abs_max_v: 1458.5\n",
      "fc layer 2 self.abs_max_out: 820.0\n",
      "fc layer 2 self.abs_max_out: 823.0\n",
      "fc layer 1 self.abs_max_out: 3328.0\n",
      "fc layer 1 self.abs_max_out: 3355.0\n",
      "fc layer 1 self.abs_max_out: 3376.0\n",
      "fc layer 1 self.abs_max_out: 3458.0\n",
      "lif layer 2 self.abs_max_v: 1504.5\n",
      "lif layer 2 self.abs_max_v: 1562.5\n",
      "lif layer 2 self.abs_max_v: 1591.5\n",
      "lif layer 2 self.abs_max_v: 1594.5\n",
      "fc layer 3 self.abs_max_out: 326.0\n",
      "lif layer 1 self.abs_max_v: 4467.0\n",
      "fc layer 2 self.abs_max_out: 885.0\n",
      "fc layer 2 self.abs_max_out: 902.0\n",
      "fc layer 2 self.abs_max_out: 930.0\n",
      "lif layer 1 self.abs_max_v: 4836.0\n",
      "lif layer 1 self.abs_max_v: 4937.5\n",
      "lif layer 1 self.abs_max_v: 4951.0\n",
      "fc layer 3 self.abs_max_out: 331.0\n",
      "fc layer 3 self.abs_max_out: 339.0\n",
      "fc layer 2 self.abs_max_out: 978.0\n",
      "fc layer 1 self.abs_max_out: 3520.0\n",
      "fc layer 3 self.abs_max_out: 340.0\n",
      "fc layer 1 self.abs_max_out: 3667.0\n",
      "fc layer 1 self.abs_max_out: 3733.0\n",
      "fc layer 1 self.abs_max_out: 3852.0\n",
      "fc layer 1 self.abs_max_out: 3911.0\n",
      "lif layer 1 self.abs_max_v: 5168.5\n",
      "lif layer 1 self.abs_max_v: 5326.0\n",
      "lif layer 1 self.abs_max_v: 5543.5\n",
      "fc layer 2 self.abs_max_out: 999.0\n",
      "lif layer 1 self.abs_max_v: 6218.0\n",
      "lif layer 1 self.abs_max_v: 6861.0\n",
      "fc layer 1 self.abs_max_out: 3926.0\n",
      "fc layer 1 self.abs_max_out: 4022.0\n",
      "fc layer 1 self.abs_max_out: 4105.0\n",
      "fc layer 1 self.abs_max_out: 4114.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss:  9.586651/100.695572, val:  25.42%, val_best:  25.42%, tr:  98.77%, tr_best:  98.77%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.1433%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.9265%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 1385  14.147%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 342.0\n",
      "lif layer 2 self.abs_max_v: 1628.0\n",
      "lif layer 2 self.abs_max_v: 1644.0\n",
      "fc layer 3 self.abs_max_out: 355.0\n",
      "fc layer 3 self.abs_max_out: 365.0\n",
      "fc layer 1 self.abs_max_out: 4180.0\n",
      "lif layer 2 self.abs_max_v: 1658.5\n",
      "lif layer 2 self.abs_max_v: 1723.5\n",
      "lif layer 2 self.abs_max_v: 1750.0\n",
      "lif layer 2 self.abs_max_v: 1763.0\n",
      "lif layer 2 self.abs_max_v: 1769.5\n",
      "lif layer 2 self.abs_max_v: 1773.0\n",
      "fc layer 3 self.abs_max_out: 381.0\n",
      "lif layer 1 self.abs_max_v: 6864.0\n",
      "lif layer 1 self.abs_max_v: 7147.0\n",
      "lif layer 1 self.abs_max_v: 7167.0\n",
      "fc layer 3 self.abs_max_out: 399.0\n",
      "fc layer 3 self.abs_max_out: 410.0\n",
      "fc layer 1 self.abs_max_out: 4263.0\n",
      "fc layer 1 self.abs_max_out: 4275.0\n",
      "fc layer 2 self.abs_max_out: 1000.0\n",
      "lif layer 2 self.abs_max_v: 1788.0\n",
      "lif layer 2 self.abs_max_v: 1806.0\n",
      "lif layer 2 self.abs_max_v: 1833.0\n",
      "lif layer 2 self.abs_max_v: 1847.5\n",
      "lif layer 2 self.abs_max_v: 1855.0\n",
      "lif layer 2 self.abs_max_v: 1858.5\n",
      "lif layer 2 self.abs_max_v: 1860.5\n",
      "lif layer 2 self.abs_max_v: 1920.5\n",
      "lif layer 2 self.abs_max_v: 1951.5\n",
      "lif layer 2 self.abs_max_v: 1967.0\n",
      "fc layer 2 self.abs_max_out: 1013.0\n",
      "fc layer 2 self.abs_max_out: 1051.0\n",
      "lif layer 2 self.abs_max_v: 1969.5\n",
      "lif layer 2 self.abs_max_v: 2036.0\n",
      "lif layer 2 self.abs_max_v: 2069.0\n",
      "lif layer 2 self.abs_max_v: 2085.5\n",
      "lif layer 2 self.abs_max_v: 2094.0\n",
      "lif layer 2 self.abs_max_v: 2098.0\n",
      "lif layer 2 self.abs_max_v: 2100.0\n",
      "fc layer 2 self.abs_max_out: 1071.0\n",
      "lif layer 2 self.abs_max_v: 2109.0\n",
      "lif layer 2 self.abs_max_v: 2125.5\n",
      "lif layer 2 self.abs_max_v: 2134.0\n",
      "lif layer 2 self.abs_max_v: 2138.0\n",
      "lif layer 2 self.abs_max_v: 2140.0\n",
      "fc layer 2 self.abs_max_out: 1086.0\n",
      "fc layer 2 self.abs_max_out: 1111.0\n",
      "fc layer 1 self.abs_max_out: 4315.0\n",
      "lif layer 2 self.abs_max_v: 2153.0\n",
      "fc layer 1 self.abs_max_out: 4436.0\n",
      "lif layer 2 self.abs_max_v: 2162.5\n",
      "lif layer 2 self.abs_max_v: 2192.5\n",
      "fc layer 2 self.abs_max_out: 1142.0\n",
      "lif layer 2 self.abs_max_v: 2193.0\n",
      "fc layer 2 self.abs_max_out: 1155.0\n",
      "fc layer 2 self.abs_max_out: 1191.0\n",
      "lif layer 2 self.abs_max_v: 2229.0\n",
      "lif layer 2 self.abs_max_v: 2305.5\n",
      "lif layer 2 self.abs_max_v: 2344.0\n",
      "lif layer 2 self.abs_max_v: 2363.0\n",
      "fc layer 2 self.abs_max_out: 1210.0\n",
      "lif layer 2 self.abs_max_v: 2382.5\n",
      "lif layer 2 self.abs_max_v: 2401.5\n",
      "lif layer 2 self.abs_max_v: 2411.0\n",
      "lif layer 2 self.abs_max_v: 2415.5\n",
      "lif layer 2 self.abs_max_v: 2418.0\n",
      "fc layer 2 self.abs_max_out: 1230.0\n",
      "lif layer 2 self.abs_max_v: 2432.0\n",
      "lif layer 2 self.abs_max_v: 2442.0\n",
      "lif layer 2 self.abs_max_v: 2451.0\n",
      "lif layer 2 self.abs_max_v: 2455.5\n",
      "lif layer 2 self.abs_max_v: 2458.0\n",
      "fc layer 1 self.abs_max_out: 4505.0\n",
      "fc layer 2 self.abs_max_out: 1265.0\n",
      "fc layer 1 self.abs_max_out: 4514.0\n",
      "lif layer 2 self.abs_max_v: 2476.5\n",
      "lif layer 2 self.abs_max_v: 2503.5\n",
      "lif layer 1 self.abs_max_v: 7521.5\n",
      "fc layer 1 self.abs_max_out: 4690.0\n",
      "lif layer 1 self.abs_max_v: 7866.5\n",
      "lif layer 1 self.abs_max_v: 8011.5\n",
      "lif layer 1 self.abs_max_v: 8129.0\n",
      "fc layer 3 self.abs_max_out: 427.0\n",
      "fc layer 2 self.abs_max_out: 1272.0\n",
      "fc layer 1 self.abs_max_out: 4873.0\n",
      "fc layer 1 self.abs_max_out: 5080.0\n",
      "fc layer 1 self.abs_max_out: 5093.0\n",
      "fc layer 2 self.abs_max_out: 1307.0\n",
      "lif layer 2 self.abs_max_v: 2512.5\n",
      "lif layer 2 self.abs_max_v: 2524.5\n",
      "lif layer 2 self.abs_max_v: 2534.5\n",
      "lif layer 2 self.abs_max_v: 2539.5\n",
      "lif layer 2 self.abs_max_v: 2542.0\n",
      "fc layer 1 self.abs_max_out: 5147.0\n",
      "fc layer 1 self.abs_max_out: 5248.0\n",
      "fc layer 1 self.abs_max_out: 5483.0\n",
      "lif layer 1 self.abs_max_v: 8545.0\n",
      "fc layer 1 self.abs_max_out: 5814.0\n",
      "lif layer 2 self.abs_max_v: 2546.5\n",
      "lif layer 2 self.abs_max_v: 2560.5\n",
      "lif layer 2 self.abs_max_v: 2567.5\n",
      "lif layer 1 self.abs_max_v: 8985.5\n",
      "lif layer 1 self.abs_max_v: 9895.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 11.013737/ 90.529602, val:  23.33%, val_best:  25.42%, tr:  98.88%, tr_best:  98.88%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.3591%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.8811%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 2718  13.882%\n",
      "fc layer 2 self.abs_max_out: 1329.0\n",
      "lif layer 2 self.abs_max_v: 2570.0\n",
      "lif layer 2 self.abs_max_v: 2577.0\n",
      "lif layer 2 self.abs_max_v: 2580.5\n",
      "lif layer 2 self.abs_max_v: 2582.5\n",
      "fc layer 2 self.abs_max_out: 1333.0\n",
      "fc layer 2 self.abs_max_out: 1349.0\n",
      "lif layer 2 self.abs_max_v: 2632.5\n",
      "lif layer 2 self.abs_max_v: 2665.5\n",
      "fc layer 1 self.abs_max_out: 5888.0\n",
      "fc layer 1 self.abs_max_out: 6013.0\n",
      "fc layer 3 self.abs_max_out: 445.0\n",
      "fc layer 3 self.abs_max_out: 448.0\n",
      "fc layer 1 self.abs_max_out: 6087.0\n",
      "fc layer 2 self.abs_max_out: 1350.0\n",
      "lif layer 2 self.abs_max_v: 2680.5\n",
      "lif layer 2 self.abs_max_v: 2690.5\n",
      "fc layer 2 self.abs_max_out: 1387.0\n",
      "lif layer 2 self.abs_max_v: 2695.0\n",
      "lif layer 2 self.abs_max_v: 2697.5\n",
      "lif layer 1 self.abs_max_v: 10477.0\n",
      "lif layer 1 self.abs_max_v: 11082.5\n",
      "lif layer 1 self.abs_max_v: 11511.5\n",
      "fc layer 1 self.abs_max_out: 6497.0\n",
      "fc layer 2 self.abs_max_out: 1408.0\n",
      "fc layer 1 self.abs_max_out: 6571.0\n",
      "fc layer 1 self.abs_max_out: 6617.0\n",
      "lif layer 1 self.abs_max_v: 12095.5\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 11.034542/ 64.509514, val:  32.08%, val_best:  32.08%, tr:  99.28%, tr_best:  99.28%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.3494%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.7872%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 4040  13.756%\n",
      "fc layer 2 self.abs_max_out: 1413.0\n",
      "fc layer 1 self.abs_max_out: 6847.0\n",
      "lif layer 2 self.abs_max_v: 2712.0\n",
      "lif layer 2 self.abs_max_v: 2722.0\n",
      "lif layer 2 self.abs_max_v: 2727.0\n",
      "fc layer 2 self.abs_max_out: 1415.0\n",
      "fc layer 2 self.abs_max_out: 1453.0\n",
      "lif layer 1 self.abs_max_v: 12141.0\n",
      "lif layer 2 self.abs_max_v: 2796.5\n",
      "lif layer 2 self.abs_max_v: 2851.5\n",
      "lif layer 2 self.abs_max_v: 2875.0\n",
      "fc layer 1 self.abs_max_out: 6857.0\n",
      "fc layer 1 self.abs_max_out: 7128.0\n",
      "fc layer 2 self.abs_max_out: 1461.0\n",
      "fc layer 2 self.abs_max_out: 1493.0\n",
      "lif layer 2 self.abs_max_v: 2896.5\n",
      "lif layer 2 self.abs_max_v: 2941.5\n",
      "lif layer 2 self.abs_max_v: 2964.0\n",
      "lif layer 2 self.abs_max_v: 2975.0\n",
      "lif layer 2 self.abs_max_v: 2980.5\n",
      "fc layer 1 self.abs_max_out: 7206.0\n",
      "fc layer 1 self.abs_max_out: 7455.0\n",
      "lif layer 1 self.abs_max_v: 12480.0\n",
      "fc layer 1 self.abs_max_out: 7494.0\n",
      "lif layer 1 self.abs_max_v: 13734.0\n",
      "lif layer 1 self.abs_max_v: 13808.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss: 11.338596/ 92.199913, val:  25.83%, val_best:  32.08%, tr:  98.98%, tr_best:  99.28%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.7642%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.0882%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 5367  13.705%\n",
      "fc layer 3 self.abs_max_out: 472.0\n",
      "fc layer 3 self.abs_max_out: 478.0\n",
      "fc layer 1 self.abs_max_out: 7872.0\n",
      "fc layer 2 self.abs_max_out: 1539.0\n",
      "fc layer 3 self.abs_max_out: 490.0\n",
      "fc layer 2 self.abs_max_out: 1606.0\n",
      "fc layer 3 self.abs_max_out: 538.0\n",
      "fc layer 3 self.abs_max_out: 541.0\n",
      "lif layer 2 self.abs_max_v: 2981.0\n",
      "lif layer 2 self.abs_max_v: 2985.0\n",
      "fc layer 2 self.abs_max_out: 1695.0\n",
      "lif layer 2 self.abs_max_v: 3010.5\n",
      "lif layer 1 self.abs_max_v: 14082.5\n",
      "lif layer 1 self.abs_max_v: 14220.0\n",
      "lif layer 2 self.abs_max_v: 3013.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss: 11.602185/ 91.572197, val:  28.33%, val_best:  32.08%, tr:  99.39%, tr_best:  99.39%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.5363%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.4436%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 6700  13.687%\n",
      "fc layer 2 self.abs_max_out: 1836.0\n",
      "lif layer 2 self.abs_max_v: 3036.5\n",
      "lif layer 2 self.abs_max_v: 3089.5\n",
      "lif layer 2 self.abs_max_v: 3116.0\n",
      "lif layer 2 self.abs_max_v: 3123.0\n",
      "fc layer 1 self.abs_max_out: 8550.0\n",
      "lif layer 1 self.abs_max_v: 15295.0\n",
      "lif layer 1 self.abs_max_v: 15564.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss: 12.093143/ 61.892879, val:  36.25%, val_best:  36.25%, tr:  99.28%, tr_best:  99.39%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0678%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.2427%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 8016  13.647%\n",
      "fc layer 1 self.abs_max_out: 8656.0\n",
      "fc layer 1 self.abs_max_out: 8777.0\n",
      "lif layer 1 self.abs_max_v: 16011.5\n",
      "fc layer 1 self.abs_max_out: 9081.0\n",
      "lif layer 1 self.abs_max_v: 16615.5\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss: 11.725549/ 80.263008, val:  43.75%, val_best:  43.75%, tr:  98.37%, tr_best:  99.39%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0583%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.8034%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 9262  13.515%\n",
      "fc layer 2 self.abs_max_out: 1851.0\n",
      "fc layer 3 self.abs_max_out: 543.0\n",
      "fc layer 2 self.abs_max_out: 1891.0\n",
      "lif layer 2 self.abs_max_v: 3150.0\n",
      "fc layer 1 self.abs_max_out: 9087.0\n",
      "lif layer 1 self.abs_max_v: 16676.5\n",
      "fc layer 1 self.abs_max_out: 9097.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss: 10.865714/ 95.729256, val:  38.75%, val_best:  43.75%, tr:  98.77%, tr_best:  99.39%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2934%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.8007%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 10432  13.320%\n",
      "lif layer 2 self.abs_max_v: 3158.5\n",
      "fc layer 3 self.abs_max_out: 614.0\n",
      "lif layer 2 self.abs_max_v: 3201.0\n",
      "lif layer 2 self.abs_max_v: 3281.5\n",
      "fc layer 1 self.abs_max_out: 9393.0\n",
      "fc layer 1 self.abs_max_out: 9540.0\n",
      "lif layer 1 self.abs_max_v: 17506.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss: 12.419036/ 81.553360, val:  41.67%, val_best:  43.75%, tr:  98.67%, tr_best:  99.39%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7715%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.3431%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 11731  13.314%\n",
      "fc layer 3 self.abs_max_out: 630.0\n",
      "fc layer 3 self.abs_max_out: 678.0\n",
      "fc layer 3 self.abs_max_out: 697.0\n",
      "fc layer 3 self.abs_max_out: 787.0\n",
      "fc layer 1 self.abs_max_out: 9596.0\n",
      "fc layer 1 self.abs_max_out: 9840.0\n",
      "lif layer 1 self.abs_max_v: 17967.0\n",
      "fc layer 1 self.abs_max_out: 9843.0\n",
      "lif layer 2 self.abs_max_v: 3295.0\n",
      "lif layer 2 self.abs_max_v: 3378.5\n",
      "lif layer 2 self.abs_max_v: 3420.5\n",
      "lif layer 2 self.abs_max_v: 3441.5\n",
      "lif layer 2 self.abs_max_v: 3452.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss: 12.430990/ 96.731262, val:  41.67%, val_best:  43.75%, tr:  99.49%, tr_best:  99.49%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4467%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.2601%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 12968  13.246%\n",
      "fc layer 2 self.abs_max_out: 1919.0\n",
      "fc layer 1 self.abs_max_out: 9991.0\n",
      "lif layer 1 self.abs_max_v: 18225.5\n",
      "fc layer 1 self.abs_max_out: 10098.0\n",
      "fc layer 1 self.abs_max_out: 10542.0\n",
      "lif layer 1 self.abs_max_v: 19334.0\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss: 12.378377/ 83.644379, val:  37.92%, val_best:  43.75%, tr:  98.57%, tr_best:  99.49%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6838%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.3967%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 14196  13.182%\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss: 12.970309/110.734451, val:  25.42%, val_best:  43.75%, tr:  99.39%, tr_best:  99.49%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3123%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.1606%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 15455  13.155%\n",
      "lif layer 2 self.abs_max_v: 3460.5\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss: 12.202307/126.042770, val:  32.92%, val_best:  43.75%, tr:  99.28%, tr_best:  99.49%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1956%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.8732%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 16637  13.072%\n",
      "fc layer 2 self.abs_max_out: 1967.0\n",
      "lif layer 2 self.abs_max_v: 3471.5\n",
      "fc layer 2 self.abs_max_out: 2039.0\n",
      "lif layer 2 self.abs_max_v: 3504.5\n",
      "lif layer 2 self.abs_max_v: 3539.0\n",
      "lif layer 2 self.abs_max_v: 3709.5\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss: 12.511147/108.487396, val:  33.75%, val_best:  43.75%, tr:  99.49%, tr_best:  99.49%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9471%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.5582%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 17822  13.003%\n",
      "fc layer 1 self.abs_max_out: 10628.0\n",
      "lif layer 1 self.abs_max_v: 19395.5\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss: 12.895878/ 85.975151, val:  39.58%, val_best:  43.75%, tr:  99.39%, tr_best:  99.49%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0341%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.7263%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 19001  12.939%\n",
      "lif layer 2 self.abs_max_v: 3752.0\n",
      "lif layer 2 self.abs_max_v: 3833.0\n",
      "lif layer 2 self.abs_max_v: 3873.5\n",
      "lif layer 2 self.abs_max_v: 3894.0\n",
      "fc layer 1 self.abs_max_out: 10805.0\n",
      "lif layer 1 self.abs_max_v: 19674.0\n",
      "fc layer 2 self.abs_max_out: 2043.0\n",
      "fc layer 2 self.abs_max_out: 2097.0\n",
      "lif layer 2 self.abs_max_v: 3897.5\n",
      "lif layer 2 self.abs_max_v: 3923.5\n",
      "lif layer 2 self.abs_max_v: 3961.5\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss: 13.398896/ 94.747871, val:  40.42%, val_best:  43.75%, tr:  99.28%, tr_best:  99.49%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7782%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.4492%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 20221  12.909%\n",
      "fc layer 2 self.abs_max_out: 2119.0\n",
      "lif layer 2 self.abs_max_v: 4017.0\n",
      "lif layer 2 self.abs_max_v: 4028.5\n",
      "fc layer 2 self.abs_max_out: 2136.0\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss: 11.903369/ 94.131729, val:  45.00%, val_best:  45.00%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7652%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.3836%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 21348  12.827%\n",
      "fc layer 2 self.abs_max_out: 2152.0\n",
      "fc layer 2 self.abs_max_out: 2192.0\n",
      "lif layer 2 self.abs_max_v: 4172.0\n",
      "lif layer 2 self.abs_max_v: 4278.0\n",
      "lif layer 2 self.abs_max_v: 4331.0\n",
      "lif layer 2 self.abs_max_v: 4350.0\n",
      "lif layer 2 self.abs_max_v: 4367.0\n",
      "lif layer 2 self.abs_max_v: 4375.5\n",
      "lif layer 2 self.abs_max_v: 4380.0\n",
      "fc layer 2 self.abs_max_out: 2211.0\n",
      "fc layer 2 self.abs_max_out: 2225.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss: 12.408980/ 71.099632, val:  47.08%, val_best:  47.08%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1362%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.0206%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 22540  12.791%\n",
      "fc layer 1 self.abs_max_out: 11148.0\n",
      "lif layer 1 self.abs_max_v: 19767.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss: 13.555391/107.188782, val:  40.83%, val_best:  47.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8224%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.2995%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 23786  12.787%\n",
      "fc layer 1 self.abs_max_out: 11497.0\n",
      "lif layer 1 self.abs_max_v: 20971.5\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss: 12.372304/ 62.927731, val:  44.17%, val_best:  47.08%, tr:  99.59%, tr_best:  99.80%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1852%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.0918%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 24946  12.741%\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss: 13.295116/114.755714, val:  37.50%, val_best:  47.08%, tr:  98.88%, tr_best:  99.80%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7869%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.7039%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 26161  12.725%\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss: 13.450785/127.874420, val:  27.08%, val_best:  47.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8078%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.6896%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 27405  12.724%\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss: 12.823573/ 80.876518, val:  43.75%, val_best:  47.08%, tr:  99.28%, tr_best:  99.80%, epoch time: 75.66 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1805%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.5351%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 28591  12.698%\n",
      "fc layer 2 self.abs_max_out: 2290.0\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss: 12.518868/100.511604, val:  44.58%, val_best:  47.08%, tr:  99.08%, tr_best:  99.80%, epoch time: 75.53 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5041%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.8605%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 29754  12.663%\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss: 13.371746/105.898331, val:  41.67%, val_best:  47.08%, tr:  98.98%, tr_best:  99.80%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7316%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.2493%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 30975  12.656%\n",
      "fc layer 1 self.abs_max_out: 11910.0\n",
      "lif layer 1 self.abs_max_v: 21627.0\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss: 13.716799/ 54.940941, val:  56.25%, val_best:  56.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6755%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.3254%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 32249  12.670%\n",
      "fc layer 2 self.abs_max_out: 2310.0\n",
      "fc layer 1 self.abs_max_out: 11934.0\n",
      "lif layer 1 self.abs_max_v: 21640.0\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss: 13.134066/ 65.473343, val:  50.42%, val_best:  56.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8299%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.7216%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 33483  12.667%\n",
      "lif layer 2 self.abs_max_v: 4422.5\n",
      "fc layer 2 self.abs_max_out: 2376.0\n",
      "fc layer 2 self.abs_max_out: 2392.0\n",
      "fc layer 2 self.abs_max_out: 2415.0\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss: 13.481198/ 84.134155, val:  47.08%, val_best:  56.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1454%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.1971%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 34707  12.661%\n",
      "fc layer 2 self.abs_max_out: 2434.0\n",
      "fc layer 1 self.abs_max_out: 12354.0\n",
      "lif layer 1 self.abs_max_v: 22541.0\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss: 12.965645/105.554306, val:  42.92%, val_best:  56.25%, tr:  99.08%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7800%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.5167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 35923  12.653%\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss: 12.717711/102.889626, val:  39.58%, val_best:  56.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9724%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.0376%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 37098  12.631%\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss: 13.141454/ 69.419601, val:  52.50%, val_best:  56.25%, tr:  98.98%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4530%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.4904%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 38288  12.616%\n",
      "lif layer 2 self.abs_max_v: 4433.5\n",
      "lif layer 2 self.abs_max_v: 4449.5\n",
      "fc layer 2 self.abs_max_out: 2435.0\n",
      "lif layer 2 self.abs_max_v: 4477.0\n",
      "lif layer 2 self.abs_max_v: 4493.5\n",
      "lif layer 2 self.abs_max_v: 4550.0\n",
      "lif layer 2 self.abs_max_v: 4578.0\n",
      "fc layer 2 self.abs_max_out: 2452.0\n",
      "fc layer 2 self.abs_max_out: 2503.0\n",
      "lif layer 2 self.abs_max_v: 4621.5\n",
      "lif layer 2 self.abs_max_v: 4669.0\n",
      "lif layer 2 self.abs_max_v: 4766.5\n",
      "lif layer 2 self.abs_max_v: 4812.0\n",
      "fc layer 2 self.abs_max_out: 2523.0\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss: 12.990442/ 83.270050, val:  40.42%, val_best:  56.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8751%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.1078%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 39458  12.595%\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss: 13.109426/ 80.579216, val:  36.67%, val_best:  56.25%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6791%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.9267%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 40661  12.586%\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss: 12.978800/ 75.216301, val:  47.08%, val_best:  56.25%, tr:  99.39%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6676%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.0438%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 41825  12.565%\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss: 12.925138/124.469597, val:  30.83%, val_best:  56.25%, tr:  99.18%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0791%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.2322%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 42998  12.549%\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss: 12.819589/ 82.484177, val:  39.58%, val_best:  56.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8604%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.4951%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 44187  12.537%\n",
      "fc layer 2 self.abs_max_out: 2530.0\n",
      "fc layer 3 self.abs_max_out: 805.0\n",
      "fc layer 3 self.abs_max_out: 837.0\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss: 13.064145/ 78.918205, val:  46.25%, val_best:  56.25%, tr:  99.28%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6929%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.4458%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 45367  12.524%\n",
      "fc layer 2 self.abs_max_out: 2571.0\n",
      "fc layer 2 self.abs_max_out: 2591.0\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss: 12.010891/ 93.433357, val:  42.08%, val_best:  56.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9745%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.2265%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 46475  12.493%\n",
      "fc layer 2 self.abs_max_out: 2612.0\n",
      "fc layer 2 self.abs_max_out: 2614.0\n",
      "fc layer 1 self.abs_max_out: 12371.0\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss: 13.332938/ 89.972824, val:  50.42%, val_best:  56.25%, tr:  99.28%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8907%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.8072%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 47704  12.494%\n",
      "fc layer 2 self.abs_max_out: 2640.0\n",
      "fc layer 1 self.abs_max_out: 12428.0\n",
      "lif layer 1 self.abs_max_v: 22587.5\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss: 13.469128/113.789062, val:  43.75%, val_best:  56.25%, tr:  99.18%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2727%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.5760%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 48931  12.495%\n",
      "fc layer 1 self.abs_max_out: 12643.0\n",
      "fc layer 2 self.abs_max_out: 2666.0\n",
      "lif layer 1 self.abs_max_v: 23058.5\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss: 13.277655/ 84.381271, val:  43.33%, val_best:  56.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0236%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.0343%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 50122  12.487%\n",
      "fc layer 2 self.abs_max_out: 2727.0\n",
      "fc layer 1 self.abs_max_out: 12711.0\n",
      "lif layer 1 self.abs_max_v: 23124.0\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss: 13.418997/107.363556, val:  30.83%, val_best:  56.25%, tr:  98.98%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9878%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.8072%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 51323  12.482%\n",
      "fc layer 2 self.abs_max_out: 2790.0\n",
      "fc layer 2 self.abs_max_out: 2810.0\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss: 13.293592/118.461067, val:  37.50%, val_best:  56.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0855%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.2949%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 52502  12.472%\n",
      "fc layer 2 self.abs_max_out: 2898.0\n",
      "fc layer 3 self.abs_max_out: 843.0\n",
      "fc layer 1 self.abs_max_out: 12751.0\n",
      "lif layer 1 self.abs_max_v: 23228.5\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss: 13.258224/ 96.648987, val:  45.00%, val_best:  56.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9054%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.9774%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 53684  12.463%\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss: 13.750975/ 77.210220, val:  48.33%, val_best:  56.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9795%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.5643%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 54875  12.456%\n",
      "fc layer 2 self.abs_max_out: 2900.0\n",
      "fc layer 1 self.abs_max_out: 12843.0\n",
      "lif layer 1 self.abs_max_v: 23429.5\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss: 12.471063/ 90.476730, val:  38.33%, val_best:  56.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9927%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.8418%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 56017  12.439%\n",
      "lif layer 2 self.abs_max_v: 4818.0\n",
      "lif layer 2 self.abs_max_v: 4863.0\n",
      "lif layer 2 self.abs_max_v: 4881.0\n",
      "lif layer 2 self.abs_max_v: 4933.5\n",
      "lif layer 2 self.abs_max_v: 4981.0\n",
      "lif layer 2 self.abs_max_v: 5004.5\n",
      "fc layer 3 self.abs_max_out: 846.0\n",
      "fc layer 1 self.abs_max_out: 12975.0\n",
      "lif layer 1 self.abs_max_v: 23758.5\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss: 13.434631/ 70.656693, val:  48.75%, val_best:  56.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0912%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.8720%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 57192  12.430%\n",
      "lif layer 2 self.abs_max_v: 5005.5\n",
      "fc layer 1 self.abs_max_out: 13195.0\n",
      "lif layer 1 self.abs_max_v: 24131.5\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss: 13.076358/110.295341, val:  33.33%, val_best:  56.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9618%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.4820%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 58367  12.421%\n",
      "lif layer 2 self.abs_max_v: 5059.0\n",
      "lif layer 2 self.abs_max_v: 5102.5\n",
      "fc layer 2 self.abs_max_out: 2920.0\n",
      "fc layer 1 self.abs_max_out: 13353.0\n",
      "lif layer 1 self.abs_max_v: 24495.0\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss: 13.385916/ 98.394737, val:  45.83%, val_best:  56.25%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2071%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.2902%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 59543  12.412%\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss: 12.761153/ 86.224716, val:  51.67%, val_best:  56.25%, tr:  99.28%, tr_best: 100.00%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5475%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.0369%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 60700  12.400%\n",
      "fc layer 2 self.abs_max_out: 2940.0\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss: 13.126451/147.777847, val:  40.42%, val_best:  56.25%, tr:  99.28%, tr_best: 100.00%, epoch time: 75.85 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6348%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.0319%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 61875  12.393%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss: 13.291677/ 76.646896, val:  50.00%, val_best:  56.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1554%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.7007%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 63062  12.387%\n",
      "lif layer 2 self.abs_max_v: 5180.0\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss: 13.537298/ 63.245689, val:  48.33%, val_best:  56.25%, tr:  99.08%, tr_best: 100.00%, epoch time: 75.13 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2733%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.3048%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 64230  12.379%\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss: 13.865233/ 91.389259, val:  42.08%, val_best:  56.25%, tr:  98.88%, tr_best: 100.00%, epoch time: 75.37 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1770%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.9652%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 65455  12.381%\n",
      "fc layer 2 self.abs_max_out: 2941.0\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss: 14.026670/123.087921, val:  42.08%, val_best:  56.25%, tr:  99.08%, tr_best: 100.00%, epoch time: 74.94 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7452%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.8075%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 66653  12.379%\n",
      "fc layer 2 self.abs_max_out: 2959.0\n",
      "fc layer 2 self.abs_max_out: 2979.0\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss: 13.523393/ 90.300171, val:  42.92%, val_best:  56.25%, tr:  99.18%, tr_best: 100.00%, epoch time: 75.62 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7637%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.1018%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 67824  12.371%\n",
      "lif layer 2 self.abs_max_v: 5214.5\n",
      "fc layer 2 self.abs_max_out: 2990.0\n",
      "fc layer 2 self.abs_max_out: 3009.0\n",
      "fc layer 2 self.abs_max_out: 3028.0\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss: 13.697084/ 98.806671, val:  33.33%, val_best:  56.25%, tr:  99.28%, tr_best: 100.00%, epoch time: 75.70 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3128%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.3351%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 69022  12.369%\n",
      "lif layer 2 self.abs_max_v: 5287.5\n",
      "fc layer 2 self.abs_max_out: 3036.0\n",
      "fc layer 2 self.abs_max_out: 3044.0\n",
      "lif layer 2 self.abs_max_v: 5304.0\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss: 12.963209/ 85.706551, val:  50.00%, val_best:  56.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9046%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.1634%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 70192  12.362%\n",
      "fc layer 2 self.abs_max_out: 3057.0\n",
      "fc layer 2 self.abs_max_out: 3070.0\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss: 13.299114/ 98.537766, val:  41.25%, val_best:  56.25%, tr:  98.88%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9456%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.8390%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 71369  12.356%\n",
      "fc layer 2 self.abs_max_out: 3096.0\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss: 13.267269/ 67.484856, val:  55.83%, val_best:  56.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5553%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.6093%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 72545  12.350%\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss: 12.909358/ 61.021862, val:  50.83%, val_best:  56.25%, tr:  99.18%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7477%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.3732%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 73712  12.343%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss: 13.745918/ 86.335403, val:  50.00%, val_best:  56.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1278%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.0915%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 74919  12.343%\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss: 14.024211/ 77.029694, val:  39.17%, val_best:  56.25%, tr:  98.98%, tr_best: 100.00%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8200%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.8950%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 76110  12.340%\n",
      "lif layer 2 self.abs_max_v: 5338.0\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss: 12.877041/114.776978, val:  38.75%, val_best:  56.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8917%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.1741%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 77247  12.329%\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss: 13.083191/ 80.498978, val:  45.42%, val_best:  56.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8040%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.2303%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 78438  12.326%\n",
      "fc layer 2 self.abs_max_out: 3102.0\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss: 13.788104/ 84.357376, val:  44.17%, val_best:  56.25%, tr:  99.39%, tr_best: 100.00%, epoch time: 75.53 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3179%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.6300%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 79627  12.323%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss: 13.531566/131.040512, val:  40.42%, val_best:  56.25%, tr:  99.28%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8956%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.4907%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 80830  12.323%\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss: 13.813703/ 78.623032, val:  54.58%, val_best:  56.25%, tr:  99.18%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0117%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.0293%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 81988  12.316%\n",
      "fc layer 2 self.abs_max_out: 3104.0\n",
      "fc layer 2 self.abs_max_out: 3124.0\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss: 13.683141/ 53.646477, val:  57.08%, val_best:  57.08%, tr:  99.28%, tr_best: 100.00%, epoch time: 75.93 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3588%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.2848%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 83163  12.311%\n",
      "lif layer 2 self.abs_max_v: 5359.5\n",
      "lif layer 2 self.abs_max_v: 5419.0\n",
      "lif layer 2 self.abs_max_v: 5448.5\n",
      "fc layer 1 self.abs_max_out: 13433.0\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss: 12.868796/ 93.367210, val:  38.33%, val_best:  57.08%, tr:  99.28%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6033%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.4569%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 84282  12.299%\n",
      "fc layer 2 self.abs_max_out: 3143.0\n",
      "fc layer 2 self.abs_max_out: 3163.0\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss: 13.509817/ 78.014015, val:  54.17%, val_best:  57.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3144%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.6760%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 85448  12.293%\n",
      "fc layer 2 self.abs_max_out: 3221.0\n",
      "fc layer 2 self.abs_max_out: 3243.0\n",
      "fc layer 2 self.abs_max_out: 3269.0\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss: 12.948911/107.317032, val:  37.92%, val_best:  57.08%, tr:  99.39%, tr_best: 100.00%, epoch time: 75.90 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3334%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.7121%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 86604  12.286%\n",
      "lif layer 2 self.abs_max_v: 5581.5\n",
      "fc layer 2 self.abs_max_out: 3275.0\n",
      "lif layer 2 self.abs_max_v: 5588.0\n",
      "lif layer 2 self.abs_max_v: 5643.0\n",
      "lif layer 2 self.abs_max_v: 5670.5\n",
      "lif layer 2 self.abs_max_v: 5684.5\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss: 13.115314/ 77.663559, val:  49.17%, val_best:  57.08%, tr:  99.28%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1818%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.7523%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 87729  12.275%\n",
      "fc layer 2 self.abs_max_out: 3282.0\n",
      "lif layer 2 self.abs_max_v: 5938.5\n",
      "lif layer 2 self.abs_max_v: 6067.5\n",
      "lif layer 2 self.abs_max_v: 6132.0\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss: 13.420039/189.879654, val:  26.67%, val_best:  57.08%, tr:  99.08%, tr_best: 100.00%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1394%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.5930%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 88912  12.273%\n",
      "lif layer 2 self.abs_max_v: 6184.5\n",
      "fc layer 1 self.abs_max_out: 13611.0\n",
      "lif layer 1 self.abs_max_v: 24765.5\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss: 13.988491/ 82.431900, val:  42.92%, val_best:  57.08%, tr:  99.18%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1206%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.5132%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 90139  12.276%\n",
      "fc layer 2 self.abs_max_out: 3322.0\n",
      "fc layer 2 self.abs_max_out: 3365.0\n",
      "fc layer 1 self.abs_max_out: 13775.0\n",
      "lif layer 1 self.abs_max_v: 25087.0\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss: 13.611649/ 64.375755, val:  51.67%, val_best:  57.08%, tr:  99.18%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2226%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.5021%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 91333  12.275%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss: 14.369805/ 72.133476, val:  48.75%, val_best:  57.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7180%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.3431%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 92584  12.282%\n",
      "fc layer 2 self.abs_max_out: 3372.0\n",
      "fc layer 2 self.abs_max_out: 3392.0\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss: 12.496802/149.563187, val:  32.50%, val_best:  57.08%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5742%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.0856%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 93719  12.273%\n",
      "fc layer 2 self.abs_max_out: 3415.0\n",
      "fc layer 2 self.abs_max_out: 3569.0\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss: 13.824484/ 98.933395, val:  40.83%, val_best:  57.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3444%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.2585%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 94906  12.271%\n",
      "fc layer 2 self.abs_max_out: 3642.0\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss: 12.910507/ 96.909615, val:  45.83%, val_best:  57.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4923%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.6055%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 96070  12.266%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss: 12.883429/109.414146, val:  42.08%, val_best:  57.08%, tr:  99.28%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4899%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.5269%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 97202  12.258%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss: 12.732696/196.334457, val:  29.58%, val_best:  57.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7421%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.7881%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 98316  12.247%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss: 13.525667/ 94.490891, val:  46.25%, val_best:  57.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3941%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.7342%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 99518  12.247%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss: 13.335876/143.630951, val:  39.58%, val_best:  57.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 75.56 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.6295%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 100686  12.244%\n",
      "fc layer 2 self.abs_max_out: 3649.0\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss: 13.487651/122.542801, val:  41.67%, val_best:  57.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7879%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.8441%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 101867  12.241%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss: 13.282081/ 92.583290, val:  46.67%, val_best:  57.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 75.20 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1177%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.8143%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 103044  12.239%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss: 13.191097/ 88.113319, val:  43.33%, val_best:  57.08%, tr:  99.39%, tr_best: 100.00%, epoch time: 75.19 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6214%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.8781%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 104201  12.234%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss: 13.506169/103.425781, val:  44.58%, val_best:  57.08%, tr:  99.08%, tr_best: 100.00%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5871%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.9416%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 105405  12.235%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss: 13.498038/119.097641, val:  43.33%, val_best:  57.08%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3200%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.7667%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 106588  12.233%\n",
      "lif layer 2 self.abs_max_v: 6227.0\n",
      "fc layer 3 self.abs_max_out: 872.0\n",
      "fc layer 2 self.abs_max_out: 3769.0\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss: 13.345376/ 86.437859, val:  44.17%, val_best:  57.08%, tr:  99.28%, tr_best: 100.00%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7506%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.6407%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 107787  12.233%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss: 14.116978/128.549057, val:  41.25%, val_best:  57.08%, tr:  99.28%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6373%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.4054%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 109014  12.237%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss: 13.719705/ 63.146324, val:  58.75%, val_best:  58.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5716%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.7244%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 110206  12.236%\n",
      "fc layer 3 self.abs_max_out: 887.0\n",
      "fc layer 2 self.abs_max_out: 3828.0\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss: 13.971575/123.637566, val:  40.00%, val_best:  58.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6002%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.6030%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 111391  12.234%\n",
      "lif layer 2 self.abs_max_v: 6252.0\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss: 13.787296/123.792603, val:  32.08%, val_best:  58.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5124%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.4551%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 112599  12.236%\n",
      "fc layer 2 self.abs_max_out: 3846.0\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss: 13.332782/115.646042, val:  41.25%, val_best:  58.75%, tr:  98.88%, tr_best: 100.00%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7551%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.5427%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 113768  12.232%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss: 13.737435/ 92.662735, val:  51.67%, val_best:  58.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1295%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.7493%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 114949  12.231%\n",
      "fc layer 2 self.abs_max_out: 3857.0\n",
      "fc layer 3 self.abs_max_out: 921.0\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss: 13.760251/114.917358, val:  46.67%, val_best:  58.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9889%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.7017%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 116143  12.230%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss: 13.999053/ 67.937103, val:  58.75%, val_best:  58.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3108%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.1676%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 117347  12.231%\n",
      "fc layer 2 self.abs_max_out: 3861.0\n",
      "lif layer 2 self.abs_max_v: 6279.0\n",
      "lif layer 2 self.abs_max_v: 6542.5\n",
      "fc layer 2 self.abs_max_out: 3880.0\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss: 13.524515/110.804527, val:  41.67%, val_best:  58.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6647%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.8432%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 118494  12.226%\n",
      "fc layer 2 self.abs_max_out: 3910.0\n",
      "fc layer 2 self.abs_max_out: 3935.0\n",
      "fc layer 2 self.abs_max_out: 3955.0\n",
      "fc layer 2 self.abs_max_out: 3978.0\n",
      "fc layer 2 self.abs_max_out: 3995.0\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss: 14.072453/ 82.773102, val:  48.33%, val_best:  58.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4314%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.7001%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 119660  12.223%\n",
      "fc layer 2 self.abs_max_out: 4018.0\n",
      "fc layer 2 self.abs_max_out: 4020.0\n",
      "fc layer 1 self.abs_max_out: 13778.0\n",
      "lif layer 1 self.abs_max_v: 25244.5\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss: 13.660767/102.858208, val:  47.50%, val_best:  58.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2911%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.7584%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 120827  12.220%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss: 14.936498/ 94.284149, val:  42.08%, val_best:  58.75%, tr:  99.39%, tr_best: 100.00%, epoch time: 75.89 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6977%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.4632%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 122055  12.223%\n",
      "fc layer 2 self.abs_max_out: 4344.0\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss: 13.776022/105.817451, val:  41.25%, val_best:  58.75%, tr:  98.98%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9520%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.6365%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 123228  12.221%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss: 13.486683/ 88.443840, val:  50.83%, val_best:  58.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7291%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.6101%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 124406  12.219%\n",
      "fc layer 2 self.abs_max_out: 4520.0\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss: 13.822046/ 87.447083, val:  40.42%, val_best:  58.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8132%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.0792%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 125559  12.215%\n",
      "fc layer 2 self.abs_max_out: 4765.0\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss: 14.273629/ 81.608208, val:  42.50%, val_best:  58.75%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5985%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.0156%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 126810  12.220%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss: 13.587641/163.322250, val:  30.00%, val_best:  58.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6657%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.0492%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 127960  12.215%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss: 13.626792/109.742950, val:  32.92%, val_best:  58.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1083%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.9004%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 129148  12.215%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss: 14.204285/ 82.106888, val:  52.50%, val_best:  58.75%, tr:  99.28%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5852%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.7328%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 130341  12.214%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss: 13.819450/166.215500, val:  32.08%, val_best:  58.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.01 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3411%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.5093%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 131525  12.213%\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss: 14.012780/ 64.245415, val:  52.92%, val_best:  58.75%, tr:  99.28%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5566%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.7130%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 132711  12.212%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss: 13.414228/ 87.003525, val:  42.92%, val_best:  58.75%, tr:  99.28%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5231%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.3722%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 133840  12.206%\n",
      "fc layer 3 self.abs_max_out: 941.0\n",
      "fc layer 3 self.abs_max_out: 976.0\n",
      "fc layer 3 self.abs_max_out: 982.0\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss: 13.702271/ 66.671623, val:  58.75%, val_best:  58.75%, tr:  99.28%, tr_best: 100.00%, epoch time: 75.56 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0532%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.7671%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 135052  12.208%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss: 15.067204/153.770493, val:  28.33%, val_best:  58.75%, tr:  99.39%, tr_best: 100.00%, epoch time: 74.54 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2328%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.4448%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 136302  12.213%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss: 14.465269/117.039536, val:  36.67%, val_best:  58.75%, tr:  99.08%, tr_best: 100.00%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8968%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.7511%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 137523  12.215%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss: 13.742702/100.490829, val:  37.92%, val_best:  58.75%, tr:  98.98%, tr_best: 100.00%, epoch time: 75.31 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1389%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.5072%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 138675  12.211%\n",
      "fc layer 2 self.abs_max_out: 4790.0\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss: 13.676563/ 96.810242, val:  50.00%, val_best:  58.75%, tr:  99.49%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7340%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.3031%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 139865  12.211%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss: 13.766810/ 87.333359, val:  50.00%, val_best:  58.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7550%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.2872%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 141010  12.206%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss: 14.497740/ 70.894386, val:  47.50%, val_best:  58.75%, tr:  99.28%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6676%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.9555%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 142196  12.206%\n",
      "lif layer 2 self.abs_max_v: 6547.5\n",
      "lif layer 2 self.abs_max_v: 6619.5\n",
      "lif layer 2 self.abs_max_v: 6687.0\n",
      "lif layer 2 self.abs_max_v: 6724.0\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss: 13.437377/102.026451, val:  48.75%, val_best:  58.75%, tr:  99.08%, tr_best: 100.00%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4457%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.0037%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 143358  12.203%\n",
      "lif layer 2 self.abs_max_v: 6776.5\n",
      "lif layer 2 self.abs_max_v: 6859.5\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss: 13.526494/ 82.100616, val:  54.58%, val_best:  58.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9370%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.9009%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 144489  12.197%\n",
      "lif layer 2 self.abs_max_v: 6896.5\n",
      "lif layer 2 self.abs_max_v: 6936.5\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss: 14.037564/ 96.790184, val:  50.83%, val_best:  58.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6994%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.8518%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 145662  12.196%\n",
      "lif layer 2 self.abs_max_v: 6986.0\n",
      "lif layer 2 self.abs_max_v: 7012.0\n",
      "lif layer 2 self.abs_max_v: 7025.0\n",
      "lif layer 2 self.abs_max_v: 7031.5\n",
      "lif layer 2 self.abs_max_v: 7035.0\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss: 14.021183/ 68.910057, val:  55.00%, val_best:  58.75%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3650%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.7812%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 146837  12.194%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss: 14.231725/ 74.633560, val:  55.42%, val_best:  58.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1577%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.7028%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 148012  12.192%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss: 14.442319/ 82.265663, val:  51.67%, val_best:  58.75%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7548%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.9720%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 149227  12.194%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss: 13.620050/111.209335, val:  42.92%, val_best:  58.75%, tr:  99.08%, tr_best: 100.00%, epoch time: 75.89 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9716%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.8989%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 150392  12.192%\n",
      "lif layer 2 self.abs_max_v: 7141.0\n",
      "lif layer 2 self.abs_max_v: 7199.5\n",
      "lif layer 2 self.abs_max_v: 7206.5\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss: 14.349803/143.584778, val:  42.08%, val_best:  58.75%, tr:  99.18%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7340%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.0767%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 151598  12.193%\n",
      "lif layer 2 self.abs_max_v: 7241.0\n",
      "lif layer 2 self.abs_max_v: 7259.5\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss: 13.227432/129.608505, val:  39.17%, val_best:  58.75%, tr:  99.28%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7980%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.2155%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 152734  12.188%\n",
      "fc layer 2 self.abs_max_out: 4954.0\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss: 13.746348/129.440353, val:  39.17%, val_best:  58.75%, tr:  99.08%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8046%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.2689%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 153892  12.186%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss: 13.933253/ 86.775856, val:  45.00%, val_best:  58.75%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8983%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.2638%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 155077  12.185%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss: 14.494870/156.453415, val:  34.17%, val_best:  58.75%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9617%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.2520%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 156277  12.185%\n",
      "fc layer 2 self.abs_max_out: 4956.0\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss: 14.442151/103.308914, val:  50.42%, val_best:  58.75%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3879%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.2510%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 157474  12.186%\n",
      "fc layer 2 self.abs_max_out: 5051.0\n",
      "fc layer 2 self.abs_max_out: 5199.0\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss: 13.387519/117.431709, val:  45.00%, val_best:  58.75%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0492%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.4235%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 158609  12.181%\n",
      "fc layer 2 self.abs_max_out: 5384.0\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss: 14.332547/ 89.992775, val:  49.58%, val_best:  58.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9724%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.0845%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 159778  12.180%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss: 13.412978/ 78.069473, val:  45.83%, val_best:  58.75%, tr:  99.18%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1508%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.1788%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 160932  12.177%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss: 14.236686/110.423492, val:  41.67%, val_best:  58.75%, tr:  98.88%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0961%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.0564%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 162123  12.177%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss: 13.795904/ 75.881096, val:  50.42%, val_best:  58.75%, tr:  99.08%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2749%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.8141%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 163233  12.170%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss: 14.072704/120.428932, val:  37.08%, val_best:  58.75%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0631%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.2286%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 164409  12.169%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss: 14.370422/110.630699, val:  47.08%, val_best:  58.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9846%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.2130%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 165624  12.171%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss: 14.946116/138.253250, val:  34.17%, val_best:  58.75%, tr:  99.08%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3026%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.3305%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 166855  12.174%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss: 13.654171/ 89.836670, val:  45.00%, val_best:  58.75%, tr:  99.08%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2410%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.6201%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 168031  12.173%\n",
      "fc layer 3 self.abs_max_out: 1010.0\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss: 13.242599/106.111763, val:  34.58%, val_best:  58.75%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3368%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.5141%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 169165  12.169%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss: 14.315413/135.625488, val:  39.17%, val_best:  58.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5114%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.4891%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 170385  12.171%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss: 14.351423/ 91.061714, val:  46.67%, val_best:  58.75%, tr:  98.98%, tr_best: 100.00%, epoch time: 75.37 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3283%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.2821%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 171572  12.170%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss: 13.742151/ 95.753677, val:  44.58%, val_best:  58.75%, tr:  99.28%, tr_best: 100.00%, epoch time: 74.89 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8598%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.2361%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 172712  12.167%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss: 13.764000/115.910515, val:  42.92%, val_best:  58.75%, tr:  99.49%, tr_best: 100.00%, epoch time: 75.42 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3323%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.2287%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 173889  12.166%\n",
      "lif layer 2 self.abs_max_v: 7419.0\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss: 14.540302/120.770004, val:  45.83%, val_best:  58.75%, tr:  99.28%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5085%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.1876%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 175096  12.167%\n",
      "lif layer 2 self.abs_max_v: 7449.0\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss: 13.939042/132.121231, val:  37.08%, val_best:  58.75%, tr:  99.39%, tr_best: 100.00%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7407%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.8340%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 176247  12.164%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss: 13.536635/156.018188, val:  34.17%, val_best:  58.75%, tr:  99.28%, tr_best: 100.00%, epoch time: 75.40 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9336%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.7384%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 177394  12.161%\n",
      "fc layer 2 self.abs_max_out: 5766.0\n",
      "fc layer 1 self.abs_max_out: 13991.0\n",
      "lif layer 1 self.abs_max_v: 25520.5\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss: 14.186915/ 69.246040, val:  56.67%, val_best:  58.75%, tr:  98.77%, tr_best: 100.00%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8247%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.6213%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 178588  12.161%\n",
      "lif layer 2 self.abs_max_v: 7482.5\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss: 13.954062/ 91.435555, val:  49.17%, val_best:  58.75%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9776%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.4926%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 179735  12.158%\n",
      "fc layer 2 self.abs_max_out: 5833.0\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss: 13.913081/ 67.819237, val:  52.08%, val_best:  58.75%, tr:  99.28%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0995%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.5731%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 180894  12.156%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss: 14.083496/107.348373, val:  38.33%, val_best:  58.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8149%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.6685%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 182074  12.156%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss: 14.263945/100.691910, val:  49.58%, val_best:  58.75%, tr:  99.18%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7901%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.5936%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 183267  12.156%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss: 13.900406/ 78.762917, val:  56.67%, val_best:  58.75%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.6791%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 184423  12.153%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss: 13.957449/ 87.953438, val:  45.00%, val_best:  58.75%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7283%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.1491%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 185599  12.153%\n",
      "fc layer 3 self.abs_max_out: 1012.0\n",
      "fc layer 3 self.abs_max_out: 1027.0\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss: 14.410556/ 95.311897, val:  48.33%, val_best:  58.75%, tr:  99.08%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1217%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.4165%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 186776  12.152%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss: 13.732217/ 77.565796, val:  60.42%, val_best:  60.42%, tr:  99.28%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9320%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.0114%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 187985  12.153%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss: 14.029437/156.725861, val:  28.33%, val_best:  60.42%, tr:  99.18%, tr_best: 100.00%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0320%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.9828%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 189181  12.153%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss: 13.422888/139.493561, val:  34.58%, val_best:  60.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5203%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.1217%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 190324  12.150%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss: 13.017373/106.641670, val:  48.33%, val_best:  60.42%, tr:  98.98%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2471%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.1580%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 191446  12.146%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss: 14.035601/104.757538, val:  47.92%, val_best:  60.42%, tr:  99.18%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9703%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.0432%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 192610  12.145%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss: 14.244226/ 96.782402, val:  49.17%, val_best:  60.42%, tr:  98.98%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6087%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.0837%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 193797  12.144%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss: 14.431794/ 70.285812, val:  55.00%, val_best:  60.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4547%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.0198%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 194992  12.145%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss: 14.009417/ 76.155731, val:  52.92%, val_best:  60.42%, tr:  99.28%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7443%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.3142%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 196174  12.144%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss: 14.593736/119.239243, val:  38.33%, val_best:  60.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5655%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.3664%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 197388  12.146%\n",
      "fc layer 2 self.abs_max_out: 5879.0\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss: 14.703305/ 86.462173, val:  46.25%, val_best:  60.42%, tr:  99.08%, tr_best: 100.00%, epoch time: 75.91 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9406%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.4633%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 198592  12.147%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss: 14.035254/114.038818, val:  45.00%, val_best:  60.42%, tr:  99.08%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6538%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.1858%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 199787  12.147%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss: 13.720558/125.581451, val:  40.00%, val_best:  60.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4434%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.1182%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 200936  12.145%\n",
      "fc layer 3 self.abs_max_out: 1037.0\n",
      "fc layer 3 self.abs_max_out: 1105.0\n",
      "lif layer 2 self.abs_max_v: 7552.0\n",
      "lif layer 2 self.abs_max_v: 7710.0\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss: 12.732010/ 76.263649, val:  49.17%, val_best:  60.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0337%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.3574%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 202062  12.141%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss: 13.863693/128.941696, val:  36.67%, val_best:  60.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1141%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2475%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.1881%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 203221  12.139%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss: 13.908792/109.605354, val:  47.08%, val_best:  60.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0966%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.2564%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 204378  12.137%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss: 14.452530/ 63.381725, val:  45.83%, val_best:  60.42%, tr:  99.08%, tr_best: 100.00%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3509%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.2920%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 205608  12.140%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss: 12.366220/ 82.316078, val:  56.25%, val_best:  60.42%, tr:  99.39%, tr_best: 100.00%, epoch time: 75.46 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4587%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.3610%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 206705  12.134%\n",
      "fc layer 2 self.abs_max_out: 5984.0\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss: 13.597708/ 88.850838, val:  45.00%, val_best:  60.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 74.96 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2652%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.3339%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 207866  12.133%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss: 13.355261/ 77.093788, val:  44.17%, val_best:  60.42%, tr:  99.39%, tr_best: 100.00%, epoch time: 75.55 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8359%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.0970%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 209030  12.131%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss: 14.103777/ 99.723587, val:  44.17%, val_best:  60.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1662%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.0837%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 210222  12.132%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss: 13.605335/ 96.628563, val:  45.42%, val_best:  60.42%, tr:  99.39%, tr_best: 100.00%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0312%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3352%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.5671%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 211385  12.130%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss: 12.965975/120.484543, val:  40.83%, val_best:  60.42%, tr:  99.28%, tr_best: 100.00%, epoch time: 75.42 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0376%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5335%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.7454%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 212515  12.127%\n",
      "fc layer 2 self.abs_max_out: 6175.0\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss: 13.941747/101.716591, val:  46.67%, val_best:  60.42%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0587%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.4565%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 213679  12.126%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss: 14.313411/ 94.639832, val:  45.42%, val_best:  60.42%, tr:  99.18%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1219%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.4243%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 214890  12.127%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss: 13.320367/ 77.766273, val:  48.33%, val_best:  60.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6294%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.5516%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 216018  12.124%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss: 14.142099/114.282394, val:  38.33%, val_best:  60.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0740%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.6790%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 217210  12.124%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss: 12.722009/147.903473, val:  36.25%, val_best:  60.42%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0426%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3888%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.5464%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 218329  12.120%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss: 13.796587/105.532188, val:  45.42%, val_best:  60.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7811%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.4267%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 219499  12.119%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss: 13.587097/132.587021, val:  41.25%, val_best:  60.42%, tr:  99.18%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6724%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.4617%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 220651  12.117%\n",
      "fc layer 2 self.abs_max_out: 6414.0\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss: 14.588840/ 80.213310, val:  55.42%, val_best:  60.42%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3522%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.6477%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 221866  12.119%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss: 13.798433/105.958786, val:  48.75%, val_best:  60.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7974%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.6053%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 223051  12.119%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss: 13.680507/ 90.902679, val:  43.75%, val_best:  60.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6487%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.6073%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 224185  12.116%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss: 12.987616/ 81.674500, val:  46.67%, val_best:  60.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3465%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.4881%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 225326  12.114%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss: 13.699133/ 99.182167, val:  51.25%, val_best:  60.42%, tr:  99.18%, tr_best: 100.00%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7859%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.4783%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 226491  12.113%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss: 14.273489/176.360184, val:  39.58%, val_best:  60.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6890%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.5374%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 227703  12.114%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss: 14.032616/ 83.846764, val:  48.75%, val_best:  60.42%, tr:  98.77%, tr_best: 100.00%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6823%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.3670%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 228873  12.113%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss: 13.574004/161.993210, val:  34.17%, val_best:  60.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6771%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.1279%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 230033  12.112%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss: 13.704201/136.279053, val:  40.42%, val_best:  60.42%, tr:  99.28%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4566%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.0807%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 231196  12.111%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss: 14.352190/ 82.719345, val:  48.75%, val_best:  60.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7393%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.0043%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 232374  12.110%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss: 14.053973/107.225662, val:  43.75%, val_best:  60.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6077%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.9139%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 233593  12.112%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss: 14.003891/ 83.296005, val:  50.00%, val_best:  60.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7241%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.7682%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 234749  12.110%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss: 13.291611/104.946114, val:  45.83%, val_best:  60.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9436%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.0041%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 235887  12.108%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss: 14.108377/114.826057, val:  43.75%, val_best:  60.42%, tr:  99.08%, tr_best: 100.00%, epoch time: 75.68 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0615%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.9233%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af93a5093464c869dca3fe18a35860f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÅ‚ñÑ‚ñÇ‚ñá‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÑ‚ñÉ‚ñÇ‚ñÖ‚ñà‚ñà‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñà‚ñá‚ñÖ‚ñá‚ñÖ‚ñÉ‚ñÜ‚ñÉ‚ñÖ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñÖ‚ñÖ‚ñÇ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñá‚ñÇ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñÖ‚ñÇ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÉ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñá</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÅ‚ñÑ‚ñÇ‚ñá‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÑ‚ñÉ‚ñÇ‚ñÖ‚ñà‚ñà‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñà‚ñá‚ñÖ‚ñá‚ñÖ‚ñÉ‚ñÜ‚ñÉ‚ñÖ</td></tr><tr><td>val_loss</td><td>‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñÖ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÜ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÜ‚ñÑ‚ñÜ‚ñÑ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99081</td></tr><tr><td>tr_epoch_loss</td><td>14.10838</td></tr><tr><td>val_acc_best</td><td>0.60417</td></tr><tr><td>val_acc_now</td><td>0.4375</td></tr><tr><td>val_loss</td><td>114.82606</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">young-sweep-8</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/20t1cjpr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/20t1cjpr</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_053126-20t1cjpr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gvo9y0uy with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.046875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0234375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_094641-gvo9y0uy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gvo9y0uy' target=\"_blank\">zesty-sweep-13</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gvo9y0uy' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gvo9y0uy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251214_094650_402', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 16, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 32, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 4, 'lif_layer_v_threshold2': 256, 'init_scaling': [0.03125, 0.046875, 0.0234375], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 32, self.v_threshold 16\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 4, self.v_threshold 256\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=16, v_reset=10000, sg_width=32, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=256, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 46.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 41.0\n",
      "lif layer 2 self.abs_max_v: 41.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 51.0\n",
      "fc layer 2 self.abs_max_out: 80.0\n",
      "lif layer 2 self.abs_max_v: 94.5\n",
      "fc layer 1 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 69.0\n",
      "lif layer 2 self.abs_max_v: 107.5\n",
      "fc layer 1 self.abs_max_out: 62.0\n",
      "lif layer 1 self.abs_max_v: 90.0\n",
      "fc layer 2 self.abs_max_out: 82.0\n",
      "lif layer 2 self.abs_max_v: 121.5\n",
      "fc layer 1 self.abs_max_out: 101.0\n",
      "lif layer 1 self.abs_max_v: 138.0\n",
      "fc layer 2 self.abs_max_out: 89.0\n",
      "lif layer 2 self.abs_max_v: 146.0\n",
      "fc layer 1 self.abs_max_out: 147.0\n",
      "lif layer 1 self.abs_max_v: 172.0\n",
      "fc layer 2 self.abs_max_out: 127.0\n",
      "lif layer 2 self.abs_max_v: 197.0\n",
      "fc layer 1 self.abs_max_out: 200.0\n",
      "lif layer 1 self.abs_max_v: 258.5\n",
      "fc layer 2 self.abs_max_out: 191.0\n",
      "lif layer 2 self.abs_max_v: 289.5\n",
      "fc layer 1 self.abs_max_out: 209.0\n",
      "lif layer 1 self.abs_max_v: 299.0\n",
      "fc layer 2 self.abs_max_out: 215.0\n",
      "lif layer 2 self.abs_max_v: 329.5\n",
      "fc layer 3 self.abs_max_out: 8.0\n",
      "lif layer 1 self.abs_max_v: 306.5\n",
      "fc layer 2 self.abs_max_out: 224.0\n",
      "lif layer 2 self.abs_max_v: 337.0\n",
      "fc layer 3 self.abs_max_out: 9.0\n",
      "lif layer 2 self.abs_max_v: 359.5\n",
      "fc layer 3 self.abs_max_out: 10.0\n",
      "fc layer 1 self.abs_max_out: 226.0\n",
      "fc layer 2 self.abs_max_out: 267.0\n",
      "fc layer 3 self.abs_max_out: 12.0\n",
      "lif layer 2 self.abs_max_v: 367.0\n",
      "fc layer 2 self.abs_max_out: 276.0\n",
      "lif layer 2 self.abs_max_v: 391.0\n",
      "lif layer 2 self.abs_max_v: 392.5\n",
      "fc layer 1 self.abs_max_out: 269.0\n",
      "lif layer 1 self.abs_max_v: 332.5\n",
      "fc layer 2 self.abs_max_out: 307.0\n",
      "lif layer 2 self.abs_max_v: 431.5\n",
      "fc layer 3 self.abs_max_out: 14.0\n",
      "lif layer 1 self.abs_max_v: 361.5\n",
      "fc layer 3 self.abs_max_out: 19.0\n",
      "fc layer 3 self.abs_max_out: 22.0\n",
      "fc layer 2 self.abs_max_out: 319.0\n",
      "lif layer 2 self.abs_max_v: 434.5\n",
      "fc layer 1 self.abs_max_out: 314.0\n",
      "fc layer 2 self.abs_max_out: 345.0\n",
      "lif layer 2 self.abs_max_v: 436.5\n",
      "fc layer 2 self.abs_max_out: 397.0\n",
      "lif layer 2 self.abs_max_v: 447.0\n",
      "lif layer 2 self.abs_max_v: 449.5\n",
      "fc layer 3 self.abs_max_out: 24.0\n",
      "lif layer 2 self.abs_max_v: 451.5\n",
      "lif layer 2 self.abs_max_v: 455.5\n",
      "lif layer 2 self.abs_max_v: 464.0\n",
      "fc layer 3 self.abs_max_out: 26.0\n",
      "lif layer 2 self.abs_max_v: 488.0\n",
      "fc layer 3 self.abs_max_out: 28.0\n",
      "lif layer 2 self.abs_max_v: 495.5\n",
      "fc layer 3 self.abs_max_out: 31.0\n",
      "fc layer 3 self.abs_max_out: 42.0\n",
      "lif layer 2 self.abs_max_v: 522.0\n",
      "lif layer 2 self.abs_max_v: 555.5\n",
      "lif layer 2 self.abs_max_v: 560.5\n",
      "lif layer 2 self.abs_max_v: 569.5\n",
      "lif layer 2 self.abs_max_v: 579.5\n",
      "lif layer 1 self.abs_max_v: 389.0\n",
      "lif layer 2 self.abs_max_v: 583.0\n",
      "lif layer 1 self.abs_max_v: 420.5\n",
      "lif layer 2 self.abs_max_v: 600.5\n",
      "lif layer 2 self.abs_max_v: 618.5\n",
      "lif layer 2 self.abs_max_v: 622.5\n",
      "fc layer 3 self.abs_max_out: 49.0\n",
      "fc layer 3 self.abs_max_out: 54.0\n",
      "fc layer 1 self.abs_max_out: 336.0\n",
      "fc layer 1 self.abs_max_out: 379.0\n",
      "lif layer 1 self.abs_max_v: 485.0\n",
      "lif layer 1 self.abs_max_v: 544.5\n",
      "lif layer 1 self.abs_max_v: 554.5\n",
      "fc layer 2 self.abs_max_out: 431.0\n",
      "lif layer 1 self.abs_max_v: 602.5\n",
      "fc layer 3 self.abs_max_out: 68.0\n",
      "fc layer 2 self.abs_max_out: 439.0\n",
      "fc layer 2 self.abs_max_out: 504.0\n",
      "lif layer 2 self.abs_max_v: 687.5\n",
      "lif layer 2 self.abs_max_v: 706.0\n",
      "fc layer 3 self.abs_max_out: 73.0\n",
      "lif layer 1 self.abs_max_v: 614.5\n",
      "lif layer 1 self.abs_max_v: 645.5\n",
      "fc layer 2 self.abs_max_out: 516.0\n",
      "lif layer 2 self.abs_max_v: 736.0\n",
      "fc layer 1 self.abs_max_out: 406.0\n",
      "lif layer 1 self.abs_max_v: 692.0\n",
      "fc layer 1 self.abs_max_out: 411.0\n",
      "lif layer 1 self.abs_max_v: 711.0\n",
      "fc layer 1 self.abs_max_out: 496.0\n",
      "lif layer 2 self.abs_max_v: 744.5\n",
      "lif layer 2 self.abs_max_v: 750.0\n",
      "lif layer 2 self.abs_max_v: 767.5\n",
      "lif layer 2 self.abs_max_v: 788.0\n",
      "fc layer 2 self.abs_max_out: 537.0\n",
      "fc layer 3 self.abs_max_out: 75.0\n",
      "lif layer 2 self.abs_max_v: 813.0\n",
      "fc layer 2 self.abs_max_out: 545.0\n",
      "lif layer 2 self.abs_max_v: 815.5\n",
      "fc layer 2 self.abs_max_out: 575.0\n",
      "fc layer 1 self.abs_max_out: 501.0\n",
      "lif layer 1 self.abs_max_v: 722.5\n",
      "fc layer 3 self.abs_max_out: 81.0\n",
      "fc layer 3 self.abs_max_out: 88.0\n",
      "fc layer 3 self.abs_max_out: 90.0\n",
      "fc layer 3 self.abs_max_out: 99.0\n",
      "fc layer 3 self.abs_max_out: 135.0\n",
      "fc layer 1 self.abs_max_out: 547.0\n",
      "lif layer 1 self.abs_max_v: 854.5\n",
      "lif layer 1 self.abs_max_v: 886.5\n",
      "fc layer 2 self.abs_max_out: 582.0\n",
      "fc layer 2 self.abs_max_out: 585.0\n",
      "fc layer 2 self.abs_max_out: 591.0\n",
      "fc layer 2 self.abs_max_out: 613.0\n",
      "fc layer 2 self.abs_max_out: 618.0\n",
      "fc layer 2 self.abs_max_out: 619.0\n",
      "fc layer 3 self.abs_max_out: 141.0\n",
      "fc layer 2 self.abs_max_out: 681.0\n",
      "fc layer 1 self.abs_max_out: 561.0\n",
      "lif layer 1 self.abs_max_v: 925.5\n",
      "fc layer 1 self.abs_max_out: 577.0\n",
      "fc layer 1 self.abs_max_out: 659.0\n",
      "fc layer 1 self.abs_max_out: 726.0\n",
      "lif layer 1 self.abs_max_v: 953.5\n",
      "fc layer 1 self.abs_max_out: 797.0\n",
      "fc layer 3 self.abs_max_out: 152.0\n",
      "fc layer 2 self.abs_max_out: 709.0\n",
      "lif layer 1 self.abs_max_v: 957.5\n",
      "lif layer 1 self.abs_max_v: 965.0\n",
      "lif layer 1 self.abs_max_v: 992.5\n",
      "lif layer 1 self.abs_max_v: 1032.5\n",
      "lif layer 1 self.abs_max_v: 1035.5\n",
      "lif layer 1 self.abs_max_v: 1040.0\n",
      "lif layer 1 self.abs_max_v: 1055.0\n",
      "lif layer 1 self.abs_max_v: 1103.5\n",
      "fc layer 1 self.abs_max_out: 884.0\n",
      "fc layer 1 self.abs_max_out: 948.0\n",
      "fc layer 2 self.abs_max_out: 740.0\n",
      "lif layer 2 self.abs_max_v: 823.0\n",
      "lif layer 2 self.abs_max_v: 859.5\n",
      "lif layer 2 self.abs_max_v: 897.5\n",
      "fc layer 2 self.abs_max_out: 755.0\n",
      "fc layer 2 self.abs_max_out: 813.0\n",
      "lif layer 2 self.abs_max_v: 903.5\n",
      "lif layer 2 self.abs_max_v: 910.0\n",
      "lif layer 1 self.abs_max_v: 1145.0\n",
      "fc layer 2 self.abs_max_out: 840.0\n",
      "fc layer 2 self.abs_max_out: 843.0\n",
      "fc layer 2 self.abs_max_out: 867.0\n",
      "lif layer 2 self.abs_max_v: 913.5\n",
      "lif layer 2 self.abs_max_v: 926.0\n",
      "lif layer 2 self.abs_max_v: 939.0\n",
      "fc layer 2 self.abs_max_out: 870.0\n",
      "fc layer 2 self.abs_max_out: 903.0\n",
      "fc layer 3 self.abs_max_out: 170.0\n",
      "lif layer 2 self.abs_max_v: 953.5\n",
      "lif layer 2 self.abs_max_v: 969.0\n",
      "fc layer 1 self.abs_max_out: 979.0\n",
      "fc layer 1 self.abs_max_out: 982.0\n",
      "lif layer 1 self.abs_max_v: 1147.0\n",
      "lif layer 2 self.abs_max_v: 972.5\n",
      "lif layer 2 self.abs_max_v: 1006.5\n",
      "lif layer 1 self.abs_max_v: 1179.0\n",
      "lif layer 2 self.abs_max_v: 1013.5\n",
      "lif layer 2 self.abs_max_v: 1034.0\n",
      "lif layer 2 self.abs_max_v: 1051.0\n",
      "lif layer 2 self.abs_max_v: 1124.5\n",
      "lif layer 2 self.abs_max_v: 1136.0\n",
      "lif layer 2 self.abs_max_v: 1137.5\n",
      "lif layer 1 self.abs_max_v: 1291.0\n",
      "fc layer 2 self.abs_max_out: 907.0\n",
      "fc layer 3 self.abs_max_out: 171.0\n",
      "fc layer 2 self.abs_max_out: 974.0\n",
      "fc layer 2 self.abs_max_out: 987.0\n",
      "fc layer 2 self.abs_max_out: 1023.0\n",
      "fc layer 2 self.abs_max_out: 1026.0\n",
      "fc layer 2 self.abs_max_out: 1034.0\n",
      "lif layer 1 self.abs_max_v: 1434.0\n",
      "fc layer 3 self.abs_max_out: 177.0\n",
      "fc layer 2 self.abs_max_out: 1036.0\n",
      "fc layer 2 self.abs_max_out: 1066.0\n",
      "fc layer 3 self.abs_max_out: 179.0\n",
      "lif layer 1 self.abs_max_v: 1456.0\n",
      "fc layer 2 self.abs_max_out: 1069.0\n",
      "lif layer 1 self.abs_max_v: 1465.5\n",
      "lif layer 1 self.abs_max_v: 1519.0\n",
      "lif layer 1 self.abs_max_v: 1582.0\n",
      "fc layer 1 self.abs_max_out: 1007.0\n",
      "lif layer 1 self.abs_max_v: 1759.0\n",
      "lif layer 1 self.abs_max_v: 1822.5\n",
      "lif layer 1 self.abs_max_v: 1906.5\n",
      "fc layer 1 self.abs_max_out: 1057.0\n",
      "lif layer 1 self.abs_max_v: 2010.5\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss:  6.082898/ 42.784412, val:  27.50%, val_best:  27.50%, tr:  97.96%, tr_best:  97.96%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3153%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.6431%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 2022  20.654%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 207.0\n",
      "fc layer 2 self.abs_max_out: 1106.0\n",
      "lif layer 2 self.abs_max_v: 1153.5\n",
      "fc layer 3 self.abs_max_out: 227.0\n",
      "lif layer 2 self.abs_max_v: 1176.0\n",
      "lif layer 2 self.abs_max_v: 1177.5\n",
      "lif layer 2 self.abs_max_v: 1179.0\n",
      "lif layer 2 self.abs_max_v: 1187.5\n",
      "lif layer 2 self.abs_max_v: 1195.5\n",
      "fc layer 2 self.abs_max_out: 1113.0\n",
      "fc layer 2 self.abs_max_out: 1130.0\n",
      "fc layer 2 self.abs_max_out: 1142.0\n",
      "fc layer 3 self.abs_max_out: 234.0\n",
      "fc layer 3 self.abs_max_out: 236.0\n",
      "fc layer 3 self.abs_max_out: 243.0\n",
      "fc layer 3 self.abs_max_out: 247.0\n",
      "fc layer 2 self.abs_max_out: 1185.0\n",
      "lif layer 2 self.abs_max_v: 1216.0\n",
      "lif layer 2 self.abs_max_v: 1230.0\n",
      "lif layer 2 self.abs_max_v: 1238.0\n",
      "lif layer 2 self.abs_max_v: 1239.0\n",
      "lif layer 2 self.abs_max_v: 1267.5\n",
      "fc layer 2 self.abs_max_out: 1234.0\n",
      "fc layer 2 self.abs_max_out: 1237.0\n",
      "fc layer 2 self.abs_max_out: 1247.0\n",
      "fc layer 2 self.abs_max_out: 1326.0\n",
      "lif layer 2 self.abs_max_v: 1326.0\n",
      "fc layer 2 self.abs_max_out: 1331.0\n",
      "lif layer 2 self.abs_max_v: 1331.0\n",
      "lif layer 2 self.abs_max_v: 1332.5\n",
      "lif layer 2 self.abs_max_v: 1374.5\n",
      "lif layer 2 self.abs_max_v: 1392.5\n",
      "lif layer 2 self.abs_max_v: 1431.5\n",
      "fc layer 2 self.abs_max_out: 1335.0\n",
      "lif layer 1 self.abs_max_v: 2027.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss:  7.614941/ 41.087399, val:  37.08%, val_best:  37.08%, tr:  99.18%, tr_best:  99.18%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3929%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.5556%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3630  18.539%\n",
      "fc layer 2 self.abs_max_out: 1379.0\n",
      "lif layer 2 self.abs_max_v: 1446.5\n",
      "lif layer 2 self.abs_max_v: 1524.5\n",
      "lif layer 2 self.abs_max_v: 1553.5\n",
      "lif layer 2 self.abs_max_v: 1557.0\n",
      "lif layer 2 self.abs_max_v: 1679.5\n",
      "lif layer 2 self.abs_max_v: 1708.5\n",
      "fc layer 1 self.abs_max_out: 1124.0\n",
      "lif layer 2 self.abs_max_v: 1709.0\n",
      "lif layer 2 self.abs_max_v: 1746.5\n",
      "lif layer 2 self.abs_max_v: 1764.5\n",
      "lif layer 2 self.abs_max_v: 1788.5\n",
      "lif layer 2 self.abs_max_v: 1808.5\n",
      "lif layer 2 self.abs_max_v: 1809.5\n",
      "fc layer 1 self.abs_max_out: 1150.0\n",
      "fc layer 1 self.abs_max_out: 1164.0\n",
      "fc layer 1 self.abs_max_out: 1250.0\n",
      "lif layer 1 self.abs_max_v: 2201.5\n",
      "lif layer 1 self.abs_max_v: 2236.5\n",
      "lif layer 1 self.abs_max_v: 2282.5\n",
      "fc layer 1 self.abs_max_out: 1256.0\n",
      "lif layer 1 self.abs_max_v: 2395.5\n",
      "fc layer 1 self.abs_max_out: 1258.0\n",
      "lif layer 1 self.abs_max_v: 2456.0\n",
      "lif layer 1 self.abs_max_v: 2466.0\n",
      "lif layer 2 self.abs_max_v: 1821.5\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss:  7.014942/ 39.664795, val:  41.67%, val_best:  41.67%, tr:  98.47%, tr_best:  99.18%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5799%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.9352%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 5261  17.913%\n",
      "lif layer 2 self.abs_max_v: 1868.5\n",
      "lif layer 2 self.abs_max_v: 1887.0\n",
      "fc layer 3 self.abs_max_out: 283.0\n",
      "fc layer 3 self.abs_max_out: 318.0\n",
      "fc layer 2 self.abs_max_out: 1389.0\n",
      "lif layer 2 self.abs_max_v: 1893.0\n",
      "fc layer 2 self.abs_max_out: 1439.0\n",
      "fc layer 2 self.abs_max_out: 1555.0\n",
      "lif layer 2 self.abs_max_v: 1943.5\n",
      "lif layer 2 self.abs_max_v: 1972.0\n",
      "lif layer 2 self.abs_max_v: 2020.0\n",
      "fc layer 1 self.abs_max_out: 1305.0\n",
      "fc layer 1 self.abs_max_out: 1441.0\n",
      "lif layer 1 self.abs_max_v: 2541.5\n",
      "fc layer 1 self.abs_max_out: 1451.0\n",
      "lif layer 1 self.abs_max_v: 2722.0\n",
      "lif layer 1 self.abs_max_v: 2804.0\n",
      "fc layer 1 self.abs_max_out: 1474.0\n",
      "lif layer 1 self.abs_max_v: 2876.0\n",
      "lif layer 2 self.abs_max_v: 2159.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  6.788880/ 42.912224, val:  41.67%, val_best:  41.67%, tr:  98.88%, tr_best:  99.18%, epoch time: 75.66 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8837%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.8287%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6774  17.298%\n",
      "lif layer 2 self.abs_max_v: 2181.5\n",
      "lif layer 2 self.abs_max_v: 2207.0\n",
      "lif layer 2 self.abs_max_v: 2212.0\n",
      "lif layer 2 self.abs_max_v: 2230.0\n",
      "lif layer 2 self.abs_max_v: 2273.0\n",
      "lif layer 2 self.abs_max_v: 2278.5\n",
      "fc layer 1 self.abs_max_out: 1541.0\n",
      "fc layer 1 self.abs_max_out: 1564.0\n",
      "lif layer 1 self.abs_max_v: 2946.0\n",
      "lif layer 1 self.abs_max_v: 3018.0\n",
      "fc layer 1 self.abs_max_out: 1573.0\n",
      "lif layer 1 self.abs_max_v: 3082.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  6.711839/ 35.672058, val:  41.67%, val_best:  41.67%, tr:  98.88%, tr_best:  99.18%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0066%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.9095%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 8344  17.046%\n",
      "lif layer 2 self.abs_max_v: 2286.0\n",
      "lif layer 2 self.abs_max_v: 2379.5\n",
      "lif layer 2 self.abs_max_v: 2405.0\n",
      "fc layer 1 self.abs_max_out: 1586.0\n",
      "fc layer 1 self.abs_max_out: 1597.0\n",
      "lif layer 1 self.abs_max_v: 3127.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  6.395207/ 38.856678, val:  38.33%, val_best:  41.67%, tr:  99.59%, tr_best:  99.59%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8014%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.9589%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9873  16.808%\n",
      "lif layer 2 self.abs_max_v: 2526.0\n",
      "fc layer 1 self.abs_max_out: 1618.0\n",
      "fc layer 1 self.abs_max_out: 1672.0\n",
      "lif layer 1 self.abs_max_v: 3153.5\n",
      "lif layer 1 self.abs_max_v: 3226.0\n",
      "fc layer 1 self.abs_max_out: 1678.0\n",
      "lif layer 1 self.abs_max_v: 3291.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  6.127626/ 40.580826, val:  46.67%, val_best:  46.67%, tr:  98.67%, tr_best:  99.59%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6639%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.6753%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 11444  16.699%\n",
      "lif layer 2 self.abs_max_v: 2538.0\n",
      "lif layer 2 self.abs_max_v: 2582.0\n",
      "fc layer 1 self.abs_max_out: 1684.0\n",
      "lif layer 1 self.abs_max_v: 3302.5\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  5.669998/ 28.415058, val:  46.67%, val_best:  46.67%, tr:  98.77%, tr_best:  99.59%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4327%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.8723%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 12923  16.500%\n",
      "lif layer 2 self.abs_max_v: 2659.5\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  5.469353/ 15.621734, val:  61.67%, val_best:  61.67%, tr:  98.88%, tr_best:  99.59%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5386%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.8806%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 14442  16.391%\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  5.227666/ 28.967741, val:  45.83%, val_best:  61.67%, tr:  98.98%, tr_best:  99.59%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7529%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.0219%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 15889  16.230%\n",
      "fc layer 1 self.abs_max_out: 1706.0\n",
      "fc layer 1 self.abs_max_out: 1712.0\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  5.358817/ 32.558456, val:  36.67%, val_best:  61.67%, tr:  98.67%, tr_best:  99.59%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3131%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.7437%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 17307  16.071%\n",
      "lif layer 2 self.abs_max_v: 2660.5\n",
      "lif layer 2 self.abs_max_v: 2679.0\n",
      "lif layer 2 self.abs_max_v: 2735.0\n",
      "fc layer 1 self.abs_max_out: 1724.0\n",
      "lif layer 1 self.abs_max_v: 3313.0\n",
      "lif layer 1 self.abs_max_v: 3314.5\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  4.776258/ 26.833702, val:  45.42%, val_best:  61.67%, tr:  99.18%, tr_best:  99.59%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8708%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0394%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 18699  15.917%\n",
      "lif layer 2 self.abs_max_v: 2751.5\n",
      "fc layer 2 self.abs_max_out: 1636.0\n",
      "lif layer 2 self.abs_max_v: 2928.0\n",
      "fc layer 1 self.abs_max_out: 1725.0\n",
      "lif layer 1 self.abs_max_v: 3315.5\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  4.766603/ 25.784237, val:  42.92%, val_best:  61.67%, tr:  99.18%, tr_best:  99.59%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2716%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.7044%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 20076  15.774%\n",
      "fc layer 2 self.abs_max_out: 1673.0\n",
      "fc layer 1 self.abs_max_out: 1768.0\n",
      "lif layer 1 self.abs_max_v: 3349.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  4.874391/ 34.002560, val:  38.33%, val_best:  61.67%, tr:  99.08%, tr_best:  99.59%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1496%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.3283%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 21479  15.671%\n",
      "lif layer 2 self.abs_max_v: 2930.5\n",
      "lif layer 2 self.abs_max_v: 2972.5\n",
      "lif layer 2 self.abs_max_v: 2978.5\n",
      "lif layer 2 self.abs_max_v: 2995.5\n",
      "fc layer 1 self.abs_max_out: 1789.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  4.770113/ 22.897284, val:  52.92%, val_best:  61.67%, tr:  99.18%, tr_best:  99.59%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3617%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1304%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 22859  15.566%\n",
      "fc layer 1 self.abs_max_out: 1826.0\n",
      "lif layer 1 self.abs_max_v: 3349.5\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  4.853323/ 19.333824, val:  45.83%, val_best:  61.67%, tr:  98.77%, tr_best:  99.59%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1694%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.5866%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 24203  15.451%\n",
      "fc layer 2 self.abs_max_out: 1688.0\n",
      "fc layer 2 self.abs_max_out: 1743.0\n",
      "lif layer 2 self.abs_max_v: 3107.0\n",
      "fc layer 1 self.abs_max_out: 1842.0\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  4.386357/ 17.214066, val:  55.00%, val_best:  61.67%, tr:  99.28%, tr_best:  99.59%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7065%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0054%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 25524  15.336%\n",
      "fc layer 2 self.abs_max_out: 1755.0\n",
      "lif layer 2 self.abs_max_v: 3165.5\n",
      "lif layer 2 self.abs_max_v: 3230.0\n",
      "lif layer 2 self.abs_max_v: 3259.0\n",
      "lif layer 2 self.abs_max_v: 3336.5\n",
      "lif layer 2 self.abs_max_v: 3393.5\n",
      "fc layer 1 self.abs_max_out: 1866.0\n",
      "lif layer 1 self.abs_max_v: 3354.5\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  4.439236/ 19.432680, val:  49.58%, val_best:  61.67%, tr:  99.08%, tr_best:  99.59%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8075%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1617%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 26896  15.263%\n",
      "fc layer 1 self.abs_max_out: 1891.0\n",
      "fc layer 1 self.abs_max_out: 1911.0\n",
      "lif layer 1 self.abs_max_v: 3393.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  4.518809/ 18.564287, val:  58.75%, val_best:  61.67%, tr:  99.08%, tr_best:  99.59%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0604%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6477%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 28241  15.183%\n",
      "fc layer 2 self.abs_max_out: 1994.0\n",
      "lif layer 2 self.abs_max_v: 3450.0\n",
      "fc layer 1 self.abs_max_out: 1958.0\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  4.090481/ 30.842684, val:  37.08%, val_best:  61.67%, tr:  98.98%, tr_best:  99.59%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3669%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6198%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 29495  15.064%\n",
      "lif layer 1 self.abs_max_v: 3433.0\n",
      "fc layer 1 self.abs_max_out: 1970.0\n",
      "lif layer 1 self.abs_max_v: 3686.5\n",
      "fc layer 1 self.abs_max_out: 2075.0\n",
      "fc layer 1 self.abs_max_out: 2104.0\n",
      "fc layer 1 self.abs_max_out: 2125.0\n",
      "lif layer 1 self.abs_max_v: 3879.0\n",
      "lif layer 1 self.abs_max_v: 3908.5\n",
      "lif layer 1 self.abs_max_v: 3950.5\n",
      "fc layer 1 self.abs_max_out: 2132.0\n",
      "lif layer 1 self.abs_max_v: 4107.5\n",
      "fc layer 1 self.abs_max_out: 2238.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  4.029996/ 28.337120, val:  42.50%, val_best:  61.67%, tr:  99.49%, tr_best:  99.59%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3773%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3731%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 30704  14.935%\n",
      "lif layer 2 self.abs_max_v: 3465.0\n",
      "lif layer 1 self.abs_max_v: 4152.0\n",
      "fc layer 1 self.abs_max_out: 2260.0\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  4.336436/ 20.501446, val:  54.58%, val_best:  61.67%, tr:  98.77%, tr_best:  99.59%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2251%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1106%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 32031  14.872%\n",
      "lif layer 2 self.abs_max_v: 3540.5\n",
      "lif layer 2 self.abs_max_v: 3612.5\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  4.080278/ 24.339739, val:  52.50%, val_best:  61.67%, tr:  98.67%, tr_best:  99.59%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5770%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5262%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 33348  14.810%\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  4.256703/ 21.532911, val:  61.25%, val_best:  61.67%, tr:  99.49%, tr_best:  99.59%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6285%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9094%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 34658  14.751%\n",
      "lif layer 2 self.abs_max_v: 3620.0\n",
      "lif layer 2 self.abs_max_v: 3650.0\n",
      "lif layer 2 self.abs_max_v: 3697.5\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  4.030813/ 20.138571, val:  57.92%, val_best:  61.67%, tr:  99.28%, tr_best:  99.59%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5945%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4623%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 35902  14.669%\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  4.231432/ 19.196115, val:  62.08%, val_best:  62.08%, tr:  98.77%, tr_best:  99.59%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3546%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0115%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 37257  14.637%\n",
      "lif layer 2 self.abs_max_v: 3713.5\n",
      "lif layer 2 self.abs_max_v: 3734.0\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  4.105965/ 17.423096, val:  55.42%, val_best:  62.08%, tr:  99.49%, tr_best:  99.59%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4050%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1643%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 38541  14.581%\n",
      "lif layer 2 self.abs_max_v: 3743.5\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  3.864552/ 16.475340, val:  57.08%, val_best:  62.08%, tr:  98.98%, tr_best:  99.59%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6137%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.3059%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 39867  14.544%\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  3.835265/ 17.677242, val:  53.33%, val_best:  62.08%, tr:  99.18%, tr_best:  99.59%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4175%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.5242%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 41105  14.478%\n",
      "fc layer 2 self.abs_max_out: 2151.0\n",
      "lif layer 2 self.abs_max_v: 3800.0\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  3.517650/ 26.250828, val:  50.00%, val_best:  62.08%, tr:  99.49%, tr_best:  99.59%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1514%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.8159%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 42334  14.414%\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  3.490613/ 17.480301, val:  52.50%, val_best:  62.08%, tr:  99.39%, tr_best:  99.59%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0508%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7523%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 43565  14.355%\n",
      "lif layer 2 self.abs_max_v: 3812.5\n",
      "lif layer 1 self.abs_max_v: 4164.5\n",
      "lif layer 1 self.abs_max_v: 4242.5\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  3.722130/ 24.713793, val:  42.50%, val_best:  62.08%, tr:  98.98%, tr_best:  99.59%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1780%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.8203%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 44846  14.315%\n",
      "fc layer 1 self.abs_max_out: 2353.0\n",
      "lif layer 1 self.abs_max_v: 4322.5\n",
      "lif layer 1 self.abs_max_v: 4373.5\n",
      "lif layer 1 self.abs_max_v: 4468.0\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  3.334353/ 25.104973, val:  55.00%, val_best:  62.08%, tr:  99.59%, tr_best:  99.59%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5236%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.5514%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 46033  14.249%\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  3.616992/ 19.210764, val:  62.08%, val_best:  62.08%, tr:  99.18%, tr_best:  99.59%, epoch time: 75.66 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.4661%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9414%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 47339  14.222%\n",
      "fc layer 1 self.abs_max_out: 2428.0\n",
      "lif layer 1 self.abs_max_v: 4513.5\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  3.592646/ 23.045662, val:  47.92%, val_best:  62.08%, tr:  98.98%, tr_best:  99.59%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1045%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0363%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 48622  14.190%\n",
      "lif layer 2 self.abs_max_v: 3888.5\n",
      "lif layer 2 self.abs_max_v: 3923.5\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  3.361601/ 15.512059, val:  53.75%, val_best:  62.08%, tr:  99.39%, tr_best:  99.59%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1103%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.8441%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 49781  14.125%\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  3.401603/ 13.134132, val:  69.17%, val_best:  69.17%, tr:  99.28%, tr_best:  99.59%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5864%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0793%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 50967  14.070%\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  3.208235/ 20.901951, val:  53.75%, val_best:  69.17%, tr:  99.49%, tr_best:  99.59%, epoch time: 75.95 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6073%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0712%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 52109  14.007%\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  3.427763/ 15.770130, val:  57.50%, val_best:  69.17%, tr:  98.98%, tr_best:  99.59%, epoch time: 75.74 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6702%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.4797%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 53324  13.966%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  3.283913/ 19.784395, val:  52.08%, val_best:  69.17%, tr:  99.59%, tr_best:  99.59%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4339%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.2928%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 54517  13.922%\n",
      "fc layer 2 self.abs_max_out: 2191.0\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  3.259437/ 15.162663, val:  62.08%, val_best:  69.17%, tr:  99.39%, tr_best:  99.59%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0114%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.4660%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 55681  13.872%\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  3.194207/ 14.843258, val:  54.58%, val_best:  69.17%, tr:  99.18%, tr_best:  99.59%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.6083%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.6461%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 56850  13.826%\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  3.181124/ 19.978813, val:  53.75%, val_best:  69.17%, tr:  98.88%, tr_best:  99.59%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.3910%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.5047%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 57992  13.776%\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  3.362866/ 13.768818, val:  66.67%, val_best:  69.17%, tr:  99.39%, tr_best:  99.59%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9212%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.2198%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 59190  13.741%\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  3.270069/ 14.729319, val:  62.92%, val_best:  69.17%, tr:  99.69%, tr_best:  99.69%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7060%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.4305%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 60329  13.694%\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  3.215549/ 20.620106, val:  57.08%, val_best:  69.17%, tr:  99.28%, tr_best:  99.69%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8841%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.4147%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 61468  13.649%\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  3.275631/ 18.731731, val:  64.17%, val_best:  69.17%, tr:  99.08%, tr_best:  99.69%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0799%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.5639%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 62680  13.622%\n",
      "fc layer 2 self.abs_max_out: 2193.0\n",
      "lif layer 2 self.abs_max_v: 4145.5\n",
      "fc layer 2 self.abs_max_out: 2213.0\n",
      "lif layer 1 self.abs_max_v: 4617.0\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  3.094408/ 23.333754, val:  40.00%, val_best:  69.17%, tr:  99.39%, tr_best:  99.69%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1490%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.6335%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 63819  13.581%\n",
      "fc layer 1 self.abs_max_out: 2463.0\n",
      "lif layer 1 self.abs_max_v: 4748.5\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  2.984290/ 13.743174, val:  52.50%, val_best:  69.17%, tr:  99.69%, tr_best:  99.69%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8235%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.6473%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 64906  13.530%\n",
      "fc layer 1 self.abs_max_out: 2575.0\n",
      "lif layer 1 self.abs_max_v: 4806.5\n",
      "lif layer 1 self.abs_max_v: 4969.5\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  3.131719/ 15.257689, val:  66.67%, val_best:  69.17%, tr:  99.39%, tr_best:  99.69%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9649%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.7384%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 66026  13.488%\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  3.082475/ 18.152794, val:  50.42%, val_best:  69.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.2465%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.9157%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 67137  13.446%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  3.068337/ 15.794326, val:  60.83%, val_best:  69.17%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5898%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.6431%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 68290  13.414%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  3.308070/ 16.967794, val:  62.50%, val_best:  69.17%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4070%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.5432%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 69436  13.382%\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  2.945912/ 21.340006, val:  64.58%, val_best:  69.17%, tr:  99.39%, tr_best:  99.90%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5027%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.8175%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 70523  13.340%\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  3.207857/ 15.362316, val:  64.17%, val_best:  69.17%, tr:  99.18%, tr_best:  99.90%, epoch time: 76.01 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0829%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.6854%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 71697  13.315%\n",
      "fc layer 1 self.abs_max_out: 2614.0\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  3.112480/ 13.557375, val:  66.25%, val_best:  69.17%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0431%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.7329%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 72815  13.282%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  2.945926/ 16.671587, val:  55.42%, val_best:  69.17%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1974%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.1747%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 73913  13.245%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  2.935407/ 15.610822, val:  60.83%, val_best:  69.17%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3352%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.7963%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 74986  13.206%\n",
      "fc layer 1 self.abs_max_out: 2638.0\n",
      "fc layer 2 self.abs_max_out: 2244.0\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  2.925436/ 14.278962, val:  68.33%, val_best:  69.17%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5134%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.9660%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 76068  13.169%\n",
      "lif layer 2 self.abs_max_v: 4215.0\n",
      "fc layer 2 self.abs_max_out: 2267.0\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  2.983798/ 17.764505, val:  55.83%, val_best:  69.17%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8815%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.6138%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 77116  13.128%\n",
      "fc layer 2 self.abs_max_out: 2344.0\n",
      "lif layer 2 self.abs_max_v: 4217.5\n",
      "lif layer 2 self.abs_max_v: 4295.0\n",
      "lif layer 2 self.abs_max_v: 4326.5\n",
      "fc layer 2 self.abs_max_out: 2345.0\n",
      "lif layer 2 self.abs_max_v: 4508.5\n",
      "fc layer 2 self.abs_max_out: 2353.0\n",
      "fc layer 2 self.abs_max_out: 2403.0\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  3.026708/ 14.580147, val:  72.08%, val_best:  72.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1348%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.9446%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 78191  13.093%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  3.030074/ 11.679786, val:  72.50%, val_best:  72.50%, tr:  99.08%, tr_best:  99.90%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8724%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.0789%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 79320  13.068%\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  2.905046/ 19.856808, val:  51.67%, val_best:  72.50%, tr:  99.18%, tr_best:  99.90%, epoch time: 75.39 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7014%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.8775%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 80394  13.035%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  2.706635/ 15.623872, val:  63.75%, val_best:  72.50%, tr:  99.28%, tr_best:  99.90%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9540%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.3150%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 81403  12.992%\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  2.750651/ 16.529888, val:  64.17%, val_best:  72.50%, tr:  99.39%, tr_best:  99.90%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9233%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.8935%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 82379  12.946%\n",
      "fc layer 2 self.abs_max_out: 2410.0\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  2.776949/ 13.455717, val:  69.58%, val_best:  72.50%, tr:  99.39%, tr_best:  99.90%, epoch time: 75.72 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3033%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.0850%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 83427  12.912%\n",
      "fc layer 2 self.abs_max_out: 2473.0\n",
      "lif layer 2 self.abs_max_v: 4532.5\n",
      "lif layer 2 self.abs_max_v: 4583.5\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  2.792648/ 15.613992, val:  60.83%, val_best:  72.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2277%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.1809%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 84455  12.876%\n",
      "fc layer 2 self.abs_max_out: 2526.0\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  2.781713/ 13.948858, val:  71.67%, val_best:  72.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 75.09 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2503%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.8376%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 85452  12.836%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  2.812413/ 12.939566, val:  67.50%, val_best:  72.50%, tr:  99.28%, tr_best:  99.90%, epoch time: 75.60 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4612%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.3490%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 86474  12.801%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  2.690135/ 18.518421, val:  50.00%, val_best:  72.50%, tr:  99.08%, tr_best:  99.90%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1887%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.0870%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 87453  12.761%\n",
      "lif layer 2 self.abs_max_v: 4696.5\n",
      "lif layer 2 self.abs_max_v: 4726.5\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  2.890877/ 15.634180, val:  62.92%, val_best:  72.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0717%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.8667%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 88503  12.733%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  2.822851/ 19.065475, val:  54.58%, val_best:  72.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9511%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.0454%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 89519  12.700%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  2.783129/ 11.023074, val:  69.17%, val_best:  72.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9300%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.4753%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 90522  12.666%\n",
      "fc layer 1 self.abs_max_out: 2645.0\n",
      "fc layer 1 self.abs_max_out: 2743.0\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  2.407522/ 16.563047, val:  59.17%, val_best:  72.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2193%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.5382%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 91469  12.626%\n",
      "fc layer 1 self.abs_max_out: 2793.0\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  2.625489/ 15.940090, val:  49.58%, val_best:  72.50%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2063%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.2617%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 92482  12.595%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  2.493393/ 12.543091, val:  73.33%, val_best:  73.33%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1975%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.2825%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 93387  12.551%\n",
      "fc layer 2 self.abs_max_out: 2717.0\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  2.713033/ 18.572588, val:  61.67%, val_best:  73.33%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4461%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.1829%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 94424  12.526%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  2.792052/ 20.189356, val:  56.25%, val_best:  73.33%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1665%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.7246%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 95398  12.493%\n",
      "fc layer 1 self.abs_max_out: 2869.0\n",
      "lif layer 2 self.abs_max_v: 4897.5\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  2.900036/ 18.644295, val:  53.75%, val_best:  73.33%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9443%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.6019%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 96394  12.464%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  2.478177/ 19.982718, val:  65.42%, val_best:  73.33%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8489%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.9846%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 97302  12.424%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  2.492603/ 19.702034, val:  56.25%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1056%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.1510%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 98208  12.385%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  2.543775/ 17.394928, val:  50.42%, val_best:  73.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2295%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.1948%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 99119  12.347%\n",
      "fc layer 1 self.abs_max_out: 2968.0\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  2.720856/ 16.502016, val:  65.83%, val_best:  73.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3506%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.8606%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 100091  12.318%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  2.609933/ 13.832254, val:  76.67%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0597%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.1601%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 101032  12.286%\n",
      "lif layer 2 self.abs_max_v: 5000.0\n",
      "lif layer 2 self.abs_max_v: 5078.0\n",
      "fc layer 1 self.abs_max_out: 2988.0\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  2.720594/ 12.755351, val:  60.00%, val_best:  76.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3185%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.2269%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 102046  12.263%\n",
      "fc layer 1 self.abs_max_out: 3035.0\n",
      "fc layer 1 self.abs_max_out: 3159.0\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  2.539931/ 16.093967, val:  67.50%, val_best:  76.67%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1976%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.2041%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 102965  12.229%\n",
      "lif layer 2 self.abs_max_v: 5117.0\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  2.576248/ 15.556162, val:  56.25%, val_best:  76.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8835%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.2308%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 103907  12.200%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  2.797307/ 15.224068, val:  67.08%, val_best:  76.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2688%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.0309%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 104875  12.173%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  2.574355/ 13.848447, val:  60.00%, val_best:  76.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9633%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.4971%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 105792  12.142%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  2.568529/ 10.206794, val:  81.25%, val_best:  81.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2118%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.5660%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 106750  12.116%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  2.602226/ 16.281437, val:  66.67%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0628%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.3416%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 107706  12.090%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  2.382671/  9.370699, val:  74.17%, val_best:  81.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0710%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.5692%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 108591  12.057%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  2.505408/ 14.944409, val:  59.17%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.90 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4082%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.4867%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 109515  12.028%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  2.423196/ 11.314335, val:  71.25%, val_best:  81.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 75.40 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6539%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.4658%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 110382  11.995%\n",
      "lif layer 1 self.abs_max_v: 5009.0\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  2.479973/ 16.102259, val:  51.67%, val_best:  81.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.30 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7373%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.5306%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 111297  11.967%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  2.545866/ 14.444765, val:  63.33%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.75 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0219%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.5296%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 112255  11.944%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  2.330217/ 18.679482, val:  73.75%, val_best:  81.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4951%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.5868%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 113135  11.914%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  2.192142/ 14.816862, val:  73.33%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7858%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.7134%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 113941  11.876%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  2.280959/ 18.639021, val:  55.42%, val_best:  81.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.31 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9544%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.5630%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 114793  11.844%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  2.343374/ 16.461275, val:  66.25%, val_best:  81.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9480%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.4520%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 115646  11.813%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  2.189667/ 12.540882, val:  65.83%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8222%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.7364%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 116465  11.779%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  2.265753/ 14.545841, val:  71.25%, val_best:  81.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7127%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.9260%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 117340  11.751%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  2.342133/ 11.210977, val:  81.25%, val_best:  81.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6121%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.6425%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 118206  11.722%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  2.168853/ 10.688495, val:  72.92%, val_best:  81.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7244%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.8987%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 119078  11.695%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  2.164605/ 12.176467, val:  80.42%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7080%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.6097%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 119912  11.665%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  2.220189/ 14.725294, val:  68.33%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5284%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.7882%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 120760  11.637%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  2.256563/ 16.862577, val:  63.33%, val_best:  81.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3436%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.7322%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 121575  11.606%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  2.210936/ 17.563330, val:  67.50%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3412%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.5580%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 122441  11.580%\n",
      "fc layer 1 self.abs_max_out: 3195.0\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  2.290220/ 14.583333, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2862%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.6924%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 123294  11.554%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  2.212322/ 13.250223, val:  72.92%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2486%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.8454%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 124127  11.526%\n",
      "fc layer 2 self.abs_max_out: 2729.0\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  2.147166/ 13.509244, val:  74.58%, val_best:  82.92%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3030%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.6545%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 124963  11.499%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  2.176596/ 16.882101, val:  61.25%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1933%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.6139%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 125800  11.473%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  2.193304/ 12.173252, val:  72.08%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1544%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.8168%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 126630  11.447%\n",
      "fc layer 1 self.abs_max_out: 3297.0\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  1.999581/ 15.352074, val:  68.33%, val_best:  82.92%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4431%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.6077%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 127393  11.415%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  2.173220/ 19.556862, val:  48.75%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0631%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.8426%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 128192  11.386%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  2.409589/ 12.628693, val:  81.67%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.4950%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 129083  11.367%\n",
      "lif layer 1 self.abs_max_v: 5208.5\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  2.223197/ 17.393221, val:  70.00%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0486%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.8696%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 129907  11.341%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  2.167660/ 14.385867, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0847%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.7806%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 130722  11.316%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  2.055474/ 14.038162, val:  69.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6898%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.7135%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 131514  11.289%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  2.239247/ 12.824986, val:  70.83%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1416%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.5340%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 132339  11.265%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  2.005545/ 14.244063, val:  70.00%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0477%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.8692%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 133094  11.235%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  2.251415/ 20.466080, val:  62.92%, val_best:  82.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7019%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.7256%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 133927  11.213%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  2.091436/ 14.078417, val:  60.83%, val_best:  82.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.72 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8545%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.9340%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 134736  11.189%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  2.095319/ 10.623759, val:  81.25%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.05 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9621%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.6487%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 135505  11.162%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  2.169050/ 11.263741, val:  76.67%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0098%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.6941%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 136314  11.139%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  2.292984/ 11.606203, val:  70.83%, val_best:  82.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.72 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7313%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.5978%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 137143  11.118%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  2.140272/ 15.493736, val:  66.25%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.87 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8488%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.8011%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 137939  11.094%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  1.997230/ 10.319293, val:  70.83%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4267%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.9352%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 138700  11.068%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  2.179927/ 11.605238, val:  74.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6590%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.8679%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 139517  11.047%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  1.860610/ 11.914362, val:  72.08%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9477%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.0502%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 140235  11.019%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  1.973698/ 13.470549, val:  71.25%, val_best:  82.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8745%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.1215%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 141001  10.994%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  2.180865/ 15.398238, val:  68.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.2198%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.9705%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 141857  10.977%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  2.041371/ 19.423508, val:  55.42%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0645%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.8914%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 142635  10.954%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  1.975164/ 12.400632, val:  82.50%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9973%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.0074%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 143386  10.930%\n",
      "lif layer 2 self.abs_max_v: 5118.5\n",
      "lif layer 2 self.abs_max_v: 5128.5\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  1.988700/ 12.753482, val:  78.75%, val_best:  82.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1239%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.0058%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 144136  10.906%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  2.027591/ 12.529889, val:  73.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0894%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.3398%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 144960  10.887%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  1.777513/ 12.397954, val:  78.75%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8216%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.2337%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 145656  10.860%\n",
      "fc layer 2 self.abs_max_out: 2820.0\n",
      "lif layer 2 self.abs_max_v: 5181.5\n",
      "lif layer 2 self.abs_max_v: 5343.0\n",
      "lif layer 1 self.abs_max_v: 5438.0\n",
      "lif layer 1 self.abs_max_v: 5652.0\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  1.877929/ 13.250894, val:  66.67%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9085%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.9916%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 146380  10.835%\n",
      "lif layer 1 self.abs_max_v: 5696.5\n",
      "lif layer 1 self.abs_max_v: 5916.5\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  1.763009/ 13.609123, val:  75.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8089%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.1687%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 147081  10.808%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  2.001459/ 13.205797, val:  75.00%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4206%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.0684%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 147871  10.789%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  1.927270/ 12.083395, val:  77.50%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7262%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.0895%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 148597  10.765%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  1.856799/ 12.614226, val:  59.58%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8820%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.2837%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 149334  10.742%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  1.911762/ 13.895290, val:  71.25%, val_best:  82.92%, tr:  99.39%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8795%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.0179%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 150076  10.720%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  1.904723/  9.438023, val:  82.92%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8192%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.1449%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 150804  10.697%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  1.718862/ 15.163132, val:  71.25%, val_best:  82.92%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7839%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.1266%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 151524  10.674%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  1.869710/ 15.710201, val:  59.58%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9733%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.1827%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 152233  10.651%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  1.897296/ 14.685905, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1010%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.1987%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 152990  10.631%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  1.857904/ 15.453184, val:  75.00%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0780%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.1465%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 153670  10.606%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  1.932104/ 15.034920, val:  70.00%, val_best:  82.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7806%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.1859%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 154410  10.585%\n",
      "lif layer 1 self.abs_max_v: 5924.0\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  1.957610/ 14.146625, val:  79.58%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8235%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.1091%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 155144  10.565%\n",
      "lif layer 1 self.abs_max_v: 6074.5\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  1.749755/ 13.360705, val:  82.92%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0917%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.2721%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 155829  10.541%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  1.743878/ 13.669332, val:  75.83%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1080%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.2915%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 156541  10.520%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  1.766372/ 14.326469, val:  77.50%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8981%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.0642%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 157209  10.496%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  1.842796/ 14.202180, val:  70.83%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7582%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.0274%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 157899  10.473%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  1.713473/ 11.872488, val:  79.17%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1446%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.3452%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 158564  10.449%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  1.819051/ 18.291727, val:  71.67%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.4894%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.0323%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 159261  10.428%\n",
      "lif layer 1 self.abs_max_v: 6086.0\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  1.910427/ 11.302415, val:  71.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.5100%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.2413%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 159983  10.409%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  1.650419/ 10.642683, val:  85.00%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9307%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.4002%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 160673  10.387%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  1.704737/ 12.232505, val:  75.42%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1052%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.2194%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 161316  10.363%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  1.705766/ 13.136877, val:  80.83%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8463%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.3728%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 161973  10.340%\n",
      "lif layer 1 self.abs_max_v: 6258.5\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  1.642792/ 12.118811, val:  76.67%, val_best:  85.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1048%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.4585%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 162640  10.319%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  1.866563/ 12.151188, val:  85.00%, val_best:  85.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9593%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.1514%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 163370  10.301%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  1.747606/ 17.587513, val:  57.92%, val_best:  85.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0717%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.3263%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 164057  10.281%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  1.705520/ 14.005074, val:  67.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9926%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.3188%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 164713  10.259%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  1.706916/ 10.330425, val:  84.58%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7566%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.3098%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 165402  10.239%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  1.720462/ 16.228224, val:  75.42%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9492%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.3383%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 166071  10.219%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  1.721733/ 12.364319, val:  81.25%, val_best:  85.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8337%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.2282%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 166737  10.198%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  1.644004/ 14.610991, val:  70.83%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0110%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.1155%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 167382  10.177%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  1.547950/ 13.192478, val:  69.17%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0950%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.3767%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 167975  10.153%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  1.672556/ 14.336000, val:  75.00%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4993%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.2373%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 168616  10.131%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  1.599460/ 13.569353, val:  80.00%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1141%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3741%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.3009%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 169222  10.108%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  1.606628/ 14.638878, val:  72.92%, val_best:  85.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5148%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.4670%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 169845  10.087%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  1.651799/ 11.669248, val:  70.42%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0142%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.5179%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 170491  10.066%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  1.510627/ 13.367755, val:  75.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1098%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.5694%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 171104  10.044%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  1.516246/ 12.027524, val:  80.83%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.2228%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.5174%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 171704  10.022%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  1.472411/ 10.821267, val:  77.08%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.3418%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.4872%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 172277   9.998%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  1.530612/ 11.008075, val:  78.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.4047%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.5445%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 172902   9.978%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  1.760171/ 10.782837, val:  84.17%, val_best:  85.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0312%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.2373%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.4274%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 173561   9.960%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  1.470491/ 13.163774, val:  76.25%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0376%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.2330%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.4588%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 174168   9.939%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  1.702252/ 12.643976, val:  74.58%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.3758%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.4403%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 174831   9.921%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  1.485369/ 11.826150, val:  84.58%, val_best:  85.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.2153%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.3564%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 175429   9.900%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  1.549334/ 12.483695, val:  71.25%, val_best:  85.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1527%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.4565%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 176001   9.878%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  1.458529/ 11.891037, val:  78.75%, val_best:  85.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8520%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.4119%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 176593   9.857%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  1.467001/ 15.997906, val:  62.92%, val_best:  85.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0426%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9979%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.5370%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 177199   9.837%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  1.510620/ 12.457738, val:  62.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0211%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.5604%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 177779   9.816%\n",
      "fc layer 1 self.abs_max_out: 3327.0\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  1.649551/ 11.374215, val:  73.33%, val_best:  85.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0939%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.4598%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 178437   9.799%\n",
      "fc layer 1 self.abs_max_out: 3420.0\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  1.603503/ 16.644440, val:  72.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1740%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.3180%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 179080   9.782%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  1.573911/ 12.677203, val:  78.75%, val_best:  85.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.3158%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.2791%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 179708   9.764%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  1.584601/ 12.318568, val:  81.67%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.3037%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.3747%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 180324   9.746%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  1.366525/ 11.009237, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0396%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.6256%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 180878   9.724%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  1.471958/ 12.093583, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9425%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.5920%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 181452   9.704%\n",
      "fc layer 1 self.abs_max_out: 3433.0\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  1.432999/ 18.039637, val:  66.67%, val_best:  86.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8284%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.4748%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 182008   9.683%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  1.619667/ 17.664963, val:  65.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1075%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.3113%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 182604   9.664%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  1.591803/ 12.120755, val:  72.92%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.2431%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.5237%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 183191   9.645%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  1.437639/ 15.724463, val:  72.50%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8216%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.4183%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 183790   9.627%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  1.542511/ 12.787819, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7762%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.3809%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 184379   9.609%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  1.522527/ 13.150824, val:  80.00%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8656%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.3260%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 184958   9.590%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  1.515853/ 10.656160, val:  85.83%, val_best:  86.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9295%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.5217%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 185570   9.573%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  1.554361/ 16.247984, val:  64.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1060%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.6057%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 186189   9.557%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  1.513105/ 13.206304, val:  78.33%, val_best:  86.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0775%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.4939%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f9623e22d4475c95dc0270fa9724f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñà‚ñÖ‚ñà‚ñá‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñá‚ñà‚ñÖ‚ñá‚ñÜ‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÉ‚ñÜ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñÜ‚ñá‚ñÜ‚ñá‚ñá</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñà‚ñÖ‚ñà‚ñá‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñá‚ñà‚ñÖ‚ñá‚ñÜ‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99796</td></tr><tr><td>tr_epoch_loss</td><td>1.5131</td></tr><tr><td>val_acc_best</td><td>0.86667</td></tr><tr><td>val_acc_now</td><td>0.78333</td></tr><tr><td>val_loss</td><td>13.2063</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zesty-sweep-13</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gvo9y0uy' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gvo9y0uy</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_094641-gvo9y0uy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8wq751rd with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.046875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0234375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_140340-8wq751rd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8wq751rd' target=\"_blank\">fresh-sweep-20</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8wq751rd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8wq751rd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251214_140349_884', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 64, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 32, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 32, 'lif_layer_v_threshold2': 256, 'init_scaling': [0.03125, 0.046875, 0.0234375], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 32, self.v_threshold 64\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 32, self.v_threshold 256\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=64, v_reset=10000, sg_width=32, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=256, v_reset=10000, sg_width=32, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 46.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 59.5\n",
      "fc layer 1 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 69.0\n",
      "fc layer 2 self.abs_max_out: 6.0\n",
      "lif layer 2 self.abs_max_v: 6.0\n",
      "fc layer 1 self.abs_max_out: 55.0\n",
      "lif layer 1 self.abs_max_v: 81.5\n",
      "fc layer 2 self.abs_max_out: 23.0\n",
      "lif layer 2 self.abs_max_v: 25.0\n",
      "fc layer 1 self.abs_max_out: 101.0\n",
      "lif layer 1 self.abs_max_v: 129.5\n",
      "fc layer 2 self.abs_max_out: 25.0\n",
      "lif layer 2 self.abs_max_v: 31.5\n",
      "fc layer 1 self.abs_max_out: 147.0\n",
      "lif layer 1 self.abs_max_v: 147.0\n",
      "fc layer 2 self.abs_max_out: 27.0\n",
      "lif layer 2 self.abs_max_v: 37.0\n",
      "fc layer 1 self.abs_max_out: 200.0\n",
      "lif layer 1 self.abs_max_v: 200.0\n",
      "fc layer 2 self.abs_max_out: 34.0\n",
      "lif layer 2 self.abs_max_v: 46.5\n",
      "fc layer 1 self.abs_max_out: 212.0\n",
      "lif layer 1 self.abs_max_v: 229.0\n",
      "lif layer 2 self.abs_max_v: 53.0\n",
      "fc layer 1 self.abs_max_out: 242.0\n",
      "lif layer 1 self.abs_max_v: 265.5\n",
      "fc layer 2 self.abs_max_out: 41.0\n",
      "lif layer 2 self.abs_max_v: 62.0\n",
      "lif layer 2 self.abs_max_v: 64.0\n",
      "fc layer 1 self.abs_max_out: 246.0\n",
      "lif layer 2 self.abs_max_v: 65.5\n",
      "fc layer 2 self.abs_max_out: 50.0\n",
      "lif layer 2 self.abs_max_v: 72.0\n",
      "lif layer 2 self.abs_max_v: 76.5\n",
      "fc layer 1 self.abs_max_out: 363.0\n",
      "lif layer 1 self.abs_max_v: 363.0\n",
      "fc layer 1 self.abs_max_out: 456.0\n",
      "lif layer 1 self.abs_max_v: 456.0\n",
      "lif layer 2 self.abs_max_v: 79.0\n",
      "lif layer 2 self.abs_max_v: 79.5\n",
      "fc layer 2 self.abs_max_out: 52.0\n",
      "lif layer 2 self.abs_max_v: 86.5\n",
      "fc layer 2 self.abs_max_out: 62.0\n",
      "lif layer 2 self.abs_max_v: 91.0\n",
      "lif layer 2 self.abs_max_v: 96.0\n",
      "lif layer 2 self.abs_max_v: 103.5\n",
      "fc layer 2 self.abs_max_out: 64.0\n",
      "lif layer 2 self.abs_max_v: 111.5\n",
      "fc layer 2 self.abs_max_out: 66.0\n",
      "lif layer 2 self.abs_max_v: 122.0\n",
      "fc layer 2 self.abs_max_out: 70.0\n",
      "lif layer 2 self.abs_max_v: 122.5\n",
      "fc layer 2 self.abs_max_out: 82.0\n",
      "lif layer 2 self.abs_max_v: 128.5\n",
      "fc layer 2 self.abs_max_out: 84.0\n",
      "lif layer 2 self.abs_max_v: 142.0\n",
      "lif layer 2 self.abs_max_v: 147.0\n",
      "lif layer 2 self.abs_max_v: 152.5\n",
      "fc layer 2 self.abs_max_out: 92.0\n",
      "lif layer 2 self.abs_max_v: 166.0\n",
      "fc layer 2 self.abs_max_out: 93.0\n",
      "fc layer 2 self.abs_max_out: 115.0\n",
      "lif layer 2 self.abs_max_v: 193.0\n",
      "lif layer 2 self.abs_max_v: 194.5\n",
      "lif layer 2 self.abs_max_v: 199.5\n",
      "fc layer 1 self.abs_max_out: 558.0\n",
      "lif layer 1 self.abs_max_v: 558.0\n",
      "lif layer 2 self.abs_max_v: 204.0\n",
      "lif layer 2 self.abs_max_v: 208.0\n",
      "lif layer 2 self.abs_max_v: 217.0\n",
      "fc layer 2 self.abs_max_out: 116.0\n",
      "fc layer 1 self.abs_max_out: 582.0\n",
      "lif layer 1 self.abs_max_v: 582.0\n",
      "fc layer 1 self.abs_max_out: 588.0\n",
      "lif layer 1 self.abs_max_v: 641.0\n",
      "fc layer 1 self.abs_max_out: 659.0\n",
      "lif layer 1 self.abs_max_v: 706.5\n",
      "lif layer 2 self.abs_max_v: 218.0\n",
      "lif layer 2 self.abs_max_v: 222.0\n",
      "fc layer 2 self.abs_max_out: 120.0\n",
      "lif layer 2 self.abs_max_v: 231.0\n",
      "fc layer 2 self.abs_max_out: 130.0\n",
      "lif layer 2 self.abs_max_v: 231.5\n",
      "fc layer 2 self.abs_max_out: 131.0\n",
      "lif layer 2 self.abs_max_v: 243.5\n",
      "fc layer 2 self.abs_max_out: 133.0\n",
      "lif layer 2 self.abs_max_v: 250.5\n",
      "fc layer 2 self.abs_max_out: 146.0\n",
      "lif layer 2 self.abs_max_v: 271.5\n",
      "fc layer 2 self.abs_max_out: 147.0\n",
      "fc layer 2 self.abs_max_out: 199.0\n",
      "lif layer 2 self.abs_max_v: 302.5\n",
      "fc layer 3 self.abs_max_out: 3.0\n",
      "fc layer 2 self.abs_max_out: 374.0\n",
      "lif layer 2 self.abs_max_v: 374.0\n",
      "fc layer 2 self.abs_max_out: 405.0\n",
      "lif layer 2 self.abs_max_v: 405.0\n",
      "fc layer 2 self.abs_max_out: 408.0\n",
      "lif layer 2 self.abs_max_v: 408.0\n",
      "fc layer 2 self.abs_max_out: 513.0\n",
      "lif layer 2 self.abs_max_v: 513.0\n",
      "fc layer 2 self.abs_max_out: 520.0\n",
      "lif layer 2 self.abs_max_v: 520.0\n",
      "fc layer 2 self.abs_max_out: 523.0\n",
      "lif layer 2 self.abs_max_v: 523.0\n",
      "fc layer 2 self.abs_max_out: 582.0\n",
      "lif layer 2 self.abs_max_v: 582.0\n",
      "fc layer 2 self.abs_max_out: 642.0\n",
      "lif layer 2 self.abs_max_v: 642.0\n",
      "fc layer 2 self.abs_max_out: 664.0\n",
      "lif layer 2 self.abs_max_v: 664.0\n",
      "fc layer 2 self.abs_max_out: 682.0\n",
      "lif layer 2 self.abs_max_v: 682.0\n",
      "fc layer 2 self.abs_max_out: 697.0\n",
      "lif layer 2 self.abs_max_v: 697.0\n",
      "fc layer 2 self.abs_max_out: 769.0\n",
      "lif layer 2 self.abs_max_v: 769.0\n",
      "fc layer 2 self.abs_max_out: 796.0\n",
      "lif layer 2 self.abs_max_v: 796.0\n",
      "fc layer 2 self.abs_max_out: 798.0\n",
      "lif layer 2 self.abs_max_v: 798.0\n",
      "fc layer 1 self.abs_max_out: 699.0\n",
      "fc layer 2 self.abs_max_out: 810.0\n",
      "lif layer 2 self.abs_max_v: 810.0\n",
      "fc layer 1 self.abs_max_out: 717.0\n",
      "lif layer 1 self.abs_max_v: 717.0\n",
      "fc layer 1 self.abs_max_out: 760.0\n",
      "lif layer 1 self.abs_max_v: 760.0\n",
      "fc layer 2 self.abs_max_out: 820.0\n",
      "lif layer 2 self.abs_max_v: 820.0\n",
      "lif layer 1 self.abs_max_v: 763.0\n",
      "fc layer 1 self.abs_max_out: 811.0\n",
      "lif layer 1 self.abs_max_v: 811.0\n",
      "fc layer 1 self.abs_max_out: 831.0\n",
      "lif layer 1 self.abs_max_v: 831.0\n",
      "fc layer 1 self.abs_max_out: 849.0\n",
      "lif layer 1 self.abs_max_v: 849.0\n",
      "fc layer 1 self.abs_max_out: 904.0\n",
      "lif layer 1 self.abs_max_v: 904.0\n",
      "fc layer 1 self.abs_max_out: 938.0\n",
      "lif layer 1 self.abs_max_v: 938.0\n",
      "fc layer 1 self.abs_max_out: 1013.0\n",
      "lif layer 1 self.abs_max_v: 1013.0\n",
      "fc layer 1 self.abs_max_out: 1024.0\n",
      "lif layer 1 self.abs_max_v: 1024.0\n",
      "fc layer 1 self.abs_max_out: 1097.0\n",
      "lif layer 1 self.abs_max_v: 1097.0\n",
      "fc layer 1 self.abs_max_out: 1148.0\n",
      "lif layer 1 self.abs_max_v: 1148.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss:  1.795977/  2.286793, val:  10.00%, val_best:  10.00%, tr:  57.00%, tr_best:  57.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.5434%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 99.6530%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 3092  31.583%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 1179.0\n",
      "lif layer 1 self.abs_max_v: 1179.0\n",
      "fc layer 1 self.abs_max_out: 1238.0\n",
      "lif layer 1 self.abs_max_v: 1238.0\n",
      "fc layer 1 self.abs_max_out: 1291.0\n",
      "lif layer 1 self.abs_max_v: 1291.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss:  1.892540/  2.335629, val:   9.17%, val_best:  10.00%, tr:  56.38%, tr_best:  57.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.7812%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 99.7247%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27780cdbe50b42e385aea1ed73b12cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñà‚ñÅ</td></tr><tr><td>tr_acc</td><td>‚ñà‚ñÅ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÅ‚ñà</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_now</td><td>‚ñà‚ñÅ</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>iter_acc</td><td>0.0</td></tr><tr><td>tr_acc</td><td>0.56384</td></tr><tr><td>tr_epoch_loss</td><td>1.89254</td></tr><tr><td>val_acc_best</td><td>0.1</td></tr><tr><td>val_acc_now</td><td>0.09167</td></tr><tr><td>val_loss</td><td>2.33563</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fresh-sweep-20</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8wq751rd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8wq751rd</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_140340-8wq751rd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 8wq751rd errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31401/1959304274.py\", line 119, in hyper_iter\n",
      "    my_snn_system(\n",
      "  File \"/tmp/ipykernel_31401/3596498357.py\", line 929, in my_snn_system\n",
      "    assert val_acc_best > 0.2\n",
      "AssertionError\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 8wq751rd errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31401/1959304274.py\", line 119, in hyper_iter\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     my_snn_system(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31401/3596498357.py\", line 929, in my_snn_system\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     assert val_acc_best > 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m AssertionError\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: c56nqp3z with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.046875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0234375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_140647-c56nqp3z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/c56nqp3z' target=\"_blank\">ethereal-sweep-21</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/c56nqp3z' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/c56nqp3z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251214_140657_031', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 32, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 64, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 1, 'lif_layer_v_threshold2': 256, 'init_scaling': [0.03125, 0.046875, 0.0234375], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 64, self.v_threshold 32\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 1, self.v_threshold 256\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=32, v_reset=10000, sg_width=64, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=256, v_reset=10000, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 46.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 6.0\n",
      "lif layer 2 self.abs_max_v: 6.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 59.5\n",
      "fc layer 2 self.abs_max_out: 36.0\n",
      "lif layer 2 self.abs_max_v: 35.5\n",
      "fc layer 1 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 69.0\n",
      "fc layer 2 self.abs_max_out: 48.0\n",
      "lif layer 2 self.abs_max_v: 53.0\n",
      "fc layer 1 self.abs_max_out: 55.0\n",
      "lif layer 1 self.abs_max_v: 70.0\n",
      "lif layer 2 self.abs_max_v: 59.5\n",
      "fc layer 1 self.abs_max_out: 101.0\n",
      "lif layer 1 self.abs_max_v: 101.0\n",
      "fc layer 2 self.abs_max_out: 50.0\n",
      "lif layer 2 self.abs_max_v: 66.0\n",
      "fc layer 1 self.abs_max_out: 147.0\n",
      "lif layer 1 self.abs_max_v: 149.5\n",
      "fc layer 2 self.abs_max_out: 80.0\n",
      "lif layer 2 self.abs_max_v: 113.0\n",
      "fc layer 1 self.abs_max_out: 200.0\n",
      "lif layer 1 self.abs_max_v: 200.0\n",
      "fc layer 2 self.abs_max_out: 102.0\n",
      "lif layer 2 self.abs_max_v: 158.5\n",
      "fc layer 2 self.abs_max_out: 108.0\n",
      "lif layer 2 self.abs_max_v: 187.5\n",
      "lif layer 1 self.abs_max_v: 209.0\n",
      "fc layer 2 self.abs_max_out: 115.0\n",
      "lif layer 2 self.abs_max_v: 203.0\n",
      "lif layer 2 self.abs_max_v: 205.5\n",
      "fc layer 1 self.abs_max_out: 210.0\n",
      "lif layer 1 self.abs_max_v: 210.0\n",
      "fc layer 2 self.abs_max_out: 155.0\n",
      "lif layer 2 self.abs_max_v: 222.5\n",
      "fc layer 2 self.abs_max_out: 183.0\n",
      "lif layer 2 self.abs_max_v: 291.0\n",
      "fc layer 3 self.abs_max_out: 3.0\n",
      "fc layer 2 self.abs_max_out: 230.0\n",
      "lif layer 2 self.abs_max_v: 368.0\n",
      "fc layer 3 self.abs_max_out: 11.0\n",
      "fc layer 1 self.abs_max_out: 214.0\n",
      "lif layer 1 self.abs_max_v: 214.0\n",
      "fc layer 2 self.abs_max_out: 254.0\n",
      "lif layer 2 self.abs_max_v: 402.5\n",
      "fc layer 1 self.abs_max_out: 308.0\n",
      "lif layer 1 self.abs_max_v: 308.0\n",
      "fc layer 2 self.abs_max_out: 286.0\n",
      "lif layer 2 self.abs_max_v: 444.5\n",
      "fc layer 2 self.abs_max_out: 351.0\n",
      "lif layer 2 self.abs_max_v: 497.5\n",
      "fc layer 3 self.abs_max_out: 26.0\n",
      "lif layer 2 self.abs_max_v: 519.0\n",
      "lif layer 2 self.abs_max_v: 527.5\n",
      "fc layer 2 self.abs_max_out: 362.0\n",
      "lif layer 2 self.abs_max_v: 560.0\n",
      "fc layer 3 self.abs_max_out: 29.0\n",
      "fc layer 3 self.abs_max_out: 31.0\n",
      "fc layer 3 self.abs_max_out: 33.0\n",
      "lif layer 2 self.abs_max_v: 590.0\n",
      "lif layer 2 self.abs_max_v: 624.0\n",
      "lif layer 1 self.abs_max_v: 353.5\n",
      "lif layer 1 self.abs_max_v: 363.0\n",
      "lif layer 1 self.abs_max_v: 371.0\n",
      "lif layer 1 self.abs_max_v: 434.5\n",
      "fc layer 1 self.abs_max_out: 321.0\n",
      "fc layer 2 self.abs_max_out: 367.0\n",
      "lif layer 2 self.abs_max_v: 664.0\n",
      "lif layer 1 self.abs_max_v: 441.0\n",
      "fc layer 2 self.abs_max_out: 387.0\n",
      "lif layer 2 self.abs_max_v: 719.0\n",
      "fc layer 1 self.abs_max_out: 322.0\n",
      "fc layer 2 self.abs_max_out: 395.0\n",
      "lif layer 2 self.abs_max_v: 728.0\n",
      "lif layer 2 self.abs_max_v: 730.0\n",
      "fc layer 1 self.abs_max_out: 340.0\n",
      "lif layer 1 self.abs_max_v: 472.0\n",
      "lif layer 1 self.abs_max_v: 500.0\n",
      "fc layer 2 self.abs_max_out: 411.0\n",
      "lif layer 2 self.abs_max_v: 767.0\n",
      "fc layer 2 self.abs_max_out: 511.0\n",
      "lif layer 2 self.abs_max_v: 792.0\n",
      "lif layer 2 self.abs_max_v: 864.0\n",
      "fc layer 3 self.abs_max_out: 41.0\n",
      "fc layer 3 self.abs_max_out: 43.0\n",
      "fc layer 2 self.abs_max_out: 512.0\n",
      "fc layer 3 self.abs_max_out: 47.0\n",
      "lif layer 2 self.abs_max_v: 884.0\n",
      "fc layer 2 self.abs_max_out: 581.0\n",
      "lif layer 2 self.abs_max_v: 940.5\n",
      "lif layer 2 self.abs_max_v: 976.5\n",
      "lif layer 2 self.abs_max_v: 1044.5\n",
      "fc layer 2 self.abs_max_out: 632.0\n",
      "lif layer 2 self.abs_max_v: 1154.5\n",
      "lif layer 2 self.abs_max_v: 1200.5\n",
      "fc layer 2 self.abs_max_out: 684.0\n",
      "fc layer 1 self.abs_max_out: 349.0\n",
      "fc layer 1 self.abs_max_out: 395.0\n",
      "lif layer 1 self.abs_max_v: 515.0\n",
      "lif layer 2 self.abs_max_v: 1219.5\n",
      "lif layer 2 self.abs_max_v: 1275.5\n",
      "fc layer 1 self.abs_max_out: 453.0\n",
      "lif layer 1 self.abs_max_v: 537.5\n",
      "lif layer 2 self.abs_max_v: 1308.0\n",
      "lif layer 1 self.abs_max_v: 550.0\n",
      "lif layer 1 self.abs_max_v: 569.0\n",
      "lif layer 1 self.abs_max_v: 573.5\n",
      "fc layer 3 self.abs_max_out: 59.0\n",
      "fc layer 2 self.abs_max_out: 692.0\n",
      "fc layer 2 self.abs_max_out: 707.0\n",
      "fc layer 2 self.abs_max_out: 735.0\n",
      "lif layer 2 self.abs_max_v: 1371.0\n",
      "lif layer 2 self.abs_max_v: 1393.5\n",
      "lif layer 2 self.abs_max_v: 1397.0\n",
      "fc layer 3 self.abs_max_out: 61.0\n",
      "fc layer 3 self.abs_max_out: 62.0\n",
      "fc layer 3 self.abs_max_out: 69.0\n",
      "lif layer 2 self.abs_max_v: 1399.0\n",
      "fc layer 3 self.abs_max_out: 75.0\n",
      "fc layer 3 self.abs_max_out: 81.0\n",
      "fc layer 3 self.abs_max_out: 83.0\n",
      "fc layer 3 self.abs_max_out: 88.0\n",
      "fc layer 3 self.abs_max_out: 89.0\n",
      "fc layer 3 self.abs_max_out: 92.0\n",
      "fc layer 3 self.abs_max_out: 105.0\n",
      "fc layer 3 self.abs_max_out: 109.0\n",
      "fc layer 2 self.abs_max_out: 753.0\n",
      "lif layer 2 self.abs_max_v: 1425.0\n",
      "fc layer 2 self.abs_max_out: 755.0\n",
      "lif layer 2 self.abs_max_v: 1454.0\n",
      "fc layer 2 self.abs_max_out: 809.0\n",
      "fc layer 2 self.abs_max_out: 881.0\n",
      "lif layer 2 self.abs_max_v: 1494.0\n",
      "lif layer 2 self.abs_max_v: 1562.0\n",
      "lif layer 2 self.abs_max_v: 1570.0\n",
      "fc layer 3 self.abs_max_out: 112.0\n",
      "fc layer 3 self.abs_max_out: 115.0\n",
      "fc layer 2 self.abs_max_out: 914.0\n",
      "lif layer 2 self.abs_max_v: 1589.5\n",
      "lif layer 2 self.abs_max_v: 1646.0\n",
      "lif layer 2 self.abs_max_v: 1671.5\n",
      "lif layer 2 self.abs_max_v: 1698.0\n",
      "lif layer 2 self.abs_max_v: 1742.0\n",
      "fc layer 3 self.abs_max_out: 119.0\n",
      "fc layer 3 self.abs_max_out: 123.0\n",
      "fc layer 3 self.abs_max_out: 128.0\n",
      "fc layer 3 self.abs_max_out: 138.0\n",
      "fc layer 1 self.abs_max_out: 469.0\n",
      "fc layer 1 self.abs_max_out: 483.0\n",
      "fc layer 1 self.abs_max_out: 489.0\n",
      "lif layer 1 self.abs_max_v: 580.5\n",
      "lif layer 1 self.abs_max_v: 603.5\n",
      "fc layer 2 self.abs_max_out: 916.0\n",
      "lif layer 2 self.abs_max_v: 1758.0\n",
      "fc layer 2 self.abs_max_out: 943.0\n",
      "lif layer 2 self.abs_max_v: 1822.0\n",
      "lif layer 1 self.abs_max_v: 729.5\n",
      "lif layer 1 self.abs_max_v: 764.0\n",
      "fc layer 3 self.abs_max_out: 140.0\n",
      "fc layer 1 self.abs_max_out: 515.0\n",
      "fc layer 3 self.abs_max_out: 160.0\n",
      "fc layer 1 self.abs_max_out: 517.0\n",
      "fc layer 2 self.abs_max_out: 948.0\n",
      "fc layer 3 self.abs_max_out: 164.0\n",
      "fc layer 1 self.abs_max_out: 550.0\n",
      "fc layer 2 self.abs_max_out: 949.0\n",
      "fc layer 2 self.abs_max_out: 990.0\n",
      "lif layer 2 self.abs_max_v: 1860.0\n",
      "fc layer 2 self.abs_max_out: 997.0\n",
      "fc layer 3 self.abs_max_out: 166.0\n",
      "fc layer 3 self.abs_max_out: 168.0\n",
      "fc layer 3 self.abs_max_out: 175.0\n",
      "fc layer 1 self.abs_max_out: 557.0\n",
      "fc layer 1 self.abs_max_out: 572.0\n",
      "fc layer 1 self.abs_max_out: 597.0\n",
      "lif layer 1 self.abs_max_v: 771.0\n",
      "lif layer 1 self.abs_max_v: 801.0\n",
      "lif layer 1 self.abs_max_v: 802.5\n",
      "fc layer 3 self.abs_max_out: 176.0\n",
      "fc layer 3 self.abs_max_out: 203.0\n",
      "lif layer 1 self.abs_max_v: 820.0\n",
      "lif layer 1 self.abs_max_v: 848.0\n",
      "lif layer 1 self.abs_max_v: 863.5\n",
      "fc layer 2 self.abs_max_out: 1009.0\n",
      "fc layer 2 self.abs_max_out: 1028.0\n",
      "lif layer 1 self.abs_max_v: 896.5\n",
      "fc layer 3 self.abs_max_out: 205.0\n",
      "fc layer 2 self.abs_max_out: 1059.0\n",
      "fc layer 2 self.abs_max_out: 1083.0\n",
      "fc layer 2 self.abs_max_out: 1113.0\n",
      "fc layer 1 self.abs_max_out: 625.0\n",
      "fc layer 1 self.abs_max_out: 664.0\n",
      "lif layer 1 self.abs_max_v: 918.5\n",
      "lif layer 1 self.abs_max_v: 943.0\n",
      "fc layer 2 self.abs_max_out: 1127.0\n",
      "fc layer 3 self.abs_max_out: 206.0\n",
      "lif layer 1 self.abs_max_v: 1055.5\n",
      "lif layer 1 self.abs_max_v: 1058.5\n",
      "lif layer 1 self.abs_max_v: 1115.5\n",
      "fc layer 1 self.abs_max_out: 694.0\n",
      "lif layer 1 self.abs_max_v: 1222.0\n",
      "fc layer 2 self.abs_max_out: 1129.0\n",
      "fc layer 2 self.abs_max_out: 1154.0\n",
      "fc layer 3 self.abs_max_out: 214.0\n",
      "fc layer 3 self.abs_max_out: 215.0\n",
      "fc layer 2 self.abs_max_out: 1218.0\n",
      "fc layer 2 self.abs_max_out: 1228.0\n",
      "fc layer 1 self.abs_max_out: 711.0\n",
      "lif layer 1 self.abs_max_v: 1274.5\n",
      "lif layer 1 self.abs_max_v: 1305.0\n",
      "fc layer 1 self.abs_max_out: 718.0\n",
      "lif layer 1 self.abs_max_v: 1344.5\n",
      "lif layer 1 self.abs_max_v: 1366.5\n",
      "fc layer 1 self.abs_max_out: 763.0\n",
      "lif layer 1 self.abs_max_v: 1446.5\n",
      "fc layer 2 self.abs_max_out: 1254.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss:  7.279467/ 47.701401, val:  31.67%, val_best:  31.67%, tr:  97.85%, tr_best:  97.85%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.8837%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.8313%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 2082  21.267%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 230.0\n",
      "fc layer 2 self.abs_max_out: 1278.0\n",
      "fc layer 2 self.abs_max_out: 1290.0\n",
      "fc layer 2 self.abs_max_out: 1320.0\n",
      "fc layer 2 self.abs_max_out: 1329.0\n",
      "fc layer 2 self.abs_max_out: 1342.0\n",
      "fc layer 3 self.abs_max_out: 238.0\n",
      "fc layer 2 self.abs_max_out: 1371.0\n",
      "fc layer 2 self.abs_max_out: 1377.0\n",
      "fc layer 2 self.abs_max_out: 1403.0\n",
      "fc layer 3 self.abs_max_out: 240.0\n",
      "fc layer 3 self.abs_max_out: 241.0\n",
      "fc layer 3 self.abs_max_out: 264.0\n",
      "fc layer 2 self.abs_max_out: 1438.0\n",
      "fc layer 2 self.abs_max_out: 1474.0\n",
      "fc layer 3 self.abs_max_out: 275.0\n",
      "lif layer 2 self.abs_max_v: 1942.0\n",
      "lif layer 2 self.abs_max_v: 1993.5\n",
      "fc layer 2 self.abs_max_out: 1476.0\n",
      "fc layer 2 self.abs_max_out: 1486.0\n",
      "fc layer 2 self.abs_max_out: 1502.0\n",
      "fc layer 2 self.abs_max_out: 1516.0\n",
      "fc layer 2 self.abs_max_out: 1579.0\n",
      "fc layer 3 self.abs_max_out: 328.0\n",
      "fc layer 1 self.abs_max_out: 767.0\n",
      "lif layer 2 self.abs_max_v: 2027.5\n",
      "fc layer 2 self.abs_max_out: 1600.0\n",
      "fc layer 2 self.abs_max_out: 1618.0\n",
      "fc layer 2 self.abs_max_out: 1660.0\n",
      "fc layer 2 self.abs_max_out: 1661.0\n",
      "fc layer 2 self.abs_max_out: 1705.0\n",
      "lif layer 2 self.abs_max_v: 2095.5\n",
      "lif layer 2 self.abs_max_v: 2099.0\n",
      "fc layer 2 self.abs_max_out: 1784.0\n",
      "fc layer 1 self.abs_max_out: 783.0\n",
      "lif layer 2 self.abs_max_v: 2186.0\n",
      "lif layer 2 self.abs_max_v: 2224.0\n",
      "lif layer 2 self.abs_max_v: 2371.0\n",
      "fc layer 2 self.abs_max_out: 1792.0\n",
      "fc layer 2 self.abs_max_out: 1869.0\n",
      "fc layer 2 self.abs_max_out: 1943.0\n",
      "fc layer 2 self.abs_max_out: 1949.0\n",
      "fc layer 1 self.abs_max_out: 801.0\n",
      "fc layer 1 self.abs_max_out: 817.0\n",
      "lif layer 1 self.abs_max_v: 1475.0\n",
      "lif layer 1 self.abs_max_v: 1506.0\n",
      "fc layer 1 self.abs_max_out: 853.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss:  8.597835/ 55.748058, val:  34.17%, val_best:  34.17%, tr:  98.88%, tr_best:  98.88%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1791%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.5952%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3745  19.127%\n",
      "fc layer 2 self.abs_max_out: 2004.0\n",
      "fc layer 2 self.abs_max_out: 2011.0\n",
      "fc layer 2 self.abs_max_out: 2012.0\n",
      "fc layer 2 self.abs_max_out: 2063.0\n",
      "fc layer 2 self.abs_max_out: 2085.0\n",
      "fc layer 2 self.abs_max_out: 2123.0\n",
      "fc layer 2 self.abs_max_out: 2171.0\n",
      "fc layer 2 self.abs_max_out: 2244.0\n",
      "fc layer 2 self.abs_max_out: 2266.0\n",
      "fc layer 2 self.abs_max_out: 2288.0\n",
      "fc layer 2 self.abs_max_out: 2294.0\n",
      "lif layer 2 self.abs_max_v: 2426.5\n",
      "lif layer 2 self.abs_max_v: 2500.5\n",
      "lif layer 2 self.abs_max_v: 2538.5\n",
      "lif layer 2 self.abs_max_v: 2745.0\n",
      "lif layer 2 self.abs_max_v: 2859.5\n",
      "fc layer 3 self.abs_max_out: 334.0\n",
      "fc layer 3 self.abs_max_out: 361.0\n",
      "lif layer 2 self.abs_max_v: 2896.5\n",
      "lif layer 2 self.abs_max_v: 2900.5\n",
      "lif layer 2 self.abs_max_v: 2907.5\n",
      "lif layer 2 self.abs_max_v: 2934.0\n",
      "fc layer 1 self.abs_max_out: 864.0\n",
      "lif layer 1 self.abs_max_v: 1595.0\n",
      "fc layer 1 self.abs_max_out: 882.0\n",
      "lif layer 1 self.abs_max_v: 1679.5\n",
      "fc layer 1 self.abs_max_out: 924.0\n",
      "lif layer 1 self.abs_max_v: 1764.0\n",
      "lif layer 1 self.abs_max_v: 1780.0\n",
      "lif layer 1 self.abs_max_v: 1808.0\n",
      "lif layer 2 self.abs_max_v: 2942.0\n",
      "lif layer 2 self.abs_max_v: 2988.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss:  9.306556/ 44.470089, val:  33.75%, val_best:  34.17%, tr:  98.98%, tr_best:  98.98%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.9432%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5944%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 5244  17.855%\n",
      "fc layer 2 self.abs_max_out: 2321.0\n",
      "fc layer 2 self.abs_max_out: 2326.0\n",
      "lif layer 2 self.abs_max_v: 3090.5\n",
      "lif layer 2 self.abs_max_v: 3392.5\n",
      "lif layer 2 self.abs_max_v: 3485.5\n",
      "fc layer 2 self.abs_max_out: 2373.0\n",
      "fc layer 3 self.abs_max_out: 407.0\n",
      "fc layer 2 self.abs_max_out: 2379.0\n",
      "fc layer 2 self.abs_max_out: 2387.0\n",
      "fc layer 2 self.abs_max_out: 2389.0\n",
      "fc layer 2 self.abs_max_out: 2462.0\n",
      "fc layer 2 self.abs_max_out: 2482.0\n",
      "fc layer 2 self.abs_max_out: 2498.0\n",
      "fc layer 2 self.abs_max_out: 2500.0\n",
      "fc layer 2 self.abs_max_out: 2509.0\n",
      "fc layer 2 self.abs_max_out: 2547.0\n",
      "fc layer 2 self.abs_max_out: 2564.0\n",
      "fc layer 2 self.abs_max_out: 2644.0\n",
      "fc layer 1 self.abs_max_out: 968.0\n",
      "fc layer 1 self.abs_max_out: 972.0\n",
      "lif layer 1 self.abs_max_v: 1867.0\n",
      "lif layer 1 self.abs_max_v: 1869.5\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  9.227834/101.089989, val:  25.00%, val_best:  34.17%, tr:  99.18%, tr_best:  99.18%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4464%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0267%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6641  16.959%\n",
      "lif layer 2 self.abs_max_v: 3490.5\n",
      "lif layer 2 self.abs_max_v: 3588.5\n",
      "lif layer 2 self.abs_max_v: 3695.0\n",
      "lif layer 2 self.abs_max_v: 3763.5\n",
      "lif layer 2 self.abs_max_v: 3796.0\n",
      "fc layer 2 self.abs_max_out: 2650.0\n",
      "fc layer 1 self.abs_max_out: 1003.0\n",
      "lif layer 1 self.abs_max_v: 1927.5\n",
      "lif layer 1 self.abs_max_v: 1945.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  9.887951/ 67.949059, val:  37.50%, val_best:  37.50%, tr:  99.18%, tr_best:  99.18%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.8190%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4432%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 8107  16.562%\n",
      "fc layer 2 self.abs_max_out: 2682.0\n",
      "lif layer 2 self.abs_max_v: 3797.0\n",
      "lif layer 2 self.abs_max_v: 4092.5\n",
      "lif layer 2 self.abs_max_v: 4171.0\n",
      "fc layer 2 self.abs_max_out: 2722.0\n",
      "fc layer 2 self.abs_max_out: 2744.0\n",
      "fc layer 2 self.abs_max_out: 2769.0\n",
      "fc layer 1 self.abs_max_out: 1016.0\n",
      "fc layer 2 self.abs_max_out: 2809.0\n",
      "fc layer 1 self.abs_max_out: 1062.0\n",
      "lif layer 2 self.abs_max_v: 4203.5\n",
      "fc layer 1 self.abs_max_out: 1092.0\n",
      "lif layer 1 self.abs_max_v: 1961.5\n",
      "lif layer 1 self.abs_max_v: 2043.0\n",
      "lif layer 1 self.abs_max_v: 2057.5\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  9.656728/ 56.810719, val:  28.75%, val_best:  37.50%, tr:  98.88%, tr_best:  99.18%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5921%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2582%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9603  16.348%\n",
      "lif layer 2 self.abs_max_v: 4208.5\n",
      "lif layer 2 self.abs_max_v: 4270.5\n",
      "lif layer 2 self.abs_max_v: 4339.0\n",
      "fc layer 1 self.abs_max_out: 1105.0\n",
      "fc layer 1 self.abs_max_out: 1138.0\n",
      "fc layer 2 self.abs_max_out: 2816.0\n",
      "fc layer 2 self.abs_max_out: 2834.0\n",
      "lif layer 1 self.abs_max_v: 2085.5\n",
      "lif layer 1 self.abs_max_v: 2102.0\n",
      "fc layer 2 self.abs_max_out: 2854.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  8.892570/ 75.487366, val:  37.92%, val_best:  37.92%, tr:  98.47%, tr_best:  99.18%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.7727%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.8851%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 11045  16.117%\n",
      "fc layer 2 self.abs_max_out: 2887.0\n",
      "fc layer 1 self.abs_max_out: 1167.0\n",
      "fc layer 1 self.abs_max_out: 1228.0\n",
      "lif layer 2 self.abs_max_v: 4410.5\n",
      "lif layer 2 self.abs_max_v: 4538.0\n",
      "lif layer 2 self.abs_max_v: 4566.5\n",
      "fc layer 2 self.abs_max_out: 2938.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  8.560625/ 49.545174, val:  38.75%, val_best:  38.75%, tr:  99.08%, tr_best:  99.18%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5459%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5615%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 12423  15.862%\n",
      "fc layer 2 self.abs_max_out: 2969.0\n",
      "lif layer 2 self.abs_max_v: 4662.0\n",
      "lif layer 2 self.abs_max_v: 4823.5\n",
      "lif layer 2 self.abs_max_v: 4868.0\n",
      "fc layer 2 self.abs_max_out: 2991.0\n",
      "fc layer 1 self.abs_max_out: 1247.0\n",
      "lif layer 1 self.abs_max_v: 2113.0\n",
      "lif layer 1 self.abs_max_v: 2232.0\n",
      "fc layer 2 self.abs_max_out: 3000.0\n",
      "fc layer 2 self.abs_max_out: 3038.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  8.820460/ 51.663200, val:  36.67%, val_best:  38.75%, tr:  98.16%, tr_best:  99.18%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.7105%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.8497%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 13892  15.767%\n",
      "fc layer 2 self.abs_max_out: 3070.0\n",
      "fc layer 2 self.abs_max_out: 3100.0\n",
      "lif layer 2 self.abs_max_v: 5081.0\n",
      "fc layer 2 self.abs_max_out: 3126.0\n",
      "fc layer 2 self.abs_max_out: 3140.0\n",
      "fc layer 2 self.abs_max_out: 3246.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  8.531243/ 62.710403, val:  30.42%, val_best:  38.75%, tr:  98.88%, tr_best:  99.18%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.1352%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6394%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 15295  15.623%\n",
      "lif layer 2 self.abs_max_v: 5241.5\n",
      "fc layer 1 self.abs_max_out: 1251.0\n",
      "fc layer 1 self.abs_max_out: 1263.0\n",
      "lif layer 1 self.abs_max_v: 2250.5\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  8.259162/ 50.287170, val:  41.67%, val_best:  41.67%, tr:  99.18%, tr_best:  99.18%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.0125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.9505%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 16692  15.500%\n",
      "lif layer 2 self.abs_max_v: 5370.0\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  7.771903/ 54.285309, val:  44.17%, val_best:  44.17%, tr:  99.18%, tr_best:  99.18%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.1670%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.3260%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 18084  15.393%\n",
      "lif layer 2 self.abs_max_v: 5549.0\n",
      "lif layer 2 self.abs_max_v: 5650.5\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  7.008252/ 47.211010, val:  37.08%, val_best:  44.17%, tr:  98.47%, tr_best:  99.18%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.7160%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.5912%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 19455  15.286%\n",
      "fc layer 1 self.abs_max_out: 1354.0\n",
      "lif layer 1 self.abs_max_v: 2278.5\n",
      "lif layer 1 self.abs_max_v: 2401.5\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  6.967988/ 55.183708, val:  35.83%, val_best:  44.17%, tr:  98.88%, tr_best:  99.18%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9946%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.8578%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 20830  15.198%\n",
      "fc layer 1 self.abs_max_out: 1403.0\n",
      "lif layer 1 self.abs_max_v: 2436.0\n",
      "lif layer 1 self.abs_max_v: 2472.0\n",
      "lif layer 1 self.abs_max_v: 2551.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  6.699904/ 36.452572, val:  40.00%, val_best:  44.17%, tr:  98.16%, tr_best:  99.18%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.7533%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.3976%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 22201  15.118%\n",
      "fc layer 1 self.abs_max_out: 1414.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  6.927282/ 31.603607, val:  44.58%, val_best:  44.58%, tr:  98.47%, tr_best:  99.18%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9863%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.8294%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 23615  15.076%\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  5.965822/ 42.363724, val:  41.67%, val_best:  44.58%, tr:  98.37%, tr_best:  99.18%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.8811%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.3982%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 24907  14.965%\n",
      "fc layer 1 self.abs_max_out: 1500.0\n",
      "lif layer 1 self.abs_max_v: 2586.5\n",
      "lif layer 1 self.abs_max_v: 2588.5\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  6.346008/ 36.734440, val:  44.58%, val_best:  44.58%, tr:  98.37%, tr_best:  99.18%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.8281%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.5348%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 26250  14.896%\n",
      "fc layer 1 self.abs_max_out: 1504.0\n",
      "lif layer 1 self.abs_max_v: 2621.5\n",
      "lif layer 1 self.abs_max_v: 2752.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  6.622814/ 38.017773, val:  43.33%, val_best:  44.58%, tr:  98.57%, tr_best:  99.18%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3220%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.9681%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 27691  14.887%\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  6.109996/ 38.211506, val:  36.25%, val_best:  44.58%, tr:  98.67%, tr_best:  99.18%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2744%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.3126%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 29035  14.829%\n",
      "lif layer 2 self.abs_max_v: 5694.5\n",
      "fc layer 1 self.abs_max_out: 1559.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  5.861501/ 50.583935, val:  32.50%, val_best:  44.58%, tr:  99.08%, tr_best:  99.18%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.8255%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.5380%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 30354  14.764%\n",
      "fc layer 2 self.abs_max_out: 3264.0\n",
      "lif layer 2 self.abs_max_v: 5921.5\n",
      "lif layer 2 self.abs_max_v: 5940.5\n",
      "lif layer 2 self.abs_max_v: 6072.5\n",
      "fc layer 2 self.abs_max_out: 3587.0\n",
      "lif layer 2 self.abs_max_v: 6257.5\n",
      "fc layer 2 self.abs_max_out: 3664.0\n",
      "lif layer 2 self.abs_max_v: 6793.0\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  6.068044/ 29.532063, val:  44.58%, val_best:  44.58%, tr:  98.26%, tr_best:  99.18%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.0633%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.2454%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 31790  14.760%\n",
      "lif layer 1 self.abs_max_v: 2764.0\n",
      "lif layer 1 self.abs_max_v: 2777.0\n",
      "lif layer 1 self.abs_max_v: 2850.5\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  5.542679/ 21.922787, val:  49.58%, val_best:  49.58%, tr:  98.77%, tr_best:  99.18%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.7742%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.0681%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 33137  14.716%\n",
      "fc layer 1 self.abs_max_out: 1603.0\n",
      "fc layer 1 self.abs_max_out: 1634.0\n",
      "lif layer 1 self.abs_max_v: 2929.5\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  5.544720/ 35.047832, val:  45.00%, val_best:  49.58%, tr:  98.06%, tr_best:  99.18%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.7364%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.9000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 34490  14.679%\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  5.750026/ 33.568939, val:  39.58%, val_best:  49.58%, tr:  98.26%, tr_best:  99.18%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.8176%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.5778%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 35851  14.648%\n",
      "fc layer 1 self.abs_max_out: 1712.0\n",
      "lif layer 1 self.abs_max_v: 2995.0\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  6.028821/ 24.686026, val:  46.25%, val_best:  49.58%, tr:  98.16%, tr_best:  99.18%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9771%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.8475%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 37333  14.667%\n",
      "fc layer 1 self.abs_max_out: 1720.0\n",
      "lif layer 1 self.abs_max_v: 3075.0\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  5.740491/ 29.397921, val:  52.08%, val_best:  52.08%, tr:  98.98%, tr_best:  99.18%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.8458%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.3991%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 38688  14.636%\n",
      "fc layer 1 self.abs_max_out: 1802.0\n",
      "lif layer 1 self.abs_max_v: 3167.0\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  5.856235/ 23.037125, val:  41.67%, val_best:  52.08%, tr:  99.08%, tr_best:  99.18%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6415%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.4988%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 40064  14.615%\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  5.493965/ 19.827261, val:  45.42%, val_best:  52.08%, tr:  98.26%, tr_best:  99.18%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.1219%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.7713%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 41436  14.595%\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  5.747784/ 46.259632, val:  40.00%, val_best:  52.08%, tr:  99.28%, tr_best:  99.28%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.0404%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.0852%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 42785  14.568%\n",
      "fc layer 1 self.abs_max_out: 1814.0\n",
      "fc layer 1 self.abs_max_out: 1865.0\n",
      "lif layer 1 self.abs_max_v: 3251.5\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  5.903088/ 23.628345, val:  54.58%, val_best:  54.58%, tr:  98.26%, tr_best:  99.28%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3812%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.6910%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 44200  14.564%\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  5.909010/ 47.897713, val:  38.33%, val_best:  54.58%, tr:  98.77%, tr_best:  99.28%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.9174%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.3947%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 45634  14.567%\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  5.674065/ 30.397676, val:  39.17%, val_best:  54.58%, tr:  97.96%, tr_best:  99.28%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5916%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.4821%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 46958  14.535%\n",
      "fc layer 1 self.abs_max_out: 1882.0\n",
      "lif layer 1 self.abs_max_v: 3351.5\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  5.507957/ 34.275478, val:  44.17%, val_best:  54.58%, tr:  98.88%, tr_best:  99.28%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.7799%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.8873%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 48296  14.509%\n",
      "fc layer 1 self.abs_max_out: 1953.0\n",
      "lif layer 1 self.abs_max_v: 3439.0\n",
      "lif layer 1 self.abs_max_v: 3465.5\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  5.560351/ 37.813557, val:  36.67%, val_best:  54.58%, tr:  98.26%, tr_best:  99.28%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.3970%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.9959%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 49666  14.495%\n",
      "fc layer 1 self.abs_max_out: 1988.0\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  5.930195/ 31.759386, val:  52.50%, val_best:  54.58%, tr:  98.67%, tr_best:  99.28%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.1682%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.7420%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 50982  14.465%\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  6.120636/ 33.931004, val:  52.50%, val_best:  54.58%, tr:  98.98%, tr_best:  99.28%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.2471%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.2306%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 52291  14.436%\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  5.491827/ 46.626659, val:  40.83%, val_best:  54.58%, tr:  98.16%, tr_best:  99.28%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.1430%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.3787%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 53553  14.395%\n",
      "fc layer 1 self.abs_max_out: 2012.0\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  5.670537/ 28.913992, val:  46.67%, val_best:  54.58%, tr:  98.88%, tr_best:  99.28%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.5720%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.2228%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 54887  14.375%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  5.487381/ 32.259388, val:  40.42%, val_best:  54.58%, tr:  98.57%, tr_best:  99.28%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.4434%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.2766%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 56191  14.349%\n",
      "fc layer 1 self.abs_max_out: 2058.0\n",
      "lif layer 1 self.abs_max_v: 3482.5\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  5.292199/ 30.492723, val:  50.42%, val_best:  54.58%, tr:  98.57%, tr_best:  99.28%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.1315%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.9049%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 57471  14.318%\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  5.131679/ 24.717951, val:  56.67%, val_best:  56.67%, tr:  98.98%, tr_best:  99.28%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.4116%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.1692%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 58773  14.294%\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  5.316960/ 35.271950, val:  47.50%, val_best:  56.67%, tr:  98.37%, tr_best:  99.28%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.8815%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.9171%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 60058  14.267%\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  5.314622/ 27.533289, val:  48.75%, val_best:  56.67%, tr:  98.67%, tr_best:  99.28%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.8251%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.0061%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 61404  14.255%\n",
      "fc layer 1 self.abs_max_out: 2155.0\n",
      "lif layer 1 self.abs_max_v: 3649.0\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  5.631305/ 30.379700, val:  44.58%, val_best:  56.67%, tr:  98.06%, tr_best:  99.28%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4775%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.6079%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 62746  14.243%\n",
      "fc layer 1 self.abs_max_out: 2182.0\n",
      "lif layer 1 self.abs_max_v: 3795.0\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  5.613573/ 35.715958, val:  46.25%, val_best:  56.67%, tr:  98.57%, tr_best:  99.28%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5023%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.1901%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 64055  14.224%\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  5.777261/ 38.417217, val:  42.08%, val_best:  56.67%, tr:  98.98%, tr_best:  99.28%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4441%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.9516%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 65387  14.211%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  5.297558/ 37.328281, val:  40.83%, val_best:  56.67%, tr:  98.88%, tr_best:  99.28%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.2707%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.5990%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 66639  14.181%\n",
      "lif layer 1 self.abs_max_v: 3808.0\n",
      "lif layer 1 self.abs_max_v: 3934.0\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  5.253793/ 29.324591, val:  50.83%, val_best:  56.67%, tr:  99.08%, tr_best:  99.28%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.2646%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.2498%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 67946  14.164%\n",
      "fc layer 1 self.abs_max_out: 2221.0\n",
      "lif layer 1 self.abs_max_v: 3936.0\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  4.976213/ 20.562969, val:  51.67%, val_best:  56.67%, tr:  98.06%, tr_best:  99.28%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.2090%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.7627%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 69239  14.145%\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  5.151643/ 30.721592, val:  48.75%, val_best:  56.67%, tr:  98.47%, tr_best:  99.28%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6296%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.5569%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 70573  14.135%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  5.179672/ 24.434610, val:  42.92%, val_best:  56.67%, tr:  98.57%, tr_best:  99.28%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.6523%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 71893  14.122%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  4.981925/ 23.213306, val:  56.25%, val_best:  56.67%, tr:  98.77%, tr_best:  99.28%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6583%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.9931%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 73196  14.107%\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  4.892381/ 23.274555, val:  50.42%, val_best:  56.67%, tr:  98.57%, tr_best:  99.28%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1807%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.0642%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 74503  14.093%\n",
      "fc layer 2 self.abs_max_out: 3666.0\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  5.114708/ 23.670315, val:  50.00%, val_best:  56.67%, tr:  98.67%, tr_best:  99.28%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.7863%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.7786%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 75839  14.085%\n",
      "fc layer 2 self.abs_max_out: 3685.0\n",
      "fc layer 2 self.abs_max_out: 3851.0\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  5.198938/ 28.904465, val:  40.42%, val_best:  56.67%, tr:  98.26%, tr_best:  99.28%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3233%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.7814%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 77181  14.078%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  5.047395/ 41.508774, val:  44.17%, val_best:  56.67%, tr:  98.77%, tr_best:  99.28%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.1142%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.6793%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 78491  14.066%\n",
      "fc layer 2 self.abs_max_out: 3921.0\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  4.956308/ 23.067106, val:  53.33%, val_best:  56.67%, tr:  98.06%, tr_best:  99.28%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.8782%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.1601%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 79788  14.052%\n",
      "fc layer 2 self.abs_max_out: 4316.0\n",
      "fc layer 1 self.abs_max_out: 2242.0\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  5.079556/ 29.538771, val:  38.75%, val_best:  56.67%, tr:  98.57%, tr_best:  99.28%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.8416%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.4258%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 81090  14.039%\n",
      "fc layer 1 self.abs_max_out: 2353.0\n",
      "fc layer 1 self.abs_max_out: 2358.0\n",
      "lif layer 1 self.abs_max_v: 4010.5\n",
      "lif layer 1 self.abs_max_v: 4166.5\n",
      "lif layer 1 self.abs_max_v: 4200.5\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  4.994524/ 38.088608, val:  41.25%, val_best:  56.67%, tr:  98.37%, tr_best:  99.28%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.1538%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.0490%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 82341  14.018%\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  4.975382/ 36.301434, val:  37.50%, val_best:  56.67%, tr:  98.67%, tr_best:  99.28%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6363%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.6020%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 83583  13.996%\n",
      "lif layer 2 self.abs_max_v: 6811.5\n",
      "lif layer 2 self.abs_max_v: 6848.0\n",
      "lif layer 2 self.abs_max_v: 7279.0\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  5.103981/ 20.630955, val:  46.67%, val_best:  56.67%, tr:  97.96%, tr_best:  99.28%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4986%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.8756%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 84901  13.987%\n",
      "fc layer 1 self.abs_max_out: 2362.0\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  4.917981/ 25.290598, val:  46.67%, val_best:  56.67%, tr:  97.85%, tr_best:  99.28%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4821%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.8456%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 86211  13.978%\n",
      "fc layer 1 self.abs_max_out: 2371.0\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  4.928606/ 32.277630, val:  51.67%, val_best:  56.67%, tr:  98.88%, tr_best:  99.28%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.7300%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.0558%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 87439  13.955%\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  4.771181/ 25.934420, val:  51.25%, val_best:  56.67%, tr:  98.37%, tr_best:  99.28%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5075%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.0735%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 88620  13.926%\n",
      "lif layer 2 self.abs_max_v: 7342.5\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  4.808095/ 34.554745, val:  46.67%, val_best:  56.67%, tr:  98.67%, tr_best:  99.28%, epoch time: 78.30 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9419%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.8056%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 89863  13.908%\n",
      "fc layer 1 self.abs_max_out: 2413.0\n",
      "fc layer 1 self.abs_max_out: 2451.0\n",
      "lif layer 1 self.abs_max_v: 4307.5\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  5.029660/ 22.884396, val:  48.75%, val_best:  56.67%, tr:  98.16%, tr_best:  99.28%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6453%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.9311%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 91152  13.897%\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  4.999393/ 33.537407, val:  44.58%, val_best:  56.67%, tr:  98.26%, tr_best:  99.28%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4524%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.5753%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 92441  13.886%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  4.869340/ 19.311392, val:  65.00%, val_best:  65.00%, tr:  98.98%, tr_best:  99.28%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5279%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.5820%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 93681  13.868%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  4.689228/ 37.120049, val:  32.92%, val_best:  65.00%, tr:  98.88%, tr_best:  99.28%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5785%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.1743%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 94908  13.849%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  4.853584/ 20.802181, val:  55.42%, val_best:  65.00%, tr:  99.18%, tr_best:  99.28%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3149%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.9254%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 96138  13.831%\n",
      "lif layer 2 self.abs_max_v: 7513.5\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  4.945227/ 32.188972, val:  48.33%, val_best:  65.00%, tr:  98.98%, tr_best:  99.28%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.8996%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.1561%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 97411  13.820%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  4.806204/ 23.825867, val:  56.25%, val_best:  65.00%, tr:  99.28%, tr_best:  99.28%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.7232%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.7453%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 98667  13.806%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  4.996521/ 40.784206, val:  50.83%, val_best:  65.00%, tr:  98.88%, tr_best:  99.28%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.8902%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.3242%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 99900  13.790%\n",
      "lif layer 1 self.abs_max_v: 4343.0\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  5.132063/ 26.530252, val:  54.17%, val_best:  65.00%, tr:  98.88%, tr_best:  99.28%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.0621%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.3114%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 101149  13.776%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  4.652339/ 28.069242, val:  53.75%, val_best:  65.00%, tr:  99.18%, tr_best:  99.28%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.0090%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.6684%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 102320  13.752%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  5.114131/ 35.750938, val:  42.50%, val_best:  65.00%, tr:  98.77%, tr_best:  99.28%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.8259%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.6455%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 103642  13.749%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  5.132769/ 27.952143, val:  48.33%, val_best:  65.00%, tr:  98.98%, tr_best:  99.28%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.9750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.1749%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 104900  13.737%\n",
      "lif layer 1 self.abs_max_v: 4353.5\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  5.090200/ 37.870117, val:  43.33%, val_best:  65.00%, tr:  98.77%, tr_best:  99.28%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4745%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.2190%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 106169  13.727%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  4.911423/ 22.800510, val:  50.83%, val_best:  65.00%, tr:  98.77%, tr_best:  99.28%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5509%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.1955%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 107360  13.708%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  5.099304/ 34.066849, val:  50.00%, val_best:  65.00%, tr:  98.77%, tr_best:  99.28%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6674%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.4693%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 108595  13.694%\n",
      "fc layer 1 self.abs_max_out: 2463.0\n",
      "lif layer 1 self.abs_max_v: 4500.5\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  4.885416/ 34.063519, val:  39.17%, val_best:  65.00%, tr:  99.08%, tr_best:  99.28%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.2729%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.5820%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 109823  13.680%\n",
      "fc layer 2 self.abs_max_out: 4330.0\n",
      "lif layer 2 self.abs_max_v: 7624.5\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  5.171717/ 36.852295, val:  47.92%, val_best:  65.00%, tr:  98.88%, tr_best:  99.28%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5074%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.5062%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 111115  13.675%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  4.968824/ 25.773125, val:  33.75%, val_best:  65.00%, tr:  98.98%, tr_best:  99.28%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6676%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.9370%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 112372  13.665%\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  4.788772/ 34.088291, val:  34.58%, val_best:  65.00%, tr:  98.67%, tr_best:  99.28%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4687%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.3974%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 113644  13.657%\n",
      "fc layer 1 self.abs_max_out: 2480.0\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  4.717165/ 25.063431, val:  47.08%, val_best:  65.00%, tr:  98.88%, tr_best:  99.28%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4658%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.1065%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 114873  13.644%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  4.587191/ 35.099812, val:  52.08%, val_best:  65.00%, tr:  98.98%, tr_best:  99.28%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3891%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.2780%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 116062  13.627%\n",
      "lif layer 2 self.abs_max_v: 7698.5\n",
      "fc layer 1 self.abs_max_out: 2543.0\n",
      "lif layer 1 self.abs_max_v: 4594.0\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  5.041975/ 29.871153, val:  52.50%, val_best:  65.00%, tr:  97.96%, tr_best:  99.28%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5006%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.8331%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 117365  13.623%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  4.663502/ 29.492517, val:  50.00%, val_best:  65.00%, tr:  98.98%, tr_best:  99.28%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.7977%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.5460%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 118596  13.611%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  4.966812/ 23.999100, val:  55.42%, val_best:  65.00%, tr:  98.57%, tr_best:  99.28%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3635%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.0173%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 119893  13.607%\n",
      "lif layer 2 self.abs_max_v: 7740.0\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  4.583384/ 25.750898, val:  52.08%, val_best:  65.00%, tr:  98.67%, tr_best:  99.28%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1364%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.3384%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 121074  13.590%\n",
      "lif layer 2 self.abs_max_v: 7792.0\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  4.634715/ 16.752110, val:  57.08%, val_best:  65.00%, tr:  98.57%, tr_best:  99.28%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9128%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9687%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 122314  13.580%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  4.554896/ 28.124464, val:  52.50%, val_best:  65.00%, tr:  97.75%, tr_best:  99.28%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.8154%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.5654%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 123535  13.568%\n",
      "fc layer 1 self.abs_max_out: 2585.0\n",
      "lif layer 1 self.abs_max_v: 4611.5\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  4.631598/ 23.738338, val:  58.75%, val_best:  65.00%, tr:  98.67%, tr_best:  99.28%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1629%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.4482%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 124764  13.557%\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  4.622882/ 31.200354, val:  43.75%, val_best:  65.00%, tr:  98.37%, tr_best:  99.28%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.8832%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.0655%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 125943  13.542%\n",
      "fc layer 2 self.abs_max_out: 4335.0\n",
      "lif layer 2 self.abs_max_v: 7878.0\n",
      "fc layer 1 self.abs_max_out: 2587.0\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  4.858023/ 23.625732, val:  63.75%, val_best:  65.00%, tr:  98.88%, tr_best:  99.28%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.0854%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.1873%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 127201  13.534%\n",
      "fc layer 2 self.abs_max_out: 4399.0\n",
      "fc layer 2 self.abs_max_out: 4491.0\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  4.714852/ 38.253498, val:  45.83%, val_best:  65.00%, tr:  98.67%, tr_best:  99.28%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2318%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.3338%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 128465  13.528%\n",
      "fc layer 1 self.abs_max_out: 2662.0\n",
      "lif layer 1 self.abs_max_v: 4723.5\n",
      "lif layer 1 self.abs_max_v: 4810.5\n",
      "lif layer 1 self.abs_max_v: 4832.5\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  4.600348/ 21.136490, val:  62.50%, val_best:  65.00%, tr:  98.77%, tr_best:  99.28%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9854%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9650%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 129697  13.518%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  4.445736/ 26.591957, val:  40.42%, val_best:  65.00%, tr:  98.77%, tr_best:  99.28%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2212%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.5998%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 130834  13.499%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  4.397706/ 20.451281, val:  53.33%, val_best:  65.00%, tr:  99.28%, tr_best:  99.28%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2063%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.7493%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 132020  13.485%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  4.377148/ 23.365257, val:  53.75%, val_best:  65.00%, tr:  98.57%, tr_best:  99.28%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2215%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8822%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 133189  13.470%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  4.418848/ 28.896273, val:  53.33%, val_best:  65.00%, tr:  98.67%, tr_best:  99.28%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2853%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.6318%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 134355  13.455%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  4.324336/ 34.850628, val:  34.58%, val_best:  65.00%, tr:  98.26%, tr_best:  99.28%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3513%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9526%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 135499  13.437%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  4.267279/ 26.192612, val:  48.75%, val_best:  65.00%, tr:  98.26%, tr_best:  99.28%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5288%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0996%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 136648  13.421%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  4.403589/ 23.166641, val:  57.92%, val_best:  65.00%, tr:  98.67%, tr_best:  99.28%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.4538%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9536%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 137832  13.408%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  4.642987/ 22.945095, val:  56.67%, val_best:  65.00%, tr:  98.47%, tr_best:  99.28%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2511%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0621%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 139028  13.397%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  4.027896/ 24.639996, val:  43.75%, val_best:  65.00%, tr:  98.37%, tr_best:  99.28%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6489%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3237%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 140131  13.377%\n",
      "fc layer 2 self.abs_max_out: 4644.0\n",
      "fc layer 1 self.abs_max_out: 2770.0\n",
      "lif layer 1 self.abs_max_v: 4912.5\n",
      "lif layer 1 self.abs_max_v: 5000.5\n",
      "lif layer 1 self.abs_max_v: 5064.5\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  4.490901/ 40.119957, val:  40.83%, val_best:  65.00%, tr:  98.16%, tr_best:  99.28%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6498%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9847%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 141345  13.368%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  4.570448/ 23.414785, val:  52.08%, val_best:  65.00%, tr:  97.65%, tr_best:  99.28%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.9632%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.7357%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 142543  13.358%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  4.475993/ 28.014177, val:  47.50%, val_best:  65.00%, tr:  98.77%, tr_best:  99.28%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.0964%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8089%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 143720  13.346%\n",
      "lif layer 2 self.abs_max_v: 7955.5\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  4.461938/ 32.406143, val:  54.17%, val_best:  65.00%, tr:  98.88%, tr_best:  99.28%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.1568%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9566%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 144892  13.333%\n",
      "lif layer 2 self.abs_max_v: 8315.0\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  4.186764/ 25.612015, val:  57.50%, val_best:  65.00%, tr:  98.16%, tr_best:  99.28%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.0508%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4184%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 146019  13.317%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  4.477290/ 22.938459, val:  57.08%, val_best:  65.00%, tr:  97.85%, tr_best:  99.28%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5379%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4292%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 147264  13.312%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  4.555824/ 22.597809, val:  52.50%, val_best:  65.00%, tr:  98.26%, tr_best:  99.28%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3011%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.6489%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 148460  13.302%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  4.369359/ 22.459108, val:  53.75%, val_best:  65.00%, tr:  97.75%, tr_best:  99.28%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.7683%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9895%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 149640  13.291%\n",
      "lif layer 2 self.abs_max_v: 8339.5\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  4.462914/ 19.906895, val:  52.08%, val_best:  65.00%, tr:  97.65%, tr_best:  99.28%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.4479%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1882%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 150818  13.280%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  4.366123/ 26.786909, val:  50.83%, val_best:  65.00%, tr:  97.45%, tr_best:  99.28%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3139%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8289%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 151966  13.267%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  4.202886/ 21.281740, val:  49.58%, val_best:  65.00%, tr:  98.47%, tr_best:  99.28%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1902%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9801%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 153077  13.251%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  4.569990/ 27.943707, val:  52.08%, val_best:  65.00%, tr:  98.26%, tr_best:  99.28%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.4248%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.5048%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 154291  13.244%\n",
      "lif layer 2 self.abs_max_v: 8495.0\n",
      "lif layer 2 self.abs_max_v: 8504.5\n",
      "lif layer 2 self.abs_max_v: 8583.0\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  4.389821/ 20.210991, val:  57.92%, val_best:  65.00%, tr:  98.06%, tr_best:  99.28%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6870%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8809%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 155426  13.230%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  4.169651/ 21.783442, val:  60.00%, val_best:  65.00%, tr:  98.57%, tr_best:  99.28%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3502%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0111%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 156545  13.215%\n",
      "fc layer 2 self.abs_max_out: 4763.0\n",
      "lif layer 2 self.abs_max_v: 9006.0\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  4.595393/ 27.206133, val:  54.17%, val_best:  65.00%, tr:  98.47%, tr_best:  99.28%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3881%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.5821%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 157747  13.207%\n",
      "fc layer 2 self.abs_max_out: 4982.0\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  4.282452/ 26.227324, val:  48.75%, val_best:  65.00%, tr:  99.39%, tr_best:  99.39%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.7327%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2365%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 158870  13.193%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  4.408916/ 23.703884, val:  55.42%, val_best:  65.00%, tr:  98.16%, tr_best:  99.39%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5708%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3978%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 160086  13.187%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  4.417097/ 20.055895, val:  64.17%, val_best:  65.00%, tr:  98.16%, tr_best:  99.39%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.4998%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2911%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 161232  13.175%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  4.166448/ 26.884657, val:  51.25%, val_best:  65.00%, tr:  98.47%, tr_best:  99.39%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3009%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1163%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 162352  13.161%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  4.317608/ 22.074501, val:  53.33%, val_best:  65.00%, tr:  98.88%, tr_best:  99.39%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.4693%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2918%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 163497  13.150%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  4.435724/ 22.380024, val:  49.17%, val_best:  65.00%, tr:  97.24%, tr_best:  99.39%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3051%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2025%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 164642  13.139%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  4.363179/ 20.207508, val:  51.25%, val_best:  65.00%, tr:  98.26%, tr_best:  99.39%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2120%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9726%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 165792  13.128%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  4.164524/ 20.184622, val:  53.33%, val_best:  65.00%, tr:  98.57%, tr_best:  99.39%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.4108%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9509%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 166884  13.113%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  4.327026/ 33.550713, val:  50.42%, val_best:  65.00%, tr:  97.75%, tr_best:  99.39%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3372%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9871%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 168076  13.105%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  4.326385/ 33.322952, val:  50.42%, val_best:  65.00%, tr:  98.47%, tr_best:  99.39%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.8714%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3489%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 169245  13.097%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  4.262324/ 32.105637, val:  47.92%, val_best:  65.00%, tr:  98.06%, tr_best:  99.39%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.9050%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9088%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 170355  13.083%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  4.411161/ 18.840948, val:  52.50%, val_best:  65.00%, tr:  99.28%, tr_best:  99.39%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6071%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1285%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 171518  13.074%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  4.178588/ 27.956411, val:  42.08%, val_best:  65.00%, tr:  98.37%, tr_best:  99.39%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6300%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0772%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 172651  13.063%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  4.322648/ 23.636391, val:  51.25%, val_best:  65.00%, tr:  98.06%, tr_best:  99.39%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5530%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1908%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 173837  13.056%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  4.066845/ 25.231829, val:  55.83%, val_best:  65.00%, tr:  98.47%, tr_best:  99.39%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3360%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0300%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 174941  13.043%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  4.295095/ 18.351175, val:  53.75%, val_best:  65.00%, tr:  98.57%, tr_best:  99.39%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1124%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1266%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 176085  13.033%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  4.349865/ 19.515579, val:  70.83%, val_best:  70.83%, tr:  98.67%, tr_best:  99.39%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2423%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8602%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 177243  13.025%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  4.343619/ 19.817017, val:  62.50%, val_best:  70.83%, tr:  97.65%, tr_best:  99.39%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3228%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3004%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 178398  13.016%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  4.578929/ 25.643141, val:  46.67%, val_best:  70.83%, tr:  97.55%, tr_best:  99.39%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1156%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8306%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 179560  13.008%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  4.278053/ 20.590126, val:  55.00%, val_best:  70.83%, tr:  98.06%, tr_best:  99.39%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2998%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2038%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 180689  12.998%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  4.485594/ 27.289650, val:  47.50%, val_best:  70.83%, tr:  98.37%, tr_best:  99.39%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3042%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3726%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 181903  12.993%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  4.132942/ 18.680887, val:  57.08%, val_best:  70.83%, tr:  98.67%, tr_best:  99.39%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1766%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3328%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 182990  12.980%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  4.272287/ 24.640177, val:  53.75%, val_best:  70.83%, tr:  98.77%, tr_best:  99.39%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3322%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3743%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 184129  12.971%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  4.063278/ 21.783501, val:  53.33%, val_best:  70.83%, tr:  98.26%, tr_best:  99.39%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.4995%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8550%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 185205  12.957%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  4.158209/ 18.194569, val:  56.67%, val_best:  70.83%, tr:  97.55%, tr_best:  99.39%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.4002%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0484%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 186327  12.947%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  3.775987/ 23.659882, val:  39.17%, val_best:  70.83%, tr:  97.24%, tr_best:  99.39%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3161%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7132%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 187366  12.931%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  4.457485/ 22.728174, val:  57.50%, val_best:  70.83%, tr:  98.26%, tr_best:  99.39%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2942%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1427%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 188553  12.926%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  4.344375/ 22.016630, val:  55.42%, val_best:  70.83%, tr:  98.47%, tr_best:  99.39%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2395%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9947%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 189686  12.917%\n",
      "fc layer 1 self.abs_max_out: 2807.0\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  4.435494/ 25.557409, val:  57.50%, val_best:  70.83%, tr:  98.47%, tr_best:  99.39%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1896%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8230%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 190818  12.908%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  4.313080/ 18.920092, val:  62.92%, val_best:  70.83%, tr:  98.16%, tr_best:  99.39%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3691%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3101%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 191965  12.900%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  4.147514/ 16.433567, val:  61.25%, val_best:  70.83%, tr:  98.16%, tr_best:  99.39%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.0829%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9655%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 193064  12.889%\n",
      "fc layer 1 self.abs_max_out: 2833.0\n",
      "lif layer 1 self.abs_max_v: 5189.5\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  4.093226/ 24.362089, val:  66.67%, val_best:  70.83%, tr:  98.37%, tr_best:  99.39%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.7978%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7122%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 194180  12.880%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  4.222222/ 19.354937, val:  59.17%, val_best:  70.83%, tr:  98.06%, tr_best:  99.39%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.4293%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4663%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 195303  12.870%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  4.218533/ 21.176014, val:  52.50%, val_best:  70.83%, tr:  97.45%, tr_best:  99.39%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.0772%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4448%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 196406  12.860%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  4.225592/ 22.295069, val:  44.58%, val_best:  70.83%, tr:  98.16%, tr_best:  99.39%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6756%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1900%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 197529  12.851%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  4.361763/ 17.775070, val:  52.92%, val_best:  70.83%, tr:  97.96%, tr_best:  99.39%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.8253%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1018%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 198663  12.843%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  4.387306/ 25.677824, val:  51.25%, val_best:  70.83%, tr:  98.16%, tr_best:  99.39%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.0813%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1618%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 199819  12.837%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  4.334458/ 19.369413, val:  60.42%, val_best:  70.83%, tr:  98.26%, tr_best:  99.39%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9809%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1912%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 200948  12.829%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  4.395792/ 27.168352, val:  55.00%, val_best:  70.83%, tr:  98.67%, tr_best:  99.39%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9789%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9973%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 202082  12.821%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  4.398734/ 19.467091, val:  60.00%, val_best:  70.83%, tr:  98.06%, tr_best:  99.39%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9072%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 203239  12.815%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  4.331702/ 31.964338, val:  47.92%, val_best:  70.83%, tr:  98.47%, tr_best:  99.39%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.8445%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1427%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 204371  12.807%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  4.143021/ 23.277090, val:  56.67%, val_best:  70.83%, tr:  98.47%, tr_best:  99.39%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2226%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2952%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 205461  12.797%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  3.966077/ 26.309427, val:  57.08%, val_best:  70.83%, tr:  97.85%, tr_best:  99.39%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2495%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3240%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 206523  12.785%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  4.267604/ 20.239084, val:  61.67%, val_best:  70.83%, tr:  98.47%, tr_best:  99.39%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.4756%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4460%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 207635  12.776%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  4.171600/ 19.897892, val:  62.92%, val_best:  70.83%, tr:  98.88%, tr_best:  99.39%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.4659%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2607%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 208723  12.766%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  4.411179/ 26.788782, val:  48.33%, val_best:  70.83%, tr:  98.77%, tr_best:  99.39%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.4547%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1966%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 209865  12.760%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  3.823678/ 22.449579, val:  60.00%, val_best:  70.83%, tr:  97.96%, tr_best:  99.39%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3939%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5575%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 210892  12.746%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  4.130909/ 34.773205, val:  45.42%, val_best:  70.83%, tr:  97.75%, tr_best:  99.39%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.0460%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3874%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 211997  12.738%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  4.062679/ 20.361912, val:  51.25%, val_best:  70.83%, tr:  98.16%, tr_best:  99.39%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1141%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2544%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3767%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 213059  12.727%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  4.086471/ 31.339846, val:  47.92%, val_best:  70.83%, tr:  97.96%, tr_best:  99.39%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9213%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5654%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 214163  12.718%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  4.350496/ 19.065556, val:  59.17%, val_best:  70.83%, tr:  97.96%, tr_best:  99.39%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1661%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8020%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 215317  12.713%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  4.105259/ 20.349705, val:  64.17%, val_best:  70.83%, tr:  97.85%, tr_best:  99.39%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1266%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7015%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 216397  12.703%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  4.184268/ 26.210382, val:  52.92%, val_best:  70.83%, tr:  98.37%, tr_best:  99.39%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2309%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1727%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 217465  12.693%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  4.196984/ 18.259726, val:  60.83%, val_best:  70.83%, tr:  97.85%, tr_best:  99.39%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.4053%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4418%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 218537  12.683%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  4.079330/ 19.386770, val:  57.92%, val_best:  70.83%, tr:  98.16%, tr_best:  99.39%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3634%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9356%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 219623  12.674%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  4.029109/ 27.976011, val:  48.75%, val_best:  70.83%, tr:  98.16%, tr_best:  99.39%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0312%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2363%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4453%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 220690  12.664%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  4.035183/ 18.960711, val:  62.92%, val_best:  70.83%, tr:  97.96%, tr_best:  99.39%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0376%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.0489%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5446%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 221726  12.653%\n",
      "fc layer 1 self.abs_max_out: 2838.0\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  4.184871/ 22.860476, val:  48.75%, val_best:  70.83%, tr:  98.98%, tr_best:  99.39%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2209%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6138%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 222831  12.645%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  4.063204/ 18.247599, val:  60.00%, val_best:  70.83%, tr:  97.34%, tr_best:  99.39%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3844%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7347%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 223942  12.638%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  3.923313/ 34.398869, val:  47.08%, val_best:  70.83%, tr:  98.47%, tr_best:  99.39%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5235%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5409%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 224989  12.627%\n",
      "fc layer 1 self.abs_max_out: 2848.0\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  4.168048/ 23.432114, val:  60.00%, val_best:  70.83%, tr:  97.85%, tr_best:  99.39%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5079%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2880%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 226071  12.619%\n",
      "fc layer 1 self.abs_max_out: 2853.0\n",
      "lif layer 1 self.abs_max_v: 5207.0\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  4.230411/ 35.887203, val:  46.25%, val_best:  70.83%, tr:  98.98%, tr_best:  99.39%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0426%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6441%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1863%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 227166  12.611%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  4.214498/ 25.474392, val:  54.17%, val_best:  70.83%, tr:  97.96%, tr_best:  99.39%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6863%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6911%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 228265  12.603%\n",
      "fc layer 1 self.abs_max_out: 2898.0\n",
      "lif layer 1 self.abs_max_v: 5274.0\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  3.937147/ 27.445312, val:  39.17%, val_best:  70.83%, tr:  98.16%, tr_best:  99.39%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5459%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7879%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 229308  12.593%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  4.051268/ 14.876637, val:  69.17%, val_best:  70.83%, tr:  97.24%, tr_best:  99.39%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1058%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0938%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 230422  12.586%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  4.237059/ 20.934423, val:  60.00%, val_best:  70.83%, tr:  98.06%, tr_best:  99.39%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3979%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6441%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 231552  12.581%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  4.370454/ 22.197594, val:  57.50%, val_best:  70.83%, tr:  98.26%, tr_best:  99.39%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6586%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1323%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 232661  12.574%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  4.161209/ 17.231741, val:  61.25%, val_best:  70.83%, tr:  98.37%, tr_best:  99.39%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.7872%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4498%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 233748  12.566%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  3.899651/ 20.040091, val:  63.75%, val_best:  70.83%, tr:  98.57%, tr_best:  99.39%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.8234%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8275%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 234792  12.556%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  3.981622/ 28.340309, val:  45.42%, val_best:  70.83%, tr:  98.26%, tr_best:  99.39%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6866%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1543%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 235859  12.548%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  4.145639/ 19.042536, val:  52.08%, val_best:  70.83%, tr:  97.04%, tr_best:  99.39%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.4072%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7548%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 236937  12.540%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  4.067384/ 21.468464, val:  46.25%, val_best:  70.83%, tr:  98.16%, tr_best:  99.39%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2116%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7558%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 238027  12.533%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  3.944515/ 26.925278, val:  46.25%, val_best:  70.83%, tr:  98.37%, tr_best:  99.39%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5657%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7689%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 239085  12.524%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  4.229220/ 23.361008, val:  58.33%, val_best:  70.83%, tr:  97.65%, tr_best:  99.39%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6991%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0327%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 240220  12.519%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  4.049059/ 19.229454, val:  52.08%, val_best:  70.83%, tr:  97.75%, tr_best:  99.39%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5732%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1023%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 241312  12.512%\n",
      "lif layer 1 self.abs_max_v: 5279.5\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  4.091936/ 28.625578, val:  44.17%, val_best:  70.83%, tr:  97.04%, tr_best:  99.39%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5539%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2061%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 242430  12.507%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  3.848567/ 29.295511, val:  53.33%, val_best:  70.83%, tr:  98.57%, tr_best:  99.39%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.9235%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8868%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 243496  12.498%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  4.034770/ 24.913929, val:  63.33%, val_best:  70.83%, tr:  98.98%, tr_best:  99.39%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6793%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7584%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75adb2c7265e4a26a7ae16101146f2a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÇ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÉ‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñá‚ñÑ‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñá‚ñá‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñá‚ñá‚ñá‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÅ‚ñà‚ñÜ‚ñÜ‚ñÅ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÉ‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñá</td></tr><tr><td>tr_epoch_loss</td><td>‚ñá‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÇ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÉ‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñá‚ñÑ‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÜ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.98979</td></tr><tr><td>tr_epoch_loss</td><td>4.03477</td></tr><tr><td>val_acc_best</td><td>0.70833</td></tr><tr><td>val_acc_now</td><td>0.63333</td></tr><tr><td>val_loss</td><td>24.91393</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ethereal-sweep-21</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/c56nqp3z' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/c56nqp3z</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_140647-c56nqp3z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nnj7uume with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.046875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0234375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_182636-nnj7uume</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nnj7uume' target=\"_blank\">avid-sweep-27</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nnj7uume' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nnj7uume</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251214_182644_735', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 128, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 64, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 64, 'lif_layer_v_threshold2': 16, 'init_scaling': [0.03125, 0.046875, 0.0234375], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 64, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 64, self.v_threshold 16\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=64, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=16, v_reset=10000, sg_width=64, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 46.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 59.5\n",
      "fc layer 1 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 69.0\n",
      "fc layer 1 self.abs_max_out: 55.0\n",
      "lif layer 1 self.abs_max_v: 75.0\n",
      "lif layer 1 self.abs_max_v: 91.5\n",
      "fc layer 1 self.abs_max_out: 63.0\n",
      "lif layer 1 self.abs_max_v: 101.5\n",
      "fc layer 1 self.abs_max_out: 86.0\n",
      "lif layer 1 self.abs_max_v: 114.0\n",
      "lif layer 1 self.abs_max_v: 121.0\n",
      "fc layer 1 self.abs_max_out: 94.0\n",
      "lif layer 1 self.abs_max_v: 129.0\n",
      "fc layer 2 self.abs_max_out: 6.0\n",
      "lif layer 2 self.abs_max_v: 6.0\n",
      "lif layer 1 self.abs_max_v: 132.5\n",
      "lif layer 1 self.abs_max_v: 139.5\n",
      "fc layer 1 self.abs_max_out: 163.0\n",
      "lif layer 1 self.abs_max_v: 203.0\n",
      "fc layer 2 self.abs_max_out: 7.0\n",
      "lif layer 2 self.abs_max_v: 8.5\n",
      "fc layer 1 self.abs_max_out: 171.0\n",
      "fc layer 2 self.abs_max_out: 8.0\n",
      "fc layer 1 self.abs_max_out: 200.0\n",
      "lif layer 2 self.abs_max_v: 12.0\n",
      "fc layer 1 self.abs_max_out: 257.0\n",
      "lif layer 1 self.abs_max_v: 257.0\n",
      "fc layer 2 self.abs_max_out: 13.0\n",
      "lif layer 2 self.abs_max_v: 17.5\n",
      "fc layer 3 self.abs_max_out: 2.0\n",
      "lif layer 2 self.abs_max_v: 18.5\n",
      "lif layer 2 self.abs_max_v: 19.5\n",
      "fc layer 3 self.abs_max_out: 4.0\n",
      "lif layer 2 self.abs_max_v: 20.0\n",
      "fc layer 3 self.abs_max_out: 9.0\n",
      "fc layer 2 self.abs_max_out: 16.0\n",
      "lif layer 2 self.abs_max_v: 22.0\n",
      "fc layer 2 self.abs_max_out: 18.0\n",
      "lif layer 2 self.abs_max_v: 23.5\n",
      "fc layer 2 self.abs_max_out: 19.0\n",
      "lif layer 2 self.abs_max_v: 26.0\n",
      "fc layer 3 self.abs_max_out: 11.0\n",
      "fc layer 2 self.abs_max_out: 24.0\n",
      "lif layer 2 self.abs_max_v: 34.0\n",
      "fc layer 3 self.abs_max_out: 13.0\n",
      "fc layer 1 self.abs_max_out: 286.0\n",
      "lif layer 1 self.abs_max_v: 286.0\n",
      "lif layer 2 self.abs_max_v: 39.0\n",
      "fc layer 3 self.abs_max_out: 25.0\n",
      "fc layer 1 self.abs_max_out: 386.0\n",
      "lif layer 1 self.abs_max_v: 386.0\n",
      "fc layer 2 self.abs_max_out: 26.0\n",
      "fc layer 1 self.abs_max_out: 390.0\n",
      "lif layer 1 self.abs_max_v: 390.0\n",
      "lif layer 2 self.abs_max_v: 40.5\n",
      "fc layer 2 self.abs_max_out: 28.0\n",
      "lif layer 2 self.abs_max_v: 43.5\n",
      "fc layer 1 self.abs_max_out: 395.0\n",
      "lif layer 1 self.abs_max_v: 395.0\n",
      "fc layer 2 self.abs_max_out: 29.0\n",
      "lif layer 2 self.abs_max_v: 46.5\n",
      "fc layer 3 self.abs_max_out: 29.0\n",
      "fc layer 2 self.abs_max_out: 33.0\n",
      "fc layer 3 self.abs_max_out: 31.0\n",
      "fc layer 2 self.abs_max_out: 35.0\n",
      "fc layer 1 self.abs_max_out: 399.0\n",
      "lif layer 1 self.abs_max_v: 399.0\n",
      "fc layer 1 self.abs_max_out: 418.0\n",
      "lif layer 1 self.abs_max_v: 418.0\n",
      "fc layer 1 self.abs_max_out: 419.0\n",
      "lif layer 1 self.abs_max_v: 419.0\n",
      "fc layer 1 self.abs_max_out: 423.0\n",
      "lif layer 1 self.abs_max_v: 423.0\n",
      "fc layer 3 self.abs_max_out: 32.0\n",
      "fc layer 3 self.abs_max_out: 35.0\n",
      "fc layer 3 self.abs_max_out: 64.0\n",
      "fc layer 1 self.abs_max_out: 523.0\n",
      "lif layer 1 self.abs_max_v: 523.0\n",
      "fc layer 1 self.abs_max_out: 589.0\n",
      "lif layer 1 self.abs_max_v: 589.0\n",
      "fc layer 2 self.abs_max_out: 42.0\n",
      "lif layer 2 self.abs_max_v: 48.5\n",
      "lif layer 2 self.abs_max_v: 51.0\n",
      "lif layer 2 self.abs_max_v: 57.0\n",
      "lif layer 2 self.abs_max_v: 60.5\n",
      "lif layer 2 self.abs_max_v: 62.5\n",
      "fc layer 2 self.abs_max_out: 43.0\n",
      "fc layer 3 self.abs_max_out: 70.0\n",
      "fc layer 2 self.abs_max_out: 45.0\n",
      "fc layer 2 self.abs_max_out: 51.0\n",
      "lif layer 2 self.abs_max_v: 63.5\n",
      "lif layer 2 self.abs_max_v: 66.0\n",
      "lif layer 2 self.abs_max_v: 71.0\n",
      "lif layer 2 self.abs_max_v: 74.5\n",
      "fc layer 2 self.abs_max_out: 59.0\n",
      "lif layer 2 self.abs_max_v: 77.5\n",
      "lif layer 2 self.abs_max_v: 83.5\n",
      "fc layer 3 self.abs_max_out: 73.0\n",
      "lif layer 2 self.abs_max_v: 84.0\n",
      "lif layer 2 self.abs_max_v: 89.0\n",
      "fc layer 3 self.abs_max_out: 77.0\n",
      "fc layer 3 self.abs_max_out: 79.0\n",
      "fc layer 3 self.abs_max_out: 80.0\n",
      "lif layer 2 self.abs_max_v: 93.5\n",
      "fc layer 3 self.abs_max_out: 96.0\n",
      "fc layer 2 self.abs_max_out: 60.0\n",
      "lif layer 2 self.abs_max_v: 95.0\n",
      "fc layer 2 self.abs_max_out: 64.0\n",
      "fc layer 3 self.abs_max_out: 98.0\n",
      "fc layer 3 self.abs_max_out: 103.0\n",
      "fc layer 3 self.abs_max_out: 104.0\n",
      "fc layer 3 self.abs_max_out: 106.0\n",
      "fc layer 3 self.abs_max_out: 107.0\n",
      "fc layer 3 self.abs_max_out: 124.0\n",
      "fc layer 3 self.abs_max_out: 131.0\n",
      "fc layer 2 self.abs_max_out: 67.0\n",
      "fc layer 3 self.abs_max_out: 132.0\n",
      "fc layer 3 self.abs_max_out: 138.0\n",
      "lif layer 2 self.abs_max_v: 101.0\n",
      "fc layer 3 self.abs_max_out: 152.0\n",
      "fc layer 1 self.abs_max_out: 602.0\n",
      "lif layer 1 self.abs_max_v: 602.0\n",
      "fc layer 2 self.abs_max_out: 69.0\n",
      "fc layer 1 self.abs_max_out: 636.0\n",
      "lif layer 1 self.abs_max_v: 636.0\n",
      "lif layer 2 self.abs_max_v: 107.5\n",
      "fc layer 1 self.abs_max_out: 694.0\n",
      "lif layer 1 self.abs_max_v: 694.0\n",
      "fc layer 2 self.abs_max_out: 70.0\n",
      "fc layer 2 self.abs_max_out: 79.0\n",
      "fc layer 2 self.abs_max_out: 86.0\n",
      "lif layer 2 self.abs_max_v: 108.5\n",
      "fc layer 3 self.abs_max_out: 153.0\n",
      "fc layer 3 self.abs_max_out: 157.0\n",
      "fc layer 3 self.abs_max_out: 158.0\n",
      "fc layer 1 self.abs_max_out: 747.0\n",
      "lif layer 1 self.abs_max_v: 747.0\n",
      "fc layer 1 self.abs_max_out: 775.0\n",
      "lif layer 1 self.abs_max_v: 775.0\n",
      "fc layer 3 self.abs_max_out: 160.0\n",
      "fc layer 3 self.abs_max_out: 164.0\n",
      "fc layer 2 self.abs_max_out: 89.0\n",
      "fc layer 2 self.abs_max_out: 95.0\n",
      "lif layer 2 self.abs_max_v: 114.5\n",
      "fc layer 1 self.abs_max_out: 793.0\n",
      "lif layer 1 self.abs_max_v: 793.0\n",
      "fc layer 1 self.abs_max_out: 843.0\n",
      "lif layer 1 self.abs_max_v: 843.0\n",
      "fc layer 1 self.abs_max_out: 845.0\n",
      "lif layer 1 self.abs_max_v: 845.0\n",
      "fc layer 1 self.abs_max_out: 892.0\n",
      "lif layer 1 self.abs_max_v: 892.0\n",
      "fc layer 1 self.abs_max_out: 922.0\n",
      "lif layer 1 self.abs_max_v: 922.0\n",
      "fc layer 3 self.abs_max_out: 168.0\n",
      "fc layer 3 self.abs_max_out: 186.0\n",
      "fc layer 2 self.abs_max_out: 106.0\n",
      "fc layer 2 self.abs_max_out: 108.0\n",
      "fc layer 2 self.abs_max_out: 115.0\n",
      "lif layer 2 self.abs_max_v: 115.0\n",
      "lif layer 2 self.abs_max_v: 125.0\n",
      "fc layer 2 self.abs_max_out: 125.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss:  6.697036/ 51.710987, val:  27.50%, val_best:  27.50%, tr:  91.73%, tr_best:  91.73%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.4059%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.4170%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 3167  32.349%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 196.0\n",
      "lif layer 2 self.abs_max_v: 128.0\n",
      "lif layer 2 self.abs_max_v: 132.0\n",
      "fc layer 2 self.abs_max_out: 126.0\n",
      "fc layer 2 self.abs_max_out: 127.0\n",
      "fc layer 3 self.abs_max_out: 205.0\n",
      "fc layer 2 self.abs_max_out: 129.0\n",
      "fc layer 2 self.abs_max_out: 143.0\n",
      "lif layer 2 self.abs_max_v: 143.0\n",
      "lif layer 2 self.abs_max_v: 146.5\n",
      "fc layer 3 self.abs_max_out: 220.0\n",
      "fc layer 3 self.abs_max_out: 223.0\n",
      "fc layer 3 self.abs_max_out: 232.0\n",
      "fc layer 1 self.abs_max_out: 942.0\n",
      "lif layer 1 self.abs_max_v: 942.0\n",
      "lif layer 2 self.abs_max_v: 149.5\n",
      "lif layer 2 self.abs_max_v: 151.0\n",
      "lif layer 2 self.abs_max_v: 152.0\n",
      "lif layer 2 self.abs_max_v: 154.0\n",
      "lif layer 2 self.abs_max_v: 155.0\n",
      "lif layer 2 self.abs_max_v: 155.5\n",
      "lif layer 2 self.abs_max_v: 156.0\n",
      "lif layer 2 self.abs_max_v: 157.0\n",
      "lif layer 2 self.abs_max_v: 168.5\n",
      "lif layer 2 self.abs_max_v: 170.5\n",
      "fc layer 3 self.abs_max_out: 247.0\n",
      "lif layer 2 self.abs_max_v: 171.0\n",
      "lif layer 2 self.abs_max_v: 182.5\n",
      "lif layer 2 self.abs_max_v: 188.5\n",
      "lif layer 2 self.abs_max_v: 191.5\n",
      "lif layer 2 self.abs_max_v: 193.0\n",
      "lif layer 2 self.abs_max_v: 193.5\n",
      "lif layer 2 self.abs_max_v: 194.0\n",
      "fc layer 3 self.abs_max_out: 255.0\n",
      "fc layer 2 self.abs_max_out: 148.0\n",
      "fc layer 2 self.abs_max_out: 150.0\n",
      "lif layer 1 self.abs_max_v: 969.5\n",
      "lif layer 1 self.abs_max_v: 988.0\n",
      "lif layer 1 self.abs_max_v: 1060.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss:  7.947637/ 42.855907, val:  33.75%, val_best:  33.75%, tr:  97.85%, tr_best:  97.85%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0027%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.1359%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 5508  28.131%\n",
      "fc layer 3 self.abs_max_out: 256.0\n",
      "fc layer 3 self.abs_max_out: 259.0\n",
      "lif layer 1 self.abs_max_v: 1103.0\n",
      "fc layer 3 self.abs_max_out: 284.0\n",
      "fc layer 3 self.abs_max_out: 287.0\n",
      "fc layer 1 self.abs_max_out: 989.0\n",
      "fc layer 3 self.abs_max_out: 298.0\n",
      "fc layer 1 self.abs_max_out: 995.0\n",
      "fc layer 1 self.abs_max_out: 1015.0\n",
      "fc layer 3 self.abs_max_out: 342.0\n",
      "lif layer 1 self.abs_max_v: 1255.5\n",
      "lif layer 1 self.abs_max_v: 1325.0\n",
      "fc layer 2 self.abs_max_out: 154.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss:  9.205202/ 59.319462, val:  30.00%, val_best:  33.75%, tr:  96.63%, tr_best:  97.85%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0498%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.5142%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 7881  26.834%\n",
      "fc layer 1 self.abs_max_out: 1018.0\n",
      "fc layer 1 self.abs_max_out: 1021.0\n",
      "fc layer 2 self.abs_max_out: 157.0\n",
      "fc layer 1 self.abs_max_out: 1024.0\n",
      "fc layer 1 self.abs_max_out: 1086.0\n",
      "fc layer 1 self.abs_max_out: 1118.0\n",
      "lif layer 2 self.abs_max_v: 199.0\n",
      "lif layer 2 self.abs_max_v: 206.0\n",
      "lif layer 2 self.abs_max_v: 207.0\n",
      "lif layer 1 self.abs_max_v: 1337.5\n",
      "lif layer 1 self.abs_max_v: 1398.0\n",
      "fc layer 2 self.abs_max_out: 158.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  8.906317/ 48.702805, val:  35.42%, val_best:  35.42%, tr:  97.65%, tr_best:  97.85%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0701%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.4613%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 10191  26.024%\n",
      "fc layer 1 self.abs_max_out: 1123.0\n",
      "fc layer 2 self.abs_max_out: 161.0\n",
      "fc layer 2 self.abs_max_out: 163.0\n",
      "fc layer 2 self.abs_max_out: 174.0\n",
      "fc layer 1 self.abs_max_out: 1155.0\n",
      "fc layer 2 self.abs_max_out: 196.0\n",
      "fc layer 2 self.abs_max_out: 197.0\n",
      "fc layer 2 self.abs_max_out: 203.0\n",
      "lif layer 2 self.abs_max_v: 239.0\n",
      "fc layer 1 self.abs_max_out: 1219.0\n",
      "fc layer 1 self.abs_max_out: 1254.0\n",
      "fc layer 2 self.abs_max_out: 241.0\n",
      "lif layer 2 self.abs_max_v: 241.0\n",
      "lif layer 1 self.abs_max_v: 1536.0\n",
      "lif layer 1 self.abs_max_v: 1641.0\n",
      "fc layer 1 self.abs_max_out: 1262.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  8.497782/ 46.730461, val:  30.42%, val_best:  35.42%, tr:  96.42%, tr_best:  97.85%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0355%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.1479%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 12601  25.743%\n",
      "lif layer 2 self.abs_max_v: 245.5\n",
      "lif layer 2 self.abs_max_v: 252.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  9.373499/ 58.986057, val:  35.00%, val_best:  35.42%, tr:  97.45%, tr_best:  97.85%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3761%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.1306%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 14779  25.160%\n",
      "fc layer 2 self.abs_max_out: 242.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  9.259144/ 73.429176, val:  38.75%, val_best:  38.75%, tr:  98.26%, tr_best:  98.26%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1407%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.9674%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 16746  24.436%\n",
      "lif layer 2 self.abs_max_v: 257.5\n",
      "lif layer 2 self.abs_max_v: 271.5\n",
      "fc layer 2 self.abs_max_out: 255.0\n",
      "lif layer 2 self.abs_max_v: 284.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss: 10.166203/ 45.510113, val:  36.67%, val_best:  38.75%, tr:  97.55%, tr_best:  98.26%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4433%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.4605%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 18842  24.058%\n",
      "lif layer 2 self.abs_max_v: 288.5\n",
      "lif layer 1 self.abs_max_v: 1912.5\n",
      "lif layer 1 self.abs_max_v: 2049.5\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss: 10.231122/ 42.688545, val:  39.17%, val_best:  39.17%, tr:  97.55%, tr_best:  98.26%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.7387%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.2023%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 20997  23.830%\n",
      "lif layer 2 self.abs_max_v: 291.0\n",
      "lif layer 2 self.abs_max_v: 311.5\n",
      "fc layer 3 self.abs_max_out: 349.0\n",
      "fc layer 3 self.abs_max_out: 398.0\n",
      "fc layer 2 self.abs_max_out: 256.0\n",
      "fc layer 2 self.abs_max_out: 270.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss: 10.188746/ 54.412373, val:  39.58%, val_best:  39.58%, tr:  96.63%, tr_best:  98.26%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8236%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.7082%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 23110  23.606%\n",
      "fc layer 1 self.abs_max_out: 1325.0\n",
      "fc layer 2 self.abs_max_out: 271.0\n",
      "lif layer 1 self.abs_max_v: 2079.0\n",
      "lif layer 1 self.abs_max_v: 2122.5\n",
      "lif layer 1 self.abs_max_v: 2155.5\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss: 10.052430/ 72.012260, val:  35.00%, val_best:  39.58%, tr:  96.73%, tr_best:  98.26%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.9443%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8523%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 25268  23.464%\n",
      "fc layer 2 self.abs_max_out: 272.0\n",
      "lif layer 1 self.abs_max_v: 2185.5\n",
      "lif layer 1 self.abs_max_v: 2261.0\n",
      "lif layer 1 self.abs_max_v: 2298.5\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss: 10.455697/ 70.591560, val:  18.33%, val_best:  39.58%, tr:  97.24%, tr_best:  98.26%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.9121%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7823%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 27383  23.309%\n",
      "fc layer 2 self.abs_max_out: 289.0\n",
      "fc layer 2 self.abs_max_out: 327.0\n",
      "lif layer 2 self.abs_max_v: 327.0\n",
      "fc layer 1 self.abs_max_out: 1348.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss: 10.255958/ 94.112007, val:  24.17%, val_best:  39.58%, tr:  97.75%, tr_best:  98.26%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8192%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7011%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 29353  23.064%\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss: 10.509660/ 64.483437, val:  31.25%, val_best:  39.58%, tr:  97.55%, tr_best:  98.26%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.9285%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0266%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 31354  22.876%\n",
      "fc layer 1 self.abs_max_out: 1356.0\n",
      "fc layer 1 self.abs_max_out: 1379.0\n",
      "fc layer 1 self.abs_max_out: 1387.0\n",
      "fc layer 3 self.abs_max_out: 415.0\n",
      "fc layer 2 self.abs_max_out: 329.0\n",
      "lif layer 2 self.abs_max_v: 329.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss: 10.066235/122.498367, val:  25.00%, val_best:  39.58%, tr:  97.24%, tr_best:  98.26%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.7007%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8656%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 33403  22.746%\n",
      "fc layer 1 self.abs_max_out: 1481.0\n",
      "lif layer 2 self.abs_max_v: 332.0\n",
      "lif layer 2 self.abs_max_v: 340.5\n",
      "lif layer 1 self.abs_max_v: 2346.0\n",
      "lif layer 1 self.abs_max_v: 2508.0\n",
      "lif layer 2 self.abs_max_v: 341.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss: 10.542712/ 58.540745, val:  38.75%, val_best:  39.58%, tr:  96.22%, tr_best:  98.26%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6914%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6644%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 35518  22.675%\n",
      "fc layer 3 self.abs_max_out: 448.0\n",
      "fc layer 2 self.abs_max_out: 340.0\n",
      "lif layer 2 self.abs_max_v: 344.5\n",
      "fc layer 2 self.abs_max_out: 376.0\n",
      "lif layer 2 self.abs_max_v: 376.0\n",
      "lif layer 2 self.abs_max_v: 381.5\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss: 10.333504/ 49.462456, val:  45.00%, val_best:  45.00%, tr:  96.12%, tr_best:  98.26%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6443%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0512%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 37573  22.576%\n",
      "lif layer 1 self.abs_max_v: 2596.5\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss: 10.841851/ 48.292667, val:  48.75%, val_best:  48.75%, tr:  97.65%, tr_best:  98.26%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.9675%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5777%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 39643  22.496%\n",
      "fc layer 2 self.abs_max_out: 385.0\n",
      "lif layer 2 self.abs_max_v: 385.0\n",
      "fc layer 1 self.abs_max_out: 1526.0\n",
      "lif layer 1 self.abs_max_v: 2779.5\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss: 11.411877/ 70.247414, val:  36.67%, val_best:  48.75%, tr:  96.32%, tr_best:  98.26%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.9394%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1700%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 41832  22.489%\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss: 10.166074/ 74.252312, val:  36.25%, val_best:  48.75%, tr:  97.04%, tr_best:  98.26%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.9943%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2837%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 43841  22.391%\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss: 10.337620/ 53.027306, val:  47.08%, val_best:  48.75%, tr:  96.83%, tr_best:  98.26%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1180%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9577%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 45968  22.359%\n",
      "fc layer 1 self.abs_max_out: 1547.0\n",
      "lif layer 1 self.abs_max_v: 2808.5\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss: 11.046958/ 69.095604, val:  32.92%, val_best:  48.75%, tr:  97.45%, tr_best:  98.26%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.7902%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4643%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 48081  22.324%\n",
      "lif layer 2 self.abs_max_v: 389.5\n",
      "lif layer 2 self.abs_max_v: 392.5\n",
      "lif layer 2 self.abs_max_v: 393.0\n",
      "lif layer 2 self.abs_max_v: 398.5\n",
      "lif layer 2 self.abs_max_v: 412.0\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss: 11.137547/ 53.009708, val:  50.00%, val_best:  50.00%, tr:  99.18%, tr_best:  99.18%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3552%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3261%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 49919  22.169%\n",
      "lif layer 2 self.abs_max_v: 416.5\n",
      "lif layer 2 self.abs_max_v: 417.5\n",
      "fc layer 3 self.abs_max_out: 458.0\n",
      "lif layer 2 self.abs_max_v: 440.5\n",
      "fc layer 3 self.abs_max_out: 462.0\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss: 11.078025/ 79.008026, val:  44.58%, val_best:  50.00%, tr:  98.57%, tr_best:  99.18%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2864%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9473%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 51609  21.965%\n",
      "fc layer 3 self.abs_max_out: 483.0\n",
      "fc layer 1 self.abs_max_out: 1569.0\n",
      "lif layer 1 self.abs_max_v: 2862.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss: 11.316517/ 78.039467, val:  45.42%, val_best:  50.00%, tr:  99.08%, tr_best:  99.18%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4249%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6842%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 53268  21.764%\n",
      "fc layer 2 self.abs_max_out: 393.0\n",
      "fc layer 1 self.abs_max_out: 1587.0\n",
      "lif layer 1 self.abs_max_v: 2912.0\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss: 11.865398/ 65.571175, val:  42.50%, val_best:  50.00%, tr:  99.08%, tr_best:  99.18%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5323%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.7488%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 54993  21.605%\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss: 10.870124/ 52.007099, val:  51.67%, val_best:  51.67%, tr:  99.28%, tr_best:  99.28%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6140%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6654%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 56629  21.424%\n",
      "fc layer 3 self.abs_max_out: 492.0\n",
      "fc layer 3 self.abs_max_out: 498.0\n",
      "fc layer 3 self.abs_max_out: 525.0\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss: 12.333500/ 50.082767, val:  54.58%, val_best:  54.58%, tr:  98.77%, tr_best:  99.28%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3541%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.4089%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 58338  21.282%\n",
      "lif layer 2 self.abs_max_v: 444.5\n",
      "fc layer 2 self.abs_max_out: 396.0\n",
      "fc layer 3 self.abs_max_out: 527.0\n",
      "fc layer 3 self.abs_max_out: 530.0\n",
      "fc layer 3 self.abs_max_out: 542.0\n",
      "fc layer 3 self.abs_max_out: 550.0\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss: 12.359153/ 43.610172, val:  56.25%, val_best:  56.25%, tr:  98.16%, tr_best:  99.28%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5196%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.1724%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 60104  21.170%\n",
      "lif layer 2 self.abs_max_v: 451.5\n",
      "fc layer 3 self.abs_max_out: 574.0\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss: 12.339394/119.332870, val:  35.83%, val_best:  56.25%, tr:  98.37%, tr_best:  99.28%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5021%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.6838%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 61903  21.077%\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss: 12.397161/ 54.182991, val:  49.17%, val_best:  56.25%, tr:  98.77%, tr_best:  99.28%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6934%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.5534%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 63664  20.977%\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss: 12.805119/ 89.141457, val:  40.83%, val_best:  56.25%, tr:  98.37%, tr_best:  99.28%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.7566%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8212%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 65560  20.927%\n",
      "fc layer 1 self.abs_max_out: 1599.0\n",
      "fc layer 1 self.abs_max_out: 1606.0\n",
      "lif layer 1 self.abs_max_v: 2965.0\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss: 11.756433/ 59.647438, val:  52.08%, val_best:  56.25%, tr:  98.26%, tr_best:  99.28%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.7453%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.5712%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 67296  20.830%\n",
      "fc layer 2 self.abs_max_out: 418.0\n",
      "lif layer 2 self.abs_max_v: 462.5\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss: 12.166808/ 66.352699, val:  50.42%, val_best:  56.25%, tr:  98.37%, tr_best:  99.28%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6259%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.3553%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 68978  20.723%\n",
      "fc layer 2 self.abs_max_out: 423.0\n",
      "fc layer 1 self.abs_max_out: 1683.0\n",
      "fc layer 1 self.abs_max_out: 1731.0\n",
      "lif layer 1 self.abs_max_v: 3163.5\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss: 11.863968/ 72.227539, val:  46.67%, val_best:  56.25%, tr:  99.39%, tr_best:  99.39%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6235%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.1947%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 70417  20.551%\n",
      "lif layer 2 self.abs_max_v: 469.0\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss: 12.284034/ 67.470459, val:  50.00%, val_best:  56.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4782%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.1433%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 71881  20.395%\n",
      "fc layer 3 self.abs_max_out: 609.0\n",
      "lif layer 2 self.abs_max_v: 477.5\n",
      "lif layer 2 self.abs_max_v: 485.5\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss: 11.628985/ 72.893219, val:  45.42%, val_best:  56.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4895%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.9938%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 73314  20.240%\n",
      "fc layer 3 self.abs_max_out: 631.0\n",
      "fc layer 3 self.abs_max_out: 639.0\n",
      "fc layer 3 self.abs_max_out: 678.0\n",
      "fc layer 1 self.abs_max_out: 1813.0\n",
      "lif layer 1 self.abs_max_v: 3259.0\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss: 11.919056/ 78.980743, val:  44.17%, val_best:  56.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4609%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.0533%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 74744  20.091%\n",
      "fc layer 1 self.abs_max_out: 1857.0\n",
      "lif layer 1 self.abs_max_v: 3323.0\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss: 12.790016/ 69.401176, val:  43.75%, val_best:  56.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3990%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.3213%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 76271  19.976%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss: 12.116913/ 90.644363, val:  39.17%, val_best:  56.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4676%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.3177%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 77732  19.850%\n",
      "fc layer 2 self.abs_max_out: 428.0\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss: 11.663368/ 64.951111, val:  52.92%, val_best:  56.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6049%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.2221%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 79154  19.720%\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss: 12.037156/ 68.283295, val:  48.75%, val_best:  56.25%, tr:  99.18%, tr_best:  99.90%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6464%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.5539%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 80574  19.596%\n",
      "fc layer 1 self.abs_max_out: 1909.0\n",
      "lif layer 1 self.abs_max_v: 3366.5\n",
      "lif layer 2 self.abs_max_v: 489.0\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss: 12.008162/ 89.797653, val:  37.92%, val_best:  56.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6865%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.0057%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 82015  19.482%\n",
      "lif layer 2 self.abs_max_v: 491.0\n",
      "lif layer 2 self.abs_max_v: 495.5\n",
      "fc layer 1 self.abs_max_out: 2066.0\n",
      "lif layer 1 self.abs_max_v: 3419.5\n",
      "lif layer 1 self.abs_max_v: 3709.0\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss: 12.707421/ 53.757275, val:  56.25%, val_best:  56.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5442%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.1228%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 83461  19.375%\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss: 12.399805/ 79.372955, val:  45.42%, val_best:  56.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6549%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.5703%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 84899  19.271%\n",
      "lif layer 2 self.abs_max_v: 507.5\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss: 12.033777/ 70.038544, val:  46.25%, val_best:  56.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4626%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.2655%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 86321  19.168%\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss: 12.439516/ 79.313644, val:  46.25%, val_best:  56.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6006%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.7406%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 87725  19.065%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss: 12.018820/ 99.098755, val:  45.83%, val_best:  56.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6826%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.6571%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 89134  18.968%\n",
      "fc layer 1 self.abs_max_out: 2115.0\n",
      "lif layer 1 self.abs_max_v: 3781.0\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss: 11.791877/ 80.335663, val:  47.08%, val_best:  56.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5943%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.9780%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 90530  18.872%\n",
      "lif layer 2 self.abs_max_v: 543.5\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss: 12.369513/ 80.540428, val:  50.83%, val_best:  56.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3958%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.8681%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 91950  18.784%\n",
      "fc layer 1 self.abs_max_out: 2123.0\n",
      "lif layer 1 self.abs_max_v: 3784.0\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss: 12.441189/ 68.569542, val:  50.00%, val_best:  56.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4538%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.0057%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 93409  18.708%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss: 12.035242/ 47.941727, val:  49.17%, val_best:  56.25%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4303%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.2626%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 94806  18.623%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss: 12.147262/ 52.069580, val:  55.42%, val_best:  56.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5105%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2854%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 96168  18.534%\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss: 12.132521/ 60.645222, val:  57.50%, val_best:  57.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3322%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0423%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 97549  18.452%\n",
      "fc layer 1 self.abs_max_out: 2136.0\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss: 13.051350/ 82.750381, val:  52.50%, val_best:  57.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2584%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1558%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 98948  18.376%\n",
      "fc layer 1 self.abs_max_out: 2293.0\n",
      "lif layer 1 self.abs_max_v: 4017.0\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss: 13.000767/ 69.222931, val:  53.33%, val_best:  57.50%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5521%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.4281%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 100373  18.308%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss: 12.487603/ 69.846466, val:  52.50%, val_best:  57.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6183%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8834%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 101753  18.234%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss: 12.017534/ 61.702965, val:  50.83%, val_best:  57.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6108%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.6989%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 103061  18.150%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss: 12.184941/ 91.088608, val:  43.75%, val_best:  57.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5694%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.4047%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 104395  18.074%\n",
      "lif layer 1 self.abs_max_v: 4018.5\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss: 12.055963/ 47.400642, val:  59.17%, val_best:  59.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5258%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3417%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 105733  18.000%\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss: 12.262658/ 87.834839, val:  49.17%, val_best:  59.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5731%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.8331%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 107064  17.928%\n",
      "fc layer 3 self.abs_max_out: 693.0\n",
      "fc layer 3 self.abs_max_out: 705.0\n",
      "fc layer 3 self.abs_max_out: 706.0\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss: 13.278988/ 78.067093, val:  54.58%, val_best:  59.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.7292%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.0826%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 108467  17.870%\n",
      "fc layer 2 self.abs_max_out: 430.0\n",
      "lif layer 2 self.abs_max_v: 552.0\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss: 13.533251/ 77.686890, val:  47.08%, val_best:  59.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8575%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.7111%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 109889  17.817%\n",
      "lif layer 2 self.abs_max_v: 562.5\n",
      "lif layer 1 self.abs_max_v: 4158.0\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss: 11.650270/ 84.768349, val:  52.08%, val_best:  59.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.7982%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.4144%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 111206  17.749%\n",
      "fc layer 3 self.abs_max_out: 713.0\n",
      "lif layer 2 self.abs_max_v: 609.5\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss: 12.217792/ 67.539757, val:  51.25%, val_best:  59.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.7219%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0692%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 112558  17.688%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss: 12.282762/ 69.600395, val:  53.33%, val_best:  59.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.7308%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0583%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 113903  17.628%\n",
      "lif layer 2 self.abs_max_v: 615.5\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss: 12.611320/ 81.667336, val:  51.67%, val_best:  59.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6578%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 115278  17.575%\n",
      "lif layer 2 self.abs_max_v: 630.5\n",
      "fc layer 3 self.abs_max_out: 717.0\n",
      "fc layer 3 self.abs_max_out: 722.0\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss: 12.879827/ 80.126411, val:  47.92%, val_best:  59.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5003%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3834%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 116633  17.520%\n",
      "fc layer 1 self.abs_max_out: 2374.0\n",
      "lif layer 1 self.abs_max_v: 4217.5\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss: 12.258777/ 65.449234, val:  53.33%, val_best:  59.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5564%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.5827%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 117978  17.465%\n",
      "fc layer 3 self.abs_max_out: 768.0\n",
      "fc layer 3 self.abs_max_out: 791.0\n",
      "lif layer 2 self.abs_max_v: 643.5\n",
      "fc layer 2 self.abs_max_out: 435.0\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss: 12.482751/ 79.640068, val:  48.75%, val_best:  59.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3785%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.1224%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 119319  17.411%\n",
      "fc layer 2 self.abs_max_out: 452.0\n",
      "fc layer 3 self.abs_max_out: 794.0\n",
      "fc layer 3 self.abs_max_out: 806.0\n",
      "fc layer 3 self.abs_max_out: 825.0\n",
      "fc layer 3 self.abs_max_out: 837.0\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss: 12.999374/ 86.121819, val:  55.00%, val_best:  59.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4116%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.0857%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 120679  17.362%\n",
      "lif layer 2 self.abs_max_v: 645.5\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss: 12.720455/ 70.049339, val:  55.00%, val_best:  59.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5185%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1320%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 122067  17.317%\n",
      "lif layer 2 self.abs_max_v: 648.5\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss: 12.313483/ 53.065235, val:  60.42%, val_best:  60.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4398%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.8646%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 123396  17.266%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss: 12.019661/ 84.726028, val:  55.42%, val_best:  60.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4982%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.6213%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 124745  17.219%\n",
      "lif layer 1 self.abs_max_v: 4218.0\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss: 13.993472/ 59.267662, val:  58.33%, val_best:  60.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5634%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.8345%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 126206  17.188%\n",
      "lif layer 2 self.abs_max_v: 668.0\n",
      "lif layer 2 self.abs_max_v: 672.0\n",
      "lif layer 2 self.abs_max_v: 694.0\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss: 12.456698/ 54.050999, val:  60.42%, val_best:  60.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6244%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3417%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 127584  17.147%\n",
      "lif layer 2 self.abs_max_v: 718.5\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss: 12.683325/ 79.197571, val:  55.00%, val_best:  60.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6364%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3888%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 128985  17.111%\n",
      "lif layer 2 self.abs_max_v: 722.0\n",
      "lif layer 2 self.abs_max_v: 753.5\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss: 12.330869/ 85.621628, val:  51.25%, val_best:  60.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5656%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.4240%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 130348  17.070%\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss: 12.950904/ 84.103493, val:  49.17%, val_best:  60.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4885%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.7799%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 131709  17.030%\n",
      "lif layer 2 self.abs_max_v: 756.5\n",
      "fc layer 1 self.abs_max_out: 2464.0\n",
      "lif layer 1 self.abs_max_v: 4283.5\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss: 12.137240/ 73.244698, val:  50.83%, val_best:  60.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5248%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.1568%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 132978  16.979%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss: 11.624847/ 69.863518, val:  57.50%, val_best:  60.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3998%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.5161%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 134225  16.926%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss: 12.617370/144.495987, val:  40.00%, val_best:  60.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5192%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.7564%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 135554  16.886%\n",
      "lif layer 2 self.abs_max_v: 757.5\n",
      "lif layer 2 self.abs_max_v: 778.5\n",
      "lif layer 2 self.abs_max_v: 790.5\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss: 13.290429/ 74.802200, val:  52.92%, val_best:  60.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6059%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.0246%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 137015  16.862%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss: 12.956659/121.159477, val:  43.33%, val_best:  60.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6004%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.7467%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 138405  16.830%\n",
      "lif layer 2 self.abs_max_v: 802.0\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss: 12.577226/ 65.456825, val:  58.33%, val_best:  60.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5340%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.9814%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 139732  16.792%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss: 13.125692/ 76.647141, val:  56.67%, val_best:  60.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5091%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.3873%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 141083  16.757%\n",
      "fc layer 2 self.abs_max_out: 474.0\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss: 12.415172/ 85.526192, val:  48.75%, val_best:  60.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4252%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.1027%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 142395  16.718%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss: 12.869032/ 74.996880, val:  60.42%, val_best:  60.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3996%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.8676%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 143751  16.686%\n",
      "fc layer 3 self.abs_max_out: 841.0\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss: 13.125962/ 84.716026, val:  47.92%, val_best:  60.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3208%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.5051%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 145102  16.653%\n",
      "fc layer 2 self.abs_max_out: 475.0\n",
      "fc layer 1 self.abs_max_out: 2494.0\n",
      "lif layer 1 self.abs_max_v: 4356.5\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss: 12.713649/ 63.161999, val:  59.58%, val_best:  60.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4607%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.0830%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 146431  16.619%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss: 13.777932/ 67.470139, val:  58.75%, val_best:  60.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.9768%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 147844  16.595%\n",
      "lif layer 2 self.abs_max_v: 815.0\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss: 12.596600/ 74.127335, val:  57.50%, val_best:  60.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2992%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.8076%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 149176  16.563%\n",
      "lif layer 2 self.abs_max_v: 817.0\n",
      "fc layer 2 self.abs_max_out: 492.0\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss: 12.714075/ 80.147751, val:  49.58%, val_best:  60.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.5925%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 150529  16.533%\n",
      "fc layer 1 self.abs_max_out: 2584.0\n",
      "lif layer 1 self.abs_max_v: 4498.5\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss: 12.702163/138.578445, val:  33.75%, val_best:  60.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3877%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.7909%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 151926  16.509%\n",
      "lif layer 2 self.abs_max_v: 826.0\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss: 12.633341/ 79.980713, val:  56.25%, val_best:  60.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4489%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.9936%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 153295  16.482%\n",
      "lif layer 2 self.abs_max_v: 850.0\n",
      "lif layer 1 self.abs_max_v: 4546.0\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss: 12.813105/ 70.641762, val:  52.92%, val_best:  60.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5326%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3708%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 154690  16.459%\n",
      "fc layer 2 self.abs_max_out: 503.0\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss: 12.385583/ 75.414688, val:  54.58%, val_best:  60.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4183%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.2791%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 156031  16.431%\n",
      "fc layer 2 self.abs_max_out: 505.0\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss: 12.039190/ 68.540863, val:  61.67%, val_best:  61.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3481%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3432%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 157358  16.401%\n",
      "fc layer 2 self.abs_max_out: 528.0\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss: 11.896548/ 77.314018, val:  50.42%, val_best:  61.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3082%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.6185%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 158635  16.367%\n",
      "fc layer 2 self.abs_max_out: 530.0\n",
      "lif layer 2 self.abs_max_v: 851.0\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss: 12.663665/ 89.032761, val:  51.67%, val_best:  61.67%, tr:  99.49%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2564%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.9117%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 159954  16.339%\n",
      "lif layer 2 self.abs_max_v: 859.5\n",
      "fc layer 1 self.abs_max_out: 2680.0\n",
      "lif layer 1 self.abs_max_v: 4766.0\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss: 12.317581/ 58.099857, val:  64.58%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2892%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.1201%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 161232  16.306%\n",
      "lif layer 2 self.abs_max_v: 861.0\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss: 12.889118/ 71.482597, val:  56.67%, val_best:  64.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3041%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.0361%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 162594  16.283%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss: 11.614197/ 99.049896, val:  41.67%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2524%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.4309%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 163881  16.252%\n",
      "lif layer 2 self.abs_max_v: 898.5\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss: 12.737060/ 93.253220, val:  52.08%, val_best:  64.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2445%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.1135%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 165204  16.226%\n",
      "fc layer 3 self.abs_max_out: 848.0\n",
      "fc layer 3 self.abs_max_out: 850.0\n",
      "fc layer 3 self.abs_max_out: 869.0\n",
      "fc layer 1 self.abs_max_out: 2683.0\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss: 12.471168/ 82.391571, val:  52.08%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4607%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.6377%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 166551  16.202%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss: 12.586144/ 90.053177, val:  46.25%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3437%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.5880%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 167916  16.181%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss: 12.308484/ 89.574738, val:  45.83%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1799%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.6385%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 169223  16.154%\n",
      "fc layer 3 self.abs_max_out: 904.0\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss: 12.751509/ 76.856812, val:  50.00%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3692%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.4428%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 170600  16.135%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss: 12.459808/ 63.552570, val:  59.17%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.0993%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.9813%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 171921  16.111%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss: 12.160739/ 97.023460, val:  49.58%, val_best:  64.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3857%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.5398%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 173236  16.087%\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        # \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [256.0,128.0,64.0,32.0,16.0]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "\n",
    "        \"learning_rate\": {\"values\": [1.0]}, \n",
    "        # \"lr_factor\": {\"values\": [-6, -7, -8, -9]}, \n",
    "        \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [14]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [-1]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [0]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        \"scale_exp_2w\": {\"values\": [0]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        \"scale_exp_3w\": {\"values\": [0]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "        \"lif_layer_sg_width2\": {\"values\": [0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0]},\n",
    "        \"lif_layer_v_threshold2\": {\"values\": [256.0,128.0,64.0,32.0,16.0]},\n",
    "        \"learning_rate2\": {\"values\": [1.0]},\n",
    "        \"init_scaling_0\": {\"values\": [4/128]},\n",
    "        \"init_scaling_1\": {\"values\": [6/128]},\n",
    "        \"init_scaling_2\": {\"values\": [3/128]},\n",
    "        \n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"0\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_2w,wandb.config.scale_exp_2w],[wandb.config.scale_exp_3w,wandb.config.scale_exp_3w]],\n",
    "        lif_layer_sg_width2  =  wandb.config.lif_layer_sg_width2,\n",
    "        lif_layer_v_threshold2  =  wandb.config.lif_layer_v_threshold2,\n",
    "        learning_rate2  =  wandb.config.learning_rate2,\n",
    "        init_scaling = [wandb.config.init_scaling_0,wandb.config.init_scaling_1,wandb.config.init_scaling_2],\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'e1m59f1o'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
