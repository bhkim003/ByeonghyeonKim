{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17781/3914466541.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' 레퍼런스\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "from spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from spikingjelly.datasets import split_to_train_test_set\n",
    "from spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "                    synapse_conv_trace_const1 = 1,\n",
    "                    synapse_conv_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "                    synapse_fc_trace_const1 = 1,\n",
    "                    synapse_fc_trace_const2 = 0.6,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    weight_count_print = False, # True # False\n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    verbose_interval = 100, #숫자 크게 하면 꺼짐\n",
    "                    validation_interval = 10, #숫자 크게 하면 꺼짐\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    gradient_verbose = False,\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = True,\n",
    "\n",
    "                    nda_net = False,\n",
    "                    \n",
    "                    domain_il_epoch = 0, # over 0, then domain il mode on\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 10005,\n",
    "\n",
    "                    OTTT_sWS_on = True, # True # False\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                 \n",
    "                    e_transport_swap = 5, # 1 이상이면 해당 숫자 에포크만큼 val_acc_best가 변화가 없으면 e_transport scheme (BP vs DFA) swap\n",
    "                    e_transport_swap_tr = 0, # 1 이상이면 해당 숫자 에포크만큼 val_acc_best가 변화가 없으면 e_transport scheme (BP vs DFA) swap\n",
    "                    e_transport_swap_coin = 0, # swap할 수 있는 coin 개수\n",
    "\n",
    "                    drop_rate = 0.5, \n",
    "\n",
    "                    exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                    merge_polarities = True, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                    denoise_on = True, \n",
    "                  ):\n",
    "    ## hyperparameter check #############################################################\n",
    "    if OTTT_sWS_on == True:\n",
    "        assert BPTT_on == False and tdBN_on == False and BN_on == False\n",
    "        if convTrue_fcFalse == False:\n",
    "            assert single_step == True\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False and any(isinstance(item, list) for item in cfg) == False\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    ## 함수 내 모든 로컬 변수 저장 ########################################################\n",
    "    hyperparameters = locals()\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "    \n",
    "    args_gpu = None\n",
    "    ## DDP settting ######################################################################\n",
    "    if (ddp_on == True):\n",
    "        parser = argparse.ArgumentParser(description='my_snn CIFAR10 Training')\n",
    "\n",
    "        # # local_rank는 command line에서 따로 줄 필요는 없지만, 선언은 필요\n",
    "        parser.add_argument(\"--local_rank\", default=0, type=int)\n",
    "\n",
    "        args = parser.parse_args() # 이거 적어줘야됨. parser argument선언하고\n",
    "\n",
    "        args.gpu = args.local_rank\n",
    "        args_gpu = args.gpu\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "        torch.distributed.init_process_group(backend=\"nccl\", init_method=\"env://\")\n",
    "        args.world_size = torch.distributed.get_world_size()\n",
    "    #######################################################################################\n",
    "\n",
    "\n",
    "    ## wandb 세팅 ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    if (ddp_on == True and torch.distributed.get_rank() != 0):\n",
    "        wandb.finish()\n",
    "    if (ddp_on == False or torch.distributed.get_rank() == 0):\n",
    "        wandb.config.update(hyperparameters)\n",
    "        wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "        wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "        wandb.run.log_code(\".\", \n",
    "                           include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "                           exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path\n",
    "                           )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader 가져오기 ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME,\n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on, )\n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "    ###########################################################################################################################################\n",
    "\n",
    "    \n",
    "    ## parameter number calculator (안 중요함) ##################################################################################################################\n",
    "    params_num = 0\n",
    "    img_size = IMAGE_SIZE \n",
    "    bias_param = 1 # 1 or 0\n",
    "    classifier_making = False\n",
    "    if (convTrue_fcFalse == True):\n",
    "        past_kernel = synapse_conv_in_channels\n",
    "        for kernel in cfg:\n",
    "            if (classifier_making == False):\n",
    "                if (type(kernel) == list):\n",
    "                    for residual_kernel in kernel:\n",
    "                        if (residual_kernel >= 10000 and residual_kernel < 20000): # separable\n",
    "                            residual_kernel -= 10000\n",
    "                            params_num += (synapse_conv_kernel_size**2 + bias_param) * past_kernel\n",
    "                            params_num += (1**2 * past_kernel + bias_param) * residual_kernel\n",
    "                            past_kernel = residual_kernel  \n",
    "                        elif (residual_kernel >= 20000 and residual_kernel < 30000): # depthwise\n",
    "                            residual_kernel -= 20000\n",
    "                            # 'past_kernel' should be same with 'kernel'\n",
    "                            params_num += (synapse_conv_kernel_size**2 + bias_param) * past_kernel\n",
    "                            past_kernel = residual_kernel  \n",
    "                        else:\n",
    "                            params_num += residual_kernel * ((synapse_conv_kernel_size**2) * past_kernel + bias_param)\n",
    "                            past_kernel = residual_kernel\n",
    "                elif (kernel == 'P' or kernel == 'M'):\n",
    "                    img_size = img_size // 2\n",
    "                elif (kernel == 'D'):\n",
    "                    img_size = 1\n",
    "                elif (kernel == 'L'):\n",
    "                    classifier_making = True\n",
    "                    past_kernel = past_kernel * (img_size**2)\n",
    "                else:\n",
    "                    if (kernel >= 10000 and kernel < 20000): # separable\n",
    "                        kernel -= 10000\n",
    "                        params_num += (synapse_conv_kernel_size**2 + bias_param) * past_kernel\n",
    "                        params_num += (1**2 * past_kernel + bias_param) * kernel\n",
    "                        past_kernel = kernel  \n",
    "                    elif (kernel >= 20000 and kernel < 30000): # depthwise\n",
    "                        kernel -= 20000\n",
    "                        # 'past_kernel' should be same with 'kernel'\n",
    "                        params_num += (synapse_conv_kernel_size**2 + bias_param) * past_kernel\n",
    "                        past_kernel = kernel  \n",
    "                    else:\n",
    "                        params_num += kernel * (synapse_conv_kernel_size**2 * past_kernel + bias_param)\n",
    "                        past_kernel = kernel    \n",
    "            else: # classifier making\n",
    "                params_num += (past_kernel + bias_param) * kernel\n",
    "                past_kernel = kernel\n",
    "        \n",
    "        \n",
    "        if classifier_making == False:\n",
    "            past_kernel = past_kernel*img_size*img_size\n",
    "\n",
    "        params_num += (past_kernel + bias_param) * synapse_fc_out_features\n",
    "    else:\n",
    "        past_in_channel = synapse_conv_in_channels*img_size*img_size\n",
    "        for in_channel in cfg:\n",
    "            if (type(in_channel) == list):\n",
    "                for residual_in_channel in in_channel:\n",
    "                    params_num += (past_in_channel + bias_param) * residual_in_channel\n",
    "                    past_in_channel = residual_in_channel\n",
    "            elif (in_channel == 'P' or in_channel == 'M'):\n",
    "                img_size = img_size // 2\n",
    "                past_in_channel = synapse_conv_in_channels*img_size*img_size\n",
    "            else:\n",
    "                params_num += (past_in_channel + bias_param) * in_channel\n",
    "                past_in_channel = in_channel\n",
    "        params_num += (past_in_channel + bias_param) * synapse_fc_out_features\n",
    "    ###########################################################################################################################################\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        if (single_step == False):\n",
    "            net = MY_SNN_FC(cfg, synapse_conv_in_channels, IMAGE_SIZE, synapse_fc_out_features,\n",
    "                        synapse_fc_trace_const1, synapse_fc_trace_const2, \n",
    "                        lif_layer_v_init, lif_layer_v_decay, \n",
    "                        lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                        lif_layer_sg_width,\n",
    "                        tdBN_on,\n",
    "                        BN_on, TIME,\n",
    "                        surrogate,\n",
    "                        BPTT_on,\n",
    "                        DFA_on,\n",
    "                        drop_rate).to(device)\n",
    "        else:\n",
    "            net = MY_SNN_FC_sstep(cfg, synapse_conv_in_channels, IMAGE_SIZE, synapse_fc_out_features,\n",
    "                        synapse_fc_trace_const1, synapse_fc_trace_const2, \n",
    "                        lif_layer_v_init, lif_layer_v_decay, \n",
    "                        lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                        lif_layer_sg_width,\n",
    "                        tdBN_on,\n",
    "                        BN_on, TIME,\n",
    "                        surrogate,\n",
    "                        BPTT_on,\n",
    "                        DFA_on,\n",
    "                        OTTT_sWS_on,\n",
    "                        drop_rate).to(device)\n",
    "    else:\n",
    "        if (single_step == False):\n",
    "            net = MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE,\n",
    "                        synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                        synapse_conv_padding, synapse_conv_trace_const1, \n",
    "                        synapse_conv_trace_const2, \n",
    "                        lif_layer_v_init, lif_layer_v_decay, \n",
    "                        lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                        lif_layer_sg_width,\n",
    "                        synapse_fc_out_features, synapse_fc_trace_const1, synapse_fc_trace_const2,\n",
    "                        tdBN_on,\n",
    "                        BN_on, TIME,\n",
    "                        surrogate,\n",
    "                        BPTT_on,\n",
    "                        OTTT_sWS_on,\n",
    "                        DFA_on,\n",
    "                        drop_rate).to(device)\n",
    "        else:\n",
    "            net = MY_SNN_CONV_sstep(cfg, synapse_conv_in_channels, IMAGE_SIZE,\n",
    "                        synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                        synapse_conv_padding, synapse_conv_trace_const1, \n",
    "                        synapse_conv_trace_const2, \n",
    "                        lif_layer_v_init, lif_layer_v_decay, \n",
    "                        lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                        lif_layer_sg_width,\n",
    "                        synapse_fc_out_features, synapse_fc_trace_const1, synapse_fc_trace_const2,\n",
    "                        tdBN_on,\n",
    "                        BN_on, TIME,\n",
    "                        surrogate,\n",
    "                        BPTT_on,\n",
    "                        OTTT_sWS_on,\n",
    "                        DFA_on,\n",
    "                        drop_rate).to(device)\n",
    "    if (nda_net == True):\n",
    "        net = VGG(cfg = cfg, num_classes=10, batch_norm = tdBN_on, in_c = synapse_conv_in_channels, \n",
    "                    lif_layer_v_threshold=lif_layer_v_threshold, lif_layer_v_decay=lif_layer_v_decay, lif_layer_sg_width=lif_layer_sg_width)\n",
    "        net.T = TIME\n",
    "    if ddp_on == False:\n",
    "        net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        net.load_state_dict(torch.load(pre_trained_path))\n",
    "    \n",
    "    if ddp_on == True:\n",
    "        device = args.gpu\n",
    "        net = net.to(args.gpu)\n",
    "        net = DDP(net, delay_allreduce=True)\n",
    "\n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        if ddp_on == False or torch.distributed.get_rank() == 0:\n",
    "            print(net)    \n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    if ddp_on == False or torch.distributed.get_rank() == 0:\n",
    "        wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter logging해줌\n",
    "    ############################################################\n",
    "\n",
    "    ## param num and memory estimation except BN with MY own calculation some lines above ##########################################\n",
    "    if ddp_on == False or torch.distributed.get_rank() == 0:\n",
    "        real_param_num = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "        if (weight_count_print == True):\n",
    "            for name, param in net.named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    print(f'Layer: {name} | Number of parameters: {param.numel()}')\n",
    "        # Batch norm 있으면 아래 두 개 서로 다를 수 있음.\n",
    "        # assert real_param_num == params_num, f'parameter number is not same. real_param_num: {real_param_num}, params_num: {params_num}'    \n",
    "        print('='*50)\n",
    "        print(f\"My Num of PARAMS: {params_num:,}, system's param_num : {real_param_num:,}\")\n",
    "        memory = params_num / 8 / 1024 / 1024 # MB\n",
    "        precision = 32\n",
    "        memory = memory * precision \n",
    "        print(f\"Memory: {memory:.2f}MiB at {precision}-bit\")\n",
    "        print('='*50)\n",
    "    ##############################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## criterion ########################################## # loss 구해주는 친구\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    if (OTTT_sWS_on == True):\n",
    "        # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "        if which_data == 'DVS_GESTURE':\n",
    "            criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    if(optimizer_what == 'SGD'):\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss= 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    elapsed_time_val = 0\n",
    "    no_val_best_growth_count = 0\n",
    "    no_tr_best_growth_count = 0\n",
    "    iter_acc_array = np.array([])\n",
    "    tr_acc_array = np.array([])\n",
    "    val_acc_now_array = np.array([])\n",
    "    DFA_current = DFA_on\n",
    "    DFA_toggle = False\n",
    "    DFA_flag = 1.0 if DFA_current == True else 0.0\n",
    "    DFA_BP_toggle_trial = 0\n",
    "    iter_of_val = False\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        if (e_transport_swap > 0 or e_transport_swap_tr > 0):\n",
    "            assert not (e_transport_swap > 0 and e_transport_swap_tr > 0)\n",
    "            if e_transport_swap > 0 and no_val_best_growth_count == e_transport_swap:\n",
    "                if DFA_BP_toggle_trial < e_transport_swap_coin:\n",
    "                    net = BP_DFA_SWAP(net, convTrue_fcFalse, single_step, ddp_on, args_gpu)\n",
    "                    no_val_best_growth_count = 0\n",
    "                    DFA_current = not DFA_current\n",
    "                    DFA_toggle = True\n",
    "                    DFA_BP_toggle_trial = DFA_BP_toggle_trial + 1\n",
    "            if e_transport_swap_tr > 0 and no_tr_best_growth_count == e_transport_swap_tr:\n",
    "                if DFA_BP_toggle_trial < e_transport_swap_coin:\n",
    "                    net = BP_DFA_SWAP(net, convTrue_fcFalse, single_step, ddp_on, args_gpu)\n",
    "                    no_tr_best_growth_count = 0\n",
    "                    DFA_current = not DFA_current\n",
    "                    DFA_toggle = True\n",
    "                    DFA_BP_toggle_trial = DFA_BP_toggle_trial + 1\n",
    "\n",
    "        if ddp_on == False or torch.distributed.get_rank() == 0:\n",
    "            # print('EPOCH', epoch)\n",
    "            pass\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # if (domain_il_epoch>0 and which_data == 'PMNIST'):\n",
    "        #     k = epoch // domain_il_epoch\n",
    "        #     xtrain=data[k]['train']['x']\n",
    "        #     ytrain=data[k]['train']['y']\n",
    "        #     xtest =data[k]['test']['x']\n",
    "        #     ytest =data[k]['test']['y']\n",
    "\n",
    "        \n",
    "        ####### iterator : input_loading & tqdm을 통한 progress_bar 생성###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        if ddp_on == False or torch.distributed.get_rank() == 0:  \n",
    "            iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "        \n",
    "        #### validation_interval이 batch size보다 작을 시 validation_interval을 batch size로 맞춰줌#############\n",
    "        validation_interval2 = validation_interval\n",
    "        if (validation_interval > len(train_loader)):\n",
    "            validation_interval2 = len(train_loader)\n",
    "        ##################################################################################################\n",
    "\n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            iter_one_train_time_start = time.time()\n",
    "            net.train() # train 모드로 바꿔줘야함\n",
    "\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "                # print('x_len',x_len)\n",
    "                # mask = padded_sequence_mask(x_len)\n",
    "                # max_time_step = x_len.max()\n",
    "                # min_time_step = x_len.min()\n",
    "            ## batch 크기 ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'n_tidigits'):\n",
    "                inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "                labels = labels[:, 0, :]\n",
    "                labels = torch.argmax(labels, dim=1)\n",
    "            elif (which_data == 'heidelberg'):\n",
    "                inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "                print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "            # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "            # print(labels)\n",
    "                \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            \n",
    "            # # dvs 데이터 시각화 코드 (확인 필요할 시 써라)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "\n",
    "            ## gradient 초기화 #######################################\n",
    "            optimizer.zero_grad()\n",
    "            ###########################################################\n",
    "            \n",
    "            ## DVS gesture에서 other label자리 매꾸기 ###############\n",
    "            if (which_data == 'DVS_GESTURE'):\n",
    "                labels[labels>2] -= 1\n",
    "            #######################################################         \n",
    "                               \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first input도 ottt trace 적용하기 위한 코드 (validation 시에는 필요X) ##########################\n",
    "                if OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_conv_trace_const2 + spike[t]*synapse_conv_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                ############################################################\n",
    "                ## weight 업데이트!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "                optimizer.step() # full step time update\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # ottt꺼 쓸때\n",
    "                labels = labels[0]\n",
    "                loss /= TIME\n",
    "            tr_epoch_loss_temp += loss.data/len(train_loader)\n",
    "\n",
    "            ## net 그림 출력해보기 #################################################################\n",
    "            # print('시각화')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch 어긋남 방지 ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            if i % verbose_interval == verbose_interval-1:\n",
    "                if ddp_on == False or torch.distributed.get_rank() == 0:\n",
    "                    print(f'{epoch}-{i} training acc: {100 * iter_acc:.2f}%, lr={[f\"{lr}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}, val_acc: {100 * val_acc_now:.2f}%')\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            iter_one_train_time_end = time.time()\n",
    "            elapsed_time = iter_one_train_time_end - iter_one_train_time_start  # 실행 시간 계산\n",
    "\n",
    "            if (i % verbose_interval == verbose_interval-1):\n",
    "                if ddp_on == False or torch.distributed.get_rank() == 0:\n",
    "                    print(f\"iter_one_train_time: {elapsed_time} seconds, last one_val_time: {elapsed_time_val} seconds\\n\")\n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i % validation_interval2 == validation_interval2-1:\n",
    "                iter_one_val_time_start = time.time()\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "                val_loss = 0\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval 모드로 바꿔줘야함 \n",
    "                    for data in test_loader:\n",
    "                        ## data loading & semi-pre-processing ##########################################################\n",
    "                        if len(data) == 2:\n",
    "                            inputs, labels = data\n",
    "                            # 처리 로직 작성\n",
    "                        elif len(data) == 3:\n",
    "                            inputs, labels, x_len = data\n",
    "                            # print('x_len',x_len)\n",
    "                            # mask = padded_sequence_mask(x_len)\n",
    "                            # max_time_step = x_len.max()\n",
    "                            # min_time_step = x_len.min()\n",
    "                            # B, T, *spatial_dims = inputs.shape\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        inputs = inputs.to(device)\n",
    "                        labels = labels.to(device)\n",
    "                        real_batch = labels.size(0)\n",
    "                        \n",
    "                        ## DVS gesture에서 other label자리 매꾸기 ###############\n",
    "                        if (which_data == 'DVS_GESTURE'):\n",
    "                            labels[labels>2] -= 1\n",
    "                        #######################################################\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs = inputs[:,:,0,:,:]\n",
    "\n",
    "                        ## network 연산 시작 ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs.permute(1, 0, 2, 3, 4)) #inputs: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs[t])\n",
    "                                val_loss_temp = criterion(outputs, labels)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct += (predicted == labels).sum().item()\n",
    "\n",
    "                    val_acc_now = correct / total\n",
    "                    # print(f'{epoch}-{i} validation acc: {100 * val_acc_now:.2f}%, lr={[f\"{lr:.10f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}')\n",
    "\n",
    "                iter_one_val_time_end = time.time()\n",
    "                elapsed_time_val = iter_one_val_time_end - iter_one_val_time_start  # 실행 시간 계산\n",
    "                # print(f\"iter_one_val_time: {elapsed_time_val} seconds\")\n",
    "\n",
    "                # network save\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    if ddp_on == False or torch.distributed.get_rank() == 0:\n",
    "                        # wandb 키면 state_dict아닌거는 저장 안됨\n",
    "                        torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "                        # torch.save(net, f\"net_save/save_now_net_{unique_name}.pth\")\n",
    "                        # torch.save(net.module.state_dict(), f\"net_save/save_now_net_weights2_{unique_name}.pth\")\n",
    "                        # torch.save(net.module, f\"net_save/save_now_net2_{unique_name}.pth\")\n",
    "                    no_val_best_growth_count = 0\n",
    "                else:\n",
    "                    no_val_best_growth_count = no_val_best_growth_count + 1\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "                    no_tr_best_growth_count = 0\n",
    "                else:\n",
    "                    no_tr_best_growth_count = no_tr_best_growth_count + 1\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "                if DFA_toggle == True:\n",
    "                    DFA_flag = 1.0 - DFA_flag\n",
    "                    DFA_toggle = False\n",
    "\n",
    "                iter_of_val = True\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            if ddp_on == False or torch.distributed.get_rank() == 0:\n",
    "                if iter_of_val == False:\n",
    "                    iterator.set_description(f\"{iter_acc_string}, iter_loss:{loss:10.6f}, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                else:\n",
    "                    iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                    iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if ddp_on == False or torch.distributed.get_rank() == 0:\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"DFA_flag\": DFA_flag}) # DFA mode 바뀌자 마자 바뀌는 게 아니고 validation 한번 했을 때 바뀜.\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            \n",
    "            ## accuray 로컬에 저장 하기 위한 코드 #####################################################################################\n",
    "            iter_acc_array = np.append(iter_acc_array, iter_acc)\n",
    "            tr_acc_array = np.append(tr_acc_array, tr_acc)\n",
    "            val_acc_now_array = np.append(val_acc_now_array, val_acc_now)\n",
    "            base_name = f'{current_time}'\n",
    "            ####################################################################################################################\n",
    "            \n",
    "            iter_acc_file_name_time = f'result_save/{base_name}_iter_acc_array_{unique_name}.npy'\n",
    "            tr_acc_file_name_time = f'result_save/{base_name}_tr_acc_array_{unique_name}.npy'\n",
    "            val_acc_file_name_time = f'result_save/{base_name}_val_acc_now_array_{unique_name}.npy'\n",
    "            hyperparameters_file_name_time = f'result_save/{base_name}_hyperparameters_{unique_name}.json'\n",
    "\n",
    "            hyperparameters['current epoch'] = epoch\n",
    "\n",
    "            ### accuracy 세이브: 덮어쓰기 하기 싫으면 주석 풀어서 사용 (시간마다 새로 쓰기) 비추천 ########################\n",
    "            # if ddp_on == False or torch.distributed.get_rank() == 0:\n",
    "            #     np.save(iter_acc_file_name_time, iter_acc_array)\n",
    "            #     np.save(tr_acc_file_name_time, iter_acc_array)\n",
    "            #     np.save(val_acc_file_name_time, val_acc_now_array)\n",
    "            #     with open(hyperparameters_file_name_time, 'w') as f:\n",
    "            #         json.dump(hyperparameters, f, indent=4)\n",
    "            #########################################################################################################\n",
    "\n",
    "            ## accuracy 세이브 ###########################################################################################\n",
    "            if ddp_on == False or torch.distributed.get_rank() == 0:\n",
    "                np.save(f'result_save/iter_acc_array_{unique_name}.npy', iter_acc_array)\n",
    "                np.save(f'result_save/tr_acc_array_{unique_name}.npy', tr_acc_array)\n",
    "                np.save(f'result_save/val_acc_now_array_{unique_name}.npy', val_acc_now_array)\n",
    "                with open(f'result_save/hyperparameters_{unique_name}.json', 'w') as f:\n",
    "                    json.dump(hyperparameters, f, indent=4)\n",
    "            ##########################################################################################################\n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "                \n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "        # 실행 시간 계산\n",
    "        epoch_time_end = time.time()\n",
    "        # print(f\"epoch_time: {epoch_time_end - epoch_start_time} seconds\\n\") \n",
    "    #======== EPOCH END ==========================================================================================\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "                    synapse_conv_trace_const1 = 1,\n",
    "                    synapse_conv_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "                    synapse_fc_trace_const1 = 1,\n",
    "                    synapse_fc_trace_const2 = 0.6,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    weight_count_print = False, # True # False\n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    verbose_interval = 100, #숫자 크게 하면 꺼짐\n",
    "                    validation_interval = 10, #숫자 크게 하면 꺼짐\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    gradient_verbose = False,\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = True,\n",
    "\n",
    "                    nda_net = False,\n",
    "                    \n",
    "                    domain_il_epoch = 0, # over 0, then domain il mode on\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 10005,\n",
    "\n",
    "                    OTTT_sWS_on = True, # True # False\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                 \n",
    "                    e_transport_swap = 5, # 1 이상이면 해당 숫자 에포크만큼 val_acc_best가 변화가 없으면 e_transport scheme (BP vs DFA) swap\n",
    "                    e_transport_swap_tr = 0, # 1 이상이면 해당 숫자 에포크만큼 val_acc_best가 변화가 없으면 e_transport scheme (BP vs DFA) swap\n",
    "                    e_transport_swap_coin = 0, # swap할 수 있는 coin 개수\n",
    "\n",
    "                    drop_rate = 0.5, \n",
    "\n",
    "                    exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                    merge_polarities = True, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                    denoise_on = True, \n",
    "                  ):\n",
    "    ## hyperparameter check #############################################################\n",
    "    if OTTT_sWS_on == True:\n",
    "        assert BPTT_on == False and tdBN_on == False and BN_on == False\n",
    "        if convTrue_fcFalse == False:\n",
    "            assert single_step == True\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False and any(isinstance(item, list) for item in cfg) == False\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    ## 함수 내 모든 로컬 변수 저장 ########################################################\n",
    "    hyperparameters = locals()\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "    \n",
    "    args_gpu = None\n",
    "    ## DDP settting ######################################################################\n",
    "    if (ddp_on == True):\n",
    "        parser = argparse.ArgumentParser(description='my_snn CIFAR10 Training')\n",
    "\n",
    "        # # local_rank는 command line에서 따로 줄 필요는 없지만, 선언은 필요\n",
    "        parser.add_argument(\"--local_rank\", default=0, type=int)\n",
    "\n",
    "        args = parser.parse_args() # 이거 적어줘야됨. parser argument선언하고\n",
    "\n",
    "        args.gpu = args.local_rank\n",
    "        args_gpu = args.gpu\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "        torch.distributed.init_process_group(backend=\"nccl\", init_method=\"env://\")\n",
    "        args.world_size = torch.distributed.get_world_size()\n",
    "    #######################################################################################\n",
    "\n",
    "\n",
    "    ## wandb 세팅 ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    if (ddp_on == True and torch.distributed.get_rank() != 0):\n",
    "        wandb.finish()\n",
    "    if (ddp_on == False or torch.distributed.get_rank() == 0):\n",
    "        wandb.config.update(hyperparameters)\n",
    "        wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "        wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "        wandb.run.log_code(\".\", \n",
    "                           include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "                           exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path\n",
    "                           )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader 가져오기 ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME,\n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on, )\n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "    ###########################################################################################################################################\n",
    "\n",
    "    \n",
    "    ## parameter number calculator (안 중요함) ##################################################################################################################\n",
    "    params_num = 0\n",
    "    img_size = IMAGE_SIZE \n",
    "    bias_param = 1 # 1 or 0\n",
    "    classifier_making = False\n",
    "    if (convTrue_fcFalse == True):\n",
    "        past_kernel = synapse_conv_in_channels\n",
    "        for kernel in cfg:\n",
    "            if (classifier_making == False):\n",
    "                if (type(kernel) == list):\n",
    "                    for residual_kernel in kernel:\n",
    "                        if (residual_kernel >= 10000 and residual_kernel < 20000): # separable\n",
    "                            residual_kernel -= 10000\n",
    "                            params_num += (synapse_conv_kernel_size**2 + bias_param) * past_kernel\n",
    "                            params_num += (1**2 * past_kernel + bias_param) * residual_kernel\n",
    "                            past_kernel = residual_kernel  \n",
    "                        elif (residual_kernel >= 20000 and residual_kernel < 30000): # depthwise\n",
    "                            residual_kernel -= 20000\n",
    "                            # 'past_kernel' should be same with 'kernel'\n",
    "                            params_num += (synapse_conv_kernel_size**2 + bias_param) * past_kernel\n",
    "                            past_kernel = residual_kernel  \n",
    "                        else:\n",
    "                            params_num += residual_kernel * ((synapse_conv_kernel_size**2) * past_kernel + bias_param)\n",
    "                            past_kernel = residual_kernel\n",
    "                elif (kernel == 'P' or kernel == 'M'):\n",
    "                    img_size = img_size // 2\n",
    "                elif (kernel == 'D'):\n",
    "                    img_size = 1\n",
    "                elif (kernel == 'L'):\n",
    "                    classifier_making = True\n",
    "                    past_kernel = past_kernel * (img_size**2)\n",
    "                else:\n",
    "                    if (kernel >= 10000 and kernel < 20000): # separable\n",
    "                        kernel -= 10000\n",
    "                        params_num += (synapse_conv_kernel_size**2 + bias_param) * past_kernel\n",
    "                        params_num += (1**2 * past_kernel + bias_param) * kernel\n",
    "                        past_kernel = kernel  \n",
    "                    elif (kernel >= 20000 and kernel < 30000): # depthwise\n",
    "                        kernel -= 20000\n",
    "                        # 'past_kernel' should be same with 'kernel'\n",
    "                        params_num += (synapse_conv_kernel_size**2 + bias_param) * past_kernel\n",
    "                        past_kernel = kernel  \n",
    "                    else:\n",
    "                        params_num += kernel * (synapse_conv_kernel_size**2 * past_kernel + bias_param)\n",
    "                        past_kernel = kernel    \n",
    "            else: # classifier making\n",
    "                params_num += (past_kernel + bias_param) * kernel\n",
    "                past_kernel = kernel\n",
    "        \n",
    "        \n",
    "        if classifier_making == False:\n",
    "            past_kernel = past_kernel*img_size*img_size\n",
    "\n",
    "        params_num += (past_kernel + bias_param) * synapse_fc_out_features\n",
    "    else:\n",
    "        past_in_channel = synapse_conv_in_channels*img_size*img_size\n",
    "        for in_channel in cfg:\n",
    "            if (type(in_channel) == list):\n",
    "                for residual_in_channel in in_channel:\n",
    "                    params_num += (past_in_channel + bias_param) * residual_in_channel\n",
    "                    past_in_channel = residual_in_channel\n",
    "            elif (in_channel == 'P' or in_channel == 'M'):\n",
    "                img_size = img_size // 2\n",
    "                past_in_channel = synapse_conv_in_channels*img_size*img_size\n",
    "            else:\n",
    "                params_num += (past_in_channel + bias_param) * in_channel\n",
    "                past_in_channel = in_channel\n",
    "        params_num += (past_in_channel + bias_param) * synapse_fc_out_features\n",
    "    ###########################################################################################################################################\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        if (single_step == False):\n",
    "            net = MY_SNN_FC(cfg, synapse_conv_in_channels, IMAGE_SIZE, synapse_fc_out_features,\n",
    "                        synapse_fc_trace_const1, synapse_fc_trace_const2, \n",
    "                        lif_layer_v_init, lif_layer_v_decay, \n",
    "                        lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                        lif_layer_sg_width,\n",
    "                        tdBN_on,\n",
    "                        BN_on, TIME,\n",
    "                        surrogate,\n",
    "                        BPTT_on,\n",
    "                        DFA_on,\n",
    "                        drop_rate).to(device)\n",
    "        else:\n",
    "            net = MY_SNN_FC_sstep(cfg, synapse_conv_in_channels, IMAGE_SIZE, synapse_fc_out_features,\n",
    "                        synapse_fc_trace_const1, synapse_fc_trace_const2, \n",
    "                        lif_layer_v_init, lif_layer_v_decay, \n",
    "                        lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                        lif_layer_sg_width,\n",
    "                        tdBN_on,\n",
    "                        BN_on, TIME,\n",
    "                        surrogate,\n",
    "                        BPTT_on,\n",
    "                        DFA_on,\n",
    "                        OTTT_sWS_on,\n",
    "                        drop_rate).to(device)\n",
    "    else:\n",
    "        if (single_step == False):\n",
    "            net = MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE,\n",
    "                        synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                        synapse_conv_padding, synapse_conv_trace_const1, \n",
    "                        synapse_conv_trace_const2, \n",
    "                        lif_layer_v_init, lif_layer_v_decay, \n",
    "                        lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                        lif_layer_sg_width,\n",
    "                        synapse_fc_out_features, synapse_fc_trace_const1, synapse_fc_trace_const2,\n",
    "                        tdBN_on,\n",
    "                        BN_on, TIME,\n",
    "                        surrogate,\n",
    "                        BPTT_on,\n",
    "                        OTTT_sWS_on,\n",
    "                        DFA_on,\n",
    "                        drop_rate).to(device)\n",
    "        else:\n",
    "            net = MY_SNN_CONV_sstep(cfg, synapse_conv_in_channels, IMAGE_SIZE,\n",
    "                        synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                        synapse_conv_padding, synapse_conv_trace_const1, \n",
    "                        synapse_conv_trace_const2, \n",
    "                        lif_layer_v_init, lif_layer_v_decay, \n",
    "                        lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                        lif_layer_sg_width,\n",
    "                        synapse_fc_out_features, synapse_fc_trace_const1, synapse_fc_trace_const2,\n",
    "                        tdBN_on,\n",
    "                        BN_on, TIME,\n",
    "                        surrogate,\n",
    "                        BPTT_on,\n",
    "                        OTTT_sWS_on,\n",
    "                        DFA_on,\n",
    "                        drop_rate).to(device)\n",
    "    if (nda_net == True):\n",
    "        net = VGG(cfg = cfg, num_classes=10, batch_norm = tdBN_on, in_c = synapse_conv_in_channels, \n",
    "                    lif_layer_v_threshold=lif_layer_v_threshold, lif_layer_v_decay=lif_layer_v_decay, lif_layer_sg_width=lif_layer_sg_width)\n",
    "        net.T = TIME\n",
    "    if ddp_on == False:\n",
    "        net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        net.load_state_dict(torch.load(pre_trained_path))\n",
    "    \n",
    "    if ddp_on == True:\n",
    "        device = args.gpu\n",
    "        net = net.to(args.gpu)\n",
    "        net = DDP(net, delay_allreduce=True)\n",
    "\n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        if ddp_on == False or torch.distributed.get_rank() == 0:\n",
    "            print(net)    \n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    if ddp_on == False or torch.distributed.get_rank() == 0:\n",
    "        wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter logging해줌\n",
    "    ############################################################\n",
    "\n",
    "    ## param num and memory estimation except BN with MY own calculation some lines above ##########################################\n",
    "    if ddp_on == False or torch.distributed.get_rank() == 0:\n",
    "        real_param_num = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "        if (weight_count_print == True):\n",
    "            for name, param in net.named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    print(f'Layer: {name} | Number of parameters: {param.numel()}')\n",
    "        # Batch norm 있으면 아래 두 개 서로 다를 수 있음.\n",
    "        # assert real_param_num == params_num, f'parameter number is not same. real_param_num: {real_param_num}, params_num: {params_num}'    \n",
    "        print('='*50)\n",
    "        print(f\"My Num of PARAMS: {params_num:,}, system's param_num : {real_param_num:,}\")\n",
    "        memory = params_num / 8 / 1024 / 1024 # MB\n",
    "        precision = 32\n",
    "        memory = memory * precision \n",
    "        print(f\"Memory: {memory:.2f}MiB at {precision}-bit\")\n",
    "        print('='*50)\n",
    "    ##############################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## criterion ########################################## # loss 구해주는 친구\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    if (OTTT_sWS_on == True):\n",
    "        # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "        if which_data == 'DVS_GESTURE':\n",
    "            criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    if(optimizer_what == 'SGD'):\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss= 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    elapsed_time_val = 0\n",
    "    no_val_best_growth_count = 0\n",
    "    no_tr_best_growth_count = 0\n",
    "    iter_acc_array = np.array([])\n",
    "    tr_acc_array = np.array([])\n",
    "    val_acc_now_array = np.array([])\n",
    "    DFA_current = DFA_on\n",
    "    DFA_toggle = False\n",
    "    DFA_flag = 1.0 if DFA_current == True else 0.0\n",
    "    DFA_BP_toggle_trial = 0\n",
    "    iter_of_val = False\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        if (e_transport_swap > 0 or e_transport_swap_tr > 0):\n",
    "            assert not (e_transport_swap > 0 and e_transport_swap_tr > 0)\n",
    "            if e_transport_swap > 0 and no_val_best_growth_count == e_transport_swap:\n",
    "                if DFA_BP_toggle_trial < e_transport_swap_coin:\n",
    "                    net = BP_DFA_SWAP(net, convTrue_fcFalse, single_step, ddp_on, args_gpu)\n",
    "                    no_val_best_growth_count = 0\n",
    "                    DFA_current = not DFA_current\n",
    "                    DFA_toggle = True\n",
    "                    DFA_BP_toggle_trial = DFA_BP_toggle_trial + 1\n",
    "            if e_transport_swap_tr > 0 and no_tr_best_growth_count == e_transport_swap_tr:\n",
    "                if DFA_BP_toggle_trial < e_transport_swap_coin:\n",
    "                    net = BP_DFA_SWAP(net, convTrue_fcFalse, single_step, ddp_on, args_gpu)\n",
    "                    no_tr_best_growth_count = 0\n",
    "                    DFA_current = not DFA_current\n",
    "                    DFA_toggle = True\n",
    "                    DFA_BP_toggle_trial = DFA_BP_toggle_trial + 1\n",
    "\n",
    "        if ddp_on == False or torch.distributed.get_rank() == 0:\n",
    "            # print('EPOCH', epoch)\n",
    "            pass\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # if (domain_il_epoch>0 and which_data == 'PMNIST'):\n",
    "        #     k = epoch // domain_il_epoch\n",
    "        #     xtrain=data[k]['train']['x']\n",
    "        #     ytrain=data[k]['train']['y']\n",
    "        #     xtest =data[k]['test']['x']\n",
    "        #     ytest =data[k]['test']['y']\n",
    "\n",
    "        \n",
    "        ####### iterator : input_loading & tqdm을 통한 progress_bar 생성###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        if ddp_on == False or torch.distributed.get_rank() == 0:  \n",
    "            iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "        \n",
    "        #### validation_interval이 batch size보다 작을 시 validation_interval을 batch size로 맞춰줌#############\n",
    "        validation_interval2 = validation_interval\n",
    "        if (validation_interval > len(train_loader)):\n",
    "            validation_interval2 = len(train_loader)\n",
    "        ##################################################################################################\n",
    "\n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            iter_one_train_time_start = time.time()\n",
    "            net.train() # train 모드로 바꿔줘야함\n",
    "\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "                # print('x_len',x_len)\n",
    "                # mask = padded_sequence_mask(x_len)\n",
    "                # max_time_step = x_len.max()\n",
    "                # min_time_step = x_len.min()\n",
    "            ## batch 크기 ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'n_tidigits'):\n",
    "                inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "                labels = labels[:, 0, :]\n",
    "                labels = torch.argmax(labels, dim=1)\n",
    "            elif (which_data == 'heidelberg'):\n",
    "                inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "                print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "            # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "            # print(labels)\n",
    "                \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            \n",
    "            # # dvs 데이터 시각화 코드 (확인 필요할 시 써라)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "\n",
    "            ## gradient 초기화 #######################################\n",
    "            optimizer.zero_grad()\n",
    "            ###########################################################\n",
    "            \n",
    "            ## DVS gesture에서 other label자리 매꾸기 ###############\n",
    "            if (which_data == 'DVS_GESTURE'):\n",
    "                labels[labels>2] -= 1\n",
    "            #######################################################         \n",
    "                               \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first input도 ottt trace 적용하기 위한 코드 (validation 시에는 필요X) ##########################\n",
    "                if OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_conv_trace_const2 + spike[t]*synapse_conv_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                ############################################################\n",
    "                ## weight 업데이트!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "                optimizer.step() # full step time update\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # ottt꺼 쓸때\n",
    "                labels = labels[0]\n",
    "                loss /= TIME\n",
    "            tr_epoch_loss_temp += loss.data/len(train_loader)\n",
    "\n",
    "            ## net 그림 출력해보기 #################################################################\n",
    "            # print('시각화')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch 어긋남 방지 ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            if i % verbose_interval == verbose_interval-1:\n",
    "                if ddp_on == False or torch.distributed.get_rank() == 0:\n",
    "                    print(f'{epoch}-{i} training acc: {100 * iter_acc:.2f}%, lr={[f\"{lr}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}, val_acc: {100 * val_acc_now:.2f}%')\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            iter_one_train_time_end = time.time()\n",
    "            elapsed_time = iter_one_train_time_end - iter_one_train_time_start  # 실행 시간 계산\n",
    "\n",
    "            if (i % verbose_interval == verbose_interval-1):\n",
    "                if ddp_on == False or torch.distributed.get_rank() == 0:\n",
    "                    print(f\"iter_one_train_time: {elapsed_time} seconds, last one_val_time: {elapsed_time_val} seconds\\n\")\n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i % validation_interval2 == validation_interval2-1:\n",
    "                iter_one_val_time_start = time.time()\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "                val_loss = 0\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval 모드로 바꿔줘야함 \n",
    "                    for data in test_loader:\n",
    "                        ## data loading & semi-pre-processing ##########################################################\n",
    "                        if len(data) == 2:\n",
    "                            inputs, labels = data\n",
    "                            # 처리 로직 작성\n",
    "                        elif len(data) == 3:\n",
    "                            inputs, labels, x_len = data\n",
    "                            # print('x_len',x_len)\n",
    "                            # mask = padded_sequence_mask(x_len)\n",
    "                            # max_time_step = x_len.max()\n",
    "                            # min_time_step = x_len.min()\n",
    "                            # B, T, *spatial_dims = inputs.shape\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        inputs = inputs.to(device)\n",
    "                        labels = labels.to(device)\n",
    "                        real_batch = labels.size(0)\n",
    "                        \n",
    "                        ## DVS gesture에서 other label자리 매꾸기 ###############\n",
    "                        if (which_data == 'DVS_GESTURE'):\n",
    "                            labels[labels>2] -= 1\n",
    "                        #######################################################\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs = inputs[:,:,0,:,:]\n",
    "\n",
    "                        ## network 연산 시작 ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs.permute(1, 0, 2, 3, 4)) #inputs: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs[t])\n",
    "                                val_loss_temp = criterion(outputs, labels)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct += (predicted == labels).sum().item()\n",
    "\n",
    "                    val_acc_now = correct / total\n",
    "                    # print(f'{epoch}-{i} validation acc: {100 * val_acc_now:.2f}%, lr={[f\"{lr:.10f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}')\n",
    "\n",
    "                iter_one_val_time_end = time.time()\n",
    "                elapsed_time_val = iter_one_val_time_end - iter_one_val_time_start  # 실행 시간 계산\n",
    "                # print(f\"iter_one_val_time: {elapsed_time_val} seconds\")\n",
    "\n",
    "                # network save\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    if ddp_on == False or torch.distributed.get_rank() == 0:\n",
    "                        # wandb 키면 state_dict아닌거는 저장 안됨\n",
    "                        torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "                        # torch.save(net, f\"net_save/save_now_net_{unique_name}.pth\")\n",
    "                        # torch.save(net.module.state_dict(), f\"net_save/save_now_net_weights2_{unique_name}.pth\")\n",
    "                        # torch.save(net.module, f\"net_save/save_now_net2_{unique_name}.pth\")\n",
    "                    no_val_best_growth_count = 0\n",
    "                else:\n",
    "                    no_val_best_growth_count = no_val_best_growth_count + 1\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "                    no_tr_best_growth_count = 0\n",
    "                else:\n",
    "                    no_tr_best_growth_count = no_tr_best_growth_count + 1\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "                if DFA_toggle == True:\n",
    "                    DFA_flag = 1.0 - DFA_flag\n",
    "                    DFA_toggle = False\n",
    "\n",
    "                iter_of_val = True\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            if ddp_on == False or torch.distributed.get_rank() == 0:\n",
    "                if iter_of_val == False:\n",
    "                    iterator.set_description(f\"{iter_acc_string}, iter_loss:{loss:10.6f}, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                else:\n",
    "                    iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                    iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if ddp_on == False or torch.distributed.get_rank() == 0:\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"DFA_flag\": DFA_flag}) # DFA mode 바뀌자 마자 바뀌는 게 아니고 validation 한번 했을 때 바뀜.\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            \n",
    "            ## accuray 로컬에 저장 하기 위한 코드 #####################################################################################\n",
    "            iter_acc_array = np.append(iter_acc_array, iter_acc)\n",
    "            tr_acc_array = np.append(tr_acc_array, tr_acc)\n",
    "            val_acc_now_array = np.append(val_acc_now_array, val_acc_now)\n",
    "            base_name = f'{current_time}'\n",
    "            ####################################################################################################################\n",
    "            \n",
    "            iter_acc_file_name_time = f'result_save/{base_name}_iter_acc_array_{unique_name}.npy'\n",
    "            tr_acc_file_name_time = f'result_save/{base_name}_tr_acc_array_{unique_name}.npy'\n",
    "            val_acc_file_name_time = f'result_save/{base_name}_val_acc_now_array_{unique_name}.npy'\n",
    "            hyperparameters_file_name_time = f'result_save/{base_name}_hyperparameters_{unique_name}.json'\n",
    "\n",
    "            hyperparameters['current epoch'] = epoch\n",
    "\n",
    "            ### accuracy 세이브: 덮어쓰기 하기 싫으면 주석 풀어서 사용 (시간마다 새로 쓰기) 비추천 ########################\n",
    "            # if ddp_on == False or torch.distributed.get_rank() == 0:\n",
    "            #     np.save(iter_acc_file_name_time, iter_acc_array)\n",
    "            #     np.save(tr_acc_file_name_time, iter_acc_array)\n",
    "            #     np.save(val_acc_file_name_time, val_acc_now_array)\n",
    "            #     with open(hyperparameters_file_name_time, 'w') as f:\n",
    "            #         json.dump(hyperparameters, f, indent=4)\n",
    "            #########################################################################################################\n",
    "\n",
    "            ## accuracy 세이브 ###########################################################################################\n",
    "            if ddp_on == False or torch.distributed.get_rank() == 0:\n",
    "                np.save(f'result_save/iter_acc_array_{unique_name}.npy', iter_acc_array)\n",
    "                np.save(f'result_save/tr_acc_array_{unique_name}.npy', tr_acc_array)\n",
    "                np.save(f'result_save/val_acc_now_array_{unique_name}.npy', val_acc_now_array)\n",
    "                with open(f'result_save/hyperparameters_{unique_name}.json', 'w') as f:\n",
    "                    json.dump(hyperparameters, f, indent=4)\n",
    "            ##########################################################################################################\n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "                \n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "        # 실행 시간 계산\n",
    "        epoch_time_end = time.time()\n",
    "        # print(f\"epoch_time: {epoch_time_end - epoch_start_time} seconds\\n\") \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.25 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "# const2 = False # trace 할거면 True, 안할거면 False\n",
    "\n",
    "# unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "# run_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "\n",
    "# if const2 == True:\n",
    "#     const2 = decay\n",
    "# else:\n",
    "#     const2 = 0.0\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=True)\n",
    "\n",
    "# my_snn_system(  devices = \"2\",\n",
    "#                 single_step = True, # True # False\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 42,\n",
    "#                 TIME = 10 , # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "#                 BATCH = 16, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 128, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "#                 #pmnist는 28로 해야 됨. 나머지는 바꿔도 돌아는 감.\n",
    "\n",
    "#                 # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 1.3102821334243646,  # 10000이상으로 하면 NDA LIF 씀. #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 0, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "#                 lif_layer_sg_width = 2.570969004857107, # sigmoid류에서는 alpha값 4.0, rectangle류에서는 width값 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "#                 synapse_conv_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "#                 synapse_conv_trace_const2 = const2, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "#                 synapse_fc_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "#                 synapse_fc_trace_const2 = const2, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "#                 # cfg = [64, 64],\n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 cfg = ['M','M',200,200],\n",
    "#                 # cfg = ['M','M',200,200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # True로 하길 추천\n",
    "#                 weight_count_print = False, # True # False\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 learning_rate = 0.01, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 60,\n",
    "#                 verbose_interval = 999999999, #이거 걍 건들지마셈 #숫자 크게 하면 꺼짐 #걍 중간중간 iter에서 끊어서 출력\n",
    "#                 validation_interval =  999999999,#999999999, #이거 걍 건들지마셈 #숫자 크게 하면 에포크 마지막 iter 때 val 함\n",
    "\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 gradient_verbose = False,  # True # False  # weight gradient 각 layer마다 띄워줌\n",
    "\n",
    "#                 BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'CosineAnnealingLR', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False,   # True # False \n",
    "#                 # 지원 DATASET: cifar10, mnist\n",
    "\n",
    "#                 nda_net = False,   # True # False\n",
    "\n",
    "#                 domain_il_epoch = 0, # over 0, then domain il mode on # pmnist 쓸거면 HLOP 코드보고 더 디벨롭하셈. 지금 개발 hold함.\n",
    "                \n",
    "#                 dvs_clipping = 2, # 숫자만큼 크면 spike 아니면 걍 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "\n",
    "#                 dvs_duration = 100_000, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # 있는 데이터들 #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # 한 숫자가 1us인듯 (spikingjelly코드에서)\n",
    "#                 # 한 장에 50 timestep만 생산함. 싫으면 my_snn/trying/spikingjelly_dvsgesture의__init__.py 를 참고해봐\n",
    "\n",
    "#                 OTTT_sWS_on = False, # True # False # BPTT끄고, CONV에만 적용됨.\n",
    "\n",
    "#                 DFA_on = False, # True # False # residual은 dfa지원안함.\n",
    "#                 OTTT_input_trace_on = False, # True # False # 맨 처음 input에 trace 적용\n",
    "                 \n",
    "#                 e_transport_swap = 0, # 1 이상이면 해당 숫자 에포크만큼 val_acc_best가 변화가 없으면 e_transport scheme (BP vs DFA) swap\n",
    "#                 e_transport_swap_tr = 0, # 1 이상이면 해당 숫자 에포크만큼 tr_acc_best가 변화가 없으면 e_transport scheme (BP vs DFA) swap\n",
    "#                 e_transport_swap_coin = 1, # swap할 수 있는 coin 개수\n",
    "\n",
    "#                 drop_rate = 0, # drop_rate만큼 0으로 만듦. ex) 0.2면 activation의 20%를 0으로 만듦.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "#                 merge_polarities = False, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "#                 ) \n",
    "# # sigmoid와 BN이 있어야 잘된다.\n",
    "# # average pooling  \n",
    "# # 이 낫다. \n",
    " \n",
    "# # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "\n",
    "# # DDP 실행 코드\n",
    "# '''\n",
    "# ddp_on 키고, gpu 개수 만큼 batch size 나눠줘\n",
    "# CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 python -m torch.distributed.launch --nproc_per_node=6 main_ddp.py\n",
    "# CUDA_VISIBLE_DEVICES=1,2,3 python -m torch.distributed.launch --nproc_per_node=3 main_ddp.py\n",
    "# CUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --nproc_per_node=4 main_ddp.py\n",
    "# '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (NMNIST) ########################\n",
    "# decay = 0.25 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "# const2 = False # trace 할거면 True, 안할거면 False\n",
    "\n",
    "# unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "# run_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "\n",
    "# if const2 == True:\n",
    "#     const2 = decay\n",
    "# else:\n",
    "#     const2 = 0.0\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=True)\n",
    "\n",
    "# my_snn_system(  devices = \"4\",\n",
    "#                 single_step = True, # True # False\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 42,\n",
    "#                 TIME = 10 , # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "#                 BATCH = 128, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 34, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "#                 #pmnist는 28로 해야 됨. 나머지는 바꿔도 돌아는 감.\n",
    "\n",
    "#                 # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "#                 which_data = 'NMNIST_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 1.0,  # 10000이상으로 하면 NDA LIF 씀. #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 0, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "#                 lif_layer_sg_width = 0.5, # # surrogate sigmoid 쓸 때는 의미없음\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "#                 synapse_conv_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "#                 synapse_conv_trace_const2 = const2, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "#                 synapse_fc_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "#                 synapse_fc_trace_const2 = const2, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "#                 # cfg = [64, 64],\n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # True로 하길 추천\n",
    "#                 weight_count_print = False, # True # False\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 learning_rate = 0.009, # 0.001, # default 0.001  # ottt 0.1 # nda 0.001 \n",
    "#                 epoch_num = 300,\n",
    "#                 verbose_interval = 999999999, #이거 걍 건들지마셈 #숫자 크게 하면 꺼짐 #걍 중간중간 iter에서 끊어서 출력\n",
    "#                 validation_interval =  999999999,#999999999, #이거 걍 건들지마셈 #숫자 크게 하면 에포크 마지막 iter 때 val 함\n",
    "\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'rectangle' 'sigmoid' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 gradient_verbose = False,  # True # False  # weight gradient 각 layer마다 띄워줌\n",
    "\n",
    "#                 BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'CosineAnnealingLR', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False,   # True # False \n",
    "#                 # 지원 DATASET: cifar10, mnist\n",
    "\n",
    "#                 nda_net = False,   # True # False\n",
    "\n",
    "#                 domain_il_epoch = 0, # over 0, then domain il mode on # pmnist 쓸거면 HLOP 코드보고 더 디벨롭하셈. 지금 개발 hold함.\n",
    "                \n",
    "#                 dvs_clipping = 1, # 숫자만큼 크면 spike 아니면 걍 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "\n",
    "#                 dvs_duration = 10_000, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # 있는 데이터들 #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # 한 숫자가 1us인듯 (spikingjelly코드에서)\n",
    "#                 # 한 장에 50 timestep만 생산함. 싫으면 my_snn/trying/spikingjelly_dvsgesture의__init__.py 를 참고해봐\n",
    "\n",
    "#                 OTTT_sWS_on = False, # True # False # BPTT끄고, CONV에만 적용됨.\n",
    "\n",
    "#                 DFA_on = True, # True # False # residual은 dfa지원안함.\n",
    "#                 OTTT_input_trace_on = False, # True # False # 맨 처음 input에 trace 적용\n",
    "                 \n",
    "#                 e_transport_swap = 5, # 1 이상이면 해당 숫자 에포크만큼 val_acc_best가 변화가 없으면 e_transport scheme (BP vs DFA) swap\n",
    "#                 e_transport_swap_tr = 0, # 1 이상이면 해당 숫자 에포크만큼 tr_acc_best가 변화가 없으면 e_transport scheme (BP vs DFA) swap\n",
    "#                 e_transport_swap_coin = 1, # swap할 수 있는 coin 개수\n",
    "                \n",
    "#                 drop_rate = 0.0, # drop_rate만큼 0으로 만듦. ex) 0.2면 activation의 20%를 0으로 만듦.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "#                 merge_polarities = False, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "#                 ) \n",
    "# # sigmoid와 BN이 있어야 잘된다.\n",
    "# # average pooling  \n",
    "# # 이 낫다. \n",
    " \n",
    "# # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "\n",
    "# # DDP 실행 코드\n",
    "# '''\n",
    "# ddp_on 키고, gpu 개수 만큼 batch size 나눠줘\n",
    "# CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 python -m torch.distributed.launch --nproc_per_node=6 main_ddp.py\n",
    "# CUDA_VISIBLE_DEVICES=1,2,3 python -m torch.distributed.launch --nproc_per_node=3 main_ddp.py\n",
    "# CUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --nproc_per_node=4 main_ddp.py\n",
    "# '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 91td8vzi\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yvmpitqi with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_sWS_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconst2: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdrop_rate: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap_coin: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap_tr: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.003908385758528434\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5.608595312389549\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.38706861803167913\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: CosineAnnealingLR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240827_011333-yvmpitqi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yvmpitqi' target=\"_blank\">cerulean-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yvmpitqi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yvmpitqi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_sWS_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap_tr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap_coin' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'drop_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_hash = 2bbd58b4e0d3c1e9ad501fad8a43feed\n",
      "cache path exists\n",
      "\n",
      "we will exclude the 'other' class. dvsgestrue 10 classes' indices exist. \n",
      "\n",
      "DataParallel(\n",
      "  (module): MY_SNN_FC_sstep(\n",
      "    (layers): MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC_sstep()\n",
      "      (3): SYNAPSE_FC_trace_sstep()\n",
      "      (4): LIF_layer_trace_sstep()\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC_trace_sstep()\n",
      "      (7): LIF_layer_trace_sstep()\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC_trace_sstep()\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "==================================================\n",
      "My Num of PARAMS: 452,010, system's param_num : 452,010\n",
      "Memory: 1.72MiB at 32-bit\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch-0   lr=['0.0039084'], tr/val_loss:  1.685219/  1.367345, tr:  42.39%, val:  46.67%, val_best:  46.67%: 100%|██████████| 62/62 [00:06<00:00,  9.42it/s]\n",
      "epoch-1   lr=['0.0039074'], tr/val_loss:  1.251507/  1.400045, tr:  56.08%, val:  50.42%, val_best:  50.42%: 100%|██████████| 62/62 [00:07<00:00,  8.83it/s]\n",
      "epoch-2   lr=['0.0039045'], tr/val_loss:  1.000695/  1.391599, tr:  65.27%, val:  55.00%, val_best:  55.00%: 100%|██████████| 62/62 [00:09<00:00,  6.86it/s]\n",
      "epoch-3   lr=['0.0038997'], tr/val_loss:  0.925174/  1.142973, tr:  69.97%, val:  64.17%, val_best:  64.17%: 100%|██████████| 62/62 [00:25<00:00,  2.46it/s]\n",
      "epoch-4   lr=['0.0038930'], tr/val_loss:  0.841484/  1.201681, tr:  69.46%, val:  59.58%, val_best:  64.17%: 100%|██████████| 62/62 [00:29<00:00,  2.07it/s]\n",
      "epoch-5   lr=['0.0038843'], tr/val_loss:  0.748438/  1.224114, tr:  74.67%, val:  65.83%, val_best:  65.83%: 100%|██████████| 62/62 [01:09<00:00,  1.11s/it]\n",
      "epoch-6   lr=['0.0038738'], tr/val_loss:  0.777904/  1.502620, tr:  76.20%, val:  59.58%, val_best:  65.83%: 100%|██████████| 62/62 [01:08<00:00,  1.10s/it]\n",
      "epoch-7   lr=['0.0038613'], tr/val_loss:  0.732607/  1.644130, tr:  74.87%, val:  55.83%, val_best:  65.83%: 100%|██████████| 62/62 [01:00<00:00,  1.03it/s]\n",
      "epoch-8   lr=['0.0038470'], tr/val_loss:  0.745237/  1.101438, tr:  75.49%, val:  66.67%, val_best:  66.67%: 100%|██████████| 62/62 [01:06<00:00,  1.07s/it]\n",
      "epoch-9   lr=['0.0038308'], tr/val_loss:  0.508388/  1.256627, tr:  83.25%, val:  69.17%, val_best:  69.17%: 100%|██████████| 62/62 [01:04<00:00,  1.05s/it]\n",
      "epoch-10  lr=['0.0038127'], tr/val_loss:  0.387579/  1.176994, tr:  91.42%, val:  69.58%, val_best:  69.58%: 100%|██████████| 62/62 [01:09<00:00,  1.13s/it]\n",
      "epoch-11  lr=['0.0037929'], tr/val_loss:  0.338739/  1.294418, tr:  92.44%, val:  72.92%, val_best:  72.92%: 100%|██████████| 62/62 [01:12<00:00,  1.17s/it]\n",
      "epoch-12  lr=['0.0037712'], tr/val_loss:  0.265903/  1.197421, tr:  96.83%, val:  78.75%, val_best:  78.75%: 100%|██████████| 62/62 [01:15<00:00,  1.21s/it]\n",
      "epoch-13  lr=['0.0037477'], tr/val_loss:  0.236808/  1.311919, tr:  96.32%, val:  74.17%, val_best:  78.75%: 100%|██████████| 62/62 [01:08<00:00,  1.11s/it]\n",
      "epoch-14  lr=['0.0037224'], tr/val_loss:  0.243444/  1.165117, tr:  94.69%, val:  77.08%, val_best:  78.75%: 100%|██████████| 62/62 [01:15<00:00,  1.22s/it]\n",
      "epoch-15  lr=['0.0036954'], tr/val_loss:  0.152184/  1.214355, tr:  98.47%, val:  82.08%, val_best:  82.08%: 100%|██████████| 62/62 [01:17<00:00,  1.26s/it]\n",
      "epoch-16  lr=['0.0036667'], tr/val_loss:  0.127246/  1.348747, tr:  99.18%, val:  75.00%, val_best:  82.08%: 100%|██████████| 62/62 [01:11<00:00,  1.15s/it]\n",
      "epoch-17  lr=['0.0036362'], tr/val_loss:  0.111079/  1.199686, tr:  99.59%, val:  80.42%, val_best:  82.08%: 100%|██████████| 62/62 [01:16<00:00,  1.24s/it]\n",
      "epoch-18  lr=['0.0036042'], tr/val_loss:  0.096097/  1.283083, tr:  99.59%, val:  82.08%, val_best:  82.08%: 100%|██████████| 62/62 [01:10<00:00,  1.13s/it]\n",
      "epoch-19  lr=['0.0035705'], tr/val_loss:  0.045119/  1.325698, tr:  99.80%, val:  79.17%, val_best:  82.08%: 100%|██████████| 62/62 [01:08<00:00,  1.10s/it]\n",
      "epoch-20  lr=['0.0035352'], tr/val_loss:  0.028558/  1.342154, tr: 100.00%, val:  78.75%, val_best:  82.08%: 100%|██████████| 62/62 [01:08<00:00,  1.10s/it]\n",
      "epoch-21  lr=['0.0034983'], tr/val_loss:  0.014498/  1.368416, tr: 100.00%, val:  80.42%, val_best:  82.08%: 100%|██████████| 62/62 [00:58<00:00,  1.07it/s]\n",
      "epoch-22  lr=['0.0034599'], tr/val_loss:  0.009627/  1.389881, tr: 100.00%, val:  80.83%, val_best:  82.08%: 100%|██████████| 62/62 [01:08<00:00,  1.10s/it]\n",
      "epoch-23  lr=['0.0034201'], tr/val_loss:  0.006253/  1.395389, tr: 100.00%, val:  82.50%, val_best:  82.50%: 100%|██████████| 62/62 [01:10<00:00,  1.14s/it]\n",
      "epoch-24  lr=['0.0033787'], tr/val_loss:  0.004529/  1.401082, tr: 100.00%, val:  81.25%, val_best:  82.50%: 100%|██████████| 62/62 [01:14<00:00,  1.20s/it]\n",
      "epoch-25  lr=['0.0033360'], tr/val_loss:  0.003767/  1.416918, tr: 100.00%, val:  81.67%, val_best:  82.50%: 100%|██████████| 62/62 [01:18<00:00,  1.27s/it]\n",
      "epoch-26  lr=['0.0032919'], tr/val_loss:  0.003114/  1.439556, tr: 100.00%, val:  81.67%, val_best:  82.50%: 100%|██████████| 62/62 [01:16<00:00,  1.23s/it]\n",
      "epoch-27  lr=['0.0032465'], tr/val_loss:  0.002872/  1.436706, tr: 100.00%, val:  81.67%, val_best:  82.50%: 100%|██████████| 62/62 [01:18<00:00,  1.26s/it]\n",
      "epoch-28  lr=['0.0031998'], tr/val_loss:  0.002508/  1.461543, tr: 100.00%, val:  82.08%, val_best:  82.50%: 100%|██████████| 62/62 [01:18<00:00,  1.26s/it]\n",
      "epoch-29  lr=['0.0031519'], tr/val_loss:  0.002406/  1.454757, tr: 100.00%, val:  81.25%, val_best:  82.50%: 100%|██████████| 62/62 [01:22<00:00,  1.33s/it]\n",
      "epoch-30  lr=['0.0031028'], tr/val_loss:  0.002228/  1.458212, tr: 100.00%, val:  82.08%, val_best:  82.50%: 100%|██████████| 62/62 [01:01<00:00,  1.01it/s]\n",
      "epoch-31  lr=['0.0030526'], tr/val_loss:  0.002093/  1.458623, tr: 100.00%, val:  81.67%, val_best:  82.50%: 100%|██████████| 62/62 [00:57<00:00,  1.07it/s]\n",
      "epoch-32  lr=['0.0030013'], tr/val_loss:  0.001965/  1.465218, tr: 100.00%, val:  81.67%, val_best:  82.50%: 100%|██████████| 62/62 [00:57<00:00,  1.07it/s]\n",
      "epoch-33  lr=['0.0029490'], tr/val_loss:  0.001884/  1.461065, tr: 100.00%, val:  82.08%, val_best:  82.50%: 100%|██████████| 62/62 [00:56<00:00,  1.09it/s]\n",
      "epoch-34  lr=['0.0028956'], tr/val_loss:  0.001767/  1.463440, tr: 100.00%, val:  82.08%, val_best:  82.50%: 100%|██████████| 62/62 [00:56<00:00,  1.10it/s]\n",
      "epoch-35  lr=['0.0028414'], tr/val_loss:  0.001751/  1.463395, tr: 100.00%, val:  82.08%, val_best:  82.50%: 100%|██████████| 62/62 [00:56<00:00,  1.10it/s]\n",
      "epoch-36  lr=['0.0027862'], tr/val_loss:  0.001622/  1.473213, tr: 100.00%, val:  82.08%, val_best:  82.50%: 100%|██████████| 62/62 [00:51<00:00,  1.21it/s]\n",
      "epoch-37  lr=['0.0027303'], tr/val_loss:  0.001531/  1.469235, tr: 100.00%, val:  82.08%, val_best:  82.50%: 100%|██████████| 62/62 [00:59<00:00,  1.05it/s]\n",
      "epoch-38  lr=['0.0026736'], tr/val_loss:  0.001490/  1.482562, tr: 100.00%, val:  82.08%, val_best:  82.50%: 100%|██████████| 62/62 [00:56<00:00,  1.09it/s]\n",
      "epoch-39  lr=['0.0026162'], tr/val_loss:  0.001462/  1.483988, tr: 100.00%, val:  82.08%, val_best:  82.50%: 100%|██████████| 62/62 [00:59<00:00,  1.05it/s]\n",
      "epoch-40  lr=['0.0025581'], tr/val_loss:  0.001412/  1.480212, tr: 100.00%, val:  82.08%, val_best:  82.50%: 100%|██████████| 62/62 [00:55<00:00,  1.11it/s]\n",
      "epoch-41  lr=['0.0024994'], tr/val_loss:  0.001402/  1.485742, tr: 100.00%, val:  81.25%, val_best:  82.50%: 100%|██████████| 62/62 [00:52<00:00,  1.18it/s]\n",
      "epoch-42  lr=['0.0024402'], tr/val_loss:  0.001314/  1.486427, tr: 100.00%, val:  81.25%, val_best:  82.50%: 100%|██████████| 62/62 [00:57<00:00,  1.08it/s]\n",
      "epoch-43  lr=['0.0023805'], tr/val_loss:  0.001309/  1.495459, tr: 100.00%, val:  81.67%, val_best:  82.50%: 100%|██████████| 62/62 [00:55<00:00,  1.12it/s]\n",
      "epoch-44  lr=['0.0023204'], tr/val_loss:  0.001262/  1.501040, tr: 100.00%, val:  81.67%, val_best:  82.50%: 100%|██████████| 62/62 [00:57<00:00,  1.07it/s]\n",
      "epoch-45  lr=['0.0022599'], tr/val_loss:  0.001225/  1.510486, tr: 100.00%, val:  81.25%, val_best:  82.50%: 100%|██████████| 62/62 [00:55<00:00,  1.11it/s]\n",
      "epoch-46  lr=['0.0021991'], tr/val_loss:  0.001198/  1.507867, tr: 100.00%, val:  81.25%, val_best:  82.50%: 100%|██████████| 62/62 [00:53<00:00,  1.15it/s]\n",
      "epoch-47  lr=['0.0021381'], tr/val_loss:  0.001155/  1.509635, tr: 100.00%, val:  81.67%, val_best:  82.50%: 100%|██████████| 62/62 [00:58<00:00,  1.05it/s]\n",
      "epoch-48  lr=['0.0020769'], tr/val_loss:  0.001141/  1.507501, tr: 100.00%, val:  81.67%, val_best:  82.50%: 100%|██████████| 62/62 [00:57<00:00,  1.09it/s]\n",
      "epoch-49  lr=['0.0020156'], tr/val_loss:  0.001112/  1.509035, tr: 100.00%, val:  81.67%, val_best:  82.50%: 100%|██████████| 62/62 [00:57<00:00,  1.09it/s]\n",
      "epoch-50  lr=['0.0019542'], tr/val_loss:  0.001100/  1.506472, tr: 100.00%, val:  82.08%, val_best:  82.50%: 100%|██████████| 62/62 [00:56<00:00,  1.09it/s]\n",
      "epoch-51  lr=['0.0018928'], tr/val_loss:  0.001075/  1.513173, tr: 100.00%, val:  82.08%, val_best:  82.50%: 100%|██████████| 62/62 [00:55<00:00,  1.11it/s]\n",
      "epoch-52  lr=['0.0018315'], tr/val_loss:  0.001053/  1.510749, tr: 100.00%, val:  82.08%, val_best:  82.50%: 100%|██████████| 62/62 [00:55<00:00,  1.11it/s]\n",
      "epoch-53  lr=['0.0017703'], tr/val_loss:  0.001034/  1.510652, tr: 100.00%, val:  82.50%, val_best:  82.50%: 100%|██████████| 62/62 [00:55<00:00,  1.12it/s]\n",
      "epoch-54  lr=['0.0017093'], tr/val_loss:  0.001045/  1.511704, tr: 100.00%, val:  82.50%, val_best:  82.50%: 100%|██████████| 62/62 [00:57<00:00,  1.08it/s]\n",
      "epoch-55  lr=['0.0016485'], tr/val_loss:  0.001011/  1.516804, tr: 100.00%, val:  82.08%, val_best:  82.50%: 100%|██████████| 62/62 [00:57<00:00,  1.08it/s]\n",
      "epoch-56  lr=['0.0015880'], tr/val_loss:  0.000997/  1.513650, tr: 100.00%, val:  82.92%, val_best:  82.92%: 100%|██████████| 62/62 [00:55<00:00,  1.12it/s]\n",
      "epoch-57  lr=['0.0015279'], tr/val_loss:  0.000983/  1.517919, tr: 100.00%, val:  82.92%, val_best:  82.92%: 100%|██████████| 62/62 [00:56<00:00,  1.09it/s]\n",
      "epoch-58  lr=['0.0014682'], tr/val_loss:  0.000958/  1.517321, tr: 100.00%, val:  82.92%, val_best:  82.92%: 100%|██████████| 62/62 [00:59<00:00,  1.05it/s]\n",
      "epoch-59  lr=['0.0014090'], tr/val_loss:  0.000951/  1.524801, tr: 100.00%, val:  82.08%, val_best:  82.92%: 100%|██████████| 62/62 [00:59<00:00,  1.05it/s]\n",
      "epoch-60  lr=['0.0013503'], tr/val_loss:  0.000943/  1.527649, tr: 100.00%, val:  82.50%, val_best:  82.92%: 100%|██████████| 62/62 [00:55<00:00,  1.11it/s]\n",
      "epoch-61  lr=['0.0012922'], tr/val_loss:  0.000946/  1.527380, tr: 100.00%, val:  82.08%, val_best:  82.92%: 100%|██████████| 62/62 [00:55<00:00,  1.13it/s]\n",
      "epoch-62  lr=['0.0012348'], tr/val_loss:  0.000921/  1.527326, tr: 100.00%, val:  82.50%, val_best:  82.92%: 100%|██████████| 62/62 [00:55<00:00,  1.12it/s]\n",
      "epoch-63  lr=['0.0011781'], tr/val_loss:  0.000929/  1.528742, tr: 100.00%, val:  82.08%, val_best:  82.92%: 100%|██████████| 62/62 [00:57<00:00,  1.08it/s]\n",
      "epoch-64  lr=['0.0011221'], tr/val_loss:  0.000916/  1.528479, tr: 100.00%, val:  82.08%, val_best:  82.92%: 100%|██████████| 62/62 [00:52<00:00,  1.18it/s]\n",
      "epoch-65  lr=['0.0010670'], tr/val_loss:  0.000911/  1.528902, tr: 100.00%, val:  82.08%, val_best:  82.92%: 100%|██████████| 62/62 [00:56<00:00,  1.09it/s]\n",
      "epoch-66  lr=['0.0010128'], tr/val_loss:  0.000898/  1.529064, tr: 100.00%, val:  82.08%, val_best:  82.92%: 100%|██████████| 62/62 [00:57<00:00,  1.08it/s]\n",
      "epoch-67  lr=['0.0009594'], tr/val_loss:  0.000907/  1.530202, tr: 100.00%, val:  82.08%, val_best:  82.92%: 100%|██████████| 62/62 [00:58<00:00,  1.07it/s]\n",
      "epoch-68  lr=['0.0009071'], tr/val_loss:  0.000888/  1.531216, tr: 100.00%, val:  81.67%, val_best:  82.92%: 100%|██████████| 62/62 [00:57<00:00,  1.07it/s]\n",
      "epoch-69  lr=['0.0008558'], tr/val_loss:  0.000875/  1.533380, tr: 100.00%, val:  81.67%, val_best:  82.92%: 100%|██████████| 62/62 [00:55<00:00,  1.11it/s]\n",
      "epoch-70  lr=['0.0008055'], tr/val_loss:  0.000872/  1.533585, tr: 100.00%, val:  82.08%, val_best:  82.92%: 100%|██████████| 62/62 [00:57<00:00,  1.09it/s]\n",
      "epoch-71  lr=['0.0007565'], tr/val_loss:  0.000878/  1.533056, tr: 100.00%, val:  82.50%, val_best:  82.92%: 100%|██████████| 62/62 [00:57<00:00,  1.08it/s]\n",
      "epoch-72  lr=['0.0007085'], tr/val_loss:  0.000865/  1.535475, tr: 100.00%, val:  82.50%, val_best:  82.92%: 100%|██████████| 62/62 [00:57<00:00,  1.08it/s]\n",
      "epoch-73  lr=['0.0006619'], tr/val_loss:  0.000902/  1.535249, tr: 100.00%, val:  82.50%, val_best:  82.92%: 100%|██████████| 62/62 [00:55<00:00,  1.11it/s]\n",
      "epoch-74  lr=['0.0006165'], tr/val_loss:  0.000853/  1.534646, tr: 100.00%, val:  82.50%, val_best:  82.92%: 100%|██████████| 62/62 [00:53<00:00,  1.15it/s]\n",
      "epoch-75  lr=['0.0005724'], tr/val_loss:  0.000843/  1.537958, tr: 100.00%, val:  82.50%, val_best:  82.92%: 100%|██████████| 62/62 [00:56<00:00,  1.09it/s]\n",
      "epoch-76  lr=['0.0005296'], tr/val_loss:  0.000871/  1.538257, tr: 100.00%, val:  82.50%, val_best:  82.92%: 100%|██████████| 62/62 [00:55<00:00,  1.11it/s]\n",
      "epoch-77  lr=['0.0004883'], tr/val_loss:  0.000858/  1.541307, tr: 100.00%, val:  82.50%, val_best:  82.92%: 100%|██████████| 62/62 [00:55<00:00,  1.12it/s]\n",
      "epoch-78  lr=['0.0004485'], tr/val_loss:  0.000836/  1.542990, tr: 100.00%, val:  82.50%, val_best:  82.92%: 100%|██████████| 62/62 [00:57<00:00,  1.07it/s]\n",
      "epoch-79  lr=['0.0004101'], tr/val_loss:  0.000838/  1.544326, tr: 100.00%, val:  82.50%, val_best:  82.92%: 100%|██████████| 62/62 [00:56<00:00,  1.09it/s]\n",
      "epoch-80  lr=['0.0003732'], tr/val_loss:  0.000831/  1.542327, tr: 100.00%, val:  82.08%, val_best:  82.92%: 100%|██████████| 62/62 [00:58<00:00,  1.07it/s]\n",
      "epoch-81  lr=['0.0003379'], tr/val_loss:  0.000842/  1.542216, tr: 100.00%, val:  82.08%, val_best:  82.92%: 100%|██████████| 62/62 [00:56<00:00,  1.10it/s]\n",
      "epoch-82  lr=['0.0003042'], tr/val_loss:  0.000834/  1.542540, tr: 100.00%, val:  82.08%, val_best:  82.92%: 100%|██████████| 62/62 [00:56<00:00,  1.09it/s]\n",
      "epoch-83  lr=['0.0002721'], tr/val_loss:  0.000838/  1.538958, tr: 100.00%, val:  82.08%, val_best:  82.92%: 100%|██████████| 62/62 [00:58<00:00,  1.07it/s]\n",
      "epoch-84  lr=['0.0002417'], tr/val_loss:  0.000834/  1.544825, tr: 100.00%, val:  82.08%, val_best:  82.92%: 100%|██████████| 62/62 [00:56<00:00,  1.11it/s]\n",
      "epoch-85  lr=['0.0002130'], tr/val_loss:  0.000826/  1.543116, tr: 100.00%, val:  82.08%, val_best:  82.92%: 100%|██████████| 62/62 [00:55<00:00,  1.11it/s]\n",
      "epoch-86  lr=['0.0001860'], tr/val_loss:  0.000822/  1.543006, tr: 100.00%, val:  82.08%, val_best:  82.92%: 100%|██████████| 62/62 [00:54<00:00,  1.14it/s]\n",
      "epoch-87  lr=['0.0001607'], tr/val_loss:  0.000830/  1.543745, tr: 100.00%, val:  82.08%, val_best:  82.92%: 100%|██████████| 62/62 [00:56<00:00,  1.10it/s]\n",
      "epoch-88  lr=['0.0001372'], tr/val_loss:  0.000826/  1.544530, tr: 100.00%, val:  82.50%, val_best:  82.92%: 100%|██████████| 62/62 [00:56<00:00,  1.09it/s]\n",
      "epoch-89  lr=['0.0001155'], tr/val_loss:  0.000821/  1.546453, tr: 100.00%, val:  82.50%, val_best:  82.92%: 100%|██████████| 62/62 [00:53<00:00,  1.17it/s]\n",
      "epoch-90  lr=['0.0000956'], tr/val_loss:  0.000826/  1.546983, tr: 100.00%, val:  82.08%, val_best:  82.92%: 100%|██████████| 62/62 [00:56<00:00,  1.11it/s]\n",
      "epoch-91  lr=['0.0000776'], tr/val_loss:  0.000832/  1.547658, tr: 100.00%, val:  82.08%, val_best:  82.92%: 100%|██████████| 62/62 [00:58<00:00,  1.06it/s]\n",
      "epoch-92  lr=['0.0000614'], tr/val_loss:  0.000829/  1.547504, tr: 100.00%, val:  82.08%, val_best:  82.92%: 100%|██████████| 62/62 [00:52<00:00,  1.18it/s]\n",
      "epoch-93  lr=['0.0000471'], tr/val_loss:  0.000829/  1.547140, tr: 100.00%, val:  82.50%, val_best:  82.92%: 100%|██████████| 62/62 [00:56<00:00,  1.09it/s]\n",
      "epoch-94  lr=['0.0000346'], tr/val_loss:  0.000829/  1.547337, tr: 100.00%, val:  82.50%, val_best:  82.92%: 100%|██████████| 62/62 [00:57<00:00,  1.09it/s]\n",
      "epoch-95  lr=['0.0000241'], tr/val_loss:  0.000835/  1.547212, tr: 100.00%, val:  82.50%, val_best:  82.92%: 100%|██████████| 62/62 [00:22<00:00,  2.75it/s]\n",
      "epoch-96  lr=['0.0000154'], tr/val_loss:  0.000826/  1.546793, tr: 100.00%, val:  82.50%, val_best:  82.92%: 100%|██████████| 62/62 [00:25<00:00,  2.48it/s]\n",
      "epoch-97  lr=['0.0000087'], tr/val_loss:  0.000832/  1.546863, tr: 100.00%, val:  82.50%, val_best:  82.92%: 100%|██████████| 62/62 [00:07<00:00,  8.70it/s]\n",
      "epoch-98  lr=['0.0000039'], tr/val_loss:  0.000829/  1.546801, tr: 100.00%, val:  82.50%, val_best:  82.92%: 100%|██████████| 62/62 [00:07<00:00,  8.41it/s]\n",
      "epoch-99  lr=['0.0000010'], tr/val_loss:  0.000817/  1.546800, tr: 100.00%, val:  82.50%, val_best:  82.92%: 100%|██████████| 62/62 [00:07<00:00,  8.76it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e026858728240178a92088e2ded45e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='3.969 MB of 3.969 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>DFA_flag</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▁▅▅█▇██████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▃▃▅▇▇█▇███████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▄▄▅▆█▇█████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▄▅▅▇▇█████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▃▃▅▇▇█▇███████████████████████████████</td></tr><tr><td>val_loss</td><td>▄▄▂█▂▁▁▂▃▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>DFA_flag</td><td>1.0</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00082</td></tr><tr><td>val_acc_best</td><td>0.82917</td></tr><tr><td>val_acc_now</td><td>0.825</td></tr><tr><td>val_loss</td><td>1.5468</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cerulean-sweep-1</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yvmpitqi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yvmpitqi</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240827_011333-yvmpitqi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rzq3qcua with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_sWS_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconst2: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdrop_rate: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap_coin: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap_tr: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.004875616852073394\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 1.517948036988241\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1.7139124411913336\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: CosineAnnealingLR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240827_024754-rzq3qcua</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rzq3qcua' target=\"_blank\">divine-sweep-4</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rzq3qcua' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rzq3qcua</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_sWS_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap_tr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap_coin' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'drop_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_hash = 2bbd58b4e0d3c1e9ad501fad8a43feed\n",
      "cache path exists\n",
      "\n",
      "we will exclude the 'other' class. dvsgestrue 10 classes' indices exist. \n",
      "\n",
      "DataParallel(\n",
      "  (module): MY_SNN_FC_sstep(\n",
      "    (layers): MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC_sstep()\n",
      "      (3): SYNAPSE_FC_trace_sstep()\n",
      "      (4): LIF_layer_trace_sstep()\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC_trace_sstep()\n",
      "      (7): LIF_layer_trace_sstep()\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC_trace_sstep()\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "==================================================\n",
      "My Num of PARAMS: 452,010, system's param_num : 452,010\n",
      "Memory: 1.72MiB at 32-bit\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch-0   lr=['0.0048756'], tr/val_loss:  2.119238/  1.573220, tr:  21.45%, val:  52.50%, val_best:  52.50%: 100%|██████████| 62/62 [00:15<00:00,  3.88it/s]\n",
      "epoch-1   lr=['0.0048744'], tr/val_loss:  1.255471/  1.241279, tr:  56.38%, val:  60.42%, val_best:  60.42%: 100%|██████████| 62/62 [00:44<00:00,  1.38it/s]\n",
      "epoch-2   lr=['0.0048708'], tr/val_loss:  1.032377/  1.211842, tr:  65.47%, val:  65.83%, val_best:  65.83%: 100%|██████████| 62/62 [00:50<00:00,  1.24it/s]\n",
      "epoch-3   lr=['0.0048648'], tr/val_loss:  0.925861/  1.126883, tr:  68.95%, val:  66.25%, val_best:  66.25%: 100%|██████████| 62/62 [00:30<00:00,  2.05it/s]\n",
      "epoch-4   lr=['0.0048564'], tr/val_loss:  0.888545/  1.240667, tr:  72.01%, val:  64.58%, val_best:  66.25%: 100%|██████████| 62/62 [00:18<00:00,  3.29it/s]\n",
      "epoch-5   lr=['0.0048456'], tr/val_loss:  0.833229/  1.277361, tr:  71.50%, val:  62.92%, val_best:  66.25%: 100%|██████████| 62/62 [00:51<00:00,  1.20it/s]\n",
      "epoch-6   lr=['0.0048324'], tr/val_loss:  0.749604/  1.170241, tr:  73.85%, val:  63.75%, val_best:  66.25%: 100%|██████████| 62/62 [00:55<00:00,  1.12it/s]\n",
      "epoch-7   lr=['0.0048169'], tr/val_loss:  0.696654/  1.266696, tr:  76.30%, val:  65.00%, val_best:  66.25%: 100%|██████████| 62/62 [00:57<00:00,  1.09it/s]\n",
      "epoch-8   lr=['0.0047990'], tr/val_loss:  0.673112/  1.142362, tr:  81.41%, val:  71.25%, val_best:  71.25%: 100%|██████████| 62/62 [00:57<00:00,  1.09it/s]\n",
      "epoch-9   lr=['0.0047788'], tr/val_loss:  0.505449/  1.297524, tr:  87.23%, val:  71.25%, val_best:  71.25%: 100%|██████████| 62/62 [00:56<00:00,  1.09it/s]\n",
      "epoch-10  lr=['0.0047563'], tr/val_loss:  0.430789/  1.272470, tr:  91.93%, val:  69.58%, val_best:  71.25%: 100%|██████████| 62/62 [00:57<00:00,  1.07it/s]\n",
      "epoch-11  lr=['0.0047315'], tr/val_loss:  0.407210/  1.344885, tr:  90.91%, val:  70.00%, val_best:  71.25%: 100%|██████████| 62/62 [00:56<00:00,  1.10it/s]\n",
      "epoch-12  lr=['0.0047044'], tr/val_loss:  0.406467/  1.153355, tr:  92.95%, val:  78.33%, val_best:  78.33%: 100%|██████████| 62/62 [00:55<00:00,  1.12it/s]\n",
      "epoch-13  lr=['0.0046751'], tr/val_loss:  0.287478/  1.267188, tr:  96.53%, val:  75.83%, val_best:  78.33%: 100%|██████████| 62/62 [00:57<00:00,  1.09it/s]\n",
      "epoch-14  lr=['0.0046436'], tr/val_loss:  0.255631/  1.274381, tr:  97.24%, val:  80.42%, val_best:  80.42%: 100%|██████████| 62/62 [00:55<00:00,  1.12it/s]\n",
      "epoch-15  lr=['0.0046099'], tr/val_loss:  0.177930/  1.295485, tr:  99.08%, val:  80.42%, val_best:  80.42%: 100%|██████████| 62/62 [00:55<00:00,  1.11it/s]\n",
      "epoch-16  lr=['0.0045741'], tr/val_loss:  0.131855/  1.435689, tr:  99.90%, val:  78.75%, val_best:  80.42%: 100%|██████████| 62/62 [00:57<00:00,  1.09it/s]\n",
      "epoch-17  lr=['0.0045361'], tr/val_loss:  0.117825/  1.455200, tr: 100.00%, val:  76.67%, val_best:  80.42%: 100%|██████████| 62/62 [00:54<00:00,  1.13it/s]\n",
      "epoch-18  lr=['0.0044961'], tr/val_loss:  0.108205/  1.491229, tr: 100.00%, val:  83.75%, val_best:  83.75%: 100%|██████████| 62/62 [00:54<00:00,  1.14it/s]\n",
      "epoch-19  lr=['0.0044541'], tr/val_loss:  0.070147/  1.573762, tr:  99.90%, val:  78.33%, val_best:  83.75%: 100%|██████████| 62/62 [00:58<00:00,  1.06it/s]\n",
      "epoch-20  lr=['0.0044100'], tr/val_loss:  0.056840/  1.605830, tr: 100.00%, val:  80.00%, val_best:  83.75%: 100%|██████████| 62/62 [00:54<00:00,  1.13it/s]\n",
      "epoch-21  lr=['0.0043641'], tr/val_loss:  0.040278/  1.618017, tr: 100.00%, val:  80.83%, val_best:  83.75%: 100%|██████████| 62/62 [00:58<00:00,  1.06it/s]\n",
      "epoch-22  lr=['0.0043162'], tr/val_loss:  0.040902/  1.634196, tr: 100.00%, val:  80.42%, val_best:  83.75%: 100%|██████████| 62/62 [00:55<00:00,  1.12it/s]\n",
      "epoch-23  lr=['0.0042664'], tr/val_loss:  0.035339/  1.667905, tr: 100.00%, val:  82.50%, val_best:  83.75%: 100%|██████████| 62/62 [00:53<00:00,  1.16it/s]\n",
      "epoch-24  lr=['0.0042149'], tr/val_loss:  0.027742/  1.745237, tr: 100.00%, val:  80.83%, val_best:  83.75%: 100%|██████████| 62/62 [00:54<00:00,  1.14it/s]\n",
      "epoch-25  lr=['0.0041616'], tr/val_loss:  0.023848/  1.769157, tr: 100.00%, val:  80.83%, val_best:  83.75%: 100%|██████████| 62/62 [00:56<00:00,  1.10it/s]\n",
      "epoch-26  lr=['0.0041066'], tr/val_loss:  0.020211/  1.747949, tr: 100.00%, val:  82.50%, val_best:  83.75%: 100%|██████████| 62/62 [00:55<00:00,  1.11it/s]\n",
      "epoch-27  lr=['0.0040500'], tr/val_loss:  0.018342/  1.771382, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:56<00:00,  1.09it/s]\n",
      "epoch-28  lr=['0.0039917'], tr/val_loss:  0.015801/  1.778883, tr: 100.00%, val:  83.33%, val_best:  83.75%: 100%|██████████| 62/62 [00:57<00:00,  1.07it/s]\n",
      "epoch-29  lr=['0.0039320'], tr/val_loss:  0.016724/  1.824400, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:55<00:00,  1.11it/s]\n",
      "epoch-30  lr=['0.0038707'], tr/val_loss:  0.012895/  1.848287, tr: 100.00%, val:  82.50%, val_best:  83.75%: 100%|██████████| 62/62 [00:51<00:00,  1.20it/s]\n",
      "epoch-31  lr=['0.0038081'], tr/val_loss:  0.012461/  1.872700, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [00:55<00:00,  1.12it/s]\n",
      "epoch-32  lr=['0.0037441'], tr/val_loss:  0.011230/  1.881236, tr: 100.00%, val:  82.08%, val_best:  83.75%: 100%|██████████| 62/62 [00:55<00:00,  1.12it/s]\n",
      "epoch-33  lr=['0.0036788'], tr/val_loss:  0.009904/  1.885665, tr: 100.00%, val:  82.50%, val_best:  83.75%: 100%|██████████| 62/62 [00:55<00:00,  1.12it/s]\n",
      "epoch-34  lr=['0.0036122'], tr/val_loss:  0.009316/  1.908385, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:53<00:00,  1.16it/s]\n",
      "epoch-35  lr=['0.0035446'], tr/val_loss:  0.009169/  1.918455, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:57<00:00,  1.08it/s]\n",
      "epoch-36  lr=['0.0034758'], tr/val_loss:  0.009138/  1.938833, tr: 100.00%, val:  82.08%, val_best:  83.75%: 100%|██████████| 62/62 [00:56<00:00,  1.09it/s]\n",
      "epoch-37  lr=['0.0034060'], tr/val_loss:  0.008292/  1.941228, tr: 100.00%, val:  82.08%, val_best:  83.75%: 100%|██████████| 62/62 [00:53<00:00,  1.15it/s]\n",
      "epoch-38  lr=['0.0033352'], tr/val_loss:  0.007847/  1.965614, tr: 100.00%, val:  82.50%, val_best:  83.75%: 100%|██████████| 62/62 [00:54<00:00,  1.13it/s]\n",
      "epoch-39  lr=['0.0032636'], tr/val_loss:  0.007411/  1.972825, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:56<00:00,  1.11it/s]\n",
      "epoch-40  lr=['0.0031911'], tr/val_loss:  0.007218/  1.974069, tr: 100.00%, val:  82.50%, val_best:  83.75%: 100%|██████████| 62/62 [00:54<00:00,  1.15it/s]\n",
      "epoch-41  lr=['0.0031179'], tr/val_loss:  0.006996/  1.994612, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [00:57<00:00,  1.07it/s]\n",
      "epoch-42  lr=['0.0030441'], tr/val_loss:  0.007219/  2.006964, tr: 100.00%, val:  82.50%, val_best:  83.75%: 100%|██████████| 62/62 [00:51<00:00,  1.19it/s]\n",
      "epoch-43  lr=['0.0029696'], tr/val_loss:  0.007236/  1.996873, tr: 100.00%, val:  82.08%, val_best:  83.75%: 100%|██████████| 62/62 [00:54<00:00,  1.14it/s]\n",
      "epoch-44  lr=['0.0028946'], tr/val_loss:  0.006081/  2.023816, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:55<00:00,  1.11it/s]\n",
      "epoch-45  lr=['0.0028192'], tr/val_loss:  0.005951/  2.030952, tr: 100.00%, val:  82.50%, val_best:  83.75%: 100%|██████████| 62/62 [00:58<00:00,  1.07it/s]\n",
      "epoch-46  lr=['0.0027433'], tr/val_loss:  0.005510/  2.020342, tr: 100.00%, val:  82.08%, val_best:  83.75%: 100%|██████████| 62/62 [00:59<00:00,  1.04it/s]\n",
      "epoch-47  lr=['0.0026672'], tr/val_loss:  0.005088/  2.031799, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:56<00:00,  1.09it/s]\n",
      "epoch-48  lr=['0.0025909'], tr/val_loss:  0.005267/  2.034713, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:51<00:00,  1.21it/s]\n",
      "epoch-49  lr=['0.0025144'], tr/val_loss:  0.005170/  2.053340, tr: 100.00%, val:  82.50%, val_best:  83.75%: 100%|██████████| 62/62 [00:53<00:00,  1.17it/s]\n",
      "epoch-50  lr=['0.0024378'], tr/val_loss:  0.004846/  2.045406, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:54<00:00,  1.13it/s]\n",
      "epoch-51  lr=['0.0023612'], tr/val_loss:  0.004715/  2.047067, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:58<00:00,  1.07it/s]\n",
      "epoch-52  lr=['0.0022847'], tr/val_loss:  0.004642/  2.043557, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:57<00:00,  1.09it/s]\n",
      "epoch-53  lr=['0.0022084'], tr/val_loss:  0.004409/  2.049461, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:52<00:00,  1.19it/s]\n",
      "epoch-54  lr=['0.0021323'], tr/val_loss:  0.004394/  2.053654, tr: 100.00%, val:  82.08%, val_best:  83.75%: 100%|██████████| 62/62 [00:54<00:00,  1.14it/s]\n",
      "epoch-55  lr=['0.0020565'], tr/val_loss:  0.004205/  2.057300, tr: 100.00%, val:  82.50%, val_best:  83.75%: 100%|██████████| 62/62 [00:57<00:00,  1.08it/s]\n",
      "epoch-56  lr=['0.0019810'], tr/val_loss:  0.004152/  2.068836, tr: 100.00%, val:  82.08%, val_best:  83.75%: 100%|██████████| 62/62 [00:58<00:00,  1.06it/s]\n",
      "epoch-57  lr=['0.0019060'], tr/val_loss:  0.004195/  2.079556, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:57<00:00,  1.08it/s]\n",
      "epoch-58  lr=['0.0018316'], tr/val_loss:  0.004057/  2.088775, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:57<00:00,  1.07it/s]\n",
      "epoch-59  lr=['0.0017577'], tr/val_loss:  0.003981/  2.090045, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:55<00:00,  1.11it/s]\n",
      "epoch-60  lr=['0.0016845'], tr/val_loss:  0.003944/  2.094601, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:54<00:00,  1.15it/s]\n",
      "epoch-61  lr=['0.0016120'], tr/val_loss:  0.004276/  2.106190, tr: 100.00%, val:  80.42%, val_best:  83.75%: 100%|██████████| 62/62 [00:56<00:00,  1.09it/s]\n",
      "epoch-62  lr=['0.0015404'], tr/val_loss:  0.003970/  2.109960, tr: 100.00%, val:  80.83%, val_best:  83.75%: 100%|██████████| 62/62 [00:56<00:00,  1.09it/s]\n",
      "epoch-63  lr=['0.0014696'], tr/val_loss:  0.003935/  2.106784, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:55<00:00,  1.11it/s]\n",
      "epoch-64  lr=['0.0013998'], tr/val_loss:  0.003795/  2.101716, tr: 100.00%, val:  80.83%, val_best:  83.75%: 100%|██████████| 62/62 [00:57<00:00,  1.08it/s]\n",
      "epoch-65  lr=['0.0013311'], tr/val_loss:  0.003763/  2.106614, tr: 100.00%, val:  80.00%, val_best:  83.75%: 100%|██████████| 62/62 [00:59<00:00,  1.05it/s]\n",
      "epoch-66  lr=['0.0012634'], tr/val_loss:  0.003686/  2.108807, tr: 100.00%, val:  80.00%, val_best:  83.75%: 100%|██████████| 62/62 [00:52<00:00,  1.18it/s]\n",
      "epoch-67  lr=['0.0011969'], tr/val_loss:  0.003681/  2.108919, tr: 100.00%, val:  80.42%, val_best:  83.75%: 100%|██████████| 62/62 [00:53<00:00,  1.16it/s]\n",
      "epoch-68  lr=['0.0011316'], tr/val_loss:  0.003702/  2.111283, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:53<00:00,  1.16it/s]\n",
      "epoch-69  lr=['0.0010676'], tr/val_loss:  0.003567/  2.105173, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:55<00:00,  1.12it/s]\n",
      "epoch-70  lr=['0.0010049'], tr/val_loss:  0.003571/  2.111202, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:56<00:00,  1.09it/s]\n",
      "epoch-71  lr=['0.0009437'], tr/val_loss:  0.003675/  2.102154, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:58<00:00,  1.06it/s]\n",
      "epoch-72  lr=['0.0008839'], tr/val_loss:  0.003627/  2.106728, tr: 100.00%, val:  80.83%, val_best:  83.75%: 100%|██████████| 62/62 [00:58<00:00,  1.05it/s]\n",
      "epoch-73  lr=['0.0008257'], tr/val_loss:  0.003643/  2.111860, tr: 100.00%, val:  80.83%, val_best:  83.75%: 100%|██████████| 62/62 [00:54<00:00,  1.13it/s]\n",
      "epoch-74  lr=['0.0007690'], tr/val_loss:  0.003582/  2.112629, tr: 100.00%, val:  80.83%, val_best:  83.75%: 100%|██████████| 62/62 [00:55<00:00,  1.12it/s]\n",
      "epoch-75  lr=['0.0007140'], tr/val_loss:  0.003536/  2.116100, tr: 100.00%, val:  80.83%, val_best:  83.75%: 100%|██████████| 62/62 [00:54<00:00,  1.13it/s]\n",
      "epoch-76  lr=['0.0006607'], tr/val_loss:  0.003429/  2.118824, tr: 100.00%, val:  80.83%, val_best:  83.75%: 100%|██████████| 62/62 [00:58<00:00,  1.05it/s]\n",
      "epoch-77  lr=['0.0006092'], tr/val_loss:  0.003357/  2.113151, tr: 100.00%, val:  80.42%, val_best:  83.75%: 100%|██████████| 62/62 [00:46<00:00,  1.34it/s]\n",
      "epoch-78  lr=['0.0005594'], tr/val_loss:  0.003430/  2.113553, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:16<00:00,  3.81it/s]\n",
      "epoch-79  lr=['0.0005116'], tr/val_loss:  0.003318/  2.115185, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:20<00:00,  3.02it/s]\n",
      "epoch-80  lr=['0.0004656'], tr/val_loss:  0.003307/  2.111876, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:17<00:00,  3.46it/s]\n",
      "epoch-81  lr=['0.0004215'], tr/val_loss:  0.003308/  2.111061, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:20<00:00,  3.08it/s]\n",
      "epoch-82  lr=['0.0003795'], tr/val_loss:  0.003276/  2.112028, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:16<00:00,  3.65it/s]\n",
      "epoch-83  lr=['0.0003395'], tr/val_loss:  0.003281/  2.115060, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:18<00:00,  3.32it/s]\n",
      "epoch-84  lr=['0.0003015'], tr/val_loss:  0.003305/  2.114753, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:18<00:00,  3.37it/s]\n",
      "epoch-85  lr=['0.0002657'], tr/val_loss:  0.003215/  2.113676, tr: 100.00%, val:  80.83%, val_best:  83.75%: 100%|██████████| 62/62 [00:18<00:00,  3.35it/s]\n",
      "epoch-86  lr=['0.0002320'], tr/val_loss:  0.003271/  2.113824, tr: 100.00%, val:  80.83%, val_best:  83.75%: 100%|██████████| 62/62 [00:18<00:00,  3.31it/s]\n",
      "epoch-87  lr=['0.0002005'], tr/val_loss:  0.003297/  2.112652, tr: 100.00%, val:  80.83%, val_best:  83.75%: 100%|██████████| 62/62 [00:19<00:00,  3.12it/s]\n",
      "epoch-88  lr=['0.0001712'], tr/val_loss:  0.003353/  2.111480, tr: 100.00%, val:  80.83%, val_best:  83.75%: 100%|██████████| 62/62 [00:18<00:00,  3.30it/s]\n",
      "epoch-89  lr=['0.0001441'], tr/val_loss:  0.003294/  2.113156, tr: 100.00%, val:  80.83%, val_best:  83.75%: 100%|██████████| 62/62 [00:16<00:00,  3.81it/s]\n",
      "epoch-90  lr=['0.0001193'], tr/val_loss:  0.003347/  2.113133, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:18<00:00,  3.38it/s]\n",
      "epoch-91  lr=['0.0000968'], tr/val_loss:  0.003276/  2.114292, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:19<00:00,  3.19it/s]\n",
      "epoch-92  lr=['0.0000766'], tr/val_loss:  0.003381/  2.114351, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:15<00:00,  4.03it/s]\n",
      "epoch-93  lr=['0.0000587'], tr/val_loss:  0.003386/  2.114810, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  8.93it/s]\n",
      "epoch-94  lr=['0.0000432'], tr/val_loss:  0.003309/  2.117275, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:05<00:00, 10.68it/s]\n",
      "epoch-95  lr=['0.0000300'], tr/val_loss:  0.003282/  2.118049, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:05<00:00, 10.35it/s]\n",
      "epoch-96  lr=['0.0000192'], tr/val_loss:  0.003284/  2.117757, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:05<00:00, 10.62it/s]\n",
      "epoch-97  lr=['0.0000108'], tr/val_loss:  0.003311/  2.117849, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:05<00:00, 10.34it/s]\n",
      "epoch-98  lr=['0.0000048'], tr/val_loss:  0.003292/  2.117707, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:12<00:00,  4.92it/s]\n",
      "epoch-99  lr=['0.0000012'], tr/val_loss:  0.003273/  2.117708, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:16<00:00,  3.75it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa52d39cc8e4406ea8f19c29eff490cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='3.958 MB of 3.958 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>DFA_flag</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▅▂▆███████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▄▄▄▅▇█▇▇█████████████████▇█████████████</td></tr><tr><td>tr_acc</td><td>▁▅▆▆▇▇██████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▄▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▄▄▅▇▇▇████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▄▄▄▅▇█▇▇█████████████████▇█████████████</td></tr><tr><td>val_loss</td><td>▄▁▂▂▂▁▂▃▄▄▅▅▆▆▇▇▇▇▇▇█▇██████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>DFA_flag</td><td>1.0</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00327</td></tr><tr><td>val_acc_best</td><td>0.8375</td></tr><tr><td>val_acc_now</td><td>0.8125</td></tr><tr><td>val_loss</td><td>2.11771</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">divine-sweep-4</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rzq3qcua' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rzq3qcua</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240827_024754-rzq3qcua/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vms6i9uj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_sWS_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconst2: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdrop_rate: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap_coin: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap_tr: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.002668933061861895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3.9439416816010127\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.8476916141431217\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: CosineAnnealingLR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240827_040444-vms6i9uj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vms6i9uj' target=\"_blank\">devoted-sweep-7</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vms6i9uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vms6i9uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_sWS_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap_tr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap_coin' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'drop_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_hash = 2bbd58b4e0d3c1e9ad501fad8a43feed\n",
      "cache path exists\n",
      "\n",
      "we will exclude the 'other' class. dvsgestrue 10 classes' indices exist. \n",
      "\n",
      "DataParallel(\n",
      "  (module): MY_SNN_FC_sstep(\n",
      "    (layers): MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC_sstep()\n",
      "      (3): SYNAPSE_FC_trace_sstep()\n",
      "      (4): LIF_layer_trace_sstep()\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC_trace_sstep()\n",
      "      (7): LIF_layer_trace_sstep()\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC_trace_sstep()\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "==================================================\n",
      "My Num of PARAMS: 452,010, system's param_num : 452,010\n",
      "Memory: 1.72MiB at 32-bit\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch-0   lr=['0.0026689'], tr/val_loss:  1.853964/  1.384805, tr:  34.83%, val:  53.33%, val_best:  53.33%: 100%|██████████| 62/62 [00:07<00:00,  8.85it/s]\n",
      "epoch-1   lr=['0.0026683'], tr/val_loss:  1.135153/  1.198079, tr:  59.96%, val:  61.25%, val_best:  61.25%: 100%|██████████| 62/62 [00:07<00:00,  8.54it/s]\n",
      "epoch-2   lr=['0.0026663'], tr/val_loss:  0.965503/  1.159609, tr:  66.70%, val:  63.75%, val_best:  63.75%: 100%|██████████| 62/62 [00:12<00:00,  4.83it/s]\n",
      "epoch-3   lr=['0.0026630'], tr/val_loss:  0.848715/  1.120237, tr:  69.36%, val:  66.25%, val_best:  66.25%: 100%|██████████| 62/62 [00:16<00:00,  3.84it/s]\n",
      "epoch-4   lr=['0.0026584'], tr/val_loss:  0.812260/  1.159926, tr:  72.32%, val:  64.58%, val_best:  66.25%: 100%|██████████| 62/62 [00:15<00:00,  3.99it/s]\n",
      "epoch-5   lr=['0.0026525'], tr/val_loss:  0.737752/  1.246404, tr:  74.16%, val:  65.42%, val_best:  66.25%: 100%|██████████| 62/62 [00:13<00:00,  4.55it/s]\n",
      "epoch-6   lr=['0.0026453'], tr/val_loss:  0.650695/  1.117563, tr:  77.53%, val:  64.58%, val_best:  66.25%: 100%|██████████| 62/62 [00:14<00:00,  4.19it/s]\n",
      "epoch-7   lr=['0.0026368'], tr/val_loss:  0.575660/  1.248326, tr:  81.10%, val:  61.67%, val_best:  66.25%: 100%|██████████| 62/62 [00:16<00:00,  3.69it/s]\n",
      "epoch-8   lr=['0.0026270'], tr/val_loss:  0.536769/  1.104303, tr:  85.50%, val:  72.08%, val_best:  72.08%: 100%|██████████| 62/62 [00:15<00:00,  4.03it/s]\n",
      "epoch-9   lr=['0.0026159'], tr/val_loss:  0.363474/  1.242332, tr:  92.24%, val:  75.42%, val_best:  75.42%: 100%|██████████| 62/62 [00:15<00:00,  4.09it/s]\n",
      "epoch-10  lr=['0.0026036'], tr/val_loss:  0.292478/  1.184822, tr:  96.73%, val:  73.33%, val_best:  75.42%: 100%|██████████| 62/62 [00:18<00:00,  3.36it/s]\n",
      "epoch-11  lr=['0.0025900'], tr/val_loss:  0.225218/  1.272481, tr:  97.65%, val:  75.42%, val_best:  75.42%: 100%|██████████| 62/62 [00:19<00:00,  3.15it/s]\n",
      "epoch-12  lr=['0.0025752'], tr/val_loss:  0.215586/  1.160870, tr:  98.06%, val:  79.58%, val_best:  79.58%: 100%|██████████| 62/62 [00:19<00:00,  3.26it/s]\n",
      "epoch-13  lr=['0.0025592'], tr/val_loss:  0.136509/  1.267365, tr:  99.69%, val:  78.75%, val_best:  79.58%: 100%|██████████| 62/62 [00:23<00:00,  2.69it/s]\n",
      "epoch-14  lr=['0.0025419'], tr/val_loss:  0.129270/  1.244384, tr:  98.67%, val:  79.17%, val_best:  79.58%: 100%|██████████| 62/62 [00:13<00:00,  4.51it/s]\n",
      "epoch-15  lr=['0.0025235'], tr/val_loss:  0.070566/  1.294455, tr: 100.00%, val:  80.83%, val_best:  80.83%: 100%|██████████| 62/62 [00:20<00:00,  3.01it/s]\n",
      "epoch-16  lr=['0.0025039'], tr/val_loss:  0.054999/  1.348607, tr: 100.00%, val:  79.58%, val_best:  80.83%: 100%|██████████| 62/62 [00:15<00:00,  3.94it/s]\n",
      "epoch-17  lr=['0.0024831'], tr/val_loss:  0.034524/  1.330210, tr: 100.00%, val:  82.50%, val_best:  82.50%: 100%|██████████| 62/62 [00:18<00:00,  3.29it/s]\n",
      "epoch-18  lr=['0.0024612'], tr/val_loss:  0.023196/  1.362289, tr: 100.00%, val:  81.25%, val_best:  82.50%: 100%|██████████| 62/62 [00:19<00:00,  3.18it/s]\n",
      "epoch-19  lr=['0.0024382'], tr/val_loss:  0.016029/  1.355414, tr: 100.00%, val:  81.67%, val_best:  82.50%: 100%|██████████| 62/62 [00:16<00:00,  3.79it/s]\n",
      "epoch-20  lr=['0.0024141'], tr/val_loss:  0.012316/  1.388396, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:19<00:00,  3.11it/s]\n",
      "epoch-21  lr=['0.0023889'], tr/val_loss:  0.010117/  1.381247, tr: 100.00%, val:  81.25%, val_best:  82.50%: 100%|██████████| 62/62 [00:19<00:00,  3.23it/s]\n",
      "epoch-22  lr=['0.0023627'], tr/val_loss:  0.008814/  1.402800, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:21<00:00,  2.94it/s]\n",
      "epoch-23  lr=['0.0023355'], tr/val_loss:  0.007824/  1.424591, tr: 100.00%, val:  81.25%, val_best:  82.50%: 100%|██████████| 62/62 [00:19<00:00,  3.21it/s]\n",
      "epoch-24  lr=['0.0023073'], tr/val_loss:  0.006799/  1.448789, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:18<00:00,  3.34it/s]\n",
      "epoch-25  lr=['0.0022781'], tr/val_loss:  0.006197/  1.465496, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:19<00:00,  3.19it/s]\n",
      "epoch-26  lr=['0.0022480'], tr/val_loss:  0.005478/  1.461039, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:17<00:00,  3.63it/s]\n",
      "epoch-27  lr=['0.0022170'], tr/val_loss:  0.005254/  1.461835, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:14<00:00,  4.13it/s]\n",
      "epoch-28  lr=['0.0021851'], tr/val_loss:  0.004679/  1.475854, tr: 100.00%, val:  81.25%, val_best:  82.50%: 100%|██████████| 62/62 [00:14<00:00,  4.13it/s]\n",
      "epoch-29  lr=['0.0021524'], tr/val_loss:  0.004441/  1.474947, tr: 100.00%, val:  82.50%, val_best:  82.50%: 100%|██████████| 62/62 [00:16<00:00,  3.80it/s]\n",
      "epoch-30  lr=['0.0021188'], tr/val_loss:  0.004044/  1.488943, tr: 100.00%, val:  82.50%, val_best:  82.50%: 100%|██████████| 62/62 [00:18<00:00,  3.27it/s]\n",
      "epoch-31  lr=['0.0020845'], tr/val_loss:  0.003857/  1.490944, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:21<00:00,  2.85it/s]\n",
      "epoch-32  lr=['0.0020495'], tr/val_loss:  0.003558/  1.500644, tr: 100.00%, val:  81.25%, val_best:  82.50%: 100%|██████████| 62/62 [00:19<00:00,  3.23it/s]\n",
      "epoch-33  lr=['0.0020138'], tr/val_loss:  0.003278/  1.508981, tr: 100.00%, val:  81.67%, val_best:  82.50%: 100%|██████████| 62/62 [00:16<00:00,  3.85it/s]\n",
      "epoch-34  lr=['0.0019774'], tr/val_loss:  0.003007/  1.513947, tr: 100.00%, val:  81.25%, val_best:  82.50%: 100%|██████████| 62/62 [00:19<00:00,  3.21it/s]\n",
      "epoch-35  lr=['0.0019403'], tr/val_loss:  0.003034/  1.521029, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:16<00:00,  3.68it/s]\n",
      "epoch-36  lr=['0.0019027'], tr/val_loss:  0.002806/  1.522453, tr: 100.00%, val:  81.25%, val_best:  82.50%: 100%|██████████| 62/62 [00:17<00:00,  3.53it/s]\n",
      "epoch-37  lr=['0.0018644'], tr/val_loss:  0.002706/  1.528490, tr: 100.00%, val:  81.25%, val_best:  82.50%: 100%|██████████| 62/62 [00:17<00:00,  3.50it/s]\n",
      "epoch-38  lr=['0.0018257'], tr/val_loss:  0.002594/  1.534269, tr: 100.00%, val:  81.25%, val_best:  82.50%: 100%|██████████| 62/62 [00:17<00:00,  3.57it/s]\n",
      "epoch-39  lr=['0.0017865'], tr/val_loss:  0.002516/  1.543858, tr: 100.00%, val:  81.67%, val_best:  82.50%: 100%|██████████| 62/62 [00:19<00:00,  3.15it/s]\n",
      "epoch-40  lr=['0.0017468'], tr/val_loss:  0.002432/  1.539092, tr: 100.00%, val:  82.08%, val_best:  82.50%: 100%|██████████| 62/62 [00:22<00:00,  2.70it/s]\n",
      "epoch-41  lr=['0.0017068'], tr/val_loss:  0.002289/  1.544928, tr: 100.00%, val:  81.67%, val_best:  82.50%: 100%|██████████| 62/62 [00:20<00:00,  3.07it/s]\n",
      "epoch-42  lr=['0.0016663'], tr/val_loss:  0.002233/  1.537557, tr: 100.00%, val:  82.92%, val_best:  82.92%: 100%|██████████| 62/62 [00:19<00:00,  3.22it/s]\n",
      "epoch-43  lr=['0.0016256'], tr/val_loss:  0.002188/  1.535629, tr: 100.00%, val:  82.92%, val_best:  82.92%: 100%|██████████| 62/62 [00:17<00:00,  3.46it/s]\n",
      "epoch-44  lr=['0.0015845'], tr/val_loss:  0.002087/  1.539773, tr: 100.00%, val:  83.33%, val_best:  83.33%: 100%|██████████| 62/62 [00:18<00:00,  3.29it/s]\n",
      "epoch-45  lr=['0.0015432'], tr/val_loss:  0.002077/  1.546455, tr: 100.00%, val:  82.50%, val_best:  83.33%: 100%|██████████| 62/62 [00:16<00:00,  3.72it/s]\n",
      "epoch-46  lr=['0.0015017'], tr/val_loss:  0.002037/  1.557059, tr: 100.00%, val:  82.50%, val_best:  83.33%: 100%|██████████| 62/62 [00:17<00:00,  3.45it/s]\n",
      "epoch-47  lr=['0.0014601'], tr/val_loss:  0.001936/  1.559755, tr: 100.00%, val:  82.50%, val_best:  83.33%: 100%|██████████| 62/62 [00:21<00:00,  2.93it/s]\n",
      "epoch-48  lr=['0.0014183'], tr/val_loss:  0.001977/  1.559776, tr: 100.00%, val:  82.08%, val_best:  83.33%: 100%|██████████| 62/62 [00:19<00:00,  3.13it/s]\n",
      "epoch-49  lr=['0.0013764'], tr/val_loss:  0.001933/  1.554883, tr: 100.00%, val:  82.08%, val_best:  83.33%: 100%|██████████| 62/62 [00:13<00:00,  4.58it/s]\n",
      "epoch-50  lr=['0.0013345'], tr/val_loss:  0.001906/  1.554011, tr: 100.00%, val:  81.25%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  9.43it/s]\n",
      "epoch-51  lr=['0.0012925'], tr/val_loss:  0.001805/  1.562811, tr: 100.00%, val:  81.67%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  9.03it/s]\n",
      "epoch-52  lr=['0.0012507'], tr/val_loss:  0.001816/  1.567083, tr: 100.00%, val:  82.08%, val_best:  83.33%: 100%|██████████| 62/62 [00:07<00:00,  8.83it/s]\n",
      "epoch-53  lr=['0.0012089'], tr/val_loss:  0.001741/  1.571621, tr: 100.00%, val:  81.25%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  9.17it/s]\n",
      "epoch-54  lr=['0.0011672'], tr/val_loss:  0.001719/  1.572006, tr: 100.00%, val:  82.08%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  9.63it/s]\n",
      "epoch-55  lr=['0.0011257'], tr/val_loss:  0.001701/  1.581367, tr: 100.00%, val:  81.67%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  9.08it/s]\n",
      "epoch-56  lr=['0.0010844'], tr/val_loss:  0.001676/  1.580680, tr: 100.00%, val:  82.08%, val_best:  83.33%: 100%|██████████| 62/62 [00:07<00:00,  8.75it/s]\n",
      "epoch-57  lr=['0.0010434'], tr/val_loss:  0.001665/  1.586006, tr: 100.00%, val:  81.67%, val_best:  83.33%: 100%|██████████| 62/62 [00:07<00:00,  8.58it/s]\n",
      "epoch-58  lr=['0.0010026'], tr/val_loss:  0.001646/  1.581885, tr: 100.00%, val:  81.67%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  8.90it/s]\n",
      "epoch-59  lr=['0.0009622'], tr/val_loss:  0.001634/  1.586597, tr: 100.00%, val:  82.08%, val_best:  83.33%: 100%|██████████| 62/62 [00:07<00:00,  8.83it/s]\n",
      "epoch-60  lr=['0.0009221'], tr/val_loss:  0.001595/  1.583586, tr: 100.00%, val:  82.50%, val_best:  83.33%: 100%|██████████| 62/62 [00:07<00:00,  7.97it/s]\n",
      "epoch-61  lr=['0.0008824'], tr/val_loss:  0.001623/  1.587607, tr: 100.00%, val:  82.50%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  9.15it/s]\n",
      "epoch-62  lr=['0.0008432'], tr/val_loss:  0.001566/  1.594918, tr: 100.00%, val:  82.50%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  9.46it/s]\n",
      "epoch-63  lr=['0.0008045'], tr/val_loss:  0.001576/  1.594425, tr: 100.00%, val:  82.92%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  9.02it/s]\n",
      "epoch-64  lr=['0.0007663'], tr/val_loss:  0.001523/  1.593845, tr: 100.00%, val:  82.50%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  9.55it/s]\n",
      "epoch-65  lr=['0.0007286'], tr/val_loss:  0.001517/  1.594591, tr: 100.00%, val:  81.67%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  9.11it/s]\n",
      "epoch-66  lr=['0.0006916'], tr/val_loss:  0.001493/  1.592413, tr: 100.00%, val:  82.08%, val_best:  83.33%: 100%|██████████| 62/62 [00:07<00:00,  8.62it/s]\n",
      "epoch-67  lr=['0.0006552'], tr/val_loss:  0.001520/  1.594803, tr: 100.00%, val:  82.08%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  8.91it/s]\n",
      "epoch-68  lr=['0.0006194'], tr/val_loss:  0.001529/  1.595293, tr: 100.00%, val:  82.50%, val_best:  83.33%: 100%|██████████| 62/62 [00:07<00:00,  8.84it/s]\n",
      "epoch-69  lr=['0.0005844'], tr/val_loss:  0.001489/  1.601197, tr: 100.00%, val:  82.50%, val_best:  83.33%: 100%|██████████| 62/62 [00:07<00:00,  8.74it/s]\n",
      "epoch-70  lr=['0.0005501'], tr/val_loss:  0.001463/  1.602175, tr: 100.00%, val:  82.92%, val_best:  83.33%: 100%|██████████| 62/62 [00:07<00:00,  8.72it/s]\n",
      "epoch-71  lr=['0.0005166'], tr/val_loss:  0.001511/  1.603884, tr: 100.00%, val:  82.50%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  9.19it/s]\n",
      "epoch-72  lr=['0.0004838'], tr/val_loss:  0.001459/  1.604642, tr: 100.00%, val:  82.50%, val_best:  83.33%: 100%|██████████| 62/62 [00:07<00:00,  8.53it/s]\n",
      "epoch-73  lr=['0.0004520'], tr/val_loss:  0.001510/  1.607300, tr: 100.00%, val:  82.08%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  9.68it/s]\n",
      "epoch-74  lr=['0.0004210'], tr/val_loss:  0.001459/  1.603417, tr: 100.00%, val:  82.08%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  9.30it/s]\n",
      "epoch-75  lr=['0.0003909'], tr/val_loss:  0.001422/  1.605114, tr: 100.00%, val:  82.08%, val_best:  83.33%: 100%|██████████| 62/62 [00:07<00:00,  8.50it/s]\n",
      "epoch-76  lr=['0.0003617'], tr/val_loss:  0.001477/  1.608628, tr: 100.00%, val:  82.08%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  9.16it/s]\n",
      "epoch-77  lr=['0.0003335'], tr/val_loss:  0.001440/  1.611247, tr: 100.00%, val:  81.67%, val_best:  83.33%: 100%|██████████| 62/62 [00:07<00:00,  8.45it/s]\n",
      "epoch-78  lr=['0.0003062'], tr/val_loss:  0.001394/  1.608717, tr: 100.00%, val:  81.67%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  8.92it/s]\n",
      "epoch-79  lr=['0.0002800'], tr/val_loss:  0.001405/  1.609456, tr: 100.00%, val:  81.67%, val_best:  83.33%: 100%|██████████| 62/62 [00:07<00:00,  8.54it/s]\n",
      "epoch-80  lr=['0.0002549'], tr/val_loss:  0.001395/  1.610019, tr: 100.00%, val:  81.67%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  8.98it/s]\n",
      "epoch-81  lr=['0.0002308'], tr/val_loss:  0.001400/  1.611927, tr: 100.00%, val:  81.67%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  9.17it/s]\n",
      "epoch-82  lr=['0.0002077'], tr/val_loss:  0.001387/  1.611433, tr: 100.00%, val:  81.67%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  8.94it/s]\n",
      "epoch-83  lr=['0.0001858'], tr/val_loss:  0.001395/  1.611859, tr: 100.00%, val:  81.67%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  9.01it/s]\n",
      "epoch-84  lr=['0.0001651'], tr/val_loss:  0.001386/  1.610609, tr: 100.00%, val:  81.67%, val_best:  83.33%: 100%|██████████| 62/62 [00:07<00:00,  8.72it/s]\n",
      "epoch-85  lr=['0.0001454'], tr/val_loss:  0.001366/  1.612342, tr: 100.00%, val:  81.67%, val_best:  83.33%: 100%|██████████| 62/62 [00:05<00:00, 10.82it/s]\n",
      "epoch-86  lr=['0.0001270'], tr/val_loss:  0.001377/  1.613168, tr: 100.00%, val:  81.67%, val_best:  83.33%: 100%|██████████| 62/62 [00:05<00:00, 11.57it/s]\n",
      "epoch-87  lr=['0.0001098'], tr/val_loss:  0.001387/  1.613011, tr: 100.00%, val:  81.67%, val_best:  83.33%: 100%|██████████| 62/62 [00:05<00:00, 11.35it/s]\n",
      "epoch-88  lr=['0.0000937'], tr/val_loss:  0.001377/  1.612253, tr: 100.00%, val:  81.67%, val_best:  83.33%: 100%|██████████| 62/62 [00:05<00:00, 11.21it/s]\n",
      "epoch-89  lr=['0.0000789'], tr/val_loss:  0.001369/  1.614063, tr: 100.00%, val:  81.67%, val_best:  83.33%: 100%|██████████| 62/62 [00:05<00:00, 11.98it/s]\n",
      "epoch-90  lr=['0.0000653'], tr/val_loss:  0.001377/  1.614239, tr: 100.00%, val:  81.25%, val_best:  83.33%: 100%|██████████| 62/62 [00:05<00:00, 11.77it/s]\n",
      "epoch-91  lr=['0.0000530'], tr/val_loss:  0.001381/  1.612641, tr: 100.00%, val:  81.25%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00, 10.22it/s]\n",
      "epoch-92  lr=['0.0000419'], tr/val_loss:  0.001359/  1.613725, tr: 100.00%, val:  81.25%, val_best:  83.33%: 100%|██████████| 62/62 [00:07<00:00,  8.73it/s]\n",
      "epoch-93  lr=['0.0000321'], tr/val_loss:  0.001363/  1.614752, tr: 100.00%, val:  81.25%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  8.88it/s]\n",
      "epoch-94  lr=['0.0000236'], tr/val_loss:  0.001365/  1.614103, tr: 100.00%, val:  81.25%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  9.24it/s]\n",
      "epoch-95  lr=['0.0000164'], tr/val_loss:  0.001366/  1.614079, tr: 100.00%, val:  81.25%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  9.04it/s]\n",
      "epoch-96  lr=['0.0000105'], tr/val_loss:  0.001353/  1.614090, tr: 100.00%, val:  81.25%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  9.20it/s]\n",
      "epoch-97  lr=['0.0000059'], tr/val_loss:  0.001357/  1.614097, tr: 100.00%, val:  81.25%, val_best:  83.33%: 100%|██████████| 62/62 [00:07<00:00,  8.65it/s]\n",
      "epoch-98  lr=['0.0000026'], tr/val_loss:  0.001360/  1.614179, tr: 100.00%, val:  81.25%, val_best:  83.33%: 100%|██████████| 62/62 [00:07<00:00,  7.94it/s]\n",
      "epoch-99  lr=['0.0000007'], tr/val_loss:  0.001340/  1.614179, tr: 100.00%, val:  81.25%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  9.32it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d6356a27f54449b817602fe678e149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='3.958 MB of 3.958 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>DFA_flag</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▅▄████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▄▃▆▇▇███▇▇██▇█████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▄▅▆▇███████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▄▄▆▇▇█████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▄▃▆▇▇███▇▇██▇█████████████████████████</td></tr><tr><td>val_loss</td><td>▄▁▁▂▂▁▂▄▄▄▅▆▆▆▇▇▇▇▇▇▇▇▇█████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>DFA_flag</td><td>1.0</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00134</td></tr><tr><td>val_acc_best</td><td>0.83333</td></tr><tr><td>val_acc_now</td><td>0.8125</td></tr><tr><td>val_loss</td><td>1.61418</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devoted-sweep-7</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vms6i9uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vms6i9uj</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240827_040444-vms6i9uj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fcmotnc4 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_sWS_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconst2: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdrop_rate: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap_coin: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap_tr: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0027711235782546728\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4.546343317272795\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1.4081712794313266\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: CosineAnnealingLR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240827_042533-fcmotnc4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fcmotnc4' target=\"_blank\">fast-sweep-9</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fcmotnc4' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fcmotnc4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_sWS_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap_tr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap_coin' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'drop_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_hash = 2bbd58b4e0d3c1e9ad501fad8a43feed\n",
      "cache path exists\n",
      "\n",
      "we will exclude the 'other' class. dvsgestrue 10 classes' indices exist. \n",
      "\n",
      "DataParallel(\n",
      "  (module): MY_SNN_FC_sstep(\n",
      "    (layers): MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC_sstep()\n",
      "      (3): SYNAPSE_FC_trace_sstep()\n",
      "      (4): LIF_layer_trace_sstep()\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC_trace_sstep()\n",
      "      (7): LIF_layer_trace_sstep()\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC_trace_sstep()\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "==================================================\n",
      "My Num of PARAMS: 452,010, system's param_num : 452,010\n",
      "Memory: 1.72MiB at 32-bit\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch-0   lr=['0.0027711'], tr/val_loss:  2.307460/  2.301194, tr:   9.19%, val:  10.83%, val_best:  10.83%: 100%|██████████| 62/62 [00:07<00:00,  8.49it/s]\n",
      "epoch-1   lr=['0.0027704'], tr/val_loss:  1.990520/  1.610771, tr:  26.35%, val:  47.50%, val_best:  47.50%: 100%|██████████| 62/62 [00:06<00:00,  8.96it/s]\n",
      "epoch-2   lr=['0.0027684'], tr/val_loss:  1.283192/  1.303336, tr:  59.14%, val:  59.17%, val_best:  59.17%: 100%|██████████| 62/62 [00:06<00:00,  9.35it/s]\n",
      "epoch-3   lr=['0.0027650'], tr/val_loss:  1.039938/  1.179252, tr:  66.60%, val:  61.25%, val_best:  61.25%: 100%|██████████| 62/62 [00:06<00:00, 10.25it/s]\n",
      "epoch-4   lr=['0.0027602'], tr/val_loss:  0.939022/  1.133468, tr:  67.52%, val:  64.17%, val_best:  64.17%: 100%|██████████| 62/62 [00:07<00:00,  8.51it/s]\n",
      "epoch-5   lr=['0.0027541'], tr/val_loss:  0.847171/  1.199431, tr:  71.81%, val:  64.58%, val_best:  64.58%: 100%|██████████| 62/62 [00:06<00:00,  9.78it/s]\n",
      "epoch-6   lr=['0.0027466'], tr/val_loss:  0.775182/  1.113349, tr:  74.67%, val:  66.67%, val_best:  66.67%: 100%|██████████| 62/62 [00:06<00:00,  9.94it/s]\n",
      "epoch-7   lr=['0.0027378'], tr/val_loss:  0.760524/  1.263057, tr:  76.10%, val:  60.42%, val_best:  66.67%: 100%|██████████| 62/62 [00:07<00:00,  8.31it/s]\n",
      "epoch-8   lr=['0.0027276'], tr/val_loss:  0.735531/  1.096318, tr:  73.03%, val:  68.33%, val_best:  68.33%: 100%|██████████| 62/62 [00:07<00:00,  8.50it/s]\n",
      "epoch-9   lr=['0.0027161'], tr/val_loss:  0.589350/  1.212417, tr:  82.94%, val:  65.42%, val_best:  68.33%: 100%|██████████| 62/62 [00:06<00:00,  9.64it/s]\n",
      "epoch-10  lr=['0.0027033'], tr/val_loss:  0.547387/  1.093759, tr:  86.31%, val:  70.83%, val_best:  70.83%: 100%|██████████| 62/62 [00:06<00:00,  9.42it/s]\n",
      "epoch-11  lr=['0.0026892'], tr/val_loss:  0.501034/  1.229827, tr:  88.46%, val:  74.58%, val_best:  74.58%: 100%|██████████| 62/62 [00:07<00:00,  8.78it/s]\n",
      "epoch-12  lr=['0.0026738'], tr/val_loss:  0.439455/  1.087776, tr:  93.56%, val:  80.42%, val_best:  80.42%: 100%|██████████| 62/62 [00:06<00:00,  9.05it/s]\n",
      "epoch-13  lr=['0.0026572'], tr/val_loss:  0.378687/  1.165965, tr:  95.51%, val:  78.33%, val_best:  80.42%: 100%|██████████| 62/62 [00:06<00:00,  9.91it/s]\n",
      "epoch-14  lr=['0.0026393'], tr/val_loss:  0.305438/  1.117154, tr:  97.96%, val:  78.75%, val_best:  80.42%: 100%|██████████| 62/62 [00:06<00:00,  9.20it/s]\n",
      "epoch-15  lr=['0.0026201'], tr/val_loss:  0.250374/  1.167959, tr:  98.88%, val:  80.83%, val_best:  80.83%: 100%|██████████| 62/62 [00:07<00:00,  8.74it/s]\n",
      "epoch-16  lr=['0.0025997'], tr/val_loss:  0.221410/  1.284464, tr:  99.18%, val:  76.67%, val_best:  80.83%: 100%|██████████| 62/62 [00:06<00:00,  9.26it/s]\n",
      "epoch-17  lr=['0.0025782'], tr/val_loss:  0.203778/  1.252129, tr:  99.28%, val:  77.50%, val_best:  80.83%: 100%|██████████| 62/62 [00:06<00:00,  9.37it/s]\n",
      "epoch-18  lr=['0.0025554'], tr/val_loss:  0.151643/  1.268036, tr:  99.90%, val:  81.25%, val_best:  81.25%: 100%|██████████| 62/62 [00:06<00:00,  9.46it/s]\n",
      "epoch-19  lr=['0.0025315'], tr/val_loss:  0.121660/  1.280488, tr:  99.90%, val:  81.25%, val_best:  81.25%: 100%|██████████| 62/62 [00:07<00:00,  8.85it/s]\n",
      "epoch-20  lr=['0.0025065'], tr/val_loss:  0.099467/  1.324516, tr: 100.00%, val:  81.67%, val_best:  81.67%: 100%|██████████| 62/62 [00:06<00:00,  9.23it/s]\n",
      "epoch-21  lr=['0.0024804'], tr/val_loss:  0.069871/  1.370979, tr: 100.00%, val:  79.58%, val_best:  81.67%: 100%|██████████| 62/62 [00:06<00:00,  9.77it/s]\n",
      "epoch-22  lr=['0.0024532'], tr/val_loss:  0.050736/  1.362868, tr: 100.00%, val:  81.25%, val_best:  81.67%: 100%|██████████| 62/62 [00:07<00:00,  8.81it/s]\n",
      "epoch-23  lr=['0.0024249'], tr/val_loss:  0.038261/  1.389333, tr: 100.00%, val:  81.25%, val_best:  81.67%: 100%|██████████| 62/62 [00:06<00:00, 10.08it/s]\n",
      "epoch-24  lr=['0.0023956'], tr/val_loss:  0.032479/  1.474524, tr: 100.00%, val:  80.42%, val_best:  81.67%: 100%|██████████| 62/62 [00:07<00:00,  8.76it/s]\n",
      "epoch-25  lr=['0.0023653'], tr/val_loss:  0.031158/  1.466861, tr: 100.00%, val:  82.50%, val_best:  82.50%: 100%|██████████| 62/62 [00:06<00:00,  9.48it/s]\n",
      "epoch-26  lr=['0.0023340'], tr/val_loss:  0.025834/  1.434904, tr: 100.00%, val:  82.92%, val_best:  82.92%: 100%|██████████| 62/62 [00:06<00:00,  9.57it/s]\n",
      "epoch-27  lr=['0.0023019'], tr/val_loss:  0.022609/  1.475336, tr: 100.00%, val:  81.67%, val_best:  82.92%: 100%|██████████| 62/62 [00:07<00:00,  7.81it/s]\n",
      "epoch-28  lr=['0.0022688'], tr/val_loss:  0.020011/  1.491994, tr: 100.00%, val:  82.50%, val_best:  82.92%: 100%|██████████| 62/62 [00:06<00:00,  9.53it/s]\n",
      "epoch-29  lr=['0.0022348'], tr/val_loss:  0.018739/  1.481011, tr: 100.00%, val:  83.33%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00, 10.22it/s]\n",
      "epoch-30  lr=['0.0022000'], tr/val_loss:  0.015272/  1.486435, tr: 100.00%, val:  82.08%, val_best:  83.33%: 100%|██████████| 62/62 [00:07<00:00,  8.48it/s]\n",
      "epoch-31  lr=['0.0021644'], tr/val_loss:  0.014388/  1.499544, tr: 100.00%, val:  82.50%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  9.35it/s]\n",
      "epoch-32  lr=['0.0021280'], tr/val_loss:  0.013171/  1.504084, tr: 100.00%, val:  82.50%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  9.06it/s]\n",
      "epoch-33  lr=['0.0020909'], tr/val_loss:  0.012344/  1.509975, tr: 100.00%, val:  82.92%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  9.37it/s]\n",
      "epoch-34  lr=['0.0020531'], tr/val_loss:  0.011695/  1.530970, tr: 100.00%, val:  82.50%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  9.85it/s]\n",
      "epoch-35  lr=['0.0020146'], tr/val_loss:  0.010432/  1.541891, tr: 100.00%, val:  82.92%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  9.49it/s]\n",
      "epoch-36  lr=['0.0019755'], tr/val_loss:  0.010258/  1.553019, tr: 100.00%, val:  81.67%, val_best:  83.33%: 100%|██████████| 62/62 [00:07<00:00,  8.68it/s]\n",
      "epoch-37  lr=['0.0019358'], tr/val_loss:  0.009424/  1.542715, tr: 100.00%, val:  82.92%, val_best:  83.33%: 100%|██████████| 62/62 [00:06<00:00,  9.83it/s]\n",
      "epoch-38  lr=['0.0018956'], tr/val_loss:  0.009046/  1.559475, tr: 100.00%, val:  83.75%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.17it/s]\n",
      "epoch-39  lr=['0.0018549'], tr/val_loss:  0.008420/  1.569952, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.49it/s]\n",
      "epoch-40  lr=['0.0018137'], tr/val_loss:  0.008194/  1.566393, tr: 100.00%, val:  83.75%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.06it/s]\n",
      "epoch-41  lr=['0.0017721'], tr/val_loss:  0.008083/  1.580337, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  8.86it/s]\n",
      "epoch-42  lr=['0.0017301'], tr/val_loss:  0.008144/  1.591089, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.92it/s]\n",
      "epoch-43  lr=['0.0016878'], tr/val_loss:  0.007475/  1.589225, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [00:07<00:00,  8.85it/s]\n",
      "epoch-44  lr=['0.0016452'], tr/val_loss:  0.007146/  1.578548, tr: 100.00%, val:  83.33%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.75it/s]\n",
      "epoch-45  lr=['0.0016023'], tr/val_loss:  0.006937/  1.573940, tr: 100.00%, val:  83.75%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.39it/s]\n",
      "epoch-46  lr=['0.0015592'], tr/val_loss:  0.006851/  1.584248, tr: 100.00%, val:  83.33%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  8.96it/s]\n",
      "epoch-47  lr=['0.0015160'], tr/val_loss:  0.006488/  1.592162, tr: 100.00%, val:  83.75%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.34it/s]\n",
      "epoch-48  lr=['0.0014726'], tr/val_loss:  0.006406/  1.620658, tr: 100.00%, val:  83.75%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  8.87it/s]\n",
      "epoch-49  lr=['0.0014291'], tr/val_loss:  0.006092/  1.611582, tr: 100.00%, val:  83.33%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.33it/s]\n",
      "epoch-50  lr=['0.0013856'], tr/val_loss:  0.005824/  1.611668, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.46it/s]\n",
      "epoch-51  lr=['0.0013420'], tr/val_loss:  0.005690/  1.616033, tr: 100.00%, val:  83.75%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.33it/s]\n",
      "epoch-52  lr=['0.0012986'], tr/val_loss:  0.005680/  1.625878, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.83it/s]\n",
      "epoch-53  lr=['0.0012552'], tr/val_loss:  0.005690/  1.629181, tr: 100.00%, val:  82.08%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.37it/s]\n",
      "epoch-54  lr=['0.0012119'], tr/val_loss:  0.005553/  1.622297, tr: 100.00%, val:  82.08%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  8.88it/s]\n",
      "epoch-55  lr=['0.0011688'], tr/val_loss:  0.005481/  1.620884, tr: 100.00%, val:  83.33%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  8.88it/s]\n",
      "epoch-56  lr=['0.0011259'], tr/val_loss:  0.005443/  1.641357, tr: 100.00%, val:  82.08%, val_best:  83.75%: 100%|██████████| 62/62 [00:07<00:00,  8.85it/s]\n",
      "epoch-57  lr=['0.0010833'], tr/val_loss:  0.005402/  1.640374, tr: 100.00%, val:  82.08%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.45it/s]\n",
      "epoch-58  lr=['0.0010410'], tr/val_loss:  0.005281/  1.637792, tr: 100.00%, val:  82.08%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.76it/s]\n",
      "epoch-59  lr=['0.0009990'], tr/val_loss:  0.005120/  1.653419, tr: 100.00%, val:  82.50%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.49it/s]\n",
      "epoch-60  lr=['0.0009574'], tr/val_loss:  0.005063/  1.660880, tr: 100.00%, val:  82.50%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.34it/s]\n",
      "epoch-61  lr=['0.0009162'], tr/val_loss:  0.006328/  1.659242, tr: 100.00%, val:  82.50%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  8.89it/s]\n",
      "epoch-62  lr=['0.0008755'], tr/val_loss:  0.004941/  1.654682, tr: 100.00%, val:  82.50%, val_best:  83.75%: 100%|██████████| 62/62 [00:07<00:00,  8.54it/s]\n",
      "epoch-63  lr=['0.0008353'], tr/val_loss:  0.004915/  1.663446, tr: 100.00%, val:  82.50%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.51it/s]\n",
      "epoch-64  lr=['0.0007956'], tr/val_loss:  0.004965/  1.663706, tr: 100.00%, val:  82.50%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.02it/s]\n",
      "epoch-65  lr=['0.0007565'], tr/val_loss:  0.004833/  1.664454, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.59it/s]\n",
      "epoch-66  lr=['0.0007181'], tr/val_loss:  0.004741/  1.667337, tr: 100.00%, val:  82.08%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.08it/s]\n",
      "epoch-67  lr=['0.0006803'], tr/val_loss:  0.004736/  1.673884, tr: 100.00%, val:  82.08%, val_best:  83.75%: 100%|██████████| 62/62 [00:07<00:00,  8.42it/s]\n",
      "epoch-68  lr=['0.0006431'], tr/val_loss:  0.004730/  1.678770, tr: 100.00%, val:  82.50%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.72it/s]\n",
      "epoch-69  lr=['0.0006068'], tr/val_loss:  0.004632/  1.667125, tr: 100.00%, val:  82.08%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.91it/s]\n",
      "epoch-70  lr=['0.0005711'], tr/val_loss:  0.004533/  1.672722, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:07<00:00,  8.63it/s]\n",
      "epoch-71  lr=['0.0005363'], tr/val_loss:  0.004609/  1.679687, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.37it/s]\n",
      "epoch-72  lr=['0.0005024'], tr/val_loss:  0.004631/  1.670813, tr: 100.00%, val:  82.08%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.41it/s]\n",
      "epoch-73  lr=['0.0004693'], tr/val_loss:  0.004703/  1.672649, tr: 100.00%, val:  82.08%, val_best:  83.75%: 100%|██████████| 62/62 [00:05<00:00, 10.50it/s]\n",
      "epoch-74  lr=['0.0004371'], tr/val_loss:  0.004497/  1.675374, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:05<00:00, 10.92it/s]\n",
      "epoch-75  lr=['0.0004058'], tr/val_loss:  0.004543/  1.675490, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:05<00:00, 10.40it/s]\n",
      "epoch-76  lr=['0.0003755'], tr/val_loss:  0.004583/  1.674604, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:05<00:00, 10.93it/s]\n",
      "epoch-77  lr=['0.0003462'], tr/val_loss:  0.004510/  1.670011, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:05<00:00, 10.77it/s]\n",
      "epoch-78  lr=['0.0003180'], tr/val_loss:  0.004453/  1.667941, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:05<00:00, 10.40it/s]\n",
      "epoch-79  lr=['0.0002908'], tr/val_loss:  0.004457/  1.668770, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:05<00:00, 10.84it/s]\n",
      "epoch-80  lr=['0.0002646'], tr/val_loss:  0.004420/  1.666814, tr: 100.00%, val:  81.25%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  8.88it/s]\n",
      "epoch-81  lr=['0.0002396'], tr/val_loss:  0.004438/  1.666588, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.89it/s]\n",
      "epoch-82  lr=['0.0002157'], tr/val_loss:  0.004387/  1.666062, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.58it/s]\n",
      "epoch-83  lr=['0.0001930'], tr/val_loss:  0.004408/  1.669587, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  8.88it/s]\n",
      "epoch-84  lr=['0.0001714'], tr/val_loss:  0.004418/  1.671218, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.15it/s]\n",
      "epoch-85  lr=['0.0001510'], tr/val_loss:  0.004317/  1.671349, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.93it/s]\n",
      "epoch-86  lr=['0.0001319'], tr/val_loss:  0.004340/  1.670138, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.37it/s]\n",
      "epoch-87  lr=['0.0001140'], tr/val_loss:  0.004326/  1.671428, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.11it/s]\n",
      "epoch-88  lr=['0.0000973'], tr/val_loss:  0.004353/  1.672677, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  8.90it/s]\n",
      "epoch-89  lr=['0.0000819'], tr/val_loss:  0.004333/  1.673512, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.93it/s]\n",
      "epoch-90  lr=['0.0000678'], tr/val_loss:  0.004379/  1.674763, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.97it/s]\n",
      "epoch-91  lr=['0.0000550'], tr/val_loss:  0.004342/  1.673819, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.86it/s]\n",
      "epoch-92  lr=['0.0000435'], tr/val_loss:  0.004330/  1.674190, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:07<00:00,  8.59it/s]\n",
      "epoch-93  lr=['0.0000334'], tr/val_loss:  0.004331/  1.674265, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.54it/s]\n",
      "epoch-94  lr=['0.0000245'], tr/val_loss:  0.004355/  1.675296, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00, 10.28it/s]\n",
      "epoch-95  lr=['0.0000171'], tr/val_loss:  0.004334/  1.676046, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.67it/s]\n",
      "epoch-96  lr=['0.0000109'], tr/val_loss:  0.004326/  1.675445, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.03it/s]\n",
      "epoch-97  lr=['0.0000061'], tr/val_loss:  0.004329/  1.675453, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.74it/s]\n",
      "epoch-98  lr=['0.0000027'], tr/val_loss:  0.004306/  1.675357, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.49it/s]\n",
      "epoch-99  lr=['0.0000007'], tr/val_loss:  0.004315/  1.675358, tr: 100.00%, val:  81.67%, val_best:  83.75%: 100%|██████████| 62/62 [00:06<00:00,  9.64it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683f909fe3c747d9b6290d302d596091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='3.958 MB of 3.958 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>DFA_flag</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▅▆███████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▆▆▆██▇████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▇███████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▆▆▇███████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▆▆▆██▇████████████████████████████████</td></tr><tr><td>val_loss</td><td>█▂▁▂▂▁▁▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>DFA_flag</td><td>1.0</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00431</td></tr><tr><td>val_acc_best</td><td>0.8375</td></tr><tr><td>val_acc_now</td><td>0.81667</td></tr><tr><td>val_loss</td><td>1.67536</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fast-sweep-9</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fcmotnc4' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fcmotnc4</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240827_042533-fcmotnc4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pbxlwl46 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_sWS_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconst2: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdrop_rate: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap_coin: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap_tr: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0017454001688072924\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5.919974959569791\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1.6283165831903574\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: CosineAnnealingLR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240827_043714-pbxlwl46</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pbxlwl46' target=\"_blank\">spring-sweep-10</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pbxlwl46' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pbxlwl46</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_sWS_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap_tr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap_coin' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'drop_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_hash = 2bbd58b4e0d3c1e9ad501fad8a43feed\n",
      "cache path exists\n",
      "\n",
      "we will exclude the 'other' class. dvsgestrue 10 classes' indices exist. \n",
      "\n",
      "DataParallel(\n",
      "  (module): MY_SNN_FC_sstep(\n",
      "    (layers): MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC_sstep()\n",
      "      (3): SYNAPSE_FC_trace_sstep()\n",
      "      (4): LIF_layer_trace_sstep()\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC_trace_sstep()\n",
      "      (7): LIF_layer_trace_sstep()\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC_trace_sstep()\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "==================================================\n",
      "My Num of PARAMS: 452,010, system's param_num : 452,010\n",
      "Memory: 1.72MiB at 32-bit\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch-0   lr=['0.0017454'], tr/val_loss:  2.306088/  2.302661, tr:   9.19%, val:  10.00%, val_best:  10.00%: 100%|██████████| 62/62 [00:07<00:00,  8.64it/s]\n",
      "epoch-1   lr=['0.0017450'], tr/val_loss:  2.306400/  2.302740, tr:   7.46%, val:  10.00%, val_best:  10.00%: 100%|██████████| 62/62 [00:06<00:00,  9.05it/s]\n",
      "epoch-2   lr=['0.0017437'], tr/val_loss:  2.305061/  2.296101, tr:  10.73%, val:  11.25%, val_best:  11.25%: 100%|██████████| 62/62 [00:06<00:00,  9.22it/s]\n",
      "epoch-3   lr=['0.0017415'], tr/val_loss:  2.195718/  2.112180, tr:  24.21%, val:  42.50%, val_best:  42.50%: 100%|██████████| 62/62 [00:06<00:00,  9.44it/s]\n",
      "epoch-4   lr=['0.0017385'], tr/val_loss:  1.954719/  1.871205, tr:  45.35%, val:  46.67%, val_best:  46.67%: 100%|██████████| 62/62 [00:06<00:00,  9.13it/s]\n",
      "epoch-5   lr=['0.0017347'], tr/val_loss:  1.659924/  1.687026, tr:  54.65%, val:  52.92%, val_best:  52.92%: 100%|██████████| 62/62 [00:06<00:00,  9.00it/s]\n",
      "epoch-6   lr=['0.0017299'], tr/val_loss:  1.432874/  1.489294, tr:  61.18%, val:  62.92%, val_best:  62.92%: 100%|██████████| 62/62 [00:06<00:00,  9.59it/s]\n",
      "epoch-7   lr=['0.0017244'], tr/val_loss:  1.253252/  1.383885, tr:  63.43%, val:  59.17%, val_best:  62.92%: 100%|██████████| 62/62 [00:06<00:00,  9.48it/s]\n",
      "epoch-8   lr=['0.0017180'], tr/val_loss:  1.108888/  1.326336, tr:  67.52%, val:  67.08%, val_best:  67.08%: 100%|██████████| 62/62 [00:06<00:00,  9.00it/s]\n",
      "epoch-9   lr=['0.0017107'], tr/val_loss:  1.013996/  1.341401, tr:  73.34%, val:  57.50%, val_best:  67.08%: 100%|██████████| 62/62 [00:06<00:00,  9.26it/s]\n",
      "epoch-10  lr=['0.0017027'], tr/val_loss:  0.938047/  1.264426, tr:  74.16%, val:  60.42%, val_best:  67.08%: 100%|██████████| 62/62 [00:06<00:00,  9.70it/s]\n",
      "epoch-11  lr=['0.0016938'], tr/val_loss:  0.887839/  1.252843, tr:  74.06%, val:  65.42%, val_best:  67.08%: 100%|██████████| 62/62 [00:07<00:00,  8.77it/s]\n",
      "epoch-12  lr=['0.0016841'], tr/val_loss:  0.821689/  1.198658, tr:  77.12%, val:  67.50%, val_best:  67.50%: 100%|██████████| 62/62 [00:06<00:00,  9.81it/s]\n",
      "epoch-13  lr=['0.0016736'], tr/val_loss:  0.778359/  1.195775, tr:  79.78%, val:  65.00%, val_best:  67.50%: 100%|██████████| 62/62 [00:06<00:00, 10.07it/s]\n",
      "epoch-14  lr=['0.0016623'], tr/val_loss:  0.732797/  1.211657, tr:  80.29%, val:  65.00%, val_best:  67.50%: 100%|██████████| 62/62 [00:07<00:00,  8.81it/s]\n",
      "epoch-15  lr=['0.0016503'], tr/val_loss:  0.688511/  1.238730, tr:  84.88%, val:  67.92%, val_best:  67.92%: 100%|██████████| 62/62 [00:06<00:00,  9.62it/s]\n",
      "epoch-16  lr=['0.0016375'], tr/val_loss:  0.658721/  1.228993, tr:  86.21%, val:  65.83%, val_best:  67.92%: 100%|██████████| 62/62 [00:06<00:00,  9.25it/s]\n",
      "epoch-17  lr=['0.0016239'], tr/val_loss:  0.634056/  1.195745, tr:  87.23%, val:  71.67%, val_best:  71.67%: 100%|██████████| 62/62 [00:06<00:00,  9.62it/s]\n",
      "epoch-18  lr=['0.0016095'], tr/val_loss:  0.600790/  1.204936, tr:  90.50%, val:  73.33%, val_best:  73.33%: 100%|██████████| 62/62 [00:06<00:00,  9.08it/s]\n",
      "epoch-19  lr=['0.0015945'], tr/val_loss:  0.510710/  1.187795, tr:  95.91%, val:  76.25%, val_best:  76.25%: 100%|██████████| 62/62 [00:06<00:00,  9.14it/s]\n",
      "epoch-20  lr=['0.0015787'], tr/val_loss:  0.465147/  1.251290, tr:  96.83%, val:  71.67%, val_best:  76.25%: 100%|██████████| 62/62 [00:06<00:00,  9.35it/s]\n",
      "epoch-21  lr=['0.0015623'], tr/val_loss:  0.419260/  1.283039, tr:  97.96%, val:  74.58%, val_best:  76.25%: 100%|██████████| 62/62 [00:06<00:00,  9.14it/s]\n",
      "epoch-22  lr=['0.0015451'], tr/val_loss:  0.399323/  1.265196, tr:  97.24%, val:  76.25%, val_best:  76.25%: 100%|██████████| 62/62 [00:06<00:00,  9.40it/s]\n",
      "epoch-23  lr=['0.0015273'], tr/val_loss:  0.357879/  1.271910, tr:  98.67%, val:  73.75%, val_best:  76.25%: 100%|██████████| 62/62 [00:06<00:00,  9.48it/s]\n",
      "epoch-24  lr=['0.0015089'], tr/val_loss:  0.346340/  1.366033, tr:  98.47%, val:  72.50%, val_best:  76.25%: 100%|██████████| 62/62 [00:06<00:00,  9.90it/s]\n",
      "epoch-25  lr=['0.0014898'], tr/val_loss:  0.295579/  1.326044, tr:  99.28%, val:  72.92%, val_best:  76.25%: 100%|██████████| 62/62 [00:06<00:00,  9.26it/s]\n",
      "epoch-26  lr=['0.0014701'], tr/val_loss:  0.263972/  1.344940, tr:  99.39%, val:  76.67%, val_best:  76.67%: 100%|██████████| 62/62 [00:06<00:00, 10.09it/s]\n",
      "epoch-27  lr=['0.0014498'], tr/val_loss:  0.241238/  1.372430, tr:  99.39%, val:  76.25%, val_best:  76.67%: 100%|██████████| 62/62 [00:06<00:00,  9.43it/s]\n",
      "epoch-28  lr=['0.0014290'], tr/val_loss:  0.225879/  1.398977, tr:  99.49%, val:  77.08%, val_best:  77.08%: 100%|██████████| 62/62 [00:06<00:00,  9.72it/s]\n",
      "epoch-29  lr=['0.0014076'], tr/val_loss:  0.204205/  1.443020, tr:  99.59%, val:  73.33%, val_best:  77.08%: 100%|██████████| 62/62 [00:06<00:00,  9.06it/s]\n",
      "epoch-30  lr=['0.0013857'], tr/val_loss:  0.190799/  1.433487, tr:  99.69%, val:  76.67%, val_best:  77.08%: 100%|██████████| 62/62 [00:06<00:00,  8.96it/s]\n",
      "epoch-31  lr=['0.0013632'], tr/val_loss:  0.166408/  1.458549, tr:  99.69%, val:  76.67%, val_best:  77.08%: 100%|██████████| 62/62 [00:06<00:00,  9.45it/s]\n",
      "epoch-32  lr=['0.0013403'], tr/val_loss:  0.151650/  1.506622, tr:  99.80%, val:  76.67%, val_best:  77.08%: 100%|██████████| 62/62 [00:06<00:00, 10.16it/s]\n",
      "epoch-33  lr=['0.0013169'], tr/val_loss:  0.134175/  1.496283, tr:  99.90%, val:  77.50%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.33it/s]\n",
      "epoch-34  lr=['0.0012931'], tr/val_loss:  0.117943/  1.528278, tr:  99.80%, val:  75.00%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  8.98it/s]\n",
      "epoch-35  lr=['0.0012689'], tr/val_loss:  0.121730/  1.513547, tr:  99.80%, val:  74.58%, val_best:  77.50%: 100%|██████████| 62/62 [00:07<00:00,  8.56it/s]\n",
      "epoch-36  lr=['0.0012443'], tr/val_loss:  0.114034/  1.543825, tr: 100.00%, val:  77.50%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.58it/s]\n",
      "epoch-37  lr=['0.0012193'], tr/val_loss:  0.093238/  1.601315, tr: 100.00%, val:  75.00%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.23it/s]\n",
      "epoch-38  lr=['0.0011940'], tr/val_loss:  0.085388/  1.592921, tr: 100.00%, val:  77.08%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.11it/s]\n",
      "epoch-39  lr=['0.0011683'], tr/val_loss:  0.080148/  1.595057, tr: 100.00%, val:  76.67%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.68it/s]\n",
      "epoch-40  lr=['0.0011424'], tr/val_loss:  0.073002/  1.599662, tr: 100.00%, val:  77.08%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.42it/s]\n",
      "epoch-41  lr=['0.0011162'], tr/val_loss:  0.069788/  1.648131, tr: 100.00%, val:  76.25%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.93it/s]\n",
      "epoch-42  lr=['0.0010897'], tr/val_loss:  0.065677/  1.627088, tr: 100.00%, val:  77.08%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.20it/s]\n",
      "epoch-43  lr=['0.0010631'], tr/val_loss:  0.061555/  1.651674, tr: 100.00%, val:  76.67%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.62it/s]\n",
      "epoch-44  lr=['0.0010362'], tr/val_loss:  0.061580/  1.672839, tr: 100.00%, val:  74.58%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.56it/s]\n",
      "epoch-45  lr=['0.0010092'], tr/val_loss:  0.053341/  1.695490, tr: 100.00%, val:  75.42%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.46it/s]\n",
      "epoch-46  lr=['0.0009821'], tr/val_loss:  0.056333/  1.694320, tr: 100.00%, val:  73.75%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.04it/s]\n",
      "epoch-47  lr=['0.0009548'], tr/val_loss:  0.050491/  1.701188, tr: 100.00%, val:  75.83%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.46it/s]\n",
      "epoch-48  lr=['0.0009275'], tr/val_loss:  0.049229/  1.691799, tr: 100.00%, val:  76.67%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.36it/s]\n",
      "epoch-49  lr=['0.0009001'], tr/val_loss:  0.045341/  1.698418, tr: 100.00%, val:  75.42%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.84it/s]\n",
      "epoch-50  lr=['0.0008727'], tr/val_loss:  0.045951/  1.682075, tr: 100.00%, val:  77.50%, val_best:  77.50%: 100%|██████████| 62/62 [00:07<00:00,  8.61it/s]\n",
      "epoch-51  lr=['0.0008453'], tr/val_loss:  0.042586/  1.710727, tr: 100.00%, val:  75.83%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.44it/s]\n",
      "epoch-52  lr=['0.0008179'], tr/val_loss:  0.040786/  1.695232, tr: 100.00%, val:  76.25%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  8.99it/s]\n",
      "epoch-53  lr=['0.0007906'], tr/val_loss:  0.039805/  1.702028, tr: 100.00%, val:  76.25%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.55it/s]\n",
      "epoch-54  lr=['0.0007633'], tr/val_loss:  0.037470/  1.721695, tr: 100.00%, val:  76.25%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.60it/s]\n",
      "epoch-55  lr=['0.0007362'], tr/val_loss:  0.036455/  1.716650, tr: 100.00%, val:  75.83%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.05it/s]\n",
      "epoch-56  lr=['0.0007092'], tr/val_loss:  0.033883/  1.710400, tr: 100.00%, val:  75.42%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.33it/s]\n",
      "epoch-57  lr=['0.0006823'], tr/val_loss:  0.034373/  1.724432, tr: 100.00%, val:  76.25%, val_best:  77.50%: 100%|██████████| 62/62 [00:07<00:00,  8.52it/s]\n",
      "epoch-58  lr=['0.0006557'], tr/val_loss:  0.033888/  1.720506, tr: 100.00%, val:  75.83%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.78it/s]\n",
      "epoch-59  lr=['0.0006292'], tr/val_loss:  0.031144/  1.721754, tr: 100.00%, val:  77.08%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.24it/s]\n",
      "epoch-60  lr=['0.0006030'], tr/val_loss:  0.031072/  1.729608, tr: 100.00%, val:  75.83%, val_best:  77.50%: 100%|██████████| 62/62 [00:05<00:00, 10.48it/s]\n",
      "epoch-61  lr=['0.0005771'], tr/val_loss:  0.035386/  1.738078, tr: 100.00%, val:  76.67%, val_best:  77.50%: 100%|██████████| 62/62 [00:05<00:00, 10.82it/s]\n",
      "epoch-62  lr=['0.0005514'], tr/val_loss:  0.029919/  1.732003, tr: 100.00%, val:  76.67%, val_best:  77.50%: 100%|██████████| 62/62 [00:05<00:00, 11.03it/s]\n",
      "epoch-63  lr=['0.0005261'], tr/val_loss:  0.030206/  1.744994, tr: 100.00%, val:  76.67%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.24it/s]\n",
      "epoch-64  lr=['0.0005011'], tr/val_loss:  0.029243/  1.748340, tr: 100.00%, val:  76.67%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00, 10.06it/s]\n",
      "epoch-65  lr=['0.0004765'], tr/val_loss:  0.029882/  1.765442, tr: 100.00%, val:  75.83%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.06it/s]\n",
      "epoch-66  lr=['0.0004523'], tr/val_loss:  0.028673/  1.749380, tr: 100.00%, val:  76.67%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00, 10.02it/s]\n",
      "epoch-67  lr=['0.0004285'], tr/val_loss:  0.028746/  1.756280, tr: 100.00%, val:  76.25%, val_best:  77.50%: 100%|██████████| 62/62 [00:05<00:00, 10.70it/s]\n",
      "epoch-68  lr=['0.0004051'], tr/val_loss:  0.028622/  1.762228, tr: 100.00%, val:  75.83%, val_best:  77.50%: 100%|██████████| 62/62 [00:05<00:00, 10.87it/s]\n",
      "epoch-69  lr=['0.0003822'], tr/val_loss:  0.027954/  1.752094, tr: 100.00%, val:  75.83%, val_best:  77.50%: 100%|██████████| 62/62 [00:05<00:00, 10.93it/s]\n",
      "epoch-70  lr=['0.0003597'], tr/val_loss:  0.028240/  1.766725, tr: 100.00%, val:  74.17%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.51it/s]\n",
      "epoch-71  lr=['0.0003378'], tr/val_loss:  0.028201/  1.767350, tr: 100.00%, val:  75.83%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.12it/s]\n",
      "epoch-72  lr=['0.0003164'], tr/val_loss:  0.027883/  1.775337, tr: 100.00%, val:  76.25%, val_best:  77.50%: 100%|██████████| 62/62 [00:07<00:00,  8.79it/s]\n",
      "epoch-73  lr=['0.0002956'], tr/val_loss:  0.026742/  1.750005, tr: 100.00%, val:  76.25%, val_best:  77.50%: 100%|██████████| 62/62 [00:07<00:00,  8.39it/s]\n",
      "epoch-74  lr=['0.0002753'], tr/val_loss:  0.026122/  1.764150, tr: 100.00%, val:  76.67%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.15it/s]\n",
      "epoch-75  lr=['0.0002556'], tr/val_loss:  0.025681/  1.752522, tr: 100.00%, val:  77.08%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.58it/s]\n",
      "epoch-76  lr=['0.0002365'], tr/val_loss:  0.025923/  1.762379, tr: 100.00%, val:  76.25%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.88it/s]\n",
      "epoch-77  lr=['0.0002181'], tr/val_loss:  0.025866/  1.760574, tr: 100.00%, val:  76.25%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.07it/s]\n",
      "epoch-78  lr=['0.0002003'], tr/val_loss:  0.025431/  1.765645, tr: 100.00%, val:  76.67%, val_best:  77.50%: 100%|██████████| 62/62 [00:07<00:00,  8.71it/s]\n",
      "epoch-79  lr=['0.0001831'], tr/val_loss:  0.025140/  1.760648, tr: 100.00%, val:  76.67%, val_best:  77.50%: 100%|██████████| 62/62 [00:07<00:00,  8.29it/s]\n",
      "epoch-80  lr=['0.0001667'], tr/val_loss:  0.024513/  1.762097, tr: 100.00%, val:  76.25%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.00it/s]\n",
      "epoch-81  lr=['0.0001509'], tr/val_loss:  0.024429/  1.765125, tr: 100.00%, val:  75.83%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  8.97it/s]\n",
      "epoch-82  lr=['0.0001359'], tr/val_loss:  0.024572/  1.763072, tr: 100.00%, val:  75.83%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  8.91it/s]\n",
      "epoch-83  lr=['0.0001215'], tr/val_loss:  0.024704/  1.765821, tr: 100.00%, val:  75.83%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  8.93it/s]\n",
      "epoch-84  lr=['0.0001079'], tr/val_loss:  0.024512/  1.766482, tr: 100.00%, val:  75.83%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.56it/s]\n",
      "epoch-85  lr=['0.0000951'], tr/val_loss:  0.024004/  1.771360, tr: 100.00%, val:  76.25%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.52it/s]\n",
      "epoch-86  lr=['0.0000831'], tr/val_loss:  0.024238/  1.771914, tr: 100.00%, val:  76.67%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.04it/s]\n",
      "epoch-87  lr=['0.0000718'], tr/val_loss:  0.024422/  1.769835, tr: 100.00%, val:  76.67%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  8.99it/s]\n",
      "epoch-88  lr=['0.0000613'], tr/val_loss:  0.024031/  1.770247, tr: 100.00%, val:  76.25%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.47it/s]\n",
      "epoch-89  lr=['0.0000516'], tr/val_loss:  0.023930/  1.766125, tr: 100.00%, val:  76.25%, val_best:  77.50%: 100%|██████████| 62/62 [00:07<00:00,  8.44it/s]\n",
      "epoch-90  lr=['0.0000427'], tr/val_loss:  0.024493/  1.769173, tr: 100.00%, val:  76.67%, val_best:  77.50%: 100%|██████████| 62/62 [00:07<00:00,  8.83it/s]\n",
      "epoch-91  lr=['0.0000347'], tr/val_loss:  0.024190/  1.772112, tr: 100.00%, val:  76.25%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.22it/s]\n",
      "epoch-92  lr=['0.0000274'], tr/val_loss:  0.024192/  1.768105, tr: 100.00%, val:  76.67%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.30it/s]\n",
      "epoch-93  lr=['0.0000210'], tr/val_loss:  0.024191/  1.770015, tr: 100.00%, val:  76.67%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.39it/s]\n",
      "epoch-94  lr=['0.0000155'], tr/val_loss:  0.023766/  1.770877, tr: 100.00%, val:  76.67%, val_best:  77.50%: 100%|██████████| 62/62 [00:07<00:00,  8.59it/s]\n",
      "epoch-95  lr=['0.0000107'], tr/val_loss:  0.023847/  1.770418, tr: 100.00%, val:  76.67%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.61it/s]\n",
      "epoch-96  lr=['0.0000069'], tr/val_loss:  0.023725/  1.770062, tr: 100.00%, val:  76.67%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.75it/s]\n",
      "epoch-97  lr=['0.0000039'], tr/val_loss:  0.023787/  1.770367, tr: 100.00%, val:  76.67%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.61it/s]\n",
      "epoch-98  lr=['0.0000017'], tr/val_loss:  0.023606/  1.770373, tr: 100.00%, val:  76.67%, val_best:  77.50%: 100%|██████████| 62/62 [00:07<00:00,  8.63it/s]\n",
      "epoch-99  lr=['0.0000004'], tr/val_loss:  0.023516/  1.770374, tr: 100.00%, val:  76.67%, val_best:  77.50%: 100%|██████████| 62/62 [00:06<00:00,  9.24it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f74104ea9ca4578a578bd81909af3e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='3.958 MB of 3.958 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>DFA_flag</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▁▃▆▆█▇▇████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▁▅▆▆▇▇▇████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▁▄▅▆▆▆▇████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>██▇▅▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▅▆▇▇▇▇████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▅▆▆▇▇▇████████████████████████████████</td></tr><tr><td>val_loss</td><td>██▅▂▂▁▁▁▁▂▂▂▃▃▃▄▄▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>DFA_flag</td><td>1.0</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.02352</td></tr><tr><td>val_acc_best</td><td>0.775</td></tr><tr><td>val_acc_now</td><td>0.76667</td></tr><tr><td>val_loss</td><td>1.77037</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">spring-sweep-10</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pbxlwl46' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pbxlwl46</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240827_043714-pbxlwl46/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: oxgayi33 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_sWS_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconst2: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdrop_rate: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap_coin: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap_tr: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0023183530116148033\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 0.6133135781811677\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.3998780610449397\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: CosineAnnealingLR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240827_044855-oxgayi33</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/oxgayi33' target=\"_blank\">legendary-sweep-11</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/oxgayi33' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/oxgayi33</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_sWS_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap_tr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap_coin' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'drop_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_hash = 2bbd58b4e0d3c1e9ad501fad8a43feed\n",
      "cache path exists\n",
      "\n",
      "we will exclude the 'other' class. dvsgestrue 10 classes' indices exist. \n",
      "\n",
      "DataParallel(\n",
      "  (module): MY_SNN_FC_sstep(\n",
      "    (layers): MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC_sstep()\n",
      "      (3): SYNAPSE_FC_trace_sstep()\n",
      "      (4): LIF_layer_trace_sstep()\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC_trace_sstep()\n",
      "      (7): LIF_layer_trace_sstep()\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC_trace_sstep()\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "==================================================\n",
      "My Num of PARAMS: 452,010, system's param_num : 452,010\n",
      "Memory: 1.72MiB at 32-bit\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch-0   lr=['0.0023184'], tr/val_loss:  1.775762/  1.457899, tr:  40.14%, val:  50.00%, val_best:  50.00%: 100%|██████████| 62/62 [00:07<00:00,  8.14it/s]\n",
      "epoch-1   lr=['0.0023178'], tr/val_loss:  1.217063/  1.316792, tr:  57.41%, val:  57.92%, val_best:  57.92%: 100%|██████████| 62/62 [00:06<00:00,  9.09it/s]\n",
      "epoch-2   lr=['0.0023161'], tr/val_loss:  1.098841/  1.248862, tr:  62.00%, val:  59.17%, val_best:  59.17%: 100%|██████████| 62/62 [00:06<00:00,  9.42it/s]\n",
      "epoch-3   lr=['0.0023132'], tr/val_loss:  1.021196/  1.242023, tr:  63.84%, val:  59.58%, val_best:  59.58%: 100%|██████████| 62/62 [00:06<00:00,  9.74it/s]\n",
      "epoch-4   lr=['0.0023092'], tr/val_loss:  1.024741/  1.209175, tr:  64.25%, val:  65.00%, val_best:  65.00%: 100%|██████████| 62/62 [00:06<00:00,  9.13it/s]\n",
      "epoch-5   lr=['0.0023041'], tr/val_loss:  0.929326/  1.236174, tr:  69.36%, val:  61.25%, val_best:  65.00%: 100%|██████████| 62/62 [00:06<00:00,  9.25it/s]\n",
      "epoch-6   lr=['0.0022978'], tr/val_loss:  0.886485/  1.174223, tr:  69.36%, val:  62.50%, val_best:  65.00%: 100%|██████████| 62/62 [00:06<00:00,  9.25it/s]\n",
      "epoch-7   lr=['0.0022904'], tr/val_loss:  0.842164/  1.176584, tr:  70.07%, val:  61.25%, val_best:  65.00%: 100%|██████████| 62/62 [00:06<00:00,  9.89it/s]\n",
      "epoch-8   lr=['0.0022819'], tr/val_loss:  0.851080/  1.097213, tr:  70.58%, val:  67.50%, val_best:  67.50%: 100%|██████████| 62/62 [00:06<00:00,  9.88it/s]\n",
      "epoch-9   lr=['0.0022723'], tr/val_loss:  0.743761/  1.178483, tr:  74.77%, val:  65.00%, val_best:  67.50%: 100%|██████████| 62/62 [00:06<00:00, 10.20it/s]\n",
      "epoch-10  lr=['0.0022616'], tr/val_loss:  0.726737/  1.148233, tr:  75.38%, val:  64.17%, val_best:  67.50%: 100%|██████████| 62/62 [00:06<00:00,  9.00it/s]\n",
      "epoch-11  lr=['0.0022498'], tr/val_loss:  0.691089/  1.147432, tr:  77.02%, val:  67.50%, val_best:  67.50%: 100%|██████████| 62/62 [00:06<00:00, 10.13it/s]\n",
      "epoch-12  lr=['0.0022370'], tr/val_loss:  0.672484/  1.080283, tr:  80.69%, val:  67.50%, val_best:  67.50%: 100%|██████████| 62/62 [00:06<00:00,  9.26it/s]\n",
      "epoch-13  lr=['0.0022230'], tr/val_loss:  0.649074/  1.177834, tr:  81.21%, val:  66.67%, val_best:  67.50%: 100%|██████████| 62/62 [00:06<00:00,  9.79it/s]\n",
      "epoch-14  lr=['0.0022080'], tr/val_loss:  0.621426/  1.120440, tr:  81.72%, val:  68.33%, val_best:  68.33%: 100%|██████████| 62/62 [00:06<00:00,  9.96it/s]\n",
      "epoch-15  lr=['0.0021920'], tr/val_loss:  0.603366/  1.123350, tr:  82.53%, val:  67.92%, val_best:  68.33%: 100%|██████████| 62/62 [00:06<00:00,  8.98it/s]\n",
      "epoch-16  lr=['0.0021750'], tr/val_loss:  0.586792/  1.156200, tr:  84.07%, val:  66.25%, val_best:  68.33%: 100%|██████████| 62/62 [00:06<00:00,  8.90it/s]\n",
      "epoch-17  lr=['0.0021569'], tr/val_loss:  0.565478/  1.166074, tr:  85.70%, val:  67.92%, val_best:  68.33%: 100%|██████████| 62/62 [00:06<00:00,  9.27it/s]\n",
      "epoch-18  lr=['0.0021379'], tr/val_loss:  0.564771/  1.159283, tr:  86.21%, val:  69.58%, val_best:  69.58%: 100%|██████████| 62/62 [00:06<00:00,  9.20it/s]\n",
      "epoch-19  lr=['0.0021179'], tr/val_loss:  0.511997/  1.213375, tr:  89.58%, val:  67.92%, val_best:  69.58%: 100%|██████████| 62/62 [00:06<00:00,  9.73it/s]\n",
      "epoch-20  lr=['0.0020970'], tr/val_loss:  0.477233/  1.262299, tr:  90.30%, val:  64.58%, val_best:  69.58%: 100%|██████████| 62/62 [00:06<00:00,  9.14it/s]\n",
      "epoch-21  lr=['0.0020751'], tr/val_loss:  0.476645/  1.241149, tr:  90.40%, val:  70.00%, val_best:  70.00%: 100%|██████████| 62/62 [00:07<00:00,  8.34it/s]\n",
      "epoch-22  lr=['0.0020523'], tr/val_loss:  0.463208/  1.220719, tr:  91.93%, val:  73.33%, val_best:  73.33%: 100%|██████████| 62/62 [00:05<00:00, 10.53it/s]\n",
      "epoch-23  lr=['0.0020287'], tr/val_loss:  0.454522/  1.297420, tr:  90.19%, val:  68.75%, val_best:  73.33%: 100%|██████████| 62/62 [00:06<00:00,  9.74it/s]\n",
      "epoch-24  lr=['0.0020042'], tr/val_loss:  0.421301/  1.390596, tr:  91.83%, val:  66.67%, val_best:  73.33%: 100%|██████████| 62/62 [00:06<00:00,  9.19it/s]\n",
      "epoch-25  lr=['0.0019788'], tr/val_loss:  0.425905/  1.339280, tr:  93.97%, val:  68.33%, val_best:  73.33%: 100%|██████████| 62/62 [00:06<00:00,  9.41it/s]\n",
      "epoch-26  lr=['0.0019527'], tr/val_loss:  0.404402/  1.267881, tr:  94.28%, val:  71.67%, val_best:  73.33%: 100%|██████████| 62/62 [00:06<00:00,  9.79it/s]\n",
      "epoch-27  lr=['0.0019258'], tr/val_loss:  0.384403/  1.315801, tr:  95.10%, val:  71.25%, val_best:  73.33%: 100%|██████████| 62/62 [00:07<00:00,  8.83it/s]\n",
      "epoch-28  lr=['0.0018981'], tr/val_loss:  0.359476/  1.283731, tr:  96.22%, val:  73.75%, val_best:  73.75%: 100%|██████████| 62/62 [00:06<00:00,  9.34it/s]\n",
      "epoch-29  lr=['0.0018696'], tr/val_loss:  0.376123/  1.338369, tr:  95.51%, val:  71.25%, val_best:  73.75%: 100%|██████████| 62/62 [00:07<00:00,  8.77it/s]\n",
      "epoch-30  lr=['0.0018405'], tr/val_loss:  0.352985/  1.320949, tr:  96.32%, val:  72.50%, val_best:  73.75%: 100%|██████████| 62/62 [00:06<00:00,  9.07it/s]\n",
      "epoch-31  lr=['0.0018107'], tr/val_loss:  0.339608/  1.374021, tr:  96.73%, val:  69.17%, val_best:  73.75%: 100%|██████████| 62/62 [00:06<00:00,  9.11it/s]\n",
      "epoch-32  lr=['0.0017803'], tr/val_loss:  0.350408/  1.381873, tr:  95.10%, val:  70.42%, val_best:  73.75%: 100%|██████████| 62/62 [00:06<00:00,  9.55it/s]\n",
      "epoch-33  lr=['0.0017492'], tr/val_loss:  0.309995/  1.391132, tr:  98.57%, val:  73.33%, val_best:  73.75%: 100%|██████████| 62/62 [00:06<00:00,  9.72it/s]\n",
      "epoch-34  lr=['0.0017176'], tr/val_loss:  0.302748/  1.368870, tr:  98.06%, val:  72.08%, val_best:  73.75%: 100%|██████████| 62/62 [00:06<00:00,  9.40it/s]\n",
      "epoch-35  lr=['0.0016854'], tr/val_loss:  0.303022/  1.388600, tr:  97.75%, val:  73.75%, val_best:  73.75%: 100%|██████████| 62/62 [00:06<00:00,  9.25it/s]\n",
      "epoch-36  lr=['0.0016527'], tr/val_loss:  0.288395/  1.398753, tr:  98.77%, val:  72.50%, val_best:  73.75%: 100%|██████████| 62/62 [00:06<00:00,  9.13it/s]\n",
      "epoch-37  lr=['0.0016195'], tr/val_loss:  0.290711/  1.415209, tr:  97.45%, val:  73.33%, val_best:  73.75%: 100%|██████████| 62/62 [00:06<00:00,  9.34it/s]\n",
      "epoch-38  lr=['0.0015859'], tr/val_loss:  0.272942/  1.440298, tr:  98.57%, val:  72.92%, val_best:  73.75%: 100%|██████████| 62/62 [00:06<00:00,  9.61it/s]\n",
      "epoch-39  lr=['0.0015518'], tr/val_loss:  0.257167/  1.483265, tr:  99.08%, val:  71.25%, val_best:  73.75%: 100%|██████████| 62/62 [00:07<00:00,  8.73it/s]\n",
      "epoch-40  lr=['0.0015174'], tr/val_loss:  0.251631/  1.471438, tr:  99.49%, val:  74.17%, val_best:  74.17%: 100%|██████████| 62/62 [00:06<00:00,  9.26it/s]\n",
      "epoch-41  lr=['0.0014826'], tr/val_loss:  0.247310/  1.481137, tr:  99.39%, val:  73.75%, val_best:  74.17%: 100%|██████████| 62/62 [00:06<00:00,  9.43it/s]\n",
      "epoch-42  lr=['0.0014475'], tr/val_loss:  0.244808/  1.529467, tr:  99.69%, val:  73.33%, val_best:  74.17%: 100%|██████████| 62/62 [00:06<00:00,  9.01it/s]\n",
      "epoch-43  lr=['0.0014120'], tr/val_loss:  0.238576/  1.494075, tr:  99.08%, val:  75.00%, val_best:  75.00%: 100%|██████████| 62/62 [00:06<00:00,  9.79it/s]\n",
      "epoch-44  lr=['0.0013764'], tr/val_loss:  0.245528/  1.503703, tr:  98.98%, val:  74.17%, val_best:  75.00%: 100%|██████████| 62/62 [00:06<00:00, 10.31it/s]\n",
      "epoch-45  lr=['0.0013405'], tr/val_loss:  0.228005/  1.571136, tr:  98.98%, val:  72.50%, val_best:  75.00%: 100%|██████████| 62/62 [00:05<00:00, 11.02it/s]\n",
      "epoch-46  lr=['0.0013045'], tr/val_loss:  0.219985/  1.557478, tr:  99.80%, val:  71.67%, val_best:  75.00%: 100%|██████████| 62/62 [00:05<00:00, 10.85it/s]\n",
      "epoch-47  lr=['0.0012683'], tr/val_loss:  0.211251/  1.567670, tr:  99.28%, val:  70.00%, val_best:  75.00%: 100%|██████████| 62/62 [00:05<00:00, 10.84it/s]\n",
      "epoch-48  lr=['0.0012320'], tr/val_loss:  0.209634/  1.552142, tr:  99.80%, val:  76.25%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.76it/s]\n",
      "epoch-49  lr=['0.0011956'], tr/val_loss:  0.205605/  1.618094, tr:  99.59%, val:  72.08%, val_best:  76.25%: 100%|██████████| 62/62 [00:06<00:00,  9.59it/s]\n",
      "epoch-50  lr=['0.0011592'], tr/val_loss:  0.201829/  1.568745, tr:  99.80%, val:  75.42%, val_best:  76.25%: 100%|██████████| 62/62 [00:06<00:00,  9.26it/s]\n",
      "epoch-51  lr=['0.0011228'], tr/val_loss:  0.194349/  1.577138, tr:  99.80%, val:  72.92%, val_best:  76.25%: 100%|██████████| 62/62 [00:06<00:00,  9.47it/s]\n",
      "epoch-52  lr=['0.0010864'], tr/val_loss:  0.189400/  1.613176, tr:  99.80%, val:  72.08%, val_best:  76.25%: 100%|██████████| 62/62 [00:06<00:00,  9.75it/s]\n",
      "epoch-53  lr=['0.0010501'], tr/val_loss:  0.190924/  1.603561, tr:  99.80%, val:  72.08%, val_best:  76.25%: 100%|██████████| 62/62 [00:06<00:00,  9.53it/s]\n",
      "epoch-54  lr=['0.0010139'], tr/val_loss:  0.185131/  1.609625, tr:  99.90%, val:  74.58%, val_best:  76.25%: 100%|██████████| 62/62 [00:06<00:00,  9.74it/s]\n",
      "epoch-55  lr=['0.0009778'], tr/val_loss:  0.175155/  1.592144, tr: 100.00%, val:  73.75%, val_best:  76.25%: 100%|██████████| 62/62 [00:06<00:00, 10.21it/s]\n",
      "epoch-56  lr=['0.0009420'], tr/val_loss:  0.177885/  1.641682, tr:  99.80%, val:  72.50%, val_best:  76.25%: 100%|██████████| 62/62 [00:06<00:00,  9.18it/s]\n",
      "epoch-57  lr=['0.0009063'], tr/val_loss:  0.168179/  1.629549, tr:  99.90%, val:  73.33%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.73it/s]\n",
      "epoch-58  lr=['0.0008709'], tr/val_loss:  0.167436/  1.650698, tr: 100.00%, val:  75.42%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.40it/s]\n",
      "epoch-59  lr=['0.0008358'], tr/val_loss:  0.166499/  1.679662, tr: 100.00%, val:  71.67%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.87it/s]\n",
      "epoch-60  lr=['0.0008010'], tr/val_loss:  0.161536/  1.675361, tr:  99.90%, val:  73.75%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 11.03it/s]\n",
      "epoch-61  lr=['0.0007665'], tr/val_loss:  0.160886/  1.704218, tr: 100.00%, val:  71.67%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.96it/s]\n",
      "epoch-62  lr=['0.0007325'], tr/val_loss:  0.155426/  1.698307, tr: 100.00%, val:  71.25%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.63it/s]\n",
      "epoch-63  lr=['0.0006988'], tr/val_loss:  0.157104/  1.684638, tr:  99.90%, val:  74.58%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.75it/s]\n",
      "epoch-64  lr=['0.0006656'], tr/val_loss:  0.151908/  1.707659, tr: 100.00%, val:  73.33%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.73it/s]\n",
      "epoch-65  lr=['0.0006329'], tr/val_loss:  0.147639/  1.727099, tr: 100.00%, val:  73.75%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 11.09it/s]\n",
      "epoch-66  lr=['0.0006007'], tr/val_loss:  0.144256/  1.734763, tr: 100.00%, val:  72.92%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.51it/s]\n",
      "epoch-67  lr=['0.0005691'], tr/val_loss:  0.142656/  1.743854, tr: 100.00%, val:  72.08%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.71it/s]\n",
      "epoch-68  lr=['0.0005381'], tr/val_loss:  0.137081/  1.749409, tr: 100.00%, val:  73.33%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.91it/s]\n",
      "epoch-69  lr=['0.0005076'], tr/val_loss:  0.136980/  1.761083, tr:  99.90%, val:  72.92%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 11.22it/s]\n",
      "epoch-70  lr=['0.0004778'], tr/val_loss:  0.135671/  1.758836, tr:  99.80%, val:  71.25%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.51it/s]\n",
      "epoch-71  lr=['0.0004487'], tr/val_loss:  0.137457/  1.771142, tr: 100.00%, val:  73.33%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.56it/s]\n",
      "epoch-72  lr=['0.0004203'], tr/val_loss:  0.129263/  1.771318, tr: 100.00%, val:  72.92%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 11.21it/s]\n",
      "epoch-73  lr=['0.0003926'], tr/val_loss:  0.132305/  1.778517, tr: 100.00%, val:  72.50%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.74it/s]\n",
      "epoch-74  lr=['0.0003657'], tr/val_loss:  0.128932/  1.780045, tr:  99.90%, val:  73.33%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 11.05it/s]\n",
      "epoch-75  lr=['0.0003395'], tr/val_loss:  0.128130/  1.807536, tr: 100.00%, val:  72.50%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.85it/s]\n",
      "epoch-76  lr=['0.0003142'], tr/val_loss:  0.127056/  1.784710, tr: 100.00%, val:  74.17%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 11.32it/s]\n",
      "epoch-77  lr=['0.0002897'], tr/val_loss:  0.126562/  1.775970, tr: 100.00%, val:  72.50%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.70it/s]\n",
      "epoch-78  lr=['0.0002660'], tr/val_loss:  0.125698/  1.802982, tr: 100.00%, val:  72.08%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.93it/s]\n",
      "epoch-79  lr=['0.0002432'], tr/val_loss:  0.124428/  1.798946, tr: 100.00%, val:  72.50%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 11.02it/s]\n",
      "epoch-80  lr=['0.0002214'], tr/val_loss:  0.123633/  1.787821, tr: 100.00%, val:  73.33%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.66it/s]\n",
      "epoch-81  lr=['0.0002004'], tr/val_loss:  0.120891/  1.798327, tr: 100.00%, val:  74.17%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.51it/s]\n",
      "epoch-82  lr=['0.0001805'], tr/val_loss:  0.124085/  1.815665, tr: 100.00%, val:  72.50%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.70it/s]\n",
      "epoch-83  lr=['0.0001614'], tr/val_loss:  0.119433/  1.810797, tr: 100.00%, val:  72.92%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 11.06it/s]\n",
      "epoch-84  lr=['0.0001434'], tr/val_loss:  0.121082/  1.801855, tr: 100.00%, val:  73.75%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.83it/s]\n",
      "epoch-85  lr=['0.0001263'], tr/val_loss:  0.120255/  1.811409, tr: 100.00%, val:  73.33%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.93it/s]\n",
      "epoch-86  lr=['0.0001103'], tr/val_loss:  0.118836/  1.816338, tr: 100.00%, val:  73.33%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.69it/s]\n",
      "epoch-87  lr=['0.0000953'], tr/val_loss:  0.120175/  1.819119, tr: 100.00%, val:  72.92%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 11.24it/s]\n",
      "epoch-88  lr=['0.0000814'], tr/val_loss:  0.117728/  1.812676, tr: 100.00%, val:  72.50%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.87it/s]\n",
      "epoch-89  lr=['0.0000685'], tr/val_loss:  0.119729/  1.815613, tr: 100.00%, val:  72.50%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 11.15it/s]\n",
      "epoch-90  lr=['0.0000567'], tr/val_loss:  0.118342/  1.818220, tr: 100.00%, val:  72.92%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.99it/s]\n",
      "epoch-91  lr=['0.0000460'], tr/val_loss:  0.117775/  1.816545, tr: 100.00%, val:  73.33%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.89it/s]\n",
      "epoch-92  lr=['0.0000364'], tr/val_loss:  0.120199/  1.816398, tr: 100.00%, val:  73.33%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.65it/s]\n",
      "epoch-93  lr=['0.0000279'], tr/val_loss:  0.118480/  1.816382, tr: 100.00%, val:  73.33%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 11.22it/s]\n",
      "epoch-94  lr=['0.0000205'], tr/val_loss:  0.118572/  1.819607, tr: 100.00%, val:  72.92%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.68it/s]\n",
      "epoch-95  lr=['0.0000143'], tr/val_loss:  0.117971/  1.818414, tr: 100.00%, val:  72.92%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.79it/s]\n",
      "epoch-96  lr=['0.0000091'], tr/val_loss:  0.119267/  1.819836, tr: 100.00%, val:  73.33%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.86it/s]\n",
      "epoch-97  lr=['0.0000051'], tr/val_loss:  0.117521/  1.819857, tr: 100.00%, val:  72.92%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.95it/s]\n",
      "epoch-98  lr=['0.0000023'], tr/val_loss:  0.117589/  1.819638, tr: 100.00%, val:  73.33%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 10.97it/s]\n",
      "epoch-99  lr=['0.0000006'], tr/val_loss:  0.116771/  1.819741, tr: 100.00%, val:  73.33%, val_best:  76.25%: 100%|██████████| 62/62 [00:05<00:00, 11.20it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0b86d105db49419b9724a3dd35f47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='3.958 MB of 3.958 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>DFA_flag</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▁▆▂▅▅▇▆▇▇██▇█▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▄▅▄▅▆▆▆▆▇▆▇▇▇██▇██▇▇▇██▇▇█▇▇█▇▇▇▇██▇███</td></tr><tr><td>tr_acc</td><td>▁▄▄▄▅▆▆▆▇▇▇▇▇▇██████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇██████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▄▅▄▅▆▆▆▆▇▆▇▇▇██▇██▇▇▇██▇▇█▇▇█▇▇▇▇██▇███</td></tr><tr><td>val_loss</td><td>▅▃▂▂▂▁▁▂▂▃▄▃▃▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇███████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>DFA_flag</td><td>1.0</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.11677</td></tr><tr><td>val_acc_best</td><td>0.7625</td></tr><tr><td>val_acc_now</td><td>0.73333</td></tr><tr><td>val_loss</td><td>1.81974</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">legendary-sweep-11</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/oxgayi33' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/oxgayi33</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240827_044855-oxgayi33/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ttallx16 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_sWS_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconst2: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdrop_rate: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap_coin: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap_tr: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.003464813079486679\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 1.4128283898404372\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1.6086530770543443\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: CosineAnnealingLR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240827_045953-ttallx16</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ttallx16' target=\"_blank\">stellar-sweep-12</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ttallx16' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ttallx16</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_sWS_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap_tr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap_coin' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'drop_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_hash = 2bbd58b4e0d3c1e9ad501fad8a43feed\n",
      "cache path exists\n",
      "\n",
      "we will exclude the 'other' class. dvsgestrue 10 classes' indices exist. \n",
      "\n",
      "DataParallel(\n",
      "  (module): MY_SNN_FC_sstep(\n",
      "    (layers): MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC_sstep()\n",
      "      (3): SYNAPSE_FC_trace_sstep()\n",
      "      (4): LIF_layer_trace_sstep()\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC_trace_sstep()\n",
      "      (7): LIF_layer_trace_sstep()\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC_trace_sstep()\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "==================================================\n",
      "My Num of PARAMS: 452,010, system's param_num : 452,010\n",
      "Memory: 1.72MiB at 32-bit\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch-0   lr=['0.0034648'], tr/val_loss:  2.128402/  1.614355, tr:  21.35%, val:  47.08%, val_best:  47.08%: 100%|██████████| 62/62 [00:05<00:00, 11.32it/s]\n",
      "epoch-1   lr=['0.0034640'], tr/val_loss:  1.253757/  1.291190, tr:  57.61%, val:  61.67%, val_best:  61.67%: 100%|██████████| 62/62 [00:05<00:00, 11.26it/s]\n",
      "epoch-2   lr=['0.0034614'], tr/val_loss:  1.054480/  1.229782, tr:  65.47%, val:  62.50%, val_best:  62.50%: 100%|██████████| 62/62 [00:05<00:00, 11.27it/s]\n",
      "epoch-3   lr=['0.0034571'], tr/val_loss:  0.928054/  1.171905, tr:  68.44%, val:  62.08%, val_best:  62.50%: 100%|██████████| 62/62 [00:05<00:00, 11.22it/s]\n",
      "epoch-4   lr=['0.0034512'], tr/val_loss:  0.937231/  1.203349, tr:  70.07%, val:  67.08%, val_best:  67.08%: 100%|██████████| 62/62 [00:05<00:00, 11.27it/s]\n",
      "epoch-5   lr=['0.0034435'], tr/val_loss:  0.834136/  1.214853, tr:  72.83%, val:  63.33%, val_best:  67.08%: 100%|██████████| 62/62 [00:05<00:00, 10.97it/s]\n",
      "epoch-6   lr=['0.0034341'], tr/val_loss:  0.764698/  1.145241, tr:  74.77%, val:  64.17%, val_best:  67.08%: 100%|██████████| 62/62 [00:05<00:00, 11.31it/s]\n",
      "epoch-7   lr=['0.0034231'], tr/val_loss:  0.729000/  1.185911, tr:  75.79%, val:  67.50%, val_best:  67.50%: 100%|██████████| 62/62 [00:05<00:00, 11.21it/s]\n",
      "epoch-8   lr=['0.0034104'], tr/val_loss:  0.703142/  1.108492, tr:  79.26%, val:  71.67%, val_best:  71.67%: 100%|██████████| 62/62 [00:05<00:00, 11.00it/s]\n",
      "epoch-9   lr=['0.0033960'], tr/val_loss:  0.564929/  1.284895, tr:  84.17%, val:  70.42%, val_best:  71.67%: 100%|██████████| 62/62 [00:05<00:00, 11.10it/s]\n",
      "epoch-10  lr=['0.0033800'], tr/val_loss:  0.502401/  1.195495, tr:  88.76%, val:  70.83%, val_best:  71.67%: 100%|██████████| 62/62 [00:05<00:00, 11.00it/s]\n",
      "epoch-11  lr=['0.0033624'], tr/val_loss:  0.463587/  1.231907, tr:  88.25%, val:  72.50%, val_best:  72.50%: 100%|██████████| 62/62 [00:05<00:00, 11.18it/s]\n",
      "epoch-12  lr=['0.0033432'], tr/val_loss:  0.445980/  1.131451, tr:  89.89%, val:  77.50%, val_best:  77.50%: 100%|██████████| 62/62 [00:05<00:00, 11.30it/s]\n",
      "epoch-13  lr=['0.0033223'], tr/val_loss:  0.355058/  1.191259, tr:  94.69%, val:  78.33%, val_best:  78.33%: 100%|██████████| 62/62 [00:05<00:00, 11.00it/s]\n",
      "epoch-14  lr=['0.0032999'], tr/val_loss:  0.314863/  1.180586, tr:  96.63%, val:  80.42%, val_best:  80.42%: 100%|██████████| 62/62 [00:05<00:00, 11.16it/s]\n",
      "epoch-15  lr=['0.0032760'], tr/val_loss:  0.259871/  1.226637, tr:  98.06%, val:  80.00%, val_best:  80.42%: 100%|██████████| 62/62 [00:05<00:00, 11.10it/s]\n",
      "epoch-16  lr=['0.0032505'], tr/val_loss:  0.223907/  1.382520, tr:  98.67%, val:  77.92%, val_best:  80.42%: 100%|██████████| 62/62 [00:05<00:00, 11.23it/s]\n",
      "epoch-17  lr=['0.0032236'], tr/val_loss:  0.221394/  1.369961, tr:  98.06%, val:  76.25%, val_best:  80.42%: 100%|██████████| 62/62 [00:05<00:00, 11.15it/s]\n",
      "epoch-18  lr=['0.0031951'], tr/val_loss:  0.185359/  1.337139, tr:  99.59%, val:  81.67%, val_best:  81.67%: 100%|██████████| 62/62 [00:05<00:00, 11.14it/s]\n",
      "epoch-19  lr=['0.0031652'], tr/val_loss:  0.158980/  1.371455, tr:  99.28%, val:  82.08%, val_best:  82.08%: 100%|██████████| 62/62 [00:05<00:00, 11.19it/s]\n",
      "epoch-20  lr=['0.0031340'], tr/val_loss:  0.121340/  1.459988, tr: 100.00%, val:  80.00%, val_best:  82.08%: 100%|██████████| 62/62 [00:05<00:00, 11.09it/s]\n",
      "epoch-21  lr=['0.0031013'], tr/val_loss:  0.092535/  1.485054, tr: 100.00%, val:  80.42%, val_best:  82.08%: 100%|██████████| 62/62 [00:05<00:00, 11.16it/s]\n",
      "epoch-22  lr=['0.0030672'], tr/val_loss:  0.086083/  1.554387, tr: 100.00%, val:  78.75%, val_best:  82.08%: 100%|██████████| 62/62 [00:05<00:00, 10.91it/s]\n",
      "epoch-23  lr=['0.0030319'], tr/val_loss:  0.079052/  1.548795, tr: 100.00%, val:  80.42%, val_best:  82.08%: 100%|██████████| 62/62 [00:05<00:00, 11.32it/s]\n",
      "epoch-24  lr=['0.0029953'], tr/val_loss:  0.058804/  1.682368, tr: 100.00%, val:  77.50%, val_best:  82.08%: 100%|██████████| 62/62 [00:05<00:00, 11.12it/s]\n",
      "epoch-25  lr=['0.0029574'], tr/val_loss:  0.056837/  1.624829, tr: 100.00%, val:  80.00%, val_best:  82.08%: 100%|██████████| 62/62 [00:05<00:00, 11.15it/s]\n",
      "epoch-26  lr=['0.0029183'], tr/val_loss:  0.043345/  1.650852, tr: 100.00%, val:  80.83%, val_best:  82.08%: 100%|██████████| 62/62 [00:05<00:00, 11.11it/s]\n",
      "epoch-27  lr=['0.0028781'], tr/val_loss:  0.041600/  1.652866, tr: 100.00%, val:  81.67%, val_best:  82.08%: 100%|██████████| 62/62 [00:05<00:00, 11.12it/s]\n",
      "epoch-28  lr=['0.0028367'], tr/val_loss:  0.035229/  1.682861, tr: 100.00%, val:  80.83%, val_best:  82.08%: 100%|██████████| 62/62 [00:05<00:00, 11.33it/s]\n",
      "epoch-29  lr=['0.0027942'], tr/val_loss:  0.032478/  1.703843, tr: 100.00%, val:  80.00%, val_best:  82.08%: 100%|██████████| 62/62 [00:05<00:00, 11.10it/s]\n",
      "epoch-30  lr=['0.0027507'], tr/val_loss:  0.029798/  1.728812, tr: 100.00%, val:  80.83%, val_best:  82.08%: 100%|██████████| 62/62 [00:05<00:00, 11.10it/s]\n",
      "epoch-31  lr=['0.0027062'], tr/val_loss:  0.032557/  1.775422, tr: 100.00%, val:  80.00%, val_best:  82.08%: 100%|██████████| 62/62 [00:05<00:00, 10.77it/s]\n",
      "epoch-32  lr=['0.0026607'], tr/val_loss:  0.024068/  1.797563, tr: 100.00%, val:  80.83%, val_best:  82.08%: 100%|██████████| 62/62 [00:05<00:00, 10.90it/s]\n",
      "epoch-33  lr=['0.0026143'], tr/val_loss:  0.020978/  1.767668, tr: 100.00%, val:  82.50%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.28it/s]\n",
      "epoch-34  lr=['0.0025670'], tr/val_loss:  0.019672/  1.797922, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.22it/s]\n",
      "epoch-35  lr=['0.0025189'], tr/val_loss:  0.018246/  1.813213, tr: 100.00%, val:  80.00%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.89it/s]\n",
      "epoch-36  lr=['0.0024700'], tr/val_loss:  0.018148/  1.866016, tr: 100.00%, val:  80.00%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.49it/s]\n",
      "epoch-37  lr=['0.0024204'], tr/val_loss:  0.015760/  1.839829, tr: 100.00%, val:  80.00%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.03it/s]\n",
      "epoch-38  lr=['0.0023701'], tr/val_loss:  0.016211/  1.870802, tr: 100.00%, val:  81.25%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.15it/s]\n",
      "epoch-39  lr=['0.0023192'], tr/val_loss:  0.014356/  1.878460, tr: 100.00%, val:  81.67%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.81it/s]\n",
      "epoch-40  lr=['0.0022677'], tr/val_loss:  0.013749/  1.901300, tr: 100.00%, val:  82.50%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.44it/s]\n",
      "epoch-41  lr=['0.0022157'], tr/val_loss:  0.012997/  1.887284, tr: 100.00%, val:  81.67%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.02it/s]\n",
      "epoch-42  lr=['0.0021632'], tr/val_loss:  0.012115/  1.898216, tr: 100.00%, val:  79.58%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.00it/s]\n",
      "epoch-43  lr=['0.0021103'], tr/val_loss:  0.011444/  1.898273, tr: 100.00%, val:  81.25%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.81it/s]\n",
      "epoch-44  lr=['0.0020570'], tr/val_loss:  0.011784/  1.900097, tr: 100.00%, val:  81.25%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.85it/s]\n",
      "epoch-45  lr=['0.0020034'], tr/val_loss:  0.011032/  1.917829, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.24it/s]\n",
      "epoch-46  lr=['0.0019495'], tr/val_loss:  0.010818/  1.944098, tr: 100.00%, val:  81.25%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.20it/s]\n",
      "epoch-47  lr=['0.0018954'], tr/val_loss:  0.010455/  1.938404, tr: 100.00%, val:  82.50%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.00it/s]\n",
      "epoch-48  lr=['0.0018412'], tr/val_loss:  0.010357/  1.956379, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.83it/s]\n",
      "epoch-49  lr=['0.0017868'], tr/val_loss:  0.009609/  1.947675, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.91it/s]\n",
      "epoch-50  lr=['0.0017324'], tr/val_loss:  0.009857/  1.950608, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.70it/s]\n",
      "epoch-51  lr=['0.0016780'], tr/val_loss:  0.009369/  1.961058, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.70it/s]\n",
      "epoch-52  lr=['0.0016236'], tr/val_loss:  0.009323/  1.982003, tr: 100.00%, val:  80.00%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.79it/s]\n",
      "epoch-53  lr=['0.0015694'], tr/val_loss:  0.009170/  1.972007, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.05it/s]\n",
      "epoch-54  lr=['0.0015153'], tr/val_loss:  0.009172/  1.979027, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.87it/s]\n",
      "epoch-55  lr=['0.0014614'], tr/val_loss:  0.008875/  1.988964, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.93it/s]\n",
      "epoch-56  lr=['0.0014078'], tr/val_loss:  0.008921/  1.995726, tr: 100.00%, val:  81.25%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.13it/s]\n",
      "epoch-57  lr=['0.0013545'], tr/val_loss:  0.008097/  1.993122, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.89it/s]\n",
      "epoch-58  lr=['0.0013016'], tr/val_loss:  0.008112/  1.994235, tr: 100.00%, val:  81.25%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.12it/s]\n",
      "epoch-59  lr=['0.0012491'], tr/val_loss:  0.007991/  2.007936, tr: 100.00%, val:  81.25%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.66it/s]\n",
      "epoch-60  lr=['0.0011971'], tr/val_loss:  0.007696/  2.015153, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.06it/s]\n",
      "epoch-61  lr=['0.0011456'], tr/val_loss:  0.008596/  2.026349, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.04it/s]\n",
      "epoch-62  lr=['0.0010947'], tr/val_loss:  0.007734/  2.018173, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.95it/s]\n",
      "epoch-63  lr=['0.0010444'], tr/val_loss:  0.007394/  2.031125, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.62it/s]\n",
      "epoch-64  lr=['0.0009948'], tr/val_loss:  0.007119/  2.042195, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.12it/s]\n",
      "epoch-65  lr=['0.0009459'], tr/val_loss:  0.007268/  2.037849, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.94it/s]\n",
      "epoch-66  lr=['0.0008978'], tr/val_loss:  0.007042/  2.034819, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.14it/s]\n",
      "epoch-67  lr=['0.0008505'], tr/val_loss:  0.006888/  2.054393, tr: 100.00%, val:  79.58%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.91it/s]\n",
      "epoch-68  lr=['0.0008041'], tr/val_loss:  0.006936/  2.041252, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.78it/s]\n",
      "epoch-69  lr=['0.0007586'], tr/val_loss:  0.006675/  2.045751, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.24it/s]\n",
      "epoch-70  lr=['0.0007141'], tr/val_loss:  0.006733/  2.044954, tr: 100.00%, val:  80.00%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.32it/s]\n",
      "epoch-71  lr=['0.0006706'], tr/val_loss:  0.006495/  2.044310, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.92it/s]\n",
      "epoch-72  lr=['0.0006281'], tr/val_loss:  0.006498/  2.041363, tr: 100.00%, val:  80.00%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.96it/s]\n",
      "epoch-73  lr=['0.0005867'], tr/val_loss:  0.006413/  2.041968, tr: 100.00%, val:  79.58%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.83it/s]\n",
      "epoch-74  lr=['0.0005465'], tr/val_loss:  0.006474/  2.044173, tr: 100.00%, val:  80.00%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.86it/s]\n",
      "epoch-75  lr=['0.0005074'], tr/val_loss:  0.006259/  2.049174, tr: 100.00%, val:  79.58%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.94it/s]\n",
      "epoch-76  lr=['0.0004695'], tr/val_loss:  0.006401/  2.045399, tr: 100.00%, val:  79.58%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.04it/s]\n",
      "epoch-77  lr=['0.0004329'], tr/val_loss:  0.006275/  2.049009, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.85it/s]\n",
      "epoch-78  lr=['0.0003976'], tr/val_loss:  0.006214/  2.051123, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.29it/s]\n",
      "epoch-79  lr=['0.0003635'], tr/val_loss:  0.006078/  2.053134, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.60it/s]\n",
      "epoch-80  lr=['0.0003309'], tr/val_loss:  0.006135/  2.051000, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.80it/s]\n",
      "epoch-81  lr=['0.0002996'], tr/val_loss:  0.005921/  2.050550, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.71it/s]\n",
      "epoch-82  lr=['0.0002697'], tr/val_loss:  0.006049/  2.051329, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.98it/s]\n",
      "epoch-83  lr=['0.0002413'], tr/val_loss:  0.005992/  2.051711, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.02it/s]\n",
      "epoch-84  lr=['0.0002143'], tr/val_loss:  0.005955/  2.046463, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.91it/s]\n",
      "epoch-85  lr=['0.0001888'], tr/val_loss:  0.005885/  2.047764, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.96it/s]\n",
      "epoch-86  lr=['0.0001649'], tr/val_loss:  0.005923/  2.049549, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.79it/s]\n",
      "epoch-87  lr=['0.0001425'], tr/val_loss:  0.005954/  2.050189, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.75it/s]\n",
      "epoch-88  lr=['0.0001217'], tr/val_loss:  0.005889/  2.048371, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.85it/s]\n",
      "epoch-89  lr=['0.0001024'], tr/val_loss:  0.005896/  2.047353, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.08it/s]\n",
      "epoch-90  lr=['0.0000848'], tr/val_loss:  0.005937/  2.051787, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.02it/s]\n",
      "epoch-91  lr=['0.0000688'], tr/val_loss:  0.005896/  2.048805, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.15it/s]\n",
      "epoch-92  lr=['0.0000544'], tr/val_loss:  0.005836/  2.048720, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.64it/s]\n",
      "epoch-93  lr=['0.0000417'], tr/val_loss:  0.005838/  2.048430, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.89it/s]\n",
      "epoch-94  lr=['0.0000307'], tr/val_loss:  0.005794/  2.048827, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.93it/s]\n",
      "epoch-95  lr=['0.0000213'], tr/val_loss:  0.005782/  2.049308, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.76it/s]\n",
      "epoch-96  lr=['0.0000137'], tr/val_loss:  0.005808/  2.049157, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.01it/s]\n",
      "epoch-97  lr=['0.0000077'], tr/val_loss:  0.005751/  2.049677, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.01it/s]\n",
      "epoch-98  lr=['0.0000034'], tr/val_loss:  0.005729/  2.049684, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.76it/s]\n",
      "epoch-99  lr=['0.0000009'], tr/val_loss:  0.005675/  2.049684, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.52it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1bbdb159e1460c9fec21be8898a10b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='3.958 MB of 3.958 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>DFA_flag</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▂▅▁▆██▇████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▄▅▅▆▇█▇██▇██████▇█████████▇██▇█████████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▇▇██████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▅▅▆▇██████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▄▅▅▆▇█▇██▇██████▇█████████▇██▇█████████</td></tr><tr><td>val_loss</td><td>▅▂▂▁▂▁▁▃▃▄▅▅▅▆▆▆▇▇▇▇▇▇▇█████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>DFA_flag</td><td>1.0</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00568</td></tr><tr><td>val_acc_best</td><td>0.825</td></tr><tr><td>val_acc_now</td><td>0.80417</td></tr><tr><td>val_loss</td><td>2.04968</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stellar-sweep-12</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ttallx16' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ttallx16</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240827_045953-ttallx16/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6pqh0xpo with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_sWS_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconst2: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdrop_rate: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap_coin: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap_tr: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.03996759422970408\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3.631391548948645\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.8680677856475025\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: CosineAnnealingLR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240827_050956-6pqh0xpo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6pqh0xpo' target=\"_blank\">laced-sweep-13</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6pqh0xpo' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6pqh0xpo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_sWS_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap_tr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap_coin' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'drop_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_hash = 2bbd58b4e0d3c1e9ad501fad8a43feed\n",
      "cache path exists\n",
      "\n",
      "we will exclude the 'other' class. dvsgestrue 10 classes' indices exist. \n",
      "\n",
      "DataParallel(\n",
      "  (module): MY_SNN_FC_sstep(\n",
      "    (layers): MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC_sstep()\n",
      "      (3): SYNAPSE_FC_trace_sstep()\n",
      "      (4): LIF_layer_trace_sstep()\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC_trace_sstep()\n",
      "      (7): LIF_layer_trace_sstep()\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC_trace_sstep()\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "==================================================\n",
      "My Num of PARAMS: 452,010, system's param_num : 452,010\n",
      "Memory: 1.72MiB at 32-bit\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch-0   lr=['0.0399676'], tr/val_loss:  3.966060/  4.006894, tr:  30.03%, val:  30.42%, val_best:  30.42%: 100%|██████████| 62/62 [00:05<00:00, 11.12it/s]\n",
      "epoch-1   lr=['0.0399577'], tr/val_loss:  3.508882/  4.019019, tr:  40.35%, val:  25.42%, val_best:  30.42%: 100%|██████████| 62/62 [00:05<00:00, 11.30it/s]\n",
      "epoch-2   lr=['0.0399282'], tr/val_loss:  3.395099/  6.465969, tr:  43.41%, val:  28.33%, val_best:  30.42%: 100%|██████████| 62/62 [00:05<00:00, 11.15it/s]\n",
      "epoch-3   lr=['0.0398789'], tr/val_loss:  3.012334/  2.908649, tr:  45.25%, val:  48.75%, val_best:  48.75%: 100%|██████████| 62/62 [00:05<00:00, 11.20it/s]\n",
      "epoch-4   lr=['0.0398100'], tr/val_loss:  5.181276/ 10.556806, tr:  42.80%, val:  37.50%, val_best:  48.75%: 100%|██████████| 62/62 [00:05<00:00, 10.85it/s]\n",
      "epoch-5   lr=['0.0397216'], tr/val_loss:  4.852954/  4.484871, tr:  44.43%, val:  37.92%, val_best:  48.75%: 100%|██████████| 62/62 [00:05<00:00, 10.93it/s]\n",
      "epoch-6   lr=['0.0396136'], tr/val_loss:  3.147024/  4.761365, tr:  47.91%, val:  32.08%, val_best:  48.75%: 100%|██████████| 62/62 [00:05<00:00, 11.19it/s]\n",
      "epoch-7   lr=['0.0394863'], tr/val_loss:  3.509954/  5.390446, tr:  46.68%, val:  45.42%, val_best:  48.75%: 100%|██████████| 62/62 [00:05<00:00, 11.08it/s]\n",
      "epoch-8   lr=['0.0393398'], tr/val_loss:  2.880146/  4.624618, tr:  52.20%, val:  36.67%, val_best:  48.75%: 100%|██████████| 62/62 [00:05<00:00, 11.51it/s]\n",
      "epoch-9   lr=['0.0391741'], tr/val_loss:  3.060921/  4.691864, tr:  53.63%, val:  40.42%, val_best:  48.75%: 100%|██████████| 62/62 [00:05<00:00, 11.01it/s]\n",
      "epoch-10  lr=['0.0389895'], tr/val_loss:  3.012911/  4.577249, tr:  52.09%, val:  44.58%, val_best:  48.75%: 100%|██████████| 62/62 [00:05<00:00, 11.04it/s]\n",
      "epoch-11  lr=['0.0387862'], tr/val_loss:  3.078010/  4.042888, tr:  51.07%, val:  35.83%, val_best:  48.75%: 100%|██████████| 62/62 [00:05<00:00, 11.13it/s]\n",
      "epoch-12  lr=['0.0385643'], tr/val_loss:  3.137000/  4.019027, tr:  54.75%, val:  44.58%, val_best:  48.75%: 100%|██████████| 62/62 [00:05<00:00, 11.04it/s]\n",
      "epoch-13  lr=['0.0383240'], tr/val_loss:  3.680529/  6.092095, tr:  49.85%, val:  45.83%, val_best:  48.75%: 100%|██████████| 62/62 [00:05<00:00, 10.95it/s]\n",
      "epoch-14  lr=['0.0380657'], tr/val_loss:  2.857428/  3.563030, tr:  54.44%, val:  37.08%, val_best:  48.75%: 100%|██████████| 62/62 [00:05<00:00, 10.68it/s]\n",
      "epoch-15  lr=['0.0377895'], tr/val_loss:  4.103817/  3.944787, tr:  45.05%, val:  42.08%, val_best:  48.75%: 100%|██████████| 62/62 [00:05<00:00, 10.69it/s]\n",
      "epoch-16  lr=['0.0374957'], tr/val_loss:  2.674348/  2.657341, tr:  57.41%, val:  50.83%, val_best:  50.83%: 100%|██████████| 62/62 [00:05<00:00, 11.11it/s]\n",
      "epoch-17  lr=['0.0371847'], tr/val_loss:  3.079842/  2.281187, tr:  51.38%, val:  55.42%, val_best:  55.42%: 100%|██████████| 62/62 [00:05<00:00, 11.03it/s]\n",
      "epoch-18  lr=['0.0368567'], tr/val_loss:  3.655563/  2.774094, tr:  47.80%, val:  42.50%, val_best:  55.42%: 100%|██████████| 62/62 [00:05<00:00, 11.34it/s]\n",
      "epoch-19  lr=['0.0365120'], tr/val_loss:  2.580594/  3.249435, tr:  53.93%, val:  48.75%, val_best:  55.42%: 100%|██████████| 62/62 [00:05<00:00, 11.14it/s]\n",
      "epoch-20  lr=['0.0361510'], tr/val_loss:  2.745680/  4.786160, tr:  55.36%, val:  47.92%, val_best:  55.42%: 100%|██████████| 62/62 [00:05<00:00, 11.29it/s]\n",
      "epoch-21  lr=['0.0357741'], tr/val_loss:  3.377844/  3.801578, tr:  48.62%, val:  49.58%, val_best:  55.42%: 100%|██████████| 62/62 [00:05<00:00, 11.24it/s]\n",
      "epoch-22  lr=['0.0353816'], tr/val_loss:  2.572030/  3.231451, tr:  56.38%, val:  48.75%, val_best:  55.42%: 100%|██████████| 62/62 [00:05<00:00, 11.58it/s]\n",
      "epoch-23  lr=['0.0349739'], tr/val_loss:  2.513602/  2.940075, tr:  57.41%, val:  46.67%, val_best:  55.42%: 100%|██████████| 62/62 [00:05<00:00, 10.98it/s]\n",
      "epoch-24  lr=['0.0345514'], tr/val_loss:  2.581002/  2.574882, tr:  55.36%, val:  55.00%, val_best:  55.42%: 100%|██████████| 62/62 [00:05<00:00, 11.16it/s]\n",
      "epoch-25  lr=['0.0341145'], tr/val_loss:  2.426420/  2.847584, tr:  55.16%, val:  46.67%, val_best:  55.42%: 100%|██████████| 62/62 [00:05<00:00, 10.78it/s]\n",
      "epoch-26  lr=['0.0336636'], tr/val_loss:  2.304191/  2.711081, tr:  54.55%, val:  56.67%, val_best:  56.67%: 100%|██████████| 62/62 [00:05<00:00, 10.94it/s]\n",
      "epoch-27  lr=['0.0331993'], tr/val_loss:  2.553407/  2.128007, tr:  55.36%, val:  57.50%, val_best:  57.50%: 100%|██████████| 62/62 [00:05<00:00, 10.55it/s]\n",
      "epoch-28  lr=['0.0327219'], tr/val_loss:  2.398366/  3.887670, tr:  57.71%, val:  47.92%, val_best:  57.50%: 100%|██████████| 62/62 [00:05<00:00, 11.00it/s]\n",
      "epoch-29  lr=['0.0322320'], tr/val_loss:  2.617450/  3.307805, tr:  56.49%, val:  48.33%, val_best:  57.50%: 100%|██████████| 62/62 [00:05<00:00, 11.05it/s]\n",
      "epoch-30  lr=['0.0317300'], tr/val_loss:  2.582027/  3.894751, tr:  57.71%, val:  45.42%, val_best:  57.50%: 100%|██████████| 62/62 [00:05<00:00, 11.05it/s]\n",
      "epoch-31  lr=['0.0312164'], tr/val_loss:  2.809681/  5.302646, tr:  56.18%, val:  35.42%, val_best:  57.50%: 100%|██████████| 62/62 [00:05<00:00, 10.60it/s]\n",
      "epoch-32  lr=['0.0306917'], tr/val_loss:  2.284166/  2.935833, tr:  59.04%, val:  43.33%, val_best:  57.50%: 100%|██████████| 62/62 [00:05<00:00, 10.83it/s]\n",
      "epoch-33  lr=['0.0301564'], tr/val_loss:  2.054229/  2.808457, tr:  58.22%, val:  52.92%, val_best:  57.50%: 100%|██████████| 62/62 [00:05<00:00, 10.52it/s]\n",
      "epoch-34  lr=['0.0296111'], tr/val_loss:  1.974068/  3.439596, tr:  59.96%, val:  46.67%, val_best:  57.50%: 100%|██████████| 62/62 [00:05<00:00, 10.67it/s]\n",
      "epoch-35  lr=['0.0290563'], tr/val_loss:  1.913776/  1.720428, tr:  59.86%, val:  56.67%, val_best:  57.50%: 100%|██████████| 62/62 [00:05<00:00, 10.96it/s]\n",
      "epoch-36  lr=['0.0284925'], tr/val_loss:  2.641226/  3.790047, tr:  58.43%, val:  43.75%, val_best:  57.50%: 100%|██████████| 62/62 [00:05<00:00, 11.01it/s]\n",
      "epoch-37  lr=['0.0279203'], tr/val_loss:  2.529927/  2.989572, tr:  59.35%, val:  55.83%, val_best:  57.50%: 100%|██████████| 62/62 [00:05<00:00, 11.03it/s]\n",
      "epoch-38  lr=['0.0273403'], tr/val_loss:  2.070611/  2.847938, tr:  59.96%, val:  54.17%, val_best:  57.50%: 100%|██████████| 62/62 [00:05<00:00, 10.98it/s]\n",
      "epoch-39  lr=['0.0267531'], tr/val_loss:  1.639313/  2.718840, tr:  63.74%, val:  51.25%, val_best:  57.50%: 100%|██████████| 62/62 [00:05<00:00, 10.95it/s]\n",
      "epoch-40  lr=['0.0261591'], tr/val_loss:  1.980320/  2.501786, tr:  63.02%, val:  48.75%, val_best:  57.50%: 100%|██████████| 62/62 [00:05<00:00, 11.08it/s]\n",
      "epoch-41  lr=['0.0255591'], tr/val_loss:  1.785801/  2.785446, tr:  61.80%, val:  48.33%, val_best:  57.50%: 100%|██████████| 62/62 [00:05<00:00, 10.85it/s]\n",
      "epoch-42  lr=['0.0249536'], tr/val_loss:  1.916447/  2.216622, tr:  62.00%, val:  57.92%, val_best:  57.92%: 100%|██████████| 62/62 [00:05<00:00, 10.88it/s]\n",
      "epoch-43  lr=['0.0243431'], tr/val_loss:  1.507611/  3.126548, tr:  64.76%, val:  46.67%, val_best:  57.92%: 100%|██████████| 62/62 [00:05<00:00, 11.19it/s]\n",
      "epoch-44  lr=['0.0237284'], tr/val_loss:  1.495096/  3.687073, tr:  66.19%, val:  44.17%, val_best:  57.92%: 100%|██████████| 62/62 [00:05<00:00, 10.72it/s]\n",
      "epoch-45  lr=['0.0231100'], tr/val_loss:  1.643387/  2.417146, tr:  62.92%, val:  63.33%, val_best:  63.33%: 100%|██████████| 62/62 [00:05<00:00, 11.18it/s]\n",
      "epoch-46  lr=['0.0224884'], tr/val_loss:  1.481619/  1.607637, tr:  67.01%, val:  63.75%, val_best:  63.75%: 100%|██████████| 62/62 [00:05<00:00, 11.17it/s]\n",
      "epoch-47  lr=['0.0218644'], tr/val_loss:  1.527452/  2.112554, tr:  68.95%, val:  60.00%, val_best:  63.75%: 100%|██████████| 62/62 [00:05<00:00, 11.25it/s]\n",
      "epoch-48  lr=['0.0212386'], tr/val_loss:  1.363610/  2.152215, tr:  68.44%, val:  54.58%, val_best:  63.75%: 100%|██████████| 62/62 [00:05<00:00, 10.96it/s]\n",
      "epoch-49  lr=['0.0206115'], tr/val_loss:  1.477223/  2.181985, tr:  66.80%, val:  59.17%, val_best:  63.75%: 100%|██████████| 62/62 [00:05<00:00, 11.09it/s]\n",
      "epoch-50  lr=['0.0199838'], tr/val_loss:  1.544932/  2.010640, tr:  69.05%, val:  55.83%, val_best:  63.75%: 100%|██████████| 62/62 [00:05<00:00, 10.90it/s]\n",
      "epoch-51  lr=['0.0193561'], tr/val_loss:  1.227614/  2.236953, tr:  71.40%, val:  53.33%, val_best:  63.75%: 100%|██████████| 62/62 [00:05<00:00, 11.18it/s]\n",
      "epoch-52  lr=['0.0187290'], tr/val_loss:  1.345373/  1.942052, tr:  71.30%, val:  58.33%, val_best:  63.75%: 100%|██████████| 62/62 [00:05<00:00, 10.68it/s]\n",
      "epoch-53  lr=['0.0181032'], tr/val_loss:  1.089199/  1.699075, tr:  70.89%, val:  56.67%, val_best:  63.75%: 100%|██████████| 62/62 [00:05<00:00, 11.15it/s]\n",
      "epoch-54  lr=['0.0174792'], tr/val_loss:  1.047406/  2.212114, tr:  74.97%, val:  52.50%, val_best:  63.75%: 100%|██████████| 62/62 [00:05<00:00, 10.95it/s]\n",
      "epoch-55  lr=['0.0168576'], tr/val_loss:  1.207838/  1.643628, tr:  71.50%, val:  60.42%, val_best:  63.75%: 100%|██████████| 62/62 [00:05<00:00, 11.12it/s]\n",
      "epoch-56  lr=['0.0162392'], tr/val_loss:  1.052097/  2.162205, tr:  72.11%, val:  59.58%, val_best:  63.75%: 100%|██████████| 62/62 [00:05<00:00, 10.92it/s]\n",
      "epoch-57  lr=['0.0156245'], tr/val_loss:  1.146291/  1.767198, tr:  71.40%, val:  58.75%, val_best:  63.75%: 100%|██████████| 62/62 [00:05<00:00, 11.01it/s]\n",
      "epoch-58  lr=['0.0150140'], tr/val_loss:  1.029533/  1.556693, tr:  72.42%, val:  62.08%, val_best:  63.75%: 100%|██████████| 62/62 [00:05<00:00, 10.75it/s]\n",
      "epoch-59  lr=['0.0144085'], tr/val_loss:  1.027746/  1.810616, tr:  72.11%, val:  59.17%, val_best:  63.75%: 100%|██████████| 62/62 [00:05<00:00, 10.66it/s]\n",
      "epoch-60  lr=['0.0138085'], tr/val_loss:  0.921588/  1.669580, tr:  72.93%, val:  61.67%, val_best:  63.75%: 100%|██████████| 62/62 [00:05<00:00, 10.95it/s]\n",
      "epoch-61  lr=['0.0132145'], tr/val_loss:  0.922465/  1.938161, tr:  73.95%, val:  56.67%, val_best:  63.75%: 100%|██████████| 62/62 [00:05<00:00, 10.89it/s]\n",
      "epoch-62  lr=['0.0126273'], tr/val_loss:  0.874949/  1.608321, tr:  72.83%, val:  60.00%, val_best:  63.75%: 100%|██████████| 62/62 [00:05<00:00, 10.98it/s]\n",
      "epoch-63  lr=['0.0120473'], tr/val_loss:  0.877240/  1.534994, tr:  74.87%, val:  65.42%, val_best:  65.42%: 100%|██████████| 62/62 [00:05<00:00, 10.75it/s]\n",
      "epoch-64  lr=['0.0114751'], tr/val_loss:  0.741017/  1.555538, tr:  77.83%, val:  60.42%, val_best:  65.42%: 100%|██████████| 62/62 [00:05<00:00, 10.78it/s]\n",
      "epoch-65  lr=['0.0109113'], tr/val_loss:  0.758363/  1.803746, tr:  78.24%, val:  60.42%, val_best:  65.42%: 100%|██████████| 62/62 [00:05<00:00, 10.76it/s]\n",
      "epoch-66  lr=['0.0103565'], tr/val_loss:  0.888495/  2.181006, tr:  75.28%, val:  58.33%, val_best:  65.42%: 100%|██████████| 62/62 [00:05<00:00, 10.89it/s]\n",
      "epoch-67  lr=['0.0098112'], tr/val_loss:  0.972772/  1.591938, tr:  75.49%, val:  65.42%, val_best:  65.42%: 100%|██████████| 62/62 [00:05<00:00, 11.24it/s]\n",
      "epoch-68  lr=['0.0092759'], tr/val_loss:  0.753097/  1.599332, tr:  77.73%, val:  61.25%, val_best:  65.42%: 100%|██████████| 62/62 [00:05<00:00, 11.14it/s]\n",
      "epoch-69  lr=['0.0087512'], tr/val_loss:  0.711188/  1.549324, tr:  78.75%, val:  63.75%, val_best:  65.42%: 100%|██████████| 62/62 [00:05<00:00, 11.11it/s]\n",
      "epoch-70  lr=['0.0082376'], tr/val_loss:  0.677825/  1.770532, tr:  79.78%, val:  60.83%, val_best:  65.42%: 100%|██████████| 62/62 [00:05<00:00, 10.82it/s]\n",
      "epoch-71  lr=['0.0077356'], tr/val_loss:  0.659359/  1.494118, tr:  79.67%, val:  67.08%, val_best:  67.08%: 100%|██████████| 62/62 [00:05<00:00, 11.30it/s]\n",
      "epoch-72  lr=['0.0072456'], tr/val_loss:  0.629813/  1.707143, tr:  81.41%, val:  64.58%, val_best:  67.08%: 100%|██████████| 62/62 [00:05<00:00, 11.11it/s]\n",
      "epoch-73  lr=['0.0067683'], tr/val_loss:  0.673006/  1.454865, tr:  78.24%, val:  68.33%, val_best:  68.33%: 100%|██████████| 62/62 [00:05<00:00, 10.69it/s]\n",
      "epoch-74  lr=['0.0063039'], tr/val_loss:  0.625592/  1.710707, tr:  79.06%, val:  66.67%, val_best:  68.33%: 100%|██████████| 62/62 [00:05<00:00, 10.76it/s]\n",
      "epoch-75  lr=['0.0058531'], tr/val_loss:  0.595862/  1.533683, tr:  80.49%, val:  63.33%, val_best:  68.33%: 100%|██████████| 62/62 [00:05<00:00, 10.71it/s]\n",
      "epoch-76  lr=['0.0054162'], tr/val_loss:  0.562335/  1.506891, tr:  81.72%, val:  66.67%, val_best:  68.33%: 100%|██████████| 62/62 [00:05<00:00, 11.07it/s]\n",
      "epoch-77  lr=['0.0049937'], tr/val_loss:  0.563237/  1.494458, tr:  83.76%, val:  68.33%, val_best:  68.33%: 100%|██████████| 62/62 [00:05<00:00, 10.80it/s]\n",
      "epoch-78  lr=['0.0045860'], tr/val_loss:  0.571977/  1.532762, tr:  82.23%, val:  60.83%, val_best:  68.33%: 100%|██████████| 62/62 [00:05<00:00, 11.27it/s]\n",
      "epoch-79  lr=['0.0041935'], tr/val_loss:  0.526454/  1.546744, tr:  84.98%, val:  65.83%, val_best:  68.33%: 100%|██████████| 62/62 [00:05<00:00, 10.92it/s]\n",
      "epoch-80  lr=['0.0038166'], tr/val_loss:  0.525363/  1.583983, tr:  87.64%, val:  66.25%, val_best:  68.33%: 100%|██████████| 62/62 [00:05<00:00, 10.83it/s]\n",
      "epoch-81  lr=['0.0034556'], tr/val_loss:  0.507850/  1.478341, tr:  84.98%, val:  67.92%, val_best:  68.33%: 100%|██████████| 62/62 [00:05<00:00, 10.94it/s]\n",
      "epoch-82  lr=['0.0031109'], tr/val_loss:  0.513567/  1.527485, tr:  84.47%, val:  65.00%, val_best:  68.33%: 100%|██████████| 62/62 [00:05<00:00, 10.98it/s]\n",
      "epoch-83  lr=['0.0027829'], tr/val_loss:  0.506787/  1.538763, tr:  85.90%, val:  67.08%, val_best:  68.33%: 100%|██████████| 62/62 [00:05<00:00, 10.81it/s]\n",
      "epoch-84  lr=['0.0024719'], tr/val_loss:  0.493468/  1.514773, tr:  88.25%, val:  67.92%, val_best:  68.33%: 100%|██████████| 62/62 [00:05<00:00, 11.52it/s]\n",
      "epoch-85  lr=['0.0021781'], tr/val_loss:  0.489974/  1.540928, tr:  86.11%, val:  64.17%, val_best:  68.33%: 100%|██████████| 62/62 [00:05<00:00, 11.00it/s]\n",
      "epoch-86  lr=['0.0019019'], tr/val_loss:  0.472024/  1.510271, tr:  88.87%, val:  67.08%, val_best:  68.33%: 100%|██████████| 62/62 [00:05<00:00, 11.04it/s]\n",
      "epoch-87  lr=['0.0016436'], tr/val_loss:  0.456210/  1.540828, tr:  91.62%, val:  67.08%, val_best:  68.33%: 100%|██████████| 62/62 [00:05<00:00, 11.05it/s]\n",
      "epoch-88  lr=['0.0014033'], tr/val_loss:  0.443554/  1.515515, tr:  92.65%, val:  68.75%, val_best:  68.75%: 100%|██████████| 62/62 [00:05<00:00, 10.86it/s]\n",
      "epoch-89  lr=['0.0011814'], tr/val_loss:  0.435564/  1.526927, tr:  92.54%, val:  67.08%, val_best:  68.75%: 100%|██████████| 62/62 [00:05<00:00, 11.14it/s]\n",
      "epoch-90  lr=['0.0009781'], tr/val_loss:  0.427767/  1.520943, tr:  94.28%, val:  71.25%, val_best:  71.25%: 100%|██████████| 62/62 [00:05<00:00, 10.92it/s]\n",
      "epoch-91  lr=['0.0007935'], tr/val_loss:  0.428744/  1.533481, tr:  94.59%, val:  65.00%, val_best:  71.25%: 100%|██████████| 62/62 [00:05<00:00, 10.84it/s]\n",
      "epoch-92  lr=['0.0006278'], tr/val_loss:  0.421324/  1.528112, tr:  94.59%, val:  67.50%, val_best:  71.25%: 100%|██████████| 62/62 [00:05<00:00, 10.91it/s]\n",
      "epoch-93  lr=['0.0004813'], tr/val_loss:  0.413984/  1.526427, tr:  93.87%, val:  68.75%, val_best:  71.25%: 100%|██████████| 62/62 [00:05<00:00, 10.71it/s]\n",
      "epoch-94  lr=['0.0003540'], tr/val_loss:  0.409560/  1.535020, tr:  96.42%, val:  67.50%, val_best:  71.25%: 100%|██████████| 62/62 [00:05<00:00, 10.82it/s]\n",
      "epoch-95  lr=['0.0002460'], tr/val_loss:  0.409071/  1.532146, tr:  96.12%, val:  68.33%, val_best:  71.25%: 100%|██████████| 62/62 [00:05<00:00, 10.73it/s]\n",
      "epoch-96  lr=['0.0001576'], tr/val_loss:  0.409926/  1.527226, tr:  96.22%, val:  67.92%, val_best:  71.25%: 100%|██████████| 62/62 [00:05<00:00, 10.93it/s]\n",
      "epoch-97  lr=['0.0000887'], tr/val_loss:  0.402995/  1.527731, tr:  96.53%, val:  68.75%, val_best:  71.25%: 100%|██████████| 62/62 [00:05<00:00, 10.82it/s]\n",
      "epoch-98  lr=['0.0000394'], tr/val_loss:  0.407407/  1.524375, tr:  96.63%, val:  68.75%, val_best:  71.25%: 100%|██████████| 62/62 [00:05<00:00, 11.24it/s]\n",
      "epoch-99  lr=['0.0000099'], tr/val_loss:  0.403894/  1.524206, tr:  96.42%, val:  68.33%, val_best:  71.25%: 100%|██████████| 62/62 [00:05<00:00, 10.94it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153978f4ba9f44c9915a0551b5b1c2b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='3.958 MB of 3.958 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>DFA_flag</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▂▄▄▄▄▂▁▄▆▆▃▄▅▅▄▃▅▅▃▅▇▇▅▆▄▅▇▇▆▇▅▇▇▆█▇▇███</td></tr><tr><td>summary_val_acc</td><td>▁▁▃▄▃▄▃▆▅▅▆▆▄▄▆▆▅▆▄▆▆▆▅▆▆▆▇▇▇▇▇█▇▇██████</td></tr><tr><td>tr_acc</td><td>▁▂▂▃▃▄▄▃▄▃▄▄▄▄▄▄▅▄▅▅▅▅▆▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>tr_epoch_loss</td><td>▆▅█▆▅▅▅▅▄▅▄▄▄▄▃▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>val_acc_now</td><td>▁▁▃▄▃▄▃▆▅▅▆▆▄▄▆▆▅▆▄▆▆▆▅▆▆▆▇▇▇▇▇█▇▇██████</td></tr><tr><td>val_loss</td><td>▃▅█▄▃▃▃▂▂▃▂▂▂▂▁▂▂▂▃▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>DFA_flag</td><td>1.0</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.96425</td></tr><tr><td>tr_epoch_loss</td><td>0.40389</td></tr><tr><td>val_acc_best</td><td>0.7125</td></tr><tr><td>val_acc_now</td><td>0.68333</td></tr><tr><td>val_loss</td><td>1.52421</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">laced-sweep-13</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6pqh0xpo' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6pqh0xpo</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240827_050956-6pqh0xpo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wrb5jm7z with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_sWS_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconst2: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdrop_rate: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap_coin: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap_tr: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.016859766931690123\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 1.3928940043938838\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1.6033262472488303\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: CosineAnnealingLR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240827_052001-wrb5jm7z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wrb5jm7z' target=\"_blank\">trim-sweep-14</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wrb5jm7z' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wrb5jm7z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_sWS_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap_tr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap_coin' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'drop_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_hash = 2bbd58b4e0d3c1e9ad501fad8a43feed\n",
      "cache path exists\n",
      "\n",
      "we will exclude the 'other' class. dvsgestrue 10 classes' indices exist. \n",
      "\n",
      "DataParallel(\n",
      "  (module): MY_SNN_FC_sstep(\n",
      "    (layers): MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC_sstep()\n",
      "      (3): SYNAPSE_FC_trace_sstep()\n",
      "      (4): LIF_layer_trace_sstep()\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC_trace_sstep()\n",
      "      (7): LIF_layer_trace_sstep()\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC_trace_sstep()\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "==================================================\n",
      "My Num of PARAMS: 452,010, system's param_num : 452,010\n",
      "Memory: 1.72MiB at 32-bit\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch-0   lr=['0.0168598'], tr/val_loss:  1.867027/  1.577115, tr:  38.20%, val:  52.92%, val_best:  52.92%: 100%|██████████| 62/62 [00:05<00:00, 11.41it/s]\n",
      "epoch-1   lr=['0.0168556'], tr/val_loss:  1.349227/  1.477657, tr:  59.04%, val:  56.67%, val_best:  56.67%: 100%|██████████| 62/62 [00:05<00:00, 11.21it/s]\n",
      "epoch-2   lr=['0.0168431'], tr/val_loss:  1.169714/  1.383806, tr:  63.53%, val:  55.83%, val_best:  56.67%: 100%|██████████| 62/62 [00:05<00:00, 11.32it/s]\n",
      "epoch-3   lr=['0.0168224'], tr/val_loss:  1.023603/  1.230142, tr:  68.64%, val:  65.00%, val_best:  65.00%: 100%|██████████| 62/62 [00:05<00:00, 11.11it/s]\n",
      "epoch-4   lr=['0.0167933'], tr/val_loss:  0.850874/  1.157880, tr:  72.11%, val:  63.75%, val_best:  65.00%: 100%|██████████| 62/62 [00:05<00:00, 11.08it/s]\n",
      "epoch-5   lr=['0.0167560'], tr/val_loss:  0.773049/  1.398859, tr:  74.36%, val:  62.92%, val_best:  65.00%: 100%|██████████| 62/62 [00:05<00:00, 10.75it/s]\n",
      "epoch-6   lr=['0.0167105'], tr/val_loss:  0.739361/  1.325216, tr:  76.20%, val:  64.17%, val_best:  65.00%: 100%|██████████| 62/62 [00:05<00:00, 11.06it/s]\n",
      "epoch-7   lr=['0.0166567'], tr/val_loss:  0.679798/  1.620959, tr:  78.86%, val:  60.00%, val_best:  65.00%: 100%|██████████| 62/62 [00:05<00:00, 11.11it/s]\n",
      "epoch-8   lr=['0.0165949'], tr/val_loss:  0.620291/  1.227409, tr:  83.15%, val:  69.17%, val_best:  69.17%: 100%|██████████| 62/62 [00:05<00:00, 11.27it/s]\n",
      "epoch-9   lr=['0.0165250'], tr/val_loss:  0.429044/  1.407031, tr:  88.05%, val:  72.92%, val_best:  72.92%: 100%|██████████| 62/62 [00:05<00:00, 11.36it/s]\n",
      "epoch-10  lr=['0.0164472'], tr/val_loss:  0.390251/  1.383159, tr:  93.46%, val:  71.67%, val_best:  72.92%: 100%|██████████| 62/62 [00:05<00:00, 11.22it/s]\n",
      "epoch-11  lr=['0.0163614'], tr/val_loss:  0.288656/  1.423967, tr:  95.10%, val:  70.42%, val_best:  72.92%: 100%|██████████| 62/62 [00:05<00:00, 11.49it/s]\n",
      "epoch-12  lr=['0.0162678'], tr/val_loss:  0.223821/  1.289703, tr:  97.04%, val:  79.58%, val_best:  79.58%: 100%|██████████| 62/62 [00:05<00:00, 11.95it/s]\n",
      "epoch-13  lr=['0.0161664'], tr/val_loss:  0.169987/  1.543249, tr:  98.98%, val:  74.58%, val_best:  79.58%: 100%|██████████| 62/62 [00:05<00:00, 11.15it/s]\n",
      "epoch-14  lr=['0.0160575'], tr/val_loss:  0.126042/  1.501390, tr:  99.49%, val:  77.08%, val_best:  79.58%: 100%|██████████| 62/62 [00:05<00:00, 11.06it/s]\n",
      "epoch-15  lr=['0.0159410'], tr/val_loss:  0.070674/  1.565948, tr:  99.90%, val:  79.58%, val_best:  79.58%: 100%|██████████| 62/62 [00:05<00:00, 10.55it/s]\n",
      "epoch-16  lr=['0.0158170'], tr/val_loss:  0.040917/  1.700276, tr: 100.00%, val:  77.08%, val_best:  79.58%: 100%|██████████| 62/62 [00:05<00:00, 10.56it/s]\n",
      "epoch-17  lr=['0.0156858'], tr/val_loss:  0.028176/  1.654794, tr: 100.00%, val:  82.50%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.82it/s]\n",
      "epoch-18  lr=['0.0155475'], tr/val_loss:  0.024682/  1.680303, tr:  99.90%, val:  81.25%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.62it/s]\n",
      "epoch-19  lr=['0.0154021'], tr/val_loss:  0.012704/  1.734593, tr: 100.00%, val:  81.25%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.41it/s]\n",
      "epoch-20  lr=['0.0152498'], tr/val_loss:  0.008932/  1.836773, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.42it/s]\n",
      "epoch-21  lr=['0.0150908'], tr/val_loss:  0.006015/  1.799097, tr: 100.00%, val:  81.25%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.59it/s]\n",
      "epoch-22  lr=['0.0149252'], tr/val_loss:  0.003961/  1.817579, tr: 100.00%, val:  81.25%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.99it/s]\n",
      "epoch-23  lr=['0.0147532'], tr/val_loss:  0.003077/  1.847309, tr: 100.00%, val:  81.25%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.96it/s]\n",
      "epoch-24  lr=['0.0145750'], tr/val_loss:  0.002724/  1.872735, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.81it/s]\n",
      "epoch-25  lr=['0.0143907'], tr/val_loss:  0.002757/  1.891482, tr: 100.00%, val:  81.25%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.10it/s]\n",
      "epoch-26  lr=['0.0142005'], tr/val_loss:  0.002282/  1.904325, tr: 100.00%, val:  80.00%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.07it/s]\n",
      "epoch-27  lr=['0.0140047'], tr/val_loss:  0.001882/  1.915881, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.09it/s]\n",
      "epoch-28  lr=['0.0138033'], tr/val_loss:  0.001618/  1.922691, tr: 100.00%, val:  80.00%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.65it/s]\n",
      "epoch-29  lr=['0.0135966'], tr/val_loss:  0.001590/  1.934718, tr: 100.00%, val:  80.00%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.60it/s]\n",
      "epoch-30  lr=['0.0133848'], tr/val_loss:  0.001452/  1.937090, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.75it/s]\n",
      "epoch-31  lr=['0.0131682'], tr/val_loss:  0.001361/  1.934481, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.81it/s]\n",
      "epoch-32  lr=['0.0129468'], tr/val_loss:  0.001327/  1.967599, tr: 100.00%, val:  80.00%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.10it/s]\n",
      "epoch-33  lr=['0.0127210'], tr/val_loss:  0.001248/  1.979272, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.82it/s]\n",
      "epoch-34  lr=['0.0124910'], tr/val_loss:  0.001237/  1.987508, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.04it/s]\n",
      "epoch-35  lr=['0.0122570'], tr/val_loss:  0.001138/  1.989231, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.93it/s]\n",
      "epoch-36  lr=['0.0120192'], tr/val_loss:  0.001086/  2.006136, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.88it/s]\n",
      "epoch-37  lr=['0.0117778'], tr/val_loss:  0.000984/  2.002276, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.91it/s]\n",
      "epoch-38  lr=['0.0115331'], tr/val_loss:  0.000963/  2.015848, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.69it/s]\n",
      "epoch-39  lr=['0.0112854'], tr/val_loss:  0.000931/  2.035532, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.63it/s]\n",
      "epoch-40  lr=['0.0110349'], tr/val_loss:  0.000876/  2.022703, tr: 100.00%, val:  80.00%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.19it/s]\n",
      "epoch-41  lr=['0.0107817'], tr/val_loss:  0.000854/  2.040436, tr: 100.00%, val:  80.00%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.73it/s]\n",
      "epoch-42  lr=['0.0105263'], tr/val_loss:  0.000821/  2.035227, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.28it/s]\n",
      "epoch-43  lr=['0.0102688'], tr/val_loss:  0.000787/  2.042663, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.66it/s]\n",
      "epoch-44  lr=['0.0100095'], tr/val_loss:  0.000774/  2.059460, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.91it/s]\n",
      "epoch-45  lr=['0.0097486'], tr/val_loss:  0.000722/  2.063525, tr: 100.00%, val:  80.00%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.88it/s]\n",
      "epoch-46  lr=['0.0094864'], tr/val_loss:  0.000669/  2.062108, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.05it/s]\n",
      "epoch-47  lr=['0.0092232'], tr/val_loss:  0.000677/  2.072317, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.09it/s]\n",
      "epoch-48  lr=['0.0089592'], tr/val_loss:  0.000691/  2.076336, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.69it/s]\n",
      "epoch-49  lr=['0.0086947'], tr/val_loss:  0.000788/  2.079664, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.76it/s]\n",
      "epoch-50  lr=['0.0084299'], tr/val_loss:  0.000669/  2.070210, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.06it/s]\n",
      "epoch-51  lr=['0.0081651'], tr/val_loss:  0.000629/  2.079739, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.89it/s]\n",
      "epoch-52  lr=['0.0079006'], tr/val_loss:  0.000614/  2.084664, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.23it/s]\n",
      "epoch-53  lr=['0.0076366'], tr/val_loss:  0.000636/  2.078828, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.48it/s]\n",
      "epoch-54  lr=['0.0073733'], tr/val_loss:  0.000601/  2.091608, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.97it/s]\n",
      "epoch-55  lr=['0.0071112'], tr/val_loss:  0.000595/  2.090932, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.80it/s]\n",
      "epoch-56  lr=['0.0068503'], tr/val_loss:  0.000588/  2.096331, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.13it/s]\n",
      "epoch-57  lr=['0.0065910'], tr/val_loss:  0.000570/  2.097275, tr: 100.00%, val:  80.42%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.14it/s]\n",
      "epoch-58  lr=['0.0063335'], tr/val_loss:  0.000566/  2.101724, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.24it/s]\n",
      "epoch-59  lr=['0.0060780'], tr/val_loss:  0.000572/  2.099816, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.88it/s]\n",
      "epoch-60  lr=['0.0058249'], tr/val_loss:  0.000567/  2.097019, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.85it/s]\n",
      "epoch-61  lr=['0.0055744'], tr/val_loss:  0.000551/  2.101507, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.77it/s]\n",
      "epoch-62  lr=['0.0053266'], tr/val_loss:  0.000533/  2.103657, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.92it/s]\n",
      "epoch-63  lr=['0.0050820'], tr/val_loss:  0.000542/  2.106452, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.94it/s]\n",
      "epoch-64  lr=['0.0048406'], tr/val_loss:  0.000534/  2.107970, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.19it/s]\n",
      "epoch-65  lr=['0.0046028'], tr/val_loss:  0.000533/  2.102680, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.83it/s]\n",
      "epoch-66  lr=['0.0043688'], tr/val_loss:  0.000540/  2.107730, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.08it/s]\n",
      "epoch-67  lr=['0.0041387'], tr/val_loss:  0.000525/  2.112253, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.90it/s]\n",
      "epoch-68  lr=['0.0039129'], tr/val_loss:  0.000528/  2.111864, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.12it/s]\n",
      "epoch-69  lr=['0.0036916'], tr/val_loss:  0.000519/  2.113158, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.10it/s]\n",
      "epoch-70  lr=['0.0034749'], tr/val_loss:  0.000508/  2.112515, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.77it/s]\n",
      "epoch-71  lr=['0.0032631'], tr/val_loss:  0.000516/  2.116413, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.82it/s]\n",
      "epoch-72  lr=['0.0030565'], tr/val_loss:  0.000512/  2.118545, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.87it/s]\n",
      "epoch-73  lr=['0.0028551'], tr/val_loss:  0.000513/  2.115110, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.22it/s]\n",
      "epoch-74  lr=['0.0026592'], tr/val_loss:  0.000496/  2.115980, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.07it/s]\n",
      "epoch-75  lr=['0.0024691'], tr/val_loss:  0.000488/  2.119717, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.04it/s]\n",
      "epoch-76  lr=['0.0022848'], tr/val_loss:  0.000492/  2.122567, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.04it/s]\n",
      "epoch-77  lr=['0.0021065'], tr/val_loss:  0.000501/  2.122530, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.75it/s]\n",
      "epoch-78  lr=['0.0019345'], tr/val_loss:  0.000492/  2.122444, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.02it/s]\n",
      "epoch-79  lr=['0.0017690'], tr/val_loss:  0.000477/  2.121880, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.82it/s]\n",
      "epoch-80  lr=['0.0016100'], tr/val_loss:  0.000479/  2.122418, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.78it/s]\n",
      "epoch-81  lr=['0.0014577'], tr/val_loss:  0.000493/  2.120747, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.81it/s]\n",
      "epoch-82  lr=['0.0013123'], tr/val_loss:  0.000476/  2.124617, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.07it/s]\n",
      "epoch-83  lr=['0.0011739'], tr/val_loss:  0.000470/  2.123701, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.02it/s]\n",
      "epoch-84  lr=['0.0010427'], tr/val_loss:  0.000478/  2.125902, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.96it/s]\n",
      "epoch-85  lr=['0.0009188'], tr/val_loss:  0.000470/  2.127571, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.97it/s]\n",
      "epoch-86  lr=['0.0008023'], tr/val_loss:  0.000473/  2.128020, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.85it/s]\n",
      "epoch-87  lr=['0.0006933'], tr/val_loss:  0.000474/  2.127777, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.75it/s]\n",
      "epoch-88  lr=['0.0005920'], tr/val_loss:  0.000473/  2.128763, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.74it/s]\n",
      "epoch-89  lr=['0.0004984'], tr/val_loss:  0.000475/  2.125298, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.88it/s]\n",
      "epoch-90  lr=['0.0004126'], tr/val_loss:  0.000476/  2.125563, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.83it/s]\n",
      "epoch-91  lr=['0.0003347'], tr/val_loss:  0.000480/  2.125639, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.06it/s]\n",
      "epoch-92  lr=['0.0002648'], tr/val_loss:  0.000477/  2.127079, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.84it/s]\n",
      "epoch-93  lr=['0.0002030'], tr/val_loss:  0.000473/  2.126188, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 10.90it/s]\n",
      "epoch-94  lr=['0.0001493'], tr/val_loss:  0.000478/  2.125624, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.00it/s]\n",
      "epoch-95  lr=['0.0001038'], tr/val_loss:  0.000482/  2.125643, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.05it/s]\n",
      "epoch-96  lr=['0.0000665'], tr/val_loss:  0.000470/  2.124827, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.08it/s]\n",
      "epoch-97  lr=['0.0000374'], tr/val_loss:  0.000478/  2.124836, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.09it/s]\n",
      "epoch-98  lr=['0.0000166'], tr/val_loss:  0.000478/  2.124840, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.27it/s]\n",
      "epoch-99  lr=['0.0000042'], tr/val_loss:  0.000473/  2.124839, tr: 100.00%, val:  80.83%, val_best:  82.50%: 100%|██████████| 62/62 [00:05<00:00, 11.09it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964be4946eb64e1f8b9674a0da8949e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='3.958 MB of 3.958 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>DFA_flag</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▆▆▇███████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▄▃▆▇▇████▇▇▇██████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▄▅▆▇███████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▄▄▆▇▇█████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▄▃▆▇▇████▇▇▇██████████████████████████</td></tr><tr><td>val_loss</td><td>▄▃▁▄▃▂▃▅▅▆▆▆▇▇▇▇▇▇██████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>DFA_flag</td><td>1.0</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00047</td></tr><tr><td>val_acc_best</td><td>0.825</td></tr><tr><td>val_acc_now</td><td>0.80833</td></tr><tr><td>val_loss</td><td>2.12484</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trim-sweep-14</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wrb5jm7z' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wrb5jm7z</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240827_052001-wrb5jm7z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mtvln8et with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_sWS_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconst2: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdrop_rate: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap_coin: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap_tr: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.026168945481113128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3.30789156944782\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1.1867367919118963\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: CosineAnnealingLR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240827_053011-mtvln8et</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mtvln8et' target=\"_blank\">snowy-sweep-15</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mtvln8et' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mtvln8et</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_sWS_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap_tr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap_coin' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'drop_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_hash = 2bbd58b4e0d3c1e9ad501fad8a43feed\n",
      "cache path exists\n",
      "\n",
      "we will exclude the 'other' class. dvsgestrue 10 classes' indices exist. \n",
      "\n",
      "DataParallel(\n",
      "  (module): MY_SNN_FC_sstep(\n",
      "    (layers): MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC_sstep()\n",
      "      (3): SYNAPSE_FC_trace_sstep()\n",
      "      (4): LIF_layer_trace_sstep()\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC_trace_sstep()\n",
      "      (7): LIF_layer_trace_sstep()\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC_trace_sstep()\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "==================================================\n",
      "My Num of PARAMS: 452,010, system's param_num : 452,010\n",
      "Memory: 1.72MiB at 32-bit\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch-0   lr=['0.0261689'], tr/val_loss:  2.038521/  1.947293, tr:  34.63%, val:  39.58%, val_best:  39.58%: 100%|██████████| 62/62 [01:17<00:00,  1.24s/it]\n",
      "epoch-1   lr=['0.0261625'], tr/val_loss:  2.060214/  1.885139, tr:  44.94%, val:  51.25%, val_best:  51.25%: 100%|██████████| 62/62 [01:21<00:00,  1.32s/it]\n",
      "epoch-2   lr=['0.0261431'], tr/val_loss:  1.670390/  3.007578, tr:  56.08%, val:  33.33%, val_best:  51.25%: 100%|██████████| 62/62 [01:22<00:00,  1.33s/it]\n",
      "epoch-3   lr=['0.0261109'], tr/val_loss:  1.653680/  1.535479, tr:  56.18%, val:  60.83%, val_best:  60.83%: 100%|██████████| 62/62 [01:24<00:00,  1.35s/it]\n",
      "epoch-4   lr=['0.0260658'], tr/val_loss:  1.993843/  2.519259, tr:  58.32%, val:  43.33%, val_best:  60.83%: 100%|██████████| 62/62 [01:22<00:00,  1.33s/it]\n",
      "epoch-5   lr=['0.0260079'], tr/val_loss:  1.570503/  1.567227, tr:  59.04%, val:  52.92%, val_best:  60.83%: 100%|██████████| 62/62 [01:23<00:00,  1.34s/it]\n",
      "epoch-6   lr=['0.0259372'], tr/val_loss:  1.229281/  1.967949, tr:  65.27%, val:  47.92%, val_best:  60.83%: 100%|██████████| 62/62 [01:23<00:00,  1.35s/it]\n",
      "epoch-7   lr=['0.0258538'], tr/val_loss:  1.224383/  2.088182, tr:  69.05%, val:  56.67%, val_best:  60.83%: 100%|██████████| 62/62 [01:22<00:00,  1.34s/it]\n",
      "epoch-8   lr=['0.0257579'], tr/val_loss:  1.284393/  1.722472, tr:  67.72%, val:  52.50%, val_best:  60.83%: 100%|██████████| 62/62 [01:23<00:00,  1.34s/it]\n",
      "epoch-9   lr=['0.0256494'], tr/val_loss:  1.307602/  2.237252, tr:  66.60%, val:  59.17%, val_best:  60.83%: 100%|██████████| 62/62 [01:23<00:00,  1.35s/it]\n",
      "epoch-10  lr=['0.0255285'], tr/val_loss:  1.225607/  1.660404, tr:  66.80%, val:  57.50%, val_best:  60.83%: 100%|██████████| 62/62 [01:23<00:00,  1.34s/it]\n",
      "epoch-11  lr=['0.0253954'], tr/val_loss:  1.006689/  1.644784, tr:  70.58%, val:  61.67%, val_best:  61.67%: 100%|██████████| 62/62 [01:21<00:00,  1.31s/it]\n",
      "epoch-12  lr=['0.0252501'], tr/val_loss:  1.312321/  1.497118, tr:  66.91%, val:  61.67%, val_best:  61.67%: 100%|██████████| 62/62 [01:23<00:00,  1.34s/it]\n",
      "epoch-13  lr=['0.0250928'], tr/val_loss:  1.118985/  1.745560, tr:  70.79%, val:  62.50%, val_best:  62.50%: 100%|██████████| 62/62 [01:13<00:00,  1.19s/it]\n",
      "epoch-14  lr=['0.0249237'], tr/val_loss:  1.042523/  1.656693, tr:  69.97%, val:  57.92%, val_best:  62.50%: 100%|██████████| 62/62 [01:18<00:00,  1.27s/it]\n",
      "epoch-15  lr=['0.0247428'], tr/val_loss:  1.225057/  1.498747, tr:  69.05%, val:  63.75%, val_best:  63.75%: 100%|██████████| 62/62 [01:23<00:00,  1.35s/it]\n",
      "epoch-16  lr=['0.0245505'], tr/val_loss:  0.966363/  2.608596, tr:  76.30%, val:  51.25%, val_best:  63.75%: 100%|██████████| 62/62 [01:20<00:00,  1.31s/it]\n",
      "epoch-17  lr=['0.0243468'], tr/val_loss:  1.197643/  1.608552, tr:  72.52%, val:  59.58%, val_best:  63.75%: 100%|██████████| 62/62 [01:22<00:00,  1.33s/it]\n",
      "epoch-18  lr=['0.0241321'], tr/val_loss:  1.028392/  1.547159, tr:  75.08%, val:  64.58%, val_best:  64.58%: 100%|██████████| 62/62 [01:22<00:00,  1.33s/it]\n",
      "epoch-19  lr=['0.0239064'], tr/val_loss:  0.797260/  2.004775, tr:  80.29%, val:  60.42%, val_best:  64.58%: 100%|██████████| 62/62 [01:18<00:00,  1.26s/it]\n",
      "epoch-20  lr=['0.0236700'], tr/val_loss:  0.922018/  1.650774, tr:  77.43%, val:  65.00%, val_best:  65.00%: 100%|██████████| 62/62 [01:21<00:00,  1.32s/it]\n",
      "epoch-21  lr=['0.0234232'], tr/val_loss:  0.827829/  1.791569, tr:  80.39%, val:  64.58%, val_best:  65.00%: 100%|██████████| 62/62 [01:21<00:00,  1.32s/it]\n",
      "epoch-22  lr=['0.0231662'], tr/val_loss:  0.825617/  1.594827, tr:  79.26%, val:  65.83%, val_best:  65.83%: 100%|██████████| 62/62 [01:20<00:00,  1.30s/it]\n",
      "epoch-23  lr=['0.0228993'], tr/val_loss:  0.786389/  1.491143, tr:  81.72%, val:  70.42%, val_best:  70.42%: 100%|██████████| 62/62 [01:24<00:00,  1.36s/it]\n",
      "epoch-24  lr=['0.0226226'], tr/val_loss:  0.646428/  1.826588, tr:  86.41%, val:  60.83%, val_best:  70.42%: 100%|██████████| 62/62 [01:23<00:00,  1.34s/it]\n",
      "epoch-25  lr=['0.0223366'], tr/val_loss:  0.665669/  1.635104, tr:  86.62%, val:  67.92%, val_best:  70.42%: 100%|██████████| 62/62 [01:23<00:00,  1.34s/it]\n",
      "epoch-26  lr=['0.0220414'], tr/val_loss:  0.615204/  1.778404, tr:  89.79%, val:  68.33%, val_best:  70.42%: 100%|██████████| 62/62 [01:22<00:00,  1.33s/it]\n",
      "epoch-27  lr=['0.0217374'], tr/val_loss:  0.611739/  1.649204, tr:  86.31%, val:  70.83%, val_best:  70.83%: 100%|██████████| 62/62 [01:20<00:00,  1.30s/it]\n",
      "epoch-28  lr=['0.0214248'], tr/val_loss:  0.586292/  1.570661, tr:  89.38%, val:  70.00%, val_best:  70.83%: 100%|██████████| 62/62 [01:23<00:00,  1.35s/it]\n",
      "epoch-29  lr=['0.0211040'], tr/val_loss:  0.516551/  1.633668, tr:  89.89%, val:  67.08%, val_best:  70.83%: 100%|██████████| 62/62 [01:21<00:00,  1.32s/it]\n",
      "epoch-30  lr=['0.0207753'], tr/val_loss:  0.575073/  1.799893, tr:  88.97%, val:  66.25%, val_best:  70.83%: 100%|██████████| 62/62 [01:22<00:00,  1.33s/it]\n",
      "epoch-31  lr=['0.0204390'], tr/val_loss:  0.598254/  1.968185, tr:  88.15%, val:  60.00%, val_best:  70.83%: 100%|██████████| 62/62 [01:22<00:00,  1.33s/it]\n",
      "epoch-32  lr=['0.0200955'], tr/val_loss:  0.595613/  1.778873, tr:  88.66%, val:  62.50%, val_best:  70.83%: 100%|██████████| 62/62 [01:18<00:00,  1.27s/it]\n",
      "epoch-33  lr=['0.0197450'], tr/val_loss:  0.483538/  1.599422, tr:  93.56%, val:  70.83%, val_best:  70.83%: 100%|██████████| 62/62 [01:20<00:00,  1.31s/it]\n",
      "epoch-34  lr=['0.0193880'], tr/val_loss:  0.569143/  1.573122, tr:  89.17%, val:  70.42%, val_best:  70.83%: 100%|██████████| 62/62 [01:22<00:00,  1.32s/it]\n",
      "epoch-35  lr=['0.0190247'], tr/val_loss:  0.509872/  1.656860, tr:  91.22%, val:  71.25%, val_best:  71.25%: 100%|██████████| 62/62 [01:21<00:00,  1.31s/it]\n",
      "epoch-36  lr=['0.0186556'], tr/val_loss:  0.461575/  1.469180, tr:  93.56%, val:  70.00%, val_best:  71.25%: 100%|██████████| 62/62 [01:23<00:00,  1.35s/it]\n",
      "epoch-37  lr=['0.0182809'], tr/val_loss:  0.428104/  1.677370, tr:  93.87%, val:  70.42%, val_best:  71.25%: 100%|██████████| 62/62 [01:19<00:00,  1.29s/it]\n",
      "epoch-38  lr=['0.0179012'], tr/val_loss:  0.429772/  1.539304, tr:  93.67%, val:  72.50%, val_best:  72.50%: 100%|██████████| 62/62 [01:24<00:00,  1.37s/it]\n",
      "epoch-39  lr=['0.0175167'], tr/val_loss:  0.388989/  1.711238, tr:  96.32%, val:  67.92%, val_best:  72.50%: 100%|██████████| 62/62 [01:22<00:00,  1.33s/it]\n",
      "epoch-40  lr=['0.0171278'], tr/val_loss:  0.446895/  1.685096, tr:  92.75%, val:  72.50%, val_best:  72.50%: 100%|██████████| 62/62 [01:21<00:00,  1.31s/it]\n",
      "epoch-41  lr=['0.0167349'], tr/val_loss:  0.354273/  1.578971, tr:  96.42%, val:  77.92%, val_best:  77.92%: 100%|██████████| 62/62 [01:21<00:00,  1.31s/it]\n",
      "epoch-42  lr=['0.0163384'], tr/val_loss:  0.337523/  1.619087, tr:  97.34%, val:  75.00%, val_best:  77.92%: 100%|██████████| 62/62 [01:22<00:00,  1.32s/it]\n",
      "epoch-43  lr=['0.0159388'], tr/val_loss:  0.313055/  1.725006, tr:  97.55%, val:  72.92%, val_best:  77.92%: 100%|██████████| 62/62 [01:19<00:00,  1.29s/it]\n",
      "epoch-44  lr=['0.0155363'], tr/val_loss:  0.304100/  1.793035, tr:  97.34%, val:  71.67%, val_best:  77.92%: 100%|██████████| 62/62 [01:17<00:00,  1.25s/it]\n",
      "epoch-45  lr=['0.0151313'], tr/val_loss:  0.304977/  1.665352, tr:  97.14%, val:  74.17%, val_best:  77.92%: 100%|██████████| 62/62 [01:21<00:00,  1.31s/it]\n",
      "epoch-46  lr=['0.0147244'], tr/val_loss:  0.254982/  1.642601, tr:  98.47%, val:  72.08%, val_best:  77.92%: 100%|██████████| 62/62 [01:16<00:00,  1.23s/it]\n",
      "epoch-47  lr=['0.0143158'], tr/val_loss:  0.231965/  1.701421, tr:  98.98%, val:  73.33%, val_best:  77.92%: 100%|██████████| 62/62 [00:57<00:00,  1.08it/s]\n",
      "epoch-48  lr=['0.0139061'], tr/val_loss:  0.230209/  1.731013, tr:  98.77%, val:  73.75%, val_best:  77.92%: 100%|██████████| 62/62 [01:21<00:00,  1.32s/it]\n",
      "epoch-49  lr=['0.0134955'], tr/val_loss:  0.257053/  1.737738, tr:  97.34%, val:  77.92%, val_best:  77.92%: 100%|██████████| 62/62 [01:21<00:00,  1.31s/it]\n",
      "epoch-50  lr=['0.0130845'], tr/val_loss:  0.248204/  1.814704, tr:  97.85%, val:  76.67%, val_best:  77.92%: 100%|██████████| 62/62 [01:20<00:00,  1.30s/it]\n",
      "epoch-51  lr=['0.0126735'], tr/val_loss:  0.174942/  1.767137, tr:  99.39%, val:  74.58%, val_best:  77.92%: 100%|██████████| 62/62 [01:25<00:00,  1.38s/it]\n",
      "epoch-52  lr=['0.0122629'], tr/val_loss:  0.149609/  1.720297, tr:  99.59%, val:  78.33%, val_best:  78.33%: 100%|██████████| 62/62 [01:23<00:00,  1.35s/it]\n",
      "epoch-53  lr=['0.0118531'], tr/val_loss:  0.133881/  1.782339, tr:  99.80%, val:  73.75%, val_best:  78.33%: 100%|██████████| 62/62 [01:20<00:00,  1.30s/it]\n",
      "epoch-54  lr=['0.0114446'], tr/val_loss:  0.115168/  1.786963, tr:  99.90%, val:  77.08%, val_best:  78.33%: 100%|██████████| 62/62 [01:18<00:00,  1.27s/it]\n",
      "epoch-55  lr=['0.0110376'], tr/val_loss:  0.115750/  1.841377, tr: 100.00%, val:  76.67%, val_best:  78.33%: 100%|██████████| 62/62 [01:21<00:00,  1.31s/it]\n",
      "epoch-56  lr=['0.0106327'], tr/val_loss:  0.105854/  1.848035, tr:  99.90%, val:  76.25%, val_best:  78.33%: 100%|██████████| 62/62 [01:23<00:00,  1.35s/it]\n",
      "epoch-57  lr=['0.0102302'], tr/val_loss:  0.098090/  1.891398, tr:  99.90%, val:  74.17%, val_best:  78.33%: 100%|██████████| 62/62 [01:23<00:00,  1.34s/it]\n",
      "epoch-58  lr=['0.0098305'], tr/val_loss:  0.089244/  1.837950, tr:  99.90%, val:  77.50%, val_best:  78.33%: 100%|██████████| 62/62 [01:22<00:00,  1.33s/it]\n",
      "epoch-59  lr=['0.0094340'], tr/val_loss:  0.078693/  1.887561, tr: 100.00%, val:  77.50%, val_best:  78.33%: 100%|██████████| 62/62 [01:04<00:00,  1.05s/it]\n",
      "epoch-60  lr=['0.0090411'], tr/val_loss:  0.071456/  1.884705, tr: 100.00%, val:  78.33%, val_best:  78.33%: 100%|██████████| 62/62 [01:21<00:00,  1.31s/it]\n",
      "epoch-61  lr=['0.0086523'], tr/val_loss:  0.069651/  1.962072, tr: 100.00%, val:  76.67%, val_best:  78.33%: 100%|██████████| 62/62 [01:24<00:00,  1.36s/it]\n",
      "epoch-62  lr=['0.0082678'], tr/val_loss:  0.057339/  1.932833, tr: 100.00%, val:  76.67%, val_best:  78.33%: 100%|██████████| 62/62 [01:22<00:00,  1.33s/it]\n",
      "epoch-63  lr=['0.0078880'], tr/val_loss:  0.057467/  1.859838, tr: 100.00%, val:  80.00%, val_best:  80.00%: 100%|██████████| 62/62 [01:22<00:00,  1.33s/it]\n",
      "epoch-64  lr=['0.0075134'], tr/val_loss:  0.037846/  1.964179, tr: 100.00%, val:  77.92%, val_best:  80.00%: 100%|██████████| 62/62 [01:21<00:00,  1.31s/it]\n",
      "epoch-65  lr=['0.0071442'], tr/val_loss:  0.029902/  2.008354, tr: 100.00%, val:  77.92%, val_best:  80.00%: 100%|██████████| 62/62 [01:22<00:00,  1.34s/it]\n",
      "epoch-66  lr=['0.0067810'], tr/val_loss:  0.026107/  2.017709, tr: 100.00%, val:  77.92%, val_best:  80.00%: 100%|██████████| 62/62 [01:21<00:00,  1.31s/it]\n",
      "epoch-67  lr=['0.0064239'], tr/val_loss:  0.020328/  2.005112, tr: 100.00%, val:  77.92%, val_best:  80.00%: 100%|██████████| 62/62 [01:22<00:00,  1.33s/it]\n",
      "epoch-68  lr=['0.0060735'], tr/val_loss:  0.019264/  1.997076, tr: 100.00%, val:  79.17%, val_best:  80.00%: 100%|██████████| 62/62 [01:22<00:00,  1.33s/it]\n",
      "epoch-69  lr=['0.0057299'], tr/val_loss:  0.017720/  1.990170, tr: 100.00%, val:  77.50%, val_best:  80.00%: 100%|██████████| 62/62 [01:23<00:00,  1.34s/it]\n",
      "epoch-70  lr=['0.0053936'], tr/val_loss:  0.013947/  1.987517, tr: 100.00%, val:  79.58%, val_best:  80.00%: 100%|██████████| 62/62 [01:20<00:00,  1.29s/it]\n",
      "epoch-71  lr=['0.0050649'], tr/val_loss:  0.014008/  1.994052, tr: 100.00%, val:  78.33%, val_best:  80.00%: 100%|██████████| 62/62 [01:20<00:00,  1.30s/it]\n",
      "epoch-72  lr=['0.0047441'], tr/val_loss:  0.011939/  1.996792, tr: 100.00%, val:  79.17%, val_best:  80.00%: 100%|██████████| 62/62 [01:23<00:00,  1.35s/it]\n",
      "epoch-73  lr=['0.0044316'], tr/val_loss:  0.011512/  2.006196, tr: 100.00%, val:  79.17%, val_best:  80.00%: 100%|██████████| 62/62 [01:19<00:00,  1.28s/it]\n",
      "epoch-74  lr=['0.0041275'], tr/val_loss:  0.010496/  2.009615, tr: 100.00%, val:  79.17%, val_best:  80.00%: 100%|██████████| 62/62 [01:21<00:00,  1.32s/it]\n",
      "epoch-75  lr=['0.0038324'], tr/val_loss:  0.009711/  2.038974, tr: 100.00%, val:  78.33%, val_best:  80.00%: 100%|██████████| 62/62 [01:23<00:00,  1.35s/it]\n",
      "epoch-76  lr=['0.0035463'], tr/val_loss:  0.010485/  2.014802, tr: 100.00%, val:  77.50%, val_best:  80.00%: 100%|██████████| 62/62 [01:23<00:00,  1.34s/it]\n",
      "epoch-77  lr=['0.0032697'], tr/val_loss:  0.009331/  2.026882, tr: 100.00%, val:  78.33%, val_best:  80.00%: 100%|██████████| 62/62 [01:23<00:00,  1.34s/it]\n",
      "epoch-78  lr=['0.0030027'], tr/val_loss:  0.008485/  2.045506, tr: 100.00%, val:  77.92%, val_best:  80.00%: 100%|██████████| 62/62 [01:20<00:00,  1.30s/it]\n",
      "epoch-79  lr=['0.0027457'], tr/val_loss:  0.008212/  2.034022, tr: 100.00%, val:  77.92%, val_best:  80.00%: 100%|██████████| 62/62 [01:21<00:00,  1.32s/it]\n",
      "epoch-80  lr=['0.0024989'], tr/val_loss:  0.008013/  2.035675, tr: 100.00%, val:  77.50%, val_best:  80.00%: 100%|██████████| 62/62 [01:22<00:00,  1.33s/it]\n",
      "epoch-81  lr=['0.0022626'], tr/val_loss:  0.008262/  2.030676, tr: 100.00%, val:  77.92%, val_best:  80.00%: 100%|██████████| 62/62 [01:21<00:00,  1.31s/it]\n",
      "epoch-82  lr=['0.0020369'], tr/val_loss:  0.007913/  2.021914, tr: 100.00%, val:  78.75%, val_best:  80.00%: 100%|██████████| 62/62 [01:17<00:00,  1.26s/it]\n",
      "epoch-83  lr=['0.0018221'], tr/val_loss:  0.007577/  2.034258, tr: 100.00%, val:  78.75%, val_best:  80.00%: 100%|██████████| 62/62 [01:19<00:00,  1.29s/it]\n",
      "epoch-84  lr=['0.0016185'], tr/val_loss:  0.007727/  2.035449, tr: 100.00%, val:  78.33%, val_best:  80.00%: 100%|██████████| 62/62 [01:23<00:00,  1.34s/it]\n",
      "epoch-85  lr=['0.0014261'], tr/val_loss:  0.007445/  2.035864, tr: 100.00%, val:  77.92%, val_best:  80.00%: 100%|██████████| 62/62 [01:20<00:00,  1.30s/it]\n",
      "epoch-86  lr=['0.0012453'], tr/val_loss:  0.007424/  2.032886, tr: 100.00%, val:  77.92%, val_best:  80.00%: 100%|██████████| 62/62 [01:23<00:00,  1.35s/it]\n",
      "epoch-87  lr=['0.0010761'], tr/val_loss:  0.007857/  2.037049, tr: 100.00%, val:  77.50%, val_best:  80.00%: 100%|██████████| 62/62 [01:20<00:00,  1.30s/it]\n",
      "epoch-88  lr=['0.0009188'], tr/val_loss:  0.007172/  2.047230, tr: 100.00%, val:  77.92%, val_best:  80.00%: 100%|██████████| 62/62 [01:20<00:00,  1.30s/it]\n",
      "epoch-89  lr=['0.0007735'], tr/val_loss:  0.007274/  2.049049, tr: 100.00%, val:  77.92%, val_best:  80.00%: 100%|██████████| 62/62 [01:23<00:00,  1.35s/it]\n",
      "epoch-90  lr=['0.0006404'], tr/val_loss:  0.007089/  2.052080, tr: 100.00%, val:  77.92%, val_best:  80.00%: 100%|██████████| 62/62 [01:21<00:00,  1.31s/it]\n",
      "epoch-91  lr=['0.0005195'], tr/val_loss:  0.007123/  2.056054, tr: 100.00%, val:  77.92%, val_best:  80.00%: 100%|██████████| 62/62 [01:22<00:00,  1.32s/it]\n",
      "epoch-92  lr=['0.0004111'], tr/val_loss:  0.007046/  2.058850, tr: 100.00%, val:  77.92%, val_best:  80.00%: 100%|██████████| 62/62 [01:22<00:00,  1.34s/it]\n",
      "epoch-93  lr=['0.0003151'], tr/val_loss:  0.007073/  2.059843, tr: 100.00%, val:  77.92%, val_best:  80.00%: 100%|██████████| 62/62 [01:21<00:00,  1.31s/it]\n",
      "epoch-94  lr=['0.0002318'], tr/val_loss:  0.006869/  2.059459, tr: 100.00%, val:  77.92%, val_best:  80.00%: 100%|██████████| 62/62 [01:20<00:00,  1.31s/it]\n",
      "epoch-95  lr=['0.0001611'], tr/val_loss:  0.006838/  2.056612, tr: 100.00%, val:  77.92%, val_best:  80.00%: 100%|██████████| 62/62 [01:18<00:00,  1.26s/it]\n",
      "epoch-96  lr=['0.0001032'], tr/val_loss:  0.006824/  2.056676, tr: 100.00%, val:  77.92%, val_best:  80.00%: 100%|██████████| 62/62 [01:13<00:00,  1.18s/it]\n",
      "epoch-97  lr=['0.0000581'], tr/val_loss:  0.006735/  2.057741, tr: 100.00%, val:  77.92%, val_best:  80.00%: 100%|██████████| 62/62 [01:14<00:00,  1.20s/it]\n",
      "epoch-98  lr=['0.0000258'], tr/val_loss:  0.006799/  2.057756, tr: 100.00%, val:  77.92%, val_best:  80.00%: 100%|██████████| 62/62 [01:18<00:00,  1.26s/it]\n",
      "epoch-99  lr=['0.0000065'], tr/val_loss:  0.006808/  2.057760, tr: 100.00%, val:  77.92%, val_best:  80.00%: 100%|██████████| 62/62 [00:40<00:00,  1.54it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dbe0ce9600b4a2bb3d96e5f40c62a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='3.958 MB of 3.958 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>DFA_flag</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▄▂▃▂▅▆▇█▇█▇▇▇▇▇█▇█████████████████████</td></tr><tr><td>summary_val_acc</td><td>▂▁▃▅▅▅▅▅▅▆▅▆▆▅▇▇▆▇▇▇███▇████████████████</td></tr><tr><td>tr_acc</td><td>▁▃▄▅▄▄▅▅▆▆▇▇▇▇▇▇████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▇█▅▅▅▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇███████████████████████</td></tr><tr><td>val_acc_now</td><td>▂▁▃▅▅▅▅▅▅▆▅▆▆▅▇▇▆▇▇▇███▇████████████████</td></tr><tr><td>val_loss</td><td>▃█▆▄▄▁▂▂▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▃▃▃▃▄▄▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>DFA_flag</td><td>1.0</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00681</td></tr><tr><td>val_acc_best</td><td>0.8</td></tr><tr><td>val_acc_now</td><td>0.77917</td></tr><tr><td>val_loss</td><td>2.05776</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">snowy-sweep-15</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mtvln8et' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mtvln8et</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240827_053011-mtvln8et/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2s3j8swr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_sWS_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconst2: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdrop_rate: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap_coin: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap_tr: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0051505621983643024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2.86088134509193\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1.5427583920259975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: CosineAnnealingLR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240827_074534-2s3j8swr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2s3j8swr' target=\"_blank\">vivid-sweep-19</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2s3j8swr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2s3j8swr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_sWS_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap_tr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap_coin' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'drop_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_hash = 2bbd58b4e0d3c1e9ad501fad8a43feed\n",
      "cache path exists\n",
      "\n",
      "we will exclude the 'other' class. dvsgestrue 10 classes' indices exist. \n",
      "\n",
      "DataParallel(\n",
      "  (module): MY_SNN_FC_sstep(\n",
      "    (layers): MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC_sstep()\n",
      "      (3): SYNAPSE_FC_trace_sstep()\n",
      "      (4): LIF_layer_trace_sstep()\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC_trace_sstep()\n",
      "      (7): LIF_layer_trace_sstep()\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC_trace_sstep()\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "==================================================\n",
      "My Num of PARAMS: 452,010, system's param_num : 452,010\n",
      "Memory: 1.72MiB at 32-bit\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch-0   lr=['0.0051506'], tr/val_loss:  2.238613/  1.931715, tr:  16.65%, val:  35.83%, val_best:  35.83%: 100%|██████████| 62/62 [01:16<00:00,  1.24s/it]\n",
      "epoch-1   lr=['0.0051493'], tr/val_loss:  1.375933/  1.265739, tr:  54.75%, val:  60.42%, val_best:  60.42%: 100%|██████████| 62/62 [01:17<00:00,  1.26s/it]\n",
      "epoch-2   lr=['0.0051455'], tr/val_loss:  1.045261/  1.257417, tr:  64.96%, val:  62.08%, val_best:  62.08%: 100%|██████████| 62/62 [01:16<00:00,  1.24s/it]\n",
      "epoch-3   lr=['0.0051391'], tr/val_loss:  0.932898/  1.085727, tr:  69.87%, val:  67.08%, val_best:  67.08%: 100%|██████████| 62/62 [01:19<00:00,  1.28s/it]\n",
      "epoch-4   lr=['0.0051303'], tr/val_loss:  0.871230/  1.135622, tr:  67.93%, val:  66.67%, val_best:  67.08%: 100%|██████████| 62/62 [01:17<00:00,  1.24s/it]\n",
      "epoch-5   lr=['0.0051189'], tr/val_loss:  0.782904/  1.225011, tr:  73.24%, val:  62.08%, val_best:  67.08%: 100%|██████████| 62/62 [01:19<00:00,  1.28s/it]\n",
      "epoch-6   lr=['0.0051049'], tr/val_loss:  0.714893/  1.117253, tr:  75.89%, val:  65.42%, val_best:  67.08%: 100%|██████████| 62/62 [01:18<00:00,  1.26s/it]\n",
      "epoch-7   lr=['0.0050885'], tr/val_loss:  0.701887/  1.262817, tr:  77.94%, val:  60.00%, val_best:  67.08%: 100%|██████████| 62/62 [01:19<00:00,  1.28s/it]\n",
      "epoch-8   lr=['0.0050697'], tr/val_loss:  0.651753/  1.106906, tr:  80.80%, val:  69.58%, val_best:  69.58%: 100%|██████████| 62/62 [01:18<00:00,  1.27s/it]\n",
      "epoch-9   lr=['0.0050483'], tr/val_loss:  0.476747/  1.307622, tr:  89.07%, val:  70.83%, val_best:  70.83%: 100%|██████████| 62/62 [01:16<00:00,  1.23s/it]\n",
      "epoch-10  lr=['0.0050245'], tr/val_loss:  0.429479/  1.142263, tr:  92.13%, val:  75.00%, val_best:  75.00%: 100%|██████████| 62/62 [01:18<00:00,  1.26s/it]\n",
      "epoch-11  lr=['0.0049983'], tr/val_loss:  0.358793/  1.239842, tr:  93.67%, val:  71.67%, val_best:  75.00%: 100%|██████████| 62/62 [01:16<00:00,  1.23s/it]\n",
      "epoch-12  lr=['0.0049697'], tr/val_loss:  0.326473/  1.217902, tr:  95.10%, val:  78.33%, val_best:  78.33%: 100%|██████████| 62/62 [01:15<00:00,  1.22s/it]\n",
      "epoch-13  lr=['0.0049388'], tr/val_loss:  0.269594/  1.303010, tr:  97.85%, val:  76.25%, val_best:  78.33%: 100%|██████████| 62/62 [01:17<00:00,  1.26s/it]\n",
      "epoch-14  lr=['0.0049055'], tr/val_loss:  0.246554/  1.213490, tr:  97.55%, val:  78.75%, val_best:  78.75%: 100%|██████████| 62/62 [01:18<00:00,  1.26s/it]\n",
      "epoch-15  lr=['0.0048699'], tr/val_loss:  0.165994/  1.232866, tr:  99.49%, val:  82.92%, val_best:  82.92%: 100%|██████████| 62/62 [01:15<00:00,  1.22s/it]\n",
      "epoch-16  lr=['0.0048320'], tr/val_loss:  0.127522/  1.473403, tr:  99.69%, val:  76.25%, val_best:  82.92%: 100%|██████████| 62/62 [01:17<00:00,  1.24s/it]\n",
      "epoch-17  lr=['0.0047919'], tr/val_loss:  0.122832/  1.378357, tr:  99.59%, val:  81.25%, val_best:  82.92%: 100%|██████████| 62/62 [01:16<00:00,  1.24s/it]\n",
      "epoch-18  lr=['0.0047497'], tr/val_loss:  0.081750/  1.313707, tr: 100.00%, val:  83.33%, val_best:  83.33%: 100%|██████████| 62/62 [01:16<00:00,  1.24s/it]\n",
      "epoch-19  lr=['0.0047052'], tr/val_loss:  0.063354/  1.374981, tr:  99.90%, val:  82.92%, val_best:  83.33%: 100%|██████████| 62/62 [01:17<00:00,  1.25s/it]\n",
      "epoch-20  lr=['0.0046587'], tr/val_loss:  0.042146/  1.484289, tr: 100.00%, val:  80.42%, val_best:  83.33%: 100%|██████████| 62/62 [01:18<00:00,  1.26s/it]\n",
      "epoch-21  lr=['0.0046102'], tr/val_loss:  0.031515/  1.478533, tr: 100.00%, val:  80.83%, val_best:  83.33%: 100%|██████████| 62/62 [01:15<00:00,  1.22s/it]\n",
      "epoch-22  lr=['0.0045596'], tr/val_loss:  0.025229/  1.490804, tr: 100.00%, val:  83.75%, val_best:  83.75%: 100%|██████████| 62/62 [01:16<00:00,  1.23s/it]\n",
      "epoch-23  lr=['0.0045070'], tr/val_loss:  0.020209/  1.540331, tr: 100.00%, val:  82.50%, val_best:  83.75%: 100%|██████████| 62/62 [01:16<00:00,  1.24s/it]\n",
      "epoch-24  lr=['0.0044526'], tr/val_loss:  0.019296/  1.586240, tr: 100.00%, val:  80.42%, val_best:  83.75%: 100%|██████████| 62/62 [01:17<00:00,  1.25s/it]\n",
      "epoch-25  lr=['0.0043963'], tr/val_loss:  0.016497/  1.551386, tr: 100.00%, val:  83.33%, val_best:  83.75%: 100%|██████████| 62/62 [01:16<00:00,  1.23s/it]\n",
      "epoch-26  lr=['0.0043382'], tr/val_loss:  0.012816/  1.628090, tr: 100.00%, val:  82.08%, val_best:  83.75%: 100%|██████████| 62/62 [01:16<00:00,  1.24s/it]\n",
      "epoch-27  lr=['0.0042783'], tr/val_loss:  0.010179/  1.601008, tr: 100.00%, val:  82.50%, val_best:  83.75%: 100%|██████████| 62/62 [01:17<00:00,  1.26s/it]\n",
      "epoch-28  lr=['0.0042168'], tr/val_loss:  0.009027/  1.614909, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:17<00:00,  1.24s/it]\n",
      "epoch-29  lr=['0.0041537'], tr/val_loss:  0.009134/  1.620472, tr: 100.00%, val:  83.33%, val_best:  83.75%: 100%|██████████| 62/62 [01:17<00:00,  1.25s/it]\n",
      "epoch-30  lr=['0.0040890'], tr/val_loss:  0.007327/  1.644143, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:14<00:00,  1.20s/it]\n",
      "epoch-31  lr=['0.0040228'], tr/val_loss:  0.007208/  1.649647, tr: 100.00%, val:  83.33%, val_best:  83.75%: 100%|██████████| 62/62 [01:17<00:00,  1.25s/it]\n",
      "epoch-32  lr=['0.0039552'], tr/val_loss:  0.006907/  1.650793, tr: 100.00%, val:  83.33%, val_best:  83.75%: 100%|██████████| 62/62 [01:17<00:00,  1.25s/it]\n",
      "epoch-33  lr=['0.0038862'], tr/val_loss:  0.006373/  1.662883, tr: 100.00%, val:  82.50%, val_best:  83.75%: 100%|██████████| 62/62 [01:18<00:00,  1.27s/it]\n",
      "epoch-34  lr=['0.0038159'], tr/val_loss:  0.005673/  1.681749, tr: 100.00%, val:  83.33%, val_best:  83.75%: 100%|██████████| 62/62 [01:16<00:00,  1.24s/it]\n",
      "epoch-35  lr=['0.0037444'], tr/val_loss:  0.005679/  1.681672, tr: 100.00%, val:  83.33%, val_best:  83.75%: 100%|██████████| 62/62 [01:17<00:00,  1.25s/it]\n",
      "epoch-36  lr=['0.0036718'], tr/val_loss:  0.005639/  1.685816, tr: 100.00%, val:  82.50%, val_best:  83.75%: 100%|██████████| 62/62 [01:18<00:00,  1.26s/it]\n",
      "epoch-37  lr=['0.0035980'], tr/val_loss:  0.005149/  1.700939, tr: 100.00%, val:  82.08%, val_best:  83.75%: 100%|██████████| 62/62 [01:16<00:00,  1.23s/it]\n",
      "epoch-38  lr=['0.0035233'], tr/val_loss:  0.005030/  1.712590, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:18<00:00,  1.27s/it]\n",
      "epoch-39  lr=['0.0034476'], tr/val_loss:  0.005126/  1.712932, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:15<00:00,  1.22s/it]\n",
      "epoch-40  lr=['0.0033711'], tr/val_loss:  0.004684/  1.731434, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:16<00:00,  1.24s/it]\n",
      "epoch-41  lr=['0.0032938'], tr/val_loss:  0.004404/  1.724546, tr: 100.00%, val:  83.33%, val_best:  83.75%: 100%|██████████| 62/62 [01:16<00:00,  1.23s/it]\n",
      "epoch-42  lr=['0.0032157'], tr/val_loss:  0.003948/  1.723883, tr: 100.00%, val:  83.33%, val_best:  83.75%: 100%|██████████| 62/62 [01:16<00:00,  1.23s/it]\n",
      "epoch-43  lr=['0.0031371'], tr/val_loss:  0.003711/  1.728762, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:15<00:00,  1.23s/it]\n",
      "epoch-44  lr=['0.0030578'], tr/val_loss:  0.003904/  1.739784, tr: 100.00%, val:  82.08%, val_best:  83.75%: 100%|██████████| 62/62 [01:16<00:00,  1.24s/it]\n",
      "epoch-45  lr=['0.0029781'], tr/val_loss:  0.003583/  1.736185, tr: 100.00%, val:  82.50%, val_best:  83.75%: 100%|██████████| 62/62 [01:10<00:00,  1.14s/it]\n",
      "epoch-46  lr=['0.0028980'], tr/val_loss:  0.003554/  1.751708, tr: 100.00%, val:  83.33%, val_best:  83.75%: 100%|██████████| 62/62 [01:14<00:00,  1.21s/it]\n",
      "epoch-47  lr=['0.0028176'], tr/val_loss:  0.003166/  1.755352, tr: 100.00%, val:  83.75%, val_best:  83.75%: 100%|██████████| 62/62 [01:17<00:00,  1.25s/it]\n",
      "epoch-48  lr=['0.0027370'], tr/val_loss:  0.002953/  1.757272, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:05<00:00,  1.06s/it]\n",
      "epoch-49  lr=['0.0026562'], tr/val_loss:  0.002934/  1.756167, tr: 100.00%, val:  82.50%, val_best:  83.75%: 100%|██████████| 62/62 [01:16<00:00,  1.24s/it]\n",
      "epoch-50  lr=['0.0025753'], tr/val_loss:  0.002827/  1.758305, tr: 100.00%, val:  83.75%, val_best:  83.75%: 100%|██████████| 62/62 [01:17<00:00,  1.24s/it]\n",
      "epoch-51  lr=['0.0024944'], tr/val_loss:  0.002876/  1.765662, tr: 100.00%, val:  83.75%, val_best:  83.75%: 100%|██████████| 62/62 [01:15<00:00,  1.21s/it]\n",
      "epoch-52  lr=['0.0024136'], tr/val_loss:  0.002712/  1.768160, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:15<00:00,  1.22s/it]\n",
      "epoch-53  lr=['0.0023329'], tr/val_loss:  0.002810/  1.775483, tr: 100.00%, val:  83.75%, val_best:  83.75%: 100%|██████████| 62/62 [01:15<00:00,  1.21s/it]\n",
      "epoch-54  lr=['0.0022525'], tr/val_loss:  0.002625/  1.780950, tr: 100.00%, val:  83.75%, val_best:  83.75%: 100%|██████████| 62/62 [01:17<00:00,  1.25s/it]\n",
      "epoch-55  lr=['0.0021724'], tr/val_loss:  0.002597/  1.779102, tr: 100.00%, val:  83.33%, val_best:  83.75%: 100%|██████████| 62/62 [01:16<00:00,  1.23s/it]\n",
      "epoch-56  lr=['0.0020927'], tr/val_loss:  0.002583/  1.783384, tr: 100.00%, val:  83.33%, val_best:  83.75%: 100%|██████████| 62/62 [01:16<00:00,  1.24s/it]\n",
      "epoch-57  lr=['0.0020135'], tr/val_loss:  0.002528/  1.784615, tr: 100.00%, val:  83.33%, val_best:  83.75%: 100%|██████████| 62/62 [01:14<00:00,  1.21s/it]\n",
      "epoch-58  lr=['0.0019348'], tr/val_loss:  0.002433/  1.790270, tr: 100.00%, val:  83.33%, val_best:  83.75%: 100%|██████████| 62/62 [01:16<00:00,  1.24s/it]\n",
      "epoch-59  lr=['0.0018568'], tr/val_loss:  0.002374/  1.780808, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:16<00:00,  1.24s/it]\n",
      "epoch-60  lr=['0.0017795'], tr/val_loss:  0.002387/  1.789872, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:17<00:00,  1.26s/it]\n",
      "epoch-61  lr=['0.0017029'], tr/val_loss:  0.002530/  1.787685, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:18<00:00,  1.26s/it]\n",
      "epoch-62  lr=['0.0016273'], tr/val_loss:  0.002247/  1.788223, tr: 100.00%, val:  83.33%, val_best:  83.75%: 100%|██████████| 62/62 [01:15<00:00,  1.23s/it]\n",
      "epoch-63  lr=['0.0015525'], tr/val_loss:  0.002248/  1.785943, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:15<00:00,  1.22s/it]\n",
      "epoch-64  lr=['0.0014788'], tr/val_loss:  0.002097/  1.791878, tr: 100.00%, val:  83.75%, val_best:  83.75%: 100%|██████████| 62/62 [01:13<00:00,  1.19s/it]\n",
      "epoch-65  lr=['0.0014061'], tr/val_loss:  0.002170/  1.791546, tr: 100.00%, val:  83.75%, val_best:  83.75%: 100%|██████████| 62/62 [01:14<00:00,  1.20s/it]\n",
      "epoch-66  lr=['0.0013346'], tr/val_loss:  0.002305/  1.788442, tr: 100.00%, val:  83.75%, val_best:  83.75%: 100%|██████████| 62/62 [01:16<00:00,  1.23s/it]\n",
      "epoch-67  lr=['0.0012644'], tr/val_loss:  0.002221/  1.790363, tr: 100.00%, val:  83.75%, val_best:  83.75%: 100%|██████████| 62/62 [01:17<00:00,  1.26s/it]\n",
      "epoch-68  lr=['0.0011954'], tr/val_loss:  0.002229/  1.789973, tr: 100.00%, val:  83.75%, val_best:  83.75%: 100%|██████████| 62/62 [01:17<00:00,  1.25s/it]\n",
      "epoch-69  lr=['0.0011278'], tr/val_loss:  0.002148/  1.789828, tr: 100.00%, val:  83.75%, val_best:  83.75%: 100%|██████████| 62/62 [01:16<00:00,  1.23s/it]\n",
      "epoch-70  lr=['0.0010616'], tr/val_loss:  0.002086/  1.782149, tr: 100.00%, val:  83.33%, val_best:  83.75%: 100%|██████████| 62/62 [01:16<00:00,  1.23s/it]\n",
      "epoch-71  lr=['0.0009969'], tr/val_loss:  0.002193/  1.785456, tr: 100.00%, val:  83.33%, val_best:  83.75%: 100%|██████████| 62/62 [01:16<00:00,  1.23s/it]\n",
      "epoch-72  lr=['0.0009337'], tr/val_loss:  0.002136/  1.786903, tr: 100.00%, val:  83.33%, val_best:  83.75%: 100%|██████████| 62/62 [01:13<00:00,  1.18s/it]\n",
      "epoch-73  lr=['0.0008722'], tr/val_loss:  0.002085/  1.792562, tr: 100.00%, val:  83.33%, val_best:  83.75%: 100%|██████████| 62/62 [01:17<00:00,  1.24s/it]\n",
      "epoch-74  lr=['0.0008124'], tr/val_loss:  0.002167/  1.795413, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:16<00:00,  1.24s/it]\n",
      "epoch-75  lr=['0.0007543'], tr/val_loss:  0.002101/  1.793628, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:17<00:00,  1.25s/it]\n",
      "epoch-76  lr=['0.0006980'], tr/val_loss:  0.002057/  1.793975, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:16<00:00,  1.24s/it]\n",
      "epoch-77  lr=['0.0006435'], tr/val_loss:  0.002170/  1.796085, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:16<00:00,  1.23s/it]\n",
      "epoch-78  lr=['0.0005910'], tr/val_loss:  0.002118/  1.790512, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:17<00:00,  1.25s/it]\n",
      "epoch-79  lr=['0.0005404'], tr/val_loss:  0.002077/  1.793424, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:17<00:00,  1.24s/it]\n",
      "epoch-80  lr=['0.0004918'], tr/val_loss:  0.001991/  1.791808, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:15<00:00,  1.22s/it]\n",
      "epoch-81  lr=['0.0004453'], tr/val_loss:  0.002102/  1.791237, tr: 100.00%, val:  82.50%, val_best:  83.75%: 100%|██████████| 62/62 [01:14<00:00,  1.21s/it]\n",
      "epoch-82  lr=['0.0004009'], tr/val_loss:  0.002055/  1.791129, tr: 100.00%, val:  82.50%, val_best:  83.75%: 100%|██████████| 62/62 [01:16<00:00,  1.23s/it]\n",
      "epoch-83  lr=['0.0003586'], tr/val_loss:  0.001981/  1.793130, tr: 100.00%, val:  82.50%, val_best:  83.75%: 100%|██████████| 62/62 [01:16<00:00,  1.23s/it]\n",
      "epoch-84  lr=['0.0003185'], tr/val_loss:  0.002026/  1.791858, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:16<00:00,  1.23s/it]\n",
      "epoch-85  lr=['0.0002807'], tr/val_loss:  0.002034/  1.793245, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:18<00:00,  1.27s/it]\n",
      "epoch-86  lr=['0.0002451'], tr/val_loss:  0.002033/  1.797555, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:18<00:00,  1.27s/it]\n",
      "epoch-87  lr=['0.0002118'], tr/val_loss:  0.001940/  1.798932, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:17<00:00,  1.24s/it]\n",
      "epoch-88  lr=['0.0001808'], tr/val_loss:  0.002023/  1.798396, tr: 100.00%, val:  83.33%, val_best:  83.75%: 100%|██████████| 62/62 [01:16<00:00,  1.23s/it]\n",
      "epoch-89  lr=['0.0001522'], tr/val_loss:  0.002016/  1.798168, tr: 100.00%, val:  83.33%, val_best:  83.75%: 100%|██████████| 62/62 [01:16<00:00,  1.24s/it]\n",
      "epoch-90  lr=['0.0001260'], tr/val_loss:  0.001923/  1.795558, tr: 100.00%, val:  83.33%, val_best:  83.75%: 100%|██████████| 62/62 [01:18<00:00,  1.27s/it]\n",
      "epoch-91  lr=['0.0001023'], tr/val_loss:  0.002004/  1.796249, tr: 100.00%, val:  83.33%, val_best:  83.75%: 100%|██████████| 62/62 [01:16<00:00,  1.24s/it]\n",
      "epoch-92  lr=['0.0000809'], tr/val_loss:  0.001989/  1.796015, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:16<00:00,  1.23s/it]\n",
      "epoch-93  lr=['0.0000620'], tr/val_loss:  0.001991/  1.796351, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:15<00:00,  1.22s/it]\n",
      "epoch-94  lr=['0.0000456'], tr/val_loss:  0.002003/  1.796710, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:07<00:00,  1.08s/it]\n",
      "epoch-95  lr=['0.0000317'], tr/val_loss:  0.001922/  1.796452, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:19<00:00,  1.29s/it]\n",
      "epoch-96  lr=['0.0000203'], tr/val_loss:  0.001988/  1.796438, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:16<00:00,  1.24s/it]\n",
      "epoch-97  lr=['0.0000114'], tr/val_loss:  0.002003/  1.796433, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:12<00:00,  1.16s/it]\n",
      "epoch-98  lr=['0.0000051'], tr/val_loss:  0.002007/  1.796497, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:14<00:00,  1.19s/it]\n",
      "epoch-99  lr=['0.0000013'], tr/val_loss:  0.001985/  1.796496, tr: 100.00%, val:  82.92%, val_best:  83.75%: 100%|██████████| 62/62 [01:18<00:00,  1.26s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f22d76adade4f0395256286e8094a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='3.958 MB of 3.958 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>DFA_flag</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▆▅████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▆▅▆▇▇█████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▇███████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▄▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▆▆▆▇▇█████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▆▅▆▇▇█████████████████████████████████</td></tr><tr><td>val_loss</td><td>█▂▁▂▃▂▂▃▃▄▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>DFA_flag</td><td>1.0</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00199</td></tr><tr><td>val_acc_best</td><td>0.8375</td></tr><tr><td>val_acc_now</td><td>0.82917</td></tr><tr><td>val_loss</td><td>1.7965</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vivid-sweep-19</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2s3j8swr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2s3j8swr</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240827_074534-2s3j8swr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: eq1lenbu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_sWS_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconst2: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdrop_rate: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap_coin: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap_tr: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.016472635997708256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4.685066204289379\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1.9715467421161776\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: CosineAnnealingLR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240827_095412-eq1lenbu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/eq1lenbu' target=\"_blank\">swift-sweep-22</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/eq1lenbu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/eq1lenbu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_sWS_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap_tr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap_coin' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'drop_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_hash = 2bbd58b4e0d3c1e9ad501fad8a43feed\n",
      "cache path exists\n",
      "\n",
      "we will exclude the 'other' class. dvsgestrue 10 classes' indices exist. \n",
      "\n",
      "DataParallel(\n",
      "  (module): MY_SNN_FC_sstep(\n",
      "    (layers): MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC_sstep()\n",
      "      (3): SYNAPSE_FC_trace_sstep()\n",
      "      (4): LIF_layer_trace_sstep()\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC_trace_sstep()\n",
      "      (7): LIF_layer_trace_sstep()\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC_trace_sstep()\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "==================================================\n",
      "My Num of PARAMS: 452,010, system's param_num : 452,010\n",
      "Memory: 1.72MiB at 32-bit\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch-0   lr=['0.0164726'], tr/val_loss:  2.326156/  2.325486, tr:   9.50%, val:  10.00%, val_best:  10.00%: 100%|██████████| 62/62 [01:07<00:00,  1.09s/it]\n",
      "epoch-1   lr=['0.0164686'], tr/val_loss:  2.322396/  2.324640, tr:   9.91%, val:  10.00%, val_best:  10.00%: 100%|██████████| 62/62 [00:57<00:00,  1.07it/s]\n",
      "epoch-2   lr=['0.0164564'], tr/val_loss:  2.321449/  2.323517, tr:   9.60%, val:  10.00%, val_best:  10.00%: 100%|██████████| 62/62 [00:56<00:00,  1.09it/s]\n",
      "epoch-3   lr=['0.0164361'], tr/val_loss:  2.319231/  2.304495, tr:  12.46%, val:  14.17%, val_best:  14.17%: 100%|██████████| 62/62 [00:53<00:00,  1.15it/s]\n",
      "epoch-4   lr=['0.0164077'], tr/val_loss:  2.099176/  1.991135, tr:  15.22%, val:  17.08%, val_best:  17.08%: 100%|██████████| 62/62 [00:54<00:00,  1.13it/s]\n",
      "epoch-5   lr=['0.0163712'], tr/val_loss:  1.927980/  1.915253, tr:  18.59%, val:  20.00%, val_best:  20.00%: 100%|██████████| 62/62 [00:55<00:00,  1.12it/s]\n",
      "epoch-6   lr=['0.0163267'], tr/val_loss:  1.932764/  1.895811, tr:  19.20%, val:  21.25%, val_best:  21.25%: 100%|██████████| 62/62 [00:58<00:00,  1.06it/s]\n",
      "epoch-7   lr=['0.0162743'], tr/val_loss:  1.770751/  1.781522, tr:  27.07%, val:  30.00%, val_best:  30.00%: 100%|██████████| 62/62 [00:56<00:00,  1.09it/s]\n",
      "epoch-8   lr=['0.0162139'], tr/val_loss:  1.680108/  1.794959, tr:  30.95%, val:  35.42%, val_best:  35.42%: 100%|██████████| 62/62 [00:54<00:00,  1.13it/s]\n",
      "epoch-9   lr=['0.0161456'], tr/val_loss:  1.596297/  1.735081, tr:  36.36%, val:  37.08%, val_best:  37.08%: 100%|██████████| 62/62 [00:55<00:00,  1.11it/s]\n",
      "epoch-10  lr=['0.0160695'], tr/val_loss:  1.401813/  1.521724, tr:  45.05%, val:  46.67%, val_best:  46.67%: 100%|██████████| 62/62 [00:58<00:00,  1.06it/s]\n",
      "epoch-11  lr=['0.0159857'], tr/val_loss:  1.299173/  1.501593, tr:  50.26%, val:  47.08%, val_best:  47.08%: 100%|██████████| 62/62 [00:53<00:00,  1.17it/s]\n",
      "epoch-12  lr=['0.0158943'], tr/val_loss:  1.259863/  1.588949, tr:  53.52%, val:  41.67%, val_best:  47.08%: 100%|██████████| 62/62 [00:57<00:00,  1.07it/s]\n",
      "epoch-13  lr=['0.0157952'], tr/val_loss:  1.226951/  1.539866, tr:  59.86%, val:  54.17%, val_best:  54.17%: 100%|██████████| 62/62 [00:56<00:00,  1.09it/s]\n",
      "epoch-14  lr=['0.0156888'], tr/val_loss:  1.113067/  1.400680, tr:  64.25%, val:  56.25%, val_best:  56.25%: 100%|██████████| 62/62 [00:55<00:00,  1.11it/s]\n",
      "epoch-15  lr=['0.0155749'], tr/val_loss:  1.050318/  1.294478, tr:  69.15%, val:  60.42%, val_best:  60.42%: 100%|██████████| 62/62 [00:57<00:00,  1.08it/s]\n",
      "epoch-16  lr=['0.0154539'], tr/val_loss:  1.098469/  1.504098, tr:  65.07%, val:  52.50%, val_best:  60.42%: 100%|██████████| 62/62 [00:59<00:00,  1.04it/s]\n",
      "epoch-17  lr=['0.0153257'], tr/val_loss:  1.018953/  1.405221, tr:  68.85%, val:  59.17%, val_best:  60.42%: 100%|██████████| 62/62 [00:57<00:00,  1.07it/s]\n",
      "epoch-18  lr=['0.0151905'], tr/val_loss:  1.042771/  1.375620, tr:  71.60%, val:  62.08%, val_best:  62.08%: 100%|██████████| 62/62 [00:55<00:00,  1.12it/s]\n",
      "epoch-19  lr=['0.0150484'], tr/val_loss:  0.956863/  1.477737, tr:  72.73%, val:  58.75%, val_best:  62.08%: 100%|██████████| 62/62 [00:56<00:00,  1.09it/s]\n",
      "epoch-20  lr=['0.0148996'], tr/val_loss:  0.988143/  1.527666, tr:  69.05%, val:  60.42%, val_best:  62.08%: 100%|██████████| 62/62 [00:56<00:00,  1.10it/s]\n",
      "epoch-21  lr=['0.0147443'], tr/val_loss:  0.955887/  1.482992, tr:  71.09%, val:  62.50%, val_best:  62.50%: 100%|██████████| 62/62 [00:55<00:00,  1.12it/s]\n",
      "epoch-22  lr=['0.0145825'], tr/val_loss:  1.058810/  1.485149, tr:  70.07%, val:  61.25%, val_best:  62.50%: 100%|██████████| 62/62 [00:54<00:00,  1.14it/s]\n",
      "epoch-23  lr=['0.0144145'], tr/val_loss:  0.982206/  1.572813, tr:  71.81%, val:  52.08%, val_best:  62.50%: 100%|██████████| 62/62 [00:57<00:00,  1.08it/s]\n",
      "epoch-24  lr=['0.0142403'], tr/val_loss:  0.974861/  1.526529, tr:  70.58%, val:  54.58%, val_best:  62.50%: 100%|██████████| 62/62 [00:56<00:00,  1.10it/s]\n",
      "epoch-25  lr=['0.0140603'], tr/val_loss:  0.962522/  1.444964, tr:  72.11%, val:  55.00%, val_best:  62.50%: 100%|██████████| 62/62 [00:56<00:00,  1.10it/s]\n",
      "epoch-26  lr=['0.0138745'], tr/val_loss:  0.926359/  1.381772, tr:  70.79%, val:  62.92%, val_best:  62.92%: 100%|██████████| 62/62 [00:57<00:00,  1.08it/s]\n",
      "epoch-27  lr=['0.0136831'], tr/val_loss:  0.901874/  1.384437, tr:  74.16%, val:  61.25%, val_best:  62.92%: 100%|██████████| 62/62 [00:53<00:00,  1.16it/s]\n",
      "epoch-28  lr=['0.0134863'], tr/val_loss:  0.880477/  1.328160, tr:  73.44%, val:  63.75%, val_best:  63.75%: 100%|██████████| 62/62 [00:57<00:00,  1.08it/s]\n",
      "epoch-29  lr=['0.0132844'], tr/val_loss:  0.910440/  1.367793, tr:  73.95%, val:  61.67%, val_best:  63.75%: 100%|██████████| 62/62 [00:58<00:00,  1.06it/s]\n",
      "epoch-30  lr=['0.0130775'], tr/val_loss:  0.868561/  1.313383, tr:  72.93%, val:  57.50%, val_best:  63.75%: 100%|██████████| 62/62 [01:23<00:00,  1.35s/it]\n",
      "epoch-31  lr=['0.0128658'], tr/val_loss:  0.870699/  1.328172, tr:  75.69%, val:  61.25%, val_best:  63.75%: 100%|██████████| 62/62 [01:23<00:00,  1.34s/it]\n",
      "epoch-32  lr=['0.0126496'], tr/val_loss:  0.843946/  1.375494, tr:  74.77%, val:  58.75%, val_best:  63.75%: 100%|██████████| 62/62 [01:21<00:00,  1.32s/it]\n",
      "epoch-33  lr=['0.0124289'], tr/val_loss:  0.836159/  1.341855, tr:  75.38%, val:  64.58%, val_best:  64.58%: 100%|██████████| 62/62 [01:20<00:00,  1.30s/it]\n",
      "epoch-34  lr=['0.0122042'], tr/val_loss:  0.827158/  1.287526, tr:  76.10%, val:  65.00%, val_best:  65.00%: 100%|██████████| 62/62 [01:24<00:00,  1.36s/it]\n",
      "epoch-35  lr=['0.0119755'], tr/val_loss:  0.858207/  1.318651, tr:  74.97%, val:  65.00%, val_best:  65.00%: 100%|██████████| 62/62 [01:21<00:00,  1.31s/it]\n",
      "epoch-36  lr=['0.0117432'], tr/val_loss:  0.808406/  1.289530, tr:  75.79%, val:  64.17%, val_best:  65.00%: 100%|██████████| 62/62 [01:23<00:00,  1.35s/it]\n",
      "epoch-37  lr=['0.0115074'], tr/val_loss:  0.805258/  1.320914, tr:  74.97%, val:  61.67%, val_best:  65.00%: 100%|██████████| 62/62 [01:21<00:00,  1.32s/it]\n",
      "epoch-38  lr=['0.0112683'], tr/val_loss:  0.784310/  1.350873, tr:  77.43%, val:  65.42%, val_best:  65.42%: 100%|██████████| 62/62 [01:12<00:00,  1.16s/it]\n",
      "epoch-39  lr=['0.0110263'], tr/val_loss:  0.800176/  1.293882, tr:  75.69%, val:  63.33%, val_best:  65.42%: 100%|██████████| 62/62 [00:56<00:00,  1.09it/s]\n",
      "epoch-40  lr=['0.0107815'], tr/val_loss:  0.780530/  1.298483, tr:  77.43%, val:  62.92%, val_best:  65.42%: 100%|██████████| 62/62 [00:44<00:00,  1.40it/s]\n",
      "epoch-41  lr=['0.0105342'], tr/val_loss:  0.758250/  1.305793, tr:  81.00%, val:  67.92%, val_best:  67.92%: 100%|██████████| 62/62 [00:16<00:00,  3.78it/s]\n",
      "epoch-42  lr=['0.0102846'], tr/val_loss:  0.754479/  1.367396, tr:  77.94%, val:  65.83%, val_best:  67.92%: 100%|██████████| 62/62 [00:41<00:00,  1.49it/s]\n",
      "epoch-43  lr=['0.0100330'], tr/val_loss:  0.724029/  1.292869, tr:  81.61%, val:  69.17%, val_best:  69.17%: 100%|██████████| 62/62 [00:55<00:00,  1.13it/s]\n",
      "epoch-44  lr=['0.0097797'], tr/val_loss:  0.698086/  1.427515, tr:  82.94%, val:  62.50%, val_best:  69.17%: 100%|██████████| 62/62 [00:59<00:00,  1.04it/s]\n",
      "epoch-45  lr=['0.0095248'], tr/val_loss:  0.709951/  1.365888, tr:  81.61%, val:  68.75%, val_best:  69.17%: 100%|██████████| 62/62 [00:56<00:00,  1.10it/s]\n",
      "epoch-46  lr=['0.0092686'], tr/val_loss:  0.725449/  1.385759, tr:  84.68%, val:  61.67%, val_best:  69.17%: 100%|██████████| 62/62 [00:56<00:00,  1.09it/s]\n",
      "epoch-47  lr=['0.0090114'], tr/val_loss:  0.695330/  1.404153, tr:  85.19%, val:  60.42%, val_best:  69.17%: 100%|██████████| 62/62 [00:57<00:00,  1.08it/s]\n",
      "epoch-48  lr=['0.0087535'], tr/val_loss:  0.683438/  1.432103, tr:  83.76%, val:  64.17%, val_best:  69.17%: 100%|██████████| 62/62 [00:57<00:00,  1.08it/s]\n",
      "epoch-49  lr=['0.0084950'], tr/val_loss:  0.660295/  1.374534, tr:  85.60%, val:  69.17%, val_best:  69.17%: 100%|██████████| 62/62 [00:55<00:00,  1.13it/s]\n",
      "epoch-50  lr=['0.0082363'], tr/val_loss:  0.649242/  1.389214, tr:  86.93%, val:  66.25%, val_best:  69.17%: 100%|██████████| 62/62 [00:53<00:00,  1.16it/s]\n",
      "epoch-51  lr=['0.0079776'], tr/val_loss:  0.636442/  1.458657, tr:  89.07%, val:  67.08%, val_best:  69.17%: 100%|██████████| 62/62 [00:56<00:00,  1.11it/s]\n",
      "epoch-52  lr=['0.0077192'], tr/val_loss:  0.644741/  1.444789, tr:  89.38%, val:  68.75%, val_best:  69.17%: 100%|██████████| 62/62 [00:54<00:00,  1.14it/s]\n",
      "epoch-53  lr=['0.0074612'], tr/val_loss:  0.587818/  1.410498, tr:  91.32%, val:  65.00%, val_best:  69.17%: 100%|██████████| 62/62 [00:54<00:00,  1.13it/s]\n",
      "epoch-54  lr=['0.0072040'], tr/val_loss:  0.608155/  1.385175, tr:  92.44%, val:  69.58%, val_best:  69.58%: 100%|██████████| 62/62 [00:51<00:00,  1.20it/s]\n",
      "epoch-55  lr=['0.0069479'], tr/val_loss:  0.623589/  1.424662, tr:  91.93%, val:  70.42%, val_best:  70.42%: 100%|██████████| 62/62 [00:52<00:00,  1.19it/s]\n",
      "epoch-56  lr=['0.0066930'], tr/val_loss:  0.588304/  1.432605, tr:  92.13%, val:  69.17%, val_best:  70.42%: 100%|██████████| 62/62 [00:55<00:00,  1.11it/s]\n",
      "epoch-57  lr=['0.0064396'], tr/val_loss:  0.562629/  1.434005, tr:  93.05%, val:  70.00%, val_best:  70.42%: 100%|██████████| 62/62 [00:54<00:00,  1.13it/s]\n",
      "epoch-58  lr=['0.0061880'], tr/val_loss:  0.561993/  1.476269, tr:  94.38%, val:  67.92%, val_best:  70.42%: 100%|██████████| 62/62 [00:53<00:00,  1.16it/s]\n",
      "epoch-59  lr=['0.0059385'], tr/val_loss:  0.545300/  1.451629, tr:  92.44%, val:  70.42%, val_best:  70.42%: 100%|██████████| 62/62 [00:55<00:00,  1.11it/s]\n",
      "epoch-60  lr=['0.0056912'], tr/val_loss:  0.543671/  1.430291, tr:  95.71%, val:  69.58%, val_best:  70.42%: 100%|██████████| 62/62 [00:52<00:00,  1.19it/s]\n",
      "epoch-61  lr=['0.0054464'], tr/val_loss:  0.549347/  1.550469, tr:  94.99%, val:  61.25%, val_best:  70.42%: 100%|██████████| 62/62 [00:57<00:00,  1.07it/s]\n",
      "epoch-62  lr=['0.0052043'], tr/val_loss:  0.538621/  1.417418, tr:  95.71%, val:  71.25%, val_best:  71.25%: 100%|██████████| 62/62 [00:53<00:00,  1.17it/s]\n",
      "epoch-63  lr=['0.0049653'], tr/val_loss:  0.519626/  1.443541, tr:  95.40%, val:  69.58%, val_best:  71.25%: 100%|██████████| 62/62 [00:57<00:00,  1.09it/s]\n",
      "epoch-64  lr=['0.0047295'], tr/val_loss:  0.480059/  1.454502, tr:  96.53%, val:  68.75%, val_best:  71.25%: 100%|██████████| 62/62 [00:55<00:00,  1.12it/s]\n",
      "epoch-65  lr=['0.0044971'], tr/val_loss:  0.481274/  1.472849, tr:  96.94%, val:  68.75%, val_best:  71.25%: 100%|██████████| 62/62 [00:58<00:00,  1.06it/s]\n",
      "epoch-66  lr=['0.0042684'], tr/val_loss:  0.465864/  1.464368, tr:  97.24%, val:  71.25%, val_best:  71.25%: 100%|██████████| 62/62 [00:57<00:00,  1.08it/s]\n",
      "epoch-67  lr=['0.0040437'], tr/val_loss:  0.455725/  1.474711, tr:  96.63%, val:  70.00%, val_best:  71.25%: 100%|██████████| 62/62 [00:56<00:00,  1.09it/s]\n",
      "epoch-68  lr=['0.0038231'], tr/val_loss:  0.458125/  1.494722, tr:  96.42%, val:  68.33%, val_best:  71.25%: 100%|██████████| 62/62 [00:54<00:00,  1.14it/s]\n",
      "epoch-69  lr=['0.0036068'], tr/val_loss:  0.440168/  1.485407, tr:  97.96%, val:  69.17%, val_best:  71.25%: 100%|██████████| 62/62 [00:55<00:00,  1.11it/s]\n",
      "epoch-70  lr=['0.0033951'], tr/val_loss:  0.408420/  1.552880, tr:  98.16%, val:  67.92%, val_best:  71.25%: 100%|██████████| 62/62 [00:57<00:00,  1.08it/s]\n",
      "epoch-71  lr=['0.0031882'], tr/val_loss:  0.413534/  1.561597, tr:  97.55%, val:  68.75%, val_best:  71.25%: 100%|██████████| 62/62 [00:55<00:00,  1.12it/s]\n",
      "epoch-72  lr=['0.0029863'], tr/val_loss:  0.397537/  1.545230, tr:  98.26%, val:  70.83%, val_best:  71.25%: 100%|██████████| 62/62 [00:53<00:00,  1.15it/s]\n",
      "epoch-73  lr=['0.0027895'], tr/val_loss:  0.386326/  1.566310, tr:  98.47%, val:  67.92%, val_best:  71.25%: 100%|██████████| 62/62 [00:57<00:00,  1.09it/s]\n",
      "epoch-74  lr=['0.0025982'], tr/val_loss:  0.366317/  1.530923, tr:  98.37%, val:  71.67%, val_best:  71.67%: 100%|██████████| 62/62 [00:56<00:00,  1.10it/s]\n",
      "epoch-75  lr=['0.0024124'], tr/val_loss:  0.344316/  1.549682, tr:  98.88%, val:  69.58%, val_best:  71.67%: 100%|██████████| 62/62 [00:52<00:00,  1.19it/s]\n",
      "epoch-76  lr=['0.0022323'], tr/val_loss:  0.343287/  1.547816, tr:  98.37%, val:  70.00%, val_best:  71.67%: 100%|██████████| 62/62 [00:58<00:00,  1.06it/s]\n",
      "epoch-77  lr=['0.0020582'], tr/val_loss:  0.361181/  1.578691, tr:  98.47%, val:  69.58%, val_best:  71.67%: 100%|██████████| 62/62 [00:52<00:00,  1.18it/s]\n",
      "epoch-78  lr=['0.0018901'], tr/val_loss:  0.338115/  1.570959, tr:  98.98%, val:  72.08%, val_best:  72.08%: 100%|██████████| 62/62 [00:57<00:00,  1.08it/s]\n",
      "epoch-79  lr=['0.0017284'], tr/val_loss:  0.313016/  1.554059, tr:  98.88%, val:  69.58%, val_best:  72.08%: 100%|██████████| 62/62 [00:54<00:00,  1.13it/s]\n",
      "epoch-80  lr=['0.0015730'], tr/val_loss:  0.305279/  1.568457, tr:  98.88%, val:  72.08%, val_best:  72.08%: 100%|██████████| 62/62 [00:57<00:00,  1.08it/s]\n",
      "epoch-81  lr=['0.0014242'], tr/val_loss:  0.297065/  1.579163, tr:  98.88%, val:  70.42%, val_best:  72.08%: 100%|██████████| 62/62 [00:56<00:00,  1.11it/s]\n",
      "epoch-82  lr=['0.0012822'], tr/val_loss:  0.288934/  1.595224, tr:  99.28%, val:  70.00%, val_best:  72.08%: 100%|██████████| 62/62 [00:57<00:00,  1.08it/s]\n",
      "epoch-83  lr=['0.0011470'], tr/val_loss:  0.278006/  1.594818, tr:  99.39%, val:  71.67%, val_best:  72.08%: 100%|██████████| 62/62 [00:55<00:00,  1.11it/s]\n",
      "epoch-84  lr=['0.0010188'], tr/val_loss:  0.270726/  1.636256, tr:  99.39%, val:  70.83%, val_best:  72.08%: 100%|██████████| 62/62 [00:58<00:00,  1.07it/s]\n",
      "epoch-85  lr=['0.0008977'], tr/val_loss:  0.265377/  1.616222, tr:  99.59%, val:  71.25%, val_best:  72.08%: 100%|██████████| 62/62 [00:46<00:00,  1.35it/s]\n",
      "epoch-86  lr=['0.0007839'], tr/val_loss:  0.265068/  1.596433, tr:  99.28%, val:  70.00%, val_best:  72.08%: 100%|██████████| 62/62 [00:23<00:00,  2.67it/s]\n",
      "epoch-87  lr=['0.0006774'], tr/val_loss:  0.263416/  1.638039, tr:  99.49%, val:  71.67%, val_best:  72.08%: 100%|██████████| 62/62 [00:57<00:00,  1.08it/s]\n",
      "epoch-88  lr=['0.0005784'], tr/val_loss:  0.255155/  1.618386, tr:  99.39%, val:  71.67%, val_best:  72.08%: 100%|██████████| 62/62 [00:49<00:00,  1.25it/s]\n",
      "epoch-89  lr=['0.0004869'], tr/val_loss:  0.252795/  1.625605, tr:  99.49%, val:  71.25%, val_best:  72.08%: 100%|██████████| 62/62 [00:56<00:00,  1.10it/s]\n",
      "epoch-90  lr=['0.0004031'], tr/val_loss:  0.252023/  1.624431, tr:  99.49%, val:  72.08%, val_best:  72.08%: 100%|██████████| 62/62 [00:54<00:00,  1.14it/s]\n",
      "epoch-91  lr=['0.0003270'], tr/val_loss:  0.251849/  1.633487, tr:  99.49%, val:  71.25%, val_best:  72.08%: 100%|██████████| 62/62 [00:59<00:00,  1.05it/s]\n",
      "epoch-92  lr=['0.0002588'], tr/val_loss:  0.250162/  1.638441, tr:  99.39%, val:  72.08%, val_best:  72.08%: 100%|██████████| 62/62 [00:53<00:00,  1.17it/s]\n",
      "epoch-93  lr=['0.0001984'], tr/val_loss:  0.247952/  1.631745, tr:  99.39%, val:  71.67%, val_best:  72.08%: 100%|██████████| 62/62 [00:58<00:00,  1.06it/s]\n",
      "epoch-94  lr=['0.0001459'], tr/val_loss:  0.244842/  1.629434, tr:  99.39%, val:  71.67%, val_best:  72.08%: 100%|██████████| 62/62 [00:55<00:00,  1.12it/s]\n",
      "epoch-95  lr=['0.0001014'], tr/val_loss:  0.242128/  1.634121, tr:  99.39%, val:  70.83%, val_best:  72.08%: 100%|██████████| 62/62 [00:56<00:00,  1.10it/s]\n",
      "epoch-96  lr=['0.0000649'], tr/val_loss:  0.241100/  1.637227, tr:  99.39%, val:  70.42%, val_best:  72.08%: 100%|██████████| 62/62 [00:19<00:00,  3.19it/s]\n",
      "epoch-97  lr=['0.0000366'], tr/val_loss:  0.240851/  1.638202, tr:  99.39%, val:  70.42%, val_best:  72.08%: 100%|██████████| 62/62 [00:08<00:00,  6.91it/s]\n",
      "epoch-98  lr=['0.0000163'], tr/val_loss:  0.245190/  1.638732, tr:  99.39%, val:  70.42%, val_best:  72.08%: 100%|██████████| 62/62 [00:29<00:00,  2.13it/s]\n",
      "epoch-99  lr=['0.0000041'], tr/val_loss:  0.240832/  1.636619, tr:  99.39%, val:  70.42%, val_best:  72.08%: 100%|██████████| 62/62 [00:56<00:00,  1.10it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b777d6ebde4178988bb673da3637b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='3.958 MB of 3.958 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>DFA_flag</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▁▁▂▄▅▆▇▇▇▇▇▆▇▇▇▇▇█▇▇▇▇▇████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▁▂▃▄▅▆▇▆▇▆▇▇▆▇▇▇▇▇▇█████▇██████████████</td></tr><tr><td>tr_acc</td><td>▁▁▁▂▃▄▅▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇███████████████</td></tr><tr><td>tr_epoch_loss</td><td>██▇▆▆▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▂▃▄▅▆▇▇▇▇▇▇▇▇▇▇███████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▂▃▄▅▆▇▆▇▆▇▇▆▇▇▇▇▇▇█████▇██████████████</td></tr><tr><td>val_loss</td><td>██▆▄▄▃▂▂▂▂▃▂▂▂▁▁▁▁▂▂▂▂▂▂▂▃▂▂▃▃▃▃▃▃▃▃▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>DFA_flag</td><td>1.0</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99387</td></tr><tr><td>tr_epoch_loss</td><td>0.24083</td></tr><tr><td>val_acc_best</td><td>0.72083</td></tr><tr><td>val_acc_now</td><td>0.70417</td></tr><tr><td>val_loss</td><td>1.63662</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">swift-sweep-22</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/eq1lenbu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/eq1lenbu</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240827_095412-eq1lenbu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: d6pmimmf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_sWS_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconst2: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdrop_rate: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap_coin: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \te_transport_swap_tr: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.006707002111414242\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5.432229653009065\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1.5371469503001631\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: CosineAnnealingLR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240827_112853-d6pmimmf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/d6pmimmf' target=\"_blank\">copper-sweep-24</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/91td8vzi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/d6pmimmf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/d6pmimmf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_sWS_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap_tr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'e_transport_swap_coin' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'drop_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_hash = 2bbd58b4e0d3c1e9ad501fad8a43feed\n",
      "cache path exists\n",
      "\n",
      "we will exclude the 'other' class. dvsgestrue 10 classes' indices exist. \n",
      "\n",
      "DataParallel(\n",
      "  (module): MY_SNN_FC_sstep(\n",
      "    (layers): MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC_sstep()\n",
      "      (3): SYNAPSE_FC_trace_sstep()\n",
      "      (4): LIF_layer_trace_sstep()\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC_trace_sstep()\n",
      "      (7): LIF_layer_trace_sstep()\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC_trace_sstep()\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "==================================================\n",
      "My Num of PARAMS: 452,010, system's param_num : 452,010\n",
      "Memory: 1.72MiB at 32-bit\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch-0   lr=['0.0067070'], tr/val_loss:  2.313099/  2.309137, tr:   9.19%, val:  10.00%, val_best:  10.00%: 100%|██████████| 62/62 [00:55<00:00,  1.11it/s]\n",
      "epoch-1   lr=['0.0067053'], tr/val_loss:  2.057918/  1.597583, tr:  25.33%, val:  48.33%, val_best:  48.33%: 100%|██████████| 62/62 [00:54<00:00,  1.14it/s]\n",
      "epoch-2   lr=['0.0067004'], tr/val_loss:  1.321591/  1.358511, tr:  58.02%, val:  55.42%, val_best:  55.42%: 100%|██████████| 62/62 [00:55<00:00,  1.12it/s]\n",
      "epoch-3   lr=['0.0066921'], tr/val_loss:  1.091063/  1.259992, tr:  65.68%, val:  60.83%, val_best:  60.83%: 100%|██████████| 62/62 [00:54<00:00,  1.13it/s]\n",
      "epoch-4   lr=['0.0066806'], tr/val_loss:  1.041639/  1.269620, tr:  63.33%, val:  62.08%, val_best:  62.08%: 100%|██████████| 62/62 [00:55<00:00,  1.13it/s]\n",
      "epoch-5   lr=['0.0066657'], tr/val_loss:  0.960158/  1.300857, tr:  69.66%, val:  60.83%, val_best:  62.08%: 100%|██████████| 62/62 [00:59<00:00,  1.05it/s]\n",
      "epoch-6   lr=['0.0066476'], tr/val_loss:  0.883951/  1.219812, tr:  69.36%, val:  62.92%, val_best:  62.92%: 100%|██████████| 62/62 [00:58<00:00,  1.06it/s]\n",
      "epoch-7   lr=['0.0066262'], tr/val_loss:  0.874601/  1.332113, tr:  70.68%, val:  58.75%, val_best:  62.92%: 100%|██████████| 62/62 [00:57<00:00,  1.07it/s]\n",
      "epoch-8   lr=['0.0066016'], tr/val_loss:  0.830870/  1.151220, tr:  71.81%, val:  68.75%, val_best:  68.75%: 100%|██████████| 62/62 [00:52<00:00,  1.18it/s]\n",
      "epoch-9   lr=['0.0065738'], tr/val_loss:  0.750134/  1.326817, tr:  75.69%, val:  63.75%, val_best:  68.75%: 100%|██████████| 62/62 [00:55<00:00,  1.11it/s]\n",
      "epoch-10  lr=['0.0065429'], tr/val_loss:  0.732945/  1.217338, tr:  76.61%, val:  63.33%, val_best:  68.75%: 100%|██████████| 62/62 [00:57<00:00,  1.08it/s]\n",
      "epoch-11  lr=['0.0065087'], tr/val_loss:  0.668440/  1.188399, tr:  77.94%, val:  64.17%, val_best:  68.75%: 100%|██████████| 62/62 [00:58<00:00,  1.06it/s]\n",
      "epoch-12  lr=['0.0064715'], tr/val_loss:  0.646981/  1.114117, tr:  84.47%, val:  70.42%, val_best:  70.42%: 100%|██████████| 62/62 [00:55<00:00,  1.12it/s]\n",
      "epoch-13  lr=['0.0064312'], tr/val_loss:  0.638953/  1.221816, tr:  82.33%, val:  64.17%, val_best:  70.42%: 100%|██████████| 62/62 [00:59<00:00,  1.05it/s]\n",
      "epoch-14  lr=['0.0063878'], tr/val_loss:  0.599045/  1.240705, tr:  84.07%, val:  67.92%, val_best:  70.42%: 100%|██████████| 62/62 [00:58<00:00,  1.07it/s]\n",
      "epoch-15  lr=['0.0063415'], tr/val_loss:  0.595815/  1.146595, tr:  85.50%, val:  69.17%, val_best:  70.42%: 100%|██████████| 62/62 [00:54<00:00,  1.15it/s]\n",
      "epoch-16  lr=['0.0062922'], tr/val_loss:  0.533937/  1.126244, tr:  86.72%, val:  77.08%, val_best:  77.08%: 100%|██████████| 62/62 [00:58<00:00,  1.07it/s]\n",
      "epoch-17  lr=['0.0062400'], tr/val_loss:  0.465148/  1.194115, tr:  90.19%, val:  71.25%, val_best:  77.08%: 100%|██████████| 62/62 [00:54<00:00,  1.13it/s]\n",
      "epoch-18  lr=['0.0061850'], tr/val_loss:  0.472277/  1.214438, tr:  92.34%, val:  75.00%, val_best:  77.08%: 100%|██████████| 62/62 [00:53<00:00,  1.16it/s]\n",
      "epoch-19  lr=['0.0061271'], tr/val_loss:  0.411107/  1.329423, tr:  96.12%, val:  73.75%, val_best:  77.08%: 100%|██████████| 62/62 [00:56<00:00,  1.10it/s]\n",
      "epoch-20  lr=['0.0060665'], tr/val_loss:  0.422632/  1.288402, tr:  94.59%, val:  72.08%, val_best:  77.08%: 100%|██████████| 62/62 [00:58<00:00,  1.06it/s]\n",
      "epoch-21  lr=['0.0060033'], tr/val_loss:  0.362351/  1.312795, tr:  96.12%, val:  75.42%, val_best:  77.08%: 100%|██████████| 62/62 [00:55<00:00,  1.11it/s]\n",
      "epoch-22  lr=['0.0059374'], tr/val_loss:  0.364462/  1.253387, tr:  94.28%, val:  76.67%, val_best:  77.08%: 100%|██████████| 62/62 [00:56<00:00,  1.11it/s]\n",
      "epoch-23  lr=['0.0058690'], tr/val_loss:  0.303317/  1.243266, tr:  97.65%, val:  78.33%, val_best:  78.33%: 100%|██████████| 62/62 [00:57<00:00,  1.08it/s]\n",
      "epoch-24  lr=['0.0057981'], tr/val_loss:  0.299602/  1.334210, tr:  98.06%, val:  71.25%, val_best:  78.33%: 100%|██████████| 62/62 [00:56<00:00,  1.09it/s]\n",
      "epoch-25  lr=['0.0057248'], tr/val_loss:  0.287298/  1.346075, tr:  98.37%, val:  72.92%, val_best:  78.33%: 100%|██████████| 62/62 [00:55<00:00,  1.13it/s]\n",
      "epoch-26  lr=['0.0056491'], tr/val_loss:  0.262367/  1.301698, tr:  98.88%, val:  75.00%, val_best:  78.33%: 100%|██████████| 62/62 [00:53<00:00,  1.16it/s]\n",
      "epoch-27  lr=['0.0055712'], tr/val_loss:  0.230744/  1.334599, tr:  99.59%, val:  78.33%, val_best:  78.33%: 100%|██████████| 62/62 [00:56<00:00,  1.10it/s]\n",
      "epoch-28  lr=['0.0054911'], tr/val_loss:  0.239410/  1.323550, tr:  99.18%, val:  77.08%, val_best:  78.33%: 100%|██████████| 62/62 [00:55<00:00,  1.12it/s]\n",
      "epoch-29  lr=['0.0054089'], tr/val_loss:  0.211774/  1.437765, tr:  99.18%, val:  73.75%, val_best:  78.33%: 100%|██████████| 62/62 [00:56<00:00,  1.10it/s]\n",
      "epoch-30  lr=['0.0053246'], tr/val_loss:  0.270166/  1.352030, tr:  98.67%, val:  79.58%, val_best:  79.58%: 100%|██████████| 62/62 [00:52<00:00,  1.18it/s]\n",
      "epoch-31  lr=['0.0052384'], tr/val_loss:  0.218990/  1.376752, tr:  99.28%, val:  77.08%, val_best:  79.58%: 100%|██████████| 62/62 [00:55<00:00,  1.11it/s]\n",
      "epoch-32  lr=['0.0051504'], tr/val_loss:  0.228220/  1.321581, tr:  99.08%, val:  78.33%, val_best:  79.58%: 100%|██████████| 62/62 [00:57<00:00,  1.08it/s]\n",
      "epoch-33  lr=['0.0050606'], tr/val_loss:  0.164570/  1.426838, tr:  99.80%, val:  80.42%, val_best:  80.42%: 100%|██████████| 62/62 [00:28<00:00,  2.14it/s]\n",
      "epoch-34  lr=['0.0049691'], tr/val_loss:  0.130498/  1.429002, tr:  99.59%, val:  78.75%, val_best:  80.42%: 100%|██████████| 62/62 [00:41<00:00,  1.48it/s]\n",
      "epoch-35  lr=['0.0048760'], tr/val_loss:  0.119718/  1.422076, tr:  99.80%, val:  77.92%, val_best:  80.42%: 100%|██████████| 62/62 [00:54<00:00,  1.14it/s]\n",
      "epoch-36  lr=['0.0047814'], tr/val_loss:  0.121624/  1.497400, tr:  99.59%, val:  77.50%, val_best:  80.42%: 100%|██████████| 62/62 [00:58<00:00,  1.05it/s]\n",
      "epoch-37  lr=['0.0046853'], tr/val_loss:  0.080025/  1.513265, tr:  99.90%, val:  76.67%, val_best:  80.42%: 100%|██████████| 62/62 [00:56<00:00,  1.09it/s]\n",
      "epoch-38  lr=['0.0045880'], tr/val_loss:  0.079296/  1.522991, tr: 100.00%, val:  77.92%, val_best:  80.42%: 100%|██████████| 62/62 [00:55<00:00,  1.12it/s]\n",
      "epoch-39  lr=['0.0044895'], tr/val_loss:  0.075558/  1.567997, tr:  99.80%, val:  79.17%, val_best:  80.42%: 100%|██████████| 62/62 [00:54<00:00,  1.13it/s]\n",
      "epoch-40  lr=['0.0043898'], tr/val_loss:  0.103054/  1.499774, tr:  99.80%, val:  79.17%, val_best:  80.42%: 100%|██████████| 62/62 [00:52<00:00,  1.18it/s]\n",
      "epoch-41  lr=['0.0042891'], tr/val_loss:  0.088744/  1.549838, tr:  99.80%, val:  79.58%, val_best:  80.42%: 100%|██████████| 62/62 [00:58<00:00,  1.06it/s]\n",
      "epoch-42  lr=['0.0041875'], tr/val_loss:  0.059151/  1.546451, tr:  99.90%, val:  79.17%, val_best:  80.42%: 100%|██████████| 62/62 [00:53<00:00,  1.16it/s]\n",
      "epoch-43  lr=['0.0040850'], tr/val_loss:  0.050606/  1.580827, tr: 100.00%, val:  80.42%, val_best:  80.42%: 100%|██████████| 62/62 [00:54<00:00,  1.14it/s]\n",
      "epoch-44  lr=['0.0039819'], tr/val_loss:  0.056046/  1.572194, tr: 100.00%, val:  79.17%, val_best:  80.42%: 100%|██████████| 62/62 [00:52<00:00,  1.17it/s]\n",
      "epoch-45  lr=['0.0038781'], tr/val_loss:  0.038757/  1.582134, tr: 100.00%, val:  79.58%, val_best:  80.42%: 100%|██████████| 62/62 [00:55<00:00,  1.11it/s]\n",
      "epoch-46  lr=['0.0037738'], tr/val_loss:  0.033592/  1.638903, tr: 100.00%, val:  80.42%, val_best:  80.42%: 100%|██████████| 62/62 [00:53<00:00,  1.15it/s]\n",
      "epoch-47  lr=['0.0036691'], tr/val_loss:  0.023875/  1.670134, tr: 100.00%, val:  79.58%, val_best:  80.42%: 100%|██████████| 62/62 [00:57<00:00,  1.08it/s]\n",
      "epoch-48  lr=['0.0035641'], tr/val_loss:  0.018985/  1.660183, tr: 100.00%, val:  80.42%, val_best:  80.42%: 100%|██████████| 62/62 [00:50<00:00,  1.24it/s]\n",
      "epoch-49  lr=['0.0034588'], tr/val_loss:  0.015982/  1.678010, tr: 100.00%, val:  80.42%, val_best:  80.42%: 100%|██████████| 62/62 [00:53<00:00,  1.15it/s]\n",
      "epoch-50  lr=['0.0033535'], tr/val_loss:  0.013845/  1.661268, tr: 100.00%, val:  79.17%, val_best:  80.42%: 100%|██████████| 62/62 [00:55<00:00,  1.13it/s]\n",
      "epoch-51  lr=['0.0032482'], tr/val_loss:  0.012642/  1.669117, tr: 100.00%, val:  80.83%, val_best:  80.83%: 100%|██████████| 62/62 [00:53<00:00,  1.15it/s]\n",
      "epoch-52  lr=['0.0031429'], tr/val_loss:  0.010395/  1.672981, tr: 100.00%, val:  79.58%, val_best:  80.83%: 100%|██████████| 62/62 [00:53<00:00,  1.15it/s]\n",
      "epoch-53  lr=['0.0030379'], tr/val_loss:  0.008650/  1.677633, tr: 100.00%, val:  82.08%, val_best:  82.08%: 100%|██████████| 62/62 [00:57<00:00,  1.08it/s]\n",
      "epoch-54  lr=['0.0029332'], tr/val_loss:  0.009230/  1.656765, tr: 100.00%, val:  80.00%, val_best:  82.08%: 100%|██████████| 62/62 [00:52<00:00,  1.17it/s]\n",
      "epoch-55  lr=['0.0028289'], tr/val_loss:  0.007540/  1.691868, tr: 100.00%, val:  80.00%, val_best:  82.08%: 100%|██████████| 62/62 [00:54<00:00,  1.14it/s]\n",
      "epoch-56  lr=['0.0027251'], tr/val_loss:  0.006741/  1.682824, tr: 100.00%, val:  80.00%, val_best:  82.08%: 100%|██████████| 62/62 [00:56<00:00,  1.10it/s]\n",
      "epoch-57  lr=['0.0026220'], tr/val_loss:  0.006037/  1.679899, tr: 100.00%, val:  79.58%, val_best:  82.08%: 100%|██████████| 62/62 [00:53<00:00,  1.15it/s]\n",
      "epoch-58  lr=['0.0025195'], tr/val_loss:  0.005865/  1.690098, tr: 100.00%, val:  79.17%, val_best:  82.08%: 100%|██████████| 62/62 [00:55<00:00,  1.13it/s]\n",
      "epoch-59  lr=['0.0024179'], tr/val_loss:  0.005562/  1.693707, tr: 100.00%, val:  79.17%, val_best:  82.08%: 100%|██████████| 62/62 [00:55<00:00,  1.11it/s]\n",
      "epoch-60  lr=['0.0023172'], tr/val_loss:  0.005296/  1.701393, tr: 100.00%, val:  78.75%, val_best:  82.08%: 100%|██████████| 62/62 [00:55<00:00,  1.11it/s]\n",
      "epoch-61  iter_acc: 100.00%, lr=['0.0022175'], iter_loss:  0.011211, val_best:  82.08%:  48%|████▊     | 30/62 [00:24<00:26,  1.20it/s]"
     ]
    }
   ],
   "source": [
    "# sweep 하는 코드, 위 셀 주석처리 해야 됨.\n",
    "\n",
    "# 이런 워닝 뜨는 거는 걍 너가 main 안에서  wandb.config.update(hyperparameters)할 때 물려서임. 어차피 근데 sweep에서 지정한 걸로 덮어짐 \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "run_name = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes',\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")} DFA',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        \"learning_rate\": {\"min\": 0.001, \"max\": 0.1}, #0.00936191669529645\n",
    "        \"BATCH\": {\"values\": [16]},\n",
    "        \"decay\": {\"values\": [0.25]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [128]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"epoch_num\": {\"values\": [100]},\n",
    "        \"dvs_duration\": {\"values\": [100_000]},\n",
    "        \"dvs_clipping\": {\"values\": [2]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"OTTT_sWS_on\": {\"values\": [False]},\n",
    "        \"const2\": {\"values\": [False]},\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "        \"cfg\": {\"values\": [['M','M',200,200]]},\n",
    "        \"e_transport_swap\": {\"values\": [0]},\n",
    "        \"e_transport_swap_tr\": {\"values\": [0]},\n",
    "        \"drop_rate\": {\"values\": [0.0]}, # \"drop_rate\": {\"values\": [0.25,0.5,0.75]}, #\"drop_rate\": {\"min\": 0.25, \"max\": 0.75},\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "        \"merge_polarities\": {\"values\": [False]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [0]},\n",
    "        \"lif_layer_sg_width\": {\"min\": 0.3, \"max\": 6.0},\n",
    "        \"e_transport_swap_coin\": {\"values\": [1]},\n",
    "        \"lif_layer_v_threshold\": {\"min\": 0.0, \"max\": 2.0},\n",
    "        \"scheduler_name\": {\"values\": ['CosineAnnealingLR']},  # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "        \"denoise_on\": {\"values\": [True]}, \n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    unique_name = unique_name_hyper ## 이거 설정하면 새로운 경로에 모두 save\n",
    "    \n",
    "    wandb.init(save_code = True)\n",
    "    learning_rate  =  wandb.config.learning_rate\n",
    "    BATCH  =  wandb.config.BATCH\n",
    "    decay  =  wandb.config.decay\n",
    "    IMAGE_SIZE  =  wandb.config.IMAGE_SIZE\n",
    "    TIME  =  wandb.config.TIME\n",
    "    epoch_num  =  wandb.config.epoch_num \n",
    "    dvs_duration  =  wandb.config.dvs_duration\n",
    "    dvs_clipping  =  wandb.config.dvs_clipping\n",
    "    which_data  =  wandb.config.which_data\n",
    "    OTTT_sWS_on  =  wandb.config.OTTT_sWS_on\n",
    "    const2  =  wandb.config.const2\n",
    "    surrogate  =  wandb.config.surrogate\n",
    "    DFA_on  =  wandb.config.DFA_on\n",
    "    OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on\n",
    "    cfg  =  wandb.config.cfg\n",
    "    e_transport_swap  =  wandb.config.e_transport_swap\n",
    "    e_transport_swap_tr  =  wandb.config.e_transport_swap_tr\n",
    "    drop_rate  =  wandb.config.drop_rate\n",
    "    exclude_class  =  wandb.config.exclude_class\n",
    "    merge_polarities  =  wandb.config.merge_polarities\n",
    "    lif_layer_v_reset  =  wandb.config.lif_layer_v_reset\n",
    "    lif_layer_sg_width  =  wandb.config.lif_layer_sg_width\n",
    "    e_transport_swap_coin  =  wandb.config.e_transport_swap_coin\n",
    "    lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold\n",
    "    scheduler_name  =  wandb.config.scheduler_name\n",
    "    denoise_on  =  wandb.config.denoise_on\n",
    "    if const2 == True:\n",
    "        const2 = decay\n",
    "    else:\n",
    "        const2 = 0.0\n",
    "\n",
    "    my_snn_system(  devices = \"2\",\n",
    "                single_step = True, # True # False\n",
    "                unique_name = run_name,\n",
    "                my_seed = 42,\n",
    "                TIME = TIME , # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "                BATCH = BATCH, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "                IMAGE_SIZE = IMAGE_SIZE, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "                # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "                #pmnist는 28로 해야 됨. 나머지는 바꿔도 돌아는 감.\n",
    "\n",
    "                # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "                which_data = which_data,\n",
    "# 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "# 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','N_CALTECH101','n_tidigits','heidelberg'\n",
    "                # CLASS_NUM = 10,\n",
    "                data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "                rate_coding = False, # True # False\n",
    "                lif_layer_v_init = 0.0,\n",
    "                lif_layer_v_decay = decay,\n",
    "                lif_layer_v_threshold = lif_layer_v_threshold,  # 10000이상으로 하면 NDA LIF 씀. #nda 0.5  #ottt 1.0\n",
    "                lif_layer_v_reset = lif_layer_v_reset, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "                lif_layer_sg_width = lif_layer_sg_width, # # surrogate sigmoid 쓸 때는 의미없음\n",
    "\n",
    "                # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                synapse_conv_kernel_size = 3,\n",
    "                synapse_conv_stride = 1,\n",
    "                synapse_conv_padding = 1,\n",
    "                synapse_conv_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "                synapse_conv_trace_const2 = const2, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "                # synapse_fc_out_features = CLASS_NUM,\n",
    "                synapse_fc_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "                synapse_fc_trace_const2 = const2, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "                pre_trained = False, # True # False\n",
    "                convTrue_fcFalse = False, # True # False\n",
    "\n",
    "                # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "                # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "                # cfg = [64, 64],\n",
    "                # cfg = [64, 124, 64, 124],\n",
    "                # cfg = ['M','M',512], \n",
    "                # cfg = [512], \n",
    "                # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "                # cfg = ['M','M',200,200],\n",
    "                # cfg = [200,200],\n",
    "                cfg = cfg,\n",
    "                # cfg = [12], #fc\n",
    "                # cfg = [12, 'M', 48, 'M', 12], \n",
    "                # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "                # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "                # cfg = [20001,10001], # depthwise, separable\n",
    "                # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "                # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "                # cfg = [], \n",
    "                \n",
    "                net_print = True, # True # False # True로 하길 추천\n",
    "                weight_count_print = False, # True # False\n",
    "                \n",
    "                pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "                learning_rate = learning_rate, # default 0.001  # ottt 0.1 # nda 0.001 \n",
    "                epoch_num = epoch_num,\n",
    "                verbose_interval = 999999999, #숫자 크게 하면 꺼짐 #걍 중간중간 iter에서 끊어서 출력\n",
    "                validation_interval =  999999999,#999999999, #숫자 크게 하면 에포크 마지막 iter 때 val 함\n",
    "\n",
    "                tdBN_on = False,  # True # False\n",
    "                BN_on = False,  # True # False\n",
    "                \n",
    "                surrogate = surrogate, # 'rectangle' 'sigmoid' 'rough_rectangle'\n",
    "                \n",
    "                gradient_verbose = False,  # True # False  # weight gradient 각 layer마다 띄워줌\n",
    "\n",
    "                BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "                optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                scheduler_name = scheduler_name, # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "                ddp_on = False,   # True # False \n",
    "                # 지원 DATASET: cifar10, mnist\n",
    "\n",
    "                nda_net = False,   # True # False\n",
    "\n",
    "                domain_il_epoch = 0, # over 0, then domain il mode on # pmnist 쓸거면 HLOP 코드보고 더 디벨롭하셈. 지금 개발 hold함.\n",
    "                \n",
    "                dvs_clipping = dvs_clipping, # 숫자만큼 크면 spike 아니면 걍 0\n",
    "                # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "\n",
    "                dvs_duration = dvs_duration, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # 있는 데이터들 #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "                # 한 숫자가 1us인듯 (spikingjelly코드에서)\n",
    "                # 한 장에 50 timestep만 생산함. 싫으면 my_snn/trying/spikingjelly_dvsgesture의__init__.py 를 참고해봐\n",
    "\n",
    "                OTTT_sWS_on = OTTT_sWS_on, # True # False # BPTT끄고, CONV에만 적용됨.\n",
    "\n",
    "                DFA_on = DFA_on, # True # False # residual은 dfa지원안함.\n",
    "                OTTT_input_trace_on = OTTT_input_trace_on, # True # False # 맨 처음 input에 trace 적용\n",
    "                 \n",
    "                e_transport_swap = e_transport_swap, # 1 이상이면 해당 숫자 에포크만큼 val_acc_best가 변화가 없으면 e_transport scheme (BP vs DFA) swap\n",
    "                e_transport_swap_tr = e_transport_swap_tr, # 1 이상이면 해당 숫자 에포크만큼 tr_acc_best가 변화가 없으면 e_transport scheme (BP vs DFA) swap\n",
    "                e_transport_swap_coin = e_transport_swap_coin, # swap할 수 있는 coin 개수\n",
    "                    \n",
    "                drop_rate = drop_rate,\n",
    "\n",
    "                exclude_class = exclude_class, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                merge_polarities = merge_polarities, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                denoise_on = denoise_on,\n",
    "                    ) \n",
    "    # sigmoid와 BN이 있어야 잘된다.\n",
    "    # average pooling\n",
    "    # 이 낫다. \n",
    "    \n",
    "    # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import json\n",
    "# run_name = 'main_FINAL_TEST'\n",
    "\n",
    "# unique_name = run_name\n",
    "# def pad_array_to_match_length(array1, array2):\n",
    "#     if len(array1) > len(array2):\n",
    "#         padded_array2 = np.pad(array2, (0, len(array1) - len(array2)), 'constant')\n",
    "#         return array1, padded_array2\n",
    "#     elif len(array2) > len(array1):\n",
    "#         padded_array1 = np.pad(array1, (0, len(array2) - len(array1)), 'constant')\n",
    "#         return padded_array1, array2\n",
    "#     else:\n",
    "#         return array1, array2\n",
    "# def load_hyperparameters(filename=f'result_save/hyperparameters_{unique_name}.json'):\n",
    "#     with open(filename, 'r') as f:\n",
    "#         return json.load(f)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# current_time = '20240628_110116'\n",
    "# base_name = f'{current_time}'\n",
    "# iter_acc_file_name = f'result_save/{base_name}_iter_acc_array_{unique_name}.npy'\n",
    "# val_acc_file_name = f'result_save/{base_name}_val_acc_now_array_{unique_name}.npy'\n",
    "# hyperparameters_file_name = f'result_save/{base_name}_hyperparameters_{unique_name}.json'\n",
    "\n",
    "# ### if you want to just see most recent train and val acc###########################\n",
    "# iter_acc_file_name = f'result_save/iter_acc_array_{unique_name}.npy'\n",
    "# tr_acc_file_name = f'result_save/tr_acc_array_{unique_name}.npy'\n",
    "# val_acc_file_name = f'result_save/val_acc_now_array_{unique_name}.npy'\n",
    "# hyperparameters_file_name = f'result_save/hyperparameters_{unique_name}.json'\n",
    "\n",
    "# loaded_iter_acc_array = np.load(iter_acc_file_name)*100\n",
    "# loaded_tr_acc_array = np.load(tr_acc_file_name)*100\n",
    "# loaded_val_acc_array = np.load(val_acc_file_name)*100\n",
    "# hyperparameters = load_hyperparameters(hyperparameters_file_name)\n",
    "\n",
    "# loaded_iter_acc_array, loaded_val_acc_array = pad_array_to_match_length(loaded_iter_acc_array, loaded_val_acc_array)\n",
    "# loaded_iter_acc_array, loaded_tr_acc_array = pad_array_to_match_length(loaded_iter_acc_array, loaded_tr_acc_array)\n",
    "# loaded_val_acc_array, loaded_tr_acc_array = pad_array_to_match_length(loaded_val_acc_array, loaded_tr_acc_array)\n",
    "\n",
    "# top_iter_acc = np.max(loaded_iter_acc_array)\n",
    "# top_tr_acc = np.max(loaded_tr_acc_array)\n",
    "# top_val_acc = np.max(loaded_val_acc_array)\n",
    "\n",
    "# which_data = hyperparameters['which_data']\n",
    "# BPTT_on = hyperparameters['BPTT_on']\n",
    "# current_epoch = hyperparameters['current epoch']\n",
    "# surrogate = hyperparameters['surrogate']\n",
    "# cfg = hyperparameters['cfg']\n",
    "# tdBN_on = hyperparameters['tdBN_on']\n",
    "# BN_on = hyperparameters['BN_on']\n",
    "\n",
    "\n",
    "# iterations = np.arange(len(loaded_iter_acc_array))\n",
    "\n",
    "# # 그래프 그리기\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(iterations, loaded_iter_acc_array, label='Iter Accuracy', color='g', alpha=0.2)\n",
    "# plt.plot(iterations, loaded_tr_acc_array, label='Training Accuracy', color='b')\n",
    "# plt.plot(iterations, loaded_val_acc_array, label='Validation Accuracy', color='r')\n",
    "\n",
    "# # # 텍스트 추가\n",
    "# # plt.text(0.05, 0.95, f'Top Training Accuracy: {100*top_iter_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='left', color='blue')\n",
    "# # plt.text(0.05, 0.90, f'Top Validation Accuracy: {100*top_val_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='left', color='red')\n",
    "# # 텍스트 추가\n",
    "# plt.text(0.5, 0.10, f'Top Training Accuracy: {top_tr_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='center', color='blue')\n",
    "# plt.text(0.5, 0.05, f'Top Validation Accuracy: {top_val_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='center', color='red')\n",
    "\n",
    "# plt.xlabel('Iterations')\n",
    "# plt.ylabel('Accuracy [%]')\n",
    "\n",
    "# # 그래프 제목에 하이퍼파라미터 정보 추가\n",
    "# title = f'Training and Validation Accuracy over Iterations\\n\\nData: {which_data}, BPTT: {\"On\" if BPTT_on else \"Off\"}, Current Epoch: {current_epoch}, Surrogate: {surrogate},\\nCFG: {cfg}, tdBN: {\"On\" if tdBN_on else \"Off\"}, BN: {\"On\" if BN_on else \"Off\"}'\n",
    "\n",
    "# plt.title(title)\n",
    "\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.xlim(0)  # x축을 0부터 시작\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
