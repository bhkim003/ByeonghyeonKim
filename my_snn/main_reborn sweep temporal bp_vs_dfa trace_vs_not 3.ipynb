{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20284/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' 레퍼런스\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loader에서 train dataset을 몇개 더 쓸건지 \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "                    ):\n",
    "    ## 함수 내 모든 로컬 변수 저장 ########################################################\n",
    "    hyperparameters = locals()\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFA랑 single_step공존하게해라'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb 세팅 ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader 가져오기 ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        net.load_state_dict(torch.load(pre_trained_path))\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter logging해줌\n",
    "    ############################################################\n",
    "\n",
    "\n",
    "    ## criterion ########################################## # loss 구해주는 친구\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    #     criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "        ####### iterator : input_loading & tqdm을 통한 progress_bar 생성###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train 모드로 바꿔줘야함\n",
    "\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "                \n",
    "            ## batch 크기 ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # 차원 전처리\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel 차원은 그대로 두고, Height, Width 차원에 대해서만 pooling 적용\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, 1).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    slice_bucket.append(slice_concat)\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs 데이터 시각화 코드 (확인 필요할 시 써라)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            ## gradient 초기화 #######################################\n",
    "            optimizer.zero_grad()\n",
    "            ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first input도 ottt trace 적용하기 위한 코드 (validation 시에는 필요X) ##########################\n",
    "                if OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight 업데이트!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "                optimizer.step() # full step time update\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # ottt꺼 쓸때\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net 그림 출력해보기 #################################################################\n",
    "            # print('시각화')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch 어긋남 방지 ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval 모드로 바꿔줘야함 \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel 차원은 그대로 두고, Height, Width 차원에 대해서만 pooling 적용\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, 1).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                slice_bucket.append(slice_concat)\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network 연산 시작 ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb 키면 state_dict아닌거는 저장 안됨\n",
    "                    # network save\n",
    "                    # torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            wandb.log({\"iter_acc\": iter_acc})\n",
    "            wandb.log({\"tr_acc\": tr_acc})\n",
    "            wandb.log({\"val_acc_now\": val_acc_now})\n",
    "            wandb.log({\"val_acc_best\": val_acc_best})\n",
    "            wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "            wandb.log({\"epoch\": epoch})\n",
    "            wandb.log({\"val_loss\": val_loss}) \n",
    "            wandb.log({\"tr_epoch_loss\": tr_epoch_loss})   \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.25 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "# const2 = False # True # False\n",
    "\n",
    "# unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "# run_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "\n",
    "# if const2 == True:\n",
    "#     const2 = decay\n",
    "# else:\n",
    "#     const2 = 0.0\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"4\",\n",
    "#                 single_step = True, # True # False # DFA_on이랑 같이 가라\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 42,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "#                 BATCH = 16, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 128, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.720291189014991,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "#                 lif_layer_sg_width = 3.555718888923306, # 2.570969004857107 # sigmoid류에서는 alpha값 4.0, rectangle류에서는 width값 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "#                 synapse_trace_const2 = const2, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # True로 하길 추천\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 10000,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 2, #일반적으로 1 또는 2 # 100ms때는 5 # 숫자만큼 크면 spike 아니면 걍 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # 있는 데이터들 #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # 한 숫자가 1us인듯 (spikingjelly코드에서)\n",
    "#                 # 한 장에 50 timestep만 생산함. 싫으면 my_snn/trying/spikingjelly_dvsgesture의__init__.py 를 참고해봐\n",
    "#                 # nmnist 5_000us, gesture는 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = False, # True # False # single_step이랑 같이 켜야 됨.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # 맨 처음 input에 trace 적용\n",
    "\n",
    "#                 exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "#                 merge_polarities = False, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "#                 denoise_on = True, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = 0, \n",
    "\n",
    "#                 num_workers = 2, # local wsl에서는 2가 맞고, 서버에서는 4가 좋더라.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = True, # True # False \n",
    "\n",
    "#                 last_lif = False,\n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 8,\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoid와 BN이 있어야 잘된다.\n",
    "# # average pooling  \n",
    "# # 이 낫다. \n",
    "\n",
    "# # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: hfzt99hb\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: efmqv2ja with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_025257-efmqv2ja</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/efmqv2ja' target=\"_blank\">stellar-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/efmqv2ja' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/efmqv2ja</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = da7e4b41c275f954fb82e708e43d2b96\n",
      "cache path doesn't exist\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=2560, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 554,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  3.816577/  9.224413, val:  10.00%, val_best:  10.00%, tr:  23.29%, tr_best:  23.29%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss: 10.265903/ 10.183496, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  23.29%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss: 10.229430/ 19.633413, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  23.29%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss: 12.068563/  9.933391, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  23.29%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss: 12.065926/ 11.243317, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  23.29%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss: 11.902559/  8.068377, val:  10.00%, val_best:  10.00%, tr:   9.70%, tr_best:  23.29%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss: 10.110721/  7.608846, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  23.29%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss: 10.240868/ 14.719351, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  23.29%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss: 12.368999/ 10.220062, val:  10.00%, val_best:  10.00%, tr:   8.38%, tr_best:  23.29%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss: 12.946035/ 16.410894, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:  23.29%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss: 11.024445/ 19.521387, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  23.29%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss: 12.335892/ 16.348295, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  23.29%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss: 12.460314/ 16.095903, val:  10.00%, val_best:  10.00%, tr:   7.66%, tr_best:  23.29%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss: 11.094676/  9.630296, val:  10.00%, val_best:  10.00%, tr:  10.42%, tr_best:  23.29%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss: 11.463687/ 10.520581, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  23.29%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss: 10.992767/  9.881374, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  23.29%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss: 12.012166/ 16.772087, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  23.29%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss: 11.395016/ 10.993787, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  23.29%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss: 12.408901/  9.379910, val:  10.00%, val_best:  10.00%, tr:  10.93%, tr_best:  23.29%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss: 11.683361/ 10.163478, val:  10.00%, val_best:  10.00%, tr:  10.42%, tr_best:  23.29%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss: 10.910822/  8.016787, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  23.29%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss: 13.516822/ 18.591354, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  23.29%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss: 11.447764/ 12.580864, val:  10.00%, val_best:  10.00%, tr:  10.52%, tr_best:  23.29%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss: 10.408908/ 17.501329, val:  10.00%, val_best:  10.00%, tr:  10.52%, tr_best:  23.29%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss: 14.813812/ 12.511620, val:  10.00%, val_best:  10.00%, tr:  11.54%, tr_best:  23.29%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss: 11.094462/  5.725636, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  23.29%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss: 11.034593/ 12.921040, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  23.29%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  8.904439/ 10.503982, val:  10.00%, val_best:  10.00%, tr:  11.85%, tr_best:  23.29%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss: 10.991140/  9.958029, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  23.29%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss: 10.802541/ 16.055315, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  23.29%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss: 12.915079/  9.156981, val:  10.00%, val_best:  10.00%, tr:   9.70%, tr_best:  23.29%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss: 11.229650/  9.253305, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  23.29%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss: 12.182318/ 10.306610, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  23.29%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss: 10.003729/  8.599459, val:  10.00%, val_best:  10.00%, tr:  10.52%, tr_best:  23.29%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss: 11.253503/ 15.084232, val:  10.00%, val_best:  10.00%, tr:  10.73%, tr_best:  23.29%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss: 12.411062/  8.874342, val:  10.00%, val_best:  10.00%, tr:  10.42%, tr_best:  23.29%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss: 12.882584/ 13.350202, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  23.29%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss: 11.313136/  6.972295, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  23.29%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  9.951011/ 12.775389, val:  10.00%, val_best:  10.00%, tr:  10.62%, tr_best:  23.29%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss: 10.964597/  9.800778, val:  10.00%, val_best:  10.00%, tr:  10.32%, tr_best:  23.29%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss: 11.003362/ 15.106037, val:  10.00%, val_best:  10.00%, tr:  12.36%, tr_best:  23.29%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  9.898082/  9.180943, val:  10.00%, val_best:  10.00%, tr:  10.32%, tr_best:  23.29%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss: 13.967167/ 11.761126, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:  23.29%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss: 14.174373/ 16.651588, val:  10.00%, val_best:  10.00%, tr:   7.66%, tr_best:  23.29%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss: 14.048003/  8.771622, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  23.29%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  9.285038/ 11.565020, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  23.29%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss: 11.801933/ 16.441250, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  23.29%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss: 12.180390/ 10.692225, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  23.29%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss: 11.340115/ 16.537891, val:  10.00%, val_best:  10.00%, tr:   8.48%, tr_best:  23.29%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss: 10.227480/  9.982451, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  23.29%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss: 10.468593/ 16.539585, val:  10.00%, val_best:  10.00%, tr:  10.62%, tr_best:  23.29%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss: 13.754898/  9.853133, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  23.29%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  8.809390/ 12.447160, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  23.29%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  9.787917/ 12.934870, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  23.29%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss: 10.782920/ 10.952007, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  23.29%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss: 13.397847/ 10.292568, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  23.29%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss: 10.823785/ 11.232095, val:  10.00%, val_best:  10.00%, tr:  12.16%, tr_best:  23.29%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss: 12.649265/  6.856357, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  23.29%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  8.400353/ 11.228507, val:  10.00%, val_best:  10.00%, tr:  11.24%, tr_best:  23.29%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss: 11.873000/ 16.169241, val:  10.00%, val_best:  10.00%, tr:  11.03%, tr_best:  23.29%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss: 10.770951/ 10.924600, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  23.29%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss: 12.370618/  6.410310, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  23.29%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  9.841721/  7.928586, val:  10.00%, val_best:  10.00%, tr:  10.73%, tr_best:  23.29%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss: 11.806448/ 13.141810, val:  10.00%, val_best:  10.00%, tr:  10.73%, tr_best:  23.29%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss: 11.971811/  9.826357, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  23.29%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss: 12.418515/  8.829686, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  23.29%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss: 11.684547/ 15.288352, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  23.29%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss: 15.196220/ 13.513825, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  23.29%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss: 11.436631/  9.026327, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  23.29%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss: 12.003304/ 10.044843, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  23.29%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss: 12.219344/ 15.892039, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  23.29%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss: 14.893333/ 13.013752, val:  10.00%, val_best:  10.00%, tr:  10.73%, tr_best:  23.29%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss: 11.736781/  5.647391, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  23.29%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss: 12.197814/  8.132871, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  23.29%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  9.477413/  9.010823, val:  10.00%, val_best:  10.00%, tr:  11.85%, tr_best:  23.29%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss: 11.226193/  9.601423, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  23.29%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss: 12.401328/  8.722935, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  23.29%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss: 12.227106/  8.930194, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  23.29%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss: 13.371003/  6.153965, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  23.29%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss: 10.370906/ 11.163114, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  23.29%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss: 11.377023/  8.082865, val:  10.00%, val_best:  10.00%, tr:  12.05%, tr_best:  23.29%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  9.112804/  8.032090, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  23.29%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss: 12.568291/  8.947873, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  23.29%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss: 13.878269/ 15.577116, val:  10.00%, val_best:  10.00%, tr:  10.52%, tr_best:  23.29%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss: 14.097907/ 10.403996, val:  10.00%, val_best:  10.00%, tr:  10.32%, tr_best:  23.29%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  9.919678/ 11.467978, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  23.29%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  9.682568/  8.970672, val:  10.00%, val_best:  10.00%, tr:  10.83%, tr_best:  23.29%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss: 12.644728/ 12.738609, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  23.29%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss: 12.116467/ 24.244865, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  23.29%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss: 13.008943/ 14.564604, val:  10.00%, val_best:  10.00%, tr:  10.62%, tr_best:  23.29%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss: 10.228969/  9.422357, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  23.29%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss: 11.951541/ 11.152939, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  23.29%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  9.906940/ 13.155832, val:  10.00%, val_best:  10.00%, tr:  10.93%, tr_best:  23.29%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss: 12.769104/  7.298407, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  23.29%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  6.639503/  4.897089, val:  10.00%, val_best:  10.00%, tr:  10.32%, tr_best:  23.29%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  9.107866/ 13.026996, val:  10.00%, val_best:  10.00%, tr:  10.42%, tr_best:  23.29%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss: 11.431705/ 11.466759, val:  10.00%, val_best:  10.00%, tr:  10.62%, tr_best:  23.29%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  9.989079/ 14.968504, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  23.29%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss: 12.182712/ 12.085588, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  23.29%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss: 13.465935/  7.763026, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  23.29%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546a18ad3ba0462ea3693a9961f795ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▂▂▁▂█▁▄▂▂▄▂▅█▄▂█▅▁▂▁▁▅▁▅▂▂▁▄▄▂▁▂▅▅▁▁▅▂▁▅</td></tr><tr><td>summary_val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>tr_acc</td><td>█▂▂▂▁▁▂▁▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>tr_epoch_loss</td><td>▁▅▆▅▇▆▆▆▆▇█▅▅▆▆▆▅▇▇▆▅▄▅▆▆▆▆█▆▆▆▆▅▆▇▆▇▅▄▅</td></tr><tr><td>val_acc_best</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_now</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃█▄▆▆▆▃▄▃▇▄▅▆▃▃▂▃▄▃▄▃▄▄▂▆▁▃▅▆▁▃▃▄▃▃▅▅▅▅▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>0.0</td></tr><tr><td>tr_acc</td><td>0.08784</td></tr><tr><td>tr_epoch_loss</td><td>13.46593</td></tr><tr><td>val_acc_best</td><td>0.1</td></tr><tr><td>val_acc_now</td><td>0.1</td></tr><tr><td>val_loss</td><td>7.76303</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stellar-sweep-1</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/efmqv2ja' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/efmqv2ja</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_025257-efmqv2ja/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: oy0aprup with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deaead9e0df84e1ea3b1d43f456ff93b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113532322148482, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_032650-oy0aprup</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/oy0aprup' target=\"_blank\">trim-sweep-3</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/oy0aprup' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/oy0aprup</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 1, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ddd2d600e882f330b6acfa4755a9bed1\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=2560, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.5, v_reset=10000, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.5, v_reset=10000, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 554,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.305364/  2.302864, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  2.305079/  2.302701, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  10.01%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  2.305126/  2.302681, val:  10.42%, val_best:  10.42%, tr:   8.38%, tr_best:  10.01%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  2.302660/  2.298214, val:  13.75%, val_best:  13.75%, tr:   8.58%, tr_best:  10.01%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  2.259439/  2.228340, val:  20.00%, val_best:  20.00%, tr:  14.71%, tr_best:  14.71%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  2.141494/  2.108390, val:  37.50%, val_best:  37.50%, tr:  23.19%, tr_best:  23.19%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  1.972995/  1.931573, val:  45.00%, val_best:  45.00%, tr:  32.69%, tr_best:  32.69%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  1.762945/  1.751060, val:  45.42%, val_best:  45.42%, tr:  47.60%, tr_best:  47.60%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  1.561936/  1.595294, val:  56.25%, val_best:  56.25%, tr:  52.91%, tr_best:  52.91%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  1.424810/  1.508673, val:  57.08%, val_best:  57.08%, tr:  60.37%, tr_best:  60.37%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  1.331438/  1.445521, val:  58.33%, val_best:  58.33%, tr:  61.18%, tr_best:  61.18%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  1.283920/  1.392315, val:  63.33%, val_best:  63.33%, tr:  62.10%, tr_best:  62.10%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  1.240236/  1.355934, val:  63.33%, val_best:  63.33%, tr:  61.18%, tr_best:  62.10%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  1.207124/  1.340919, val:  60.00%, val_best:  63.33%, tr:  64.04%, tr_best:  64.04%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  1.149838/  1.324069, val:  57.08%, val_best:  63.33%, tr:  64.86%, tr_best:  64.86%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  1.144018/  1.294100, val:  61.25%, val_best:  63.33%, tr:  64.35%, tr_best:  64.86%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  1.106115/  1.255889, val:  64.17%, val_best:  64.17%, tr:  64.35%, tr_best:  64.86%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  1.080720/  1.251679, val:  64.17%, val_best:  64.17%, tr:  68.23%, tr_best:  68.23%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  1.061985/  1.256705, val:  64.17%, val_best:  64.17%, tr:  68.54%, tr_best:  68.54%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  1.056405/  1.247626, val:  63.33%, val_best:  64.17%, tr:  67.31%, tr_best:  68.54%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  1.028237/  1.219240, val:  65.83%, val_best:  65.83%, tr:  70.48%, tr_best:  70.48%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  1.006416/  1.213852, val:  65.00%, val_best:  65.83%, tr:  69.36%, tr_best:  70.48%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  1.007272/  1.209428, val:  63.75%, val_best:  65.83%, tr:  68.85%, tr_best:  70.48%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  0.979795/  1.202830, val:  65.00%, val_best:  65.83%, tr:  70.58%, tr_best:  70.58%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  0.965609/  1.192009, val:  66.25%, val_best:  66.25%, tr:  71.30%, tr_best:  71.30%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  0.951585/  1.179743, val:  65.00%, val_best:  66.25%, tr:  72.11%, tr_best:  72.11%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  0.938655/  1.148666, val:  67.92%, val_best:  67.92%, tr:  71.30%, tr_best:  72.11%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  0.922068/  1.188757, val:  63.33%, val_best:  67.92%, tr:  73.34%, tr_best:  73.34%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  0.929983/  1.163930, val:  66.25%, val_best:  67.92%, tr:  72.42%, tr_best:  73.34%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  0.907094/  1.167277, val:  67.50%, val_best:  67.92%, tr:  71.20%, tr_best:  73.34%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  0.892458/  1.162411, val:  64.17%, val_best:  67.92%, tr:  73.14%, tr_best:  73.34%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  0.888458/  1.169931, val:  67.08%, val_best:  67.92%, tr:  72.63%, tr_best:  73.34%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  0.878055/  1.172327, val:  69.58%, val_best:  69.58%, tr:  74.06%, tr_best:  74.06%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.875673/  1.164616, val:  68.33%, val_best:  69.58%, tr:  74.06%, tr_best:  74.06%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.864312/  1.180640, val:  66.67%, val_best:  69.58%, tr:  74.06%, tr_best:  74.06%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.853112/  1.211283, val:  66.25%, val_best:  69.58%, tr:  74.97%, tr_best:  74.97%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.845514/  1.207863, val:  64.58%, val_best:  69.58%, tr:  75.79%, tr_best:  75.79%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.838598/  1.173988, val:  68.33%, val_best:  69.58%, tr:  76.81%, tr_best:  76.81%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.834208/  1.174110, val:  67.50%, val_best:  69.58%, tr:  78.04%, tr_best:  78.04%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.823395/  1.172830, val:  68.75%, val_best:  69.58%, tr:  76.81%, tr_best:  78.04%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.812903/  1.192753, val:  69.58%, val_best:  69.58%, tr:  76.00%, tr_best:  78.04%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.804604/  1.214868, val:  69.58%, val_best:  69.58%, tr:  81.00%, tr_best:  81.00%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.791567/  1.207574, val:  66.67%, val_best:  69.58%, tr:  79.98%, tr_best:  81.00%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.785612/  1.202776, val:  67.92%, val_best:  69.58%, tr:  79.88%, tr_best:  81.00%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.780678/  1.195896, val:  70.42%, val_best:  70.42%, tr:  79.06%, tr_best:  81.00%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.764351/  1.206043, val:  67.50%, val_best:  70.42%, tr:  80.90%, tr_best:  81.00%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.766171/  1.237715, val:  67.08%, val_best:  70.42%, tr:  79.67%, tr_best:  81.00%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.759577/  1.236583, val:  70.00%, val_best:  70.42%, tr:  80.80%, tr_best:  81.00%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.754034/  1.220549, val:  67.50%, val_best:  70.42%, tr:  84.47%, tr_best:  84.47%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.743680/  1.251751, val:  73.75%, val_best:  73.75%, tr:  81.31%, tr_best:  84.47%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.739169/  1.264134, val:  70.42%, val_best:  73.75%, tr:  84.17%, tr_best:  84.47%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.726494/  1.245366, val:  71.67%, val_best:  73.75%, tr:  83.15%, tr_best:  84.47%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.723090/  1.279227, val:  69.58%, val_best:  73.75%, tr:  82.74%, tr_best:  84.47%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.726910/  1.237259, val:  76.67%, val_best:  76.67%, tr:  82.33%, tr_best:  84.47%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.698242/  1.255934, val:  70.42%, val_best:  76.67%, tr:  86.11%, tr_best:  86.11%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.697759/  1.273977, val:  75.00%, val_best:  76.67%, tr:  85.19%, tr_best:  86.11%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.685503/  1.339709, val:  71.67%, val_best:  76.67%, tr:  85.90%, tr_best:  86.11%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.681925/  1.274377, val:  70.42%, val_best:  76.67%, tr:  87.03%, tr_best:  87.03%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.661968/  1.330673, val:  70.42%, val_best:  76.67%, tr:  86.72%, tr_best:  87.03%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.661401/  1.320356, val:  71.25%, val_best:  76.67%, tr:  87.95%, tr_best:  87.95%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.649778/  1.334624, val:  70.00%, val_best:  76.67%, tr:  87.84%, tr_best:  87.95%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.648686/  1.351306, val:  73.33%, val_best:  76.67%, tr:  89.68%, tr_best:  89.68%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.651119/  1.323158, val:  70.00%, val_best:  76.67%, tr:  88.76%, tr_best:  89.68%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.631515/  1.309836, val:  74.17%, val_best:  76.67%, tr:  90.09%, tr_best:  90.09%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.613097/  1.385577, val:  73.33%, val_best:  76.67%, tr:  90.70%, tr_best:  90.70%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.624572/  1.397686, val:  72.50%, val_best:  76.67%, tr:  90.30%, tr_best:  90.70%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.604997/  1.350224, val:  74.58%, val_best:  76.67%, tr:  90.40%, tr_best:  90.70%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.615711/  1.362163, val:  77.08%, val_best:  77.08%, tr:  90.30%, tr_best:  90.70%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.595184/  1.374885, val:  72.92%, val_best:  77.08%, tr:  90.09%, tr_best:  90.70%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.591361/  1.381231, val:  75.83%, val_best:  77.08%, tr:  92.24%, tr_best:  92.24%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.590977/  1.376829, val:  77.08%, val_best:  77.08%, tr:  91.73%, tr_best:  92.24%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.578282/  1.454146, val:  70.42%, val_best:  77.08%, tr:  92.34%, tr_best:  92.34%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.562713/  1.499583, val:  72.92%, val_best:  77.08%, tr:  93.56%, tr_best:  93.56%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.588765/  1.460736, val:  72.50%, val_best:  77.08%, tr:  91.83%, tr_best:  93.56%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.548453/  1.434129, val:  78.33%, val_best:  78.33%, tr:  94.38%, tr_best:  94.38%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.558784/  1.460740, val:  77.50%, val_best:  78.33%, tr:  93.97%, tr_best:  94.38%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.537370/  1.480600, val:  75.83%, val_best:  78.33%, tr:  94.48%, tr_best:  94.48%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.548305/  1.499487, val:  74.58%, val_best:  78.33%, tr:  94.79%, tr_best:  94.79%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.554073/  1.511299, val:  73.75%, val_best:  78.33%, tr:  93.36%, tr_best:  94.79%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.539164/  1.487878, val:  76.67%, val_best:  78.33%, tr:  94.69%, tr_best:  94.79%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.518530/  1.527898, val:  76.25%, val_best:  78.33%, tr:  95.71%, tr_best:  95.71%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.516416/  1.514460, val:  75.83%, val_best:  78.33%, tr:  94.99%, tr_best:  95.71%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.510867/  1.561926, val:  75.83%, val_best:  78.33%, tr:  94.38%, tr_best:  95.71%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.516671/  1.602878, val:  76.67%, val_best:  78.33%, tr:  96.22%, tr_best:  96.22%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.510090/  1.536895, val:  76.67%, val_best:  78.33%, tr:  96.32%, tr_best:  96.32%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.481515/  1.597254, val:  74.58%, val_best:  78.33%, tr:  96.53%, tr_best:  96.53%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.487893/  1.579435, val:  79.58%, val_best:  79.58%, tr:  95.30%, tr_best:  96.53%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.483485/  1.592036, val:  76.67%, val_best:  79.58%, tr:  96.53%, tr_best:  96.53%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.473938/  1.607665, val:  77.50%, val_best:  79.58%, tr:  97.14%, tr_best:  97.14%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.461620/  1.627813, val:  77.08%, val_best:  79.58%, tr:  97.04%, tr_best:  97.14%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.458294/  1.593552, val:  78.75%, val_best:  79.58%, tr:  97.24%, tr_best:  97.24%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.447412/  1.640827, val:  75.00%, val_best:  79.58%, tr:  97.55%, tr_best:  97.55%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.442647/  1.693637, val:  75.83%, val_best:  79.58%, tr:  97.55%, tr_best:  97.55%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.457884/  1.664643, val:  77.08%, val_best:  79.58%, tr:  96.12%, tr_best:  97.55%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.440259/  1.705986, val:  75.83%, val_best:  79.58%, tr:  97.45%, tr_best:  97.55%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.438903/  1.778543, val:  74.17%, val_best:  79.58%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.423789/  1.766955, val:  76.67%, val_best:  79.58%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.437624/  1.766390, val:  78.33%, val_best:  79.58%, tr:  97.14%, tr_best:  98.06%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.415872/  1.792264, val:  75.83%, val_best:  79.58%, tr:  97.96%, tr_best:  98.06%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.419557/  1.779010, val:  74.58%, val_best:  79.58%, tr:  97.55%, tr_best:  98.06%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de6380093bb42bb85dd3443d1cac64d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▂▁▄▅▅▆▄▅▆▇▇▆▆▆▆▇▇█▇▆▇███▇▇▇█▆█▇▆██▇▇▇██</td></tr><tr><td>summary_val_acc</td><td>▁▁▂▅▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇██▇██████████</td></tr><tr><td>tr_acc</td><td>▁▁▁▄▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇████████████</td></tr><tr><td>tr_epoch_loss</td><td>███▆▅▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▂▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▂▅▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇██▇██████████</td></tr><tr><td>val_loss</td><td>███▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▃▂▂▃▃▃▃▄▃▄▄▄▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.97549</td></tr><tr><td>tr_epoch_loss</td><td>0.41956</td></tr><tr><td>val_acc_best</td><td>0.79583</td></tr><tr><td>val_acc_now</td><td>0.74583</td></tr><tr><td>val_loss</td><td>1.77901</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trim-sweep-3</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/oy0aprup' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/oy0aprup</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_032650-oy0aprup/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nsap7e5o with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_040059-nsap7e5o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nsap7e5o' target=\"_blank\">fresh-sweep-5</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nsap7e5o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nsap7e5o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ddd2d600e882f330b6acfa4755a9bed1\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=2560, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 554,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.392905/  2.912891, val:  49.17%, val_best:  49.17%, tr:  35.44%, tr_best:  35.44%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.373508/  3.326828, val:  49.58%, val_best:  49.58%, tr:  49.03%, tr_best:  49.03%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.437456/  3.847586, val:  47.92%, val_best:  49.58%, tr:  57.20%, tr_best:  57.20%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  2.090055/  2.921011, val:  56.67%, val_best:  56.67%, tr:  60.88%, tr_best:  60.88%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.744821/  2.074502, val:  58.75%, val_best:  58.75%, tr:  66.70%, tr_best:  66.70%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.307056/  1.784210, val:  62.50%, val_best:  62.50%, tr:  74.77%, tr_best:  74.77%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.794937/  1.314875, val:  77.08%, val_best:  77.08%, tr:  87.23%, tr_best:  87.23%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.601461/  1.369465, val:  79.17%, val_best:  79.17%, tr:  89.79%, tr_best:  89.79%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.398779/  1.258995, val:  80.83%, val_best:  80.83%, tr:  93.36%, tr_best:  93.36%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.346408/  1.541623, val:  81.25%, val_best:  81.25%, tr:  94.38%, tr_best:  94.38%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.276294/  1.337192, val:  81.67%, val_best:  81.67%, tr:  96.53%, tr_best:  96.53%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.188794/  1.399137, val:  82.08%, val_best:  82.08%, tr:  97.65%, tr_best:  97.65%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.191128/  1.273639, val:  82.92%, val_best:  82.92%, tr:  97.55%, tr_best:  97.65%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.119016/  1.211610, val:  86.67%, val_best:  86.67%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.090027/  1.407529, val:  84.17%, val_best:  86.67%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.061766/  1.277857, val:  86.67%, val_best:  86.67%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.061936/  1.437105, val:  85.00%, val_best:  86.67%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.056861/  1.260981, val:  85.42%, val_best:  86.67%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.028151/  1.473787, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.020954/  1.419016, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.012499/  1.367328, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.009513/  1.389126, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.010555/  1.387444, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.007321/  1.440743, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.005107/  1.412533, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.004516/  1.423748, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.004117/  1.401003, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.003543/  1.435213, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.003542/  1.431524, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.003499/  1.418162, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.002755/  1.455907, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.002531/  1.458795, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.002430/  1.461442, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.002596/  1.447733, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.002585/  1.454916, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.002730/  1.496912, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.002208/  1.480235, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.002063/  1.482374, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.001868/  1.494069, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.001723/  1.500222, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.001647/  1.513336, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.001588/  1.507533, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.001499/  1.503790, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.001549/  1.517520, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.001454/  1.535363, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.001379/  1.531108, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.001266/  1.531989, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.001287/  1.537985, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.001275/  1.544889, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.001290/  1.556381, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.001155/  1.559744, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.001133/  1.558950, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.001175/  1.558477, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.001110/  1.556642, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.001083/  1.579094, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.001018/  1.577024, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.001024/  1.570238, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.000932/  1.581772, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.000898/  1.588838, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.000874/  1.589931, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.000862/  1.597732, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.000824/  1.597108, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.000877/  1.600501, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.000765/  1.608138, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.000798/  1.609397, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.000774/  1.605326, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.000760/  1.614278, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.000776/  1.616740, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.000739/  1.623392, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.000764/  1.623194, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.000696/  1.616719, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.000713/  1.622298, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.000664/  1.622595, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.000634/  1.624390, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.000603/  1.631578, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.000610/  1.630131, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.000598/  1.625356, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.000657/  1.641207, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.000618/  1.637814, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.000621/  1.642375, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.000637/  1.643525, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.000605/  1.644788, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.000567/  1.648073, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.000581/  1.653202, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.000606/  1.657871, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.000578/  1.662153, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.000581/  1.662706, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.000528/  1.665960, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.000534/  1.665938, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.000502/  1.671752, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.000514/  1.673800, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.000508/  1.671839, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.000524/  1.674185, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.000534/  1.683802, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.000513/  1.685245, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.000541/  1.687866, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.000476/  1.677355, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.000482/  1.681558, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.000497/  1.689740, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.000476/  1.692058, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1412cd887f40b884c0660eb9c17462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▃█▆███████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▁▃▆▇▇▇▇████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▃▄▇▇███████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>██▆▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▃▆▇▇██████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▃▆▇▇▇▇████████████████████████████████</td></tr><tr><td>val_loss</td><td>▅█▃▁▂▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00048</td></tr><tr><td>val_acc_best</td><td>0.8875</td></tr><tr><td>val_acc_now</td><td>0.875</td></tr><tr><td>val_loss</td><td>1.69206</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fresh-sweep-5</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nsap7e5o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nsap7e5o</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_040059-nsap7e5o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: m4r1856k with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_043552-m4r1856k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m4r1856k' target=\"_blank\">daily-sweep-7</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m4r1856k' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m4r1856k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = da7e4b41c275f954fb82e708e43d2b96\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=2560, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 554,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.305372/  2.302864, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  2.305082/  2.302657, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  10.01%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  2.305162/  2.302672, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:  10.01%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  2.304701/  2.302695, val:  10.00%, val_best:  10.00%, tr:   7.97%, tr_best:  10.01%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  2.304502/  2.301768, val:  13.75%, val_best:  13.75%, tr:   7.56%, tr_best:  10.01%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  2.265980/  2.183557, val:  25.83%, val_best:  25.83%, tr:  16.24%, tr_best:  16.24%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  2.005016/  1.894917, val:  42.50%, val_best:  42.50%, tr:  28.80%, tr_best:  28.80%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  1.696492/  1.635059, val:  45.83%, val_best:  45.83%, tr:  42.70%, tr_best:  42.70%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  1.492126/  1.507682, val:  50.83%, val_best:  50.83%, tr:  51.38%, tr_best:  51.38%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  1.369725/  1.421565, val:  55.00%, val_best:  55.00%, tr:  56.18%, tr_best:  56.18%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  1.279556/  1.375942, val:  52.92%, val_best:  55.00%, tr:  57.00%, tr_best:  57.00%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  1.213787/  1.317358, val:  58.75%, val_best:  58.75%, tr:  59.86%, tr_best:  59.86%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  1.165139/  1.294104, val:  58.33%, val_best:  58.75%, tr:  62.00%, tr_best:  62.00%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  1.123067/  1.286368, val:  57.50%, val_best:  58.75%, tr:  61.08%, tr_best:  62.00%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  1.055848/  1.305581, val:  56.25%, val_best:  58.75%, tr:  65.07%, tr_best:  65.07%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  1.038989/  1.238899, val:  60.42%, val_best:  60.42%, tr:  65.78%, tr_best:  65.78%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  1.004619/  1.221122, val:  61.67%, val_best:  61.67%, tr:  65.17%, tr_best:  65.78%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  0.957720/  1.221170, val:  61.67%, val_best:  61.67%, tr:  69.66%, tr_best:  69.66%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  0.934925/  1.251928, val:  61.25%, val_best:  61.67%, tr:  70.07%, tr_best:  70.07%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  0.932782/  1.237321, val:  60.83%, val_best:  61.67%, tr:  69.97%, tr_best:  70.07%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  0.880682/  1.218651, val:  62.08%, val_best:  62.08%, tr:  71.91%, tr_best:  71.91%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  0.863714/  1.215521, val:  62.92%, val_best:  62.92%, tr:  70.38%, tr_best:  71.91%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  0.843326/  1.202889, val:  67.08%, val_best:  67.08%, tr:  73.03%, tr_best:  73.03%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  0.808981/  1.212849, val:  66.25%, val_best:  67.08%, tr:  73.34%, tr_best:  73.34%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  0.786384/  1.239447, val:  65.42%, val_best:  67.08%, tr:  76.92%, tr_best:  76.92%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  0.767560/  1.212136, val:  68.33%, val_best:  68.33%, tr:  73.65%, tr_best:  76.92%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  0.745561/  1.214286, val:  61.67%, val_best:  68.33%, tr:  76.30%, tr_best:  76.92%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  0.728167/  1.230253, val:  64.17%, val_best:  68.33%, tr:  77.73%, tr_best:  77.73%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  0.721717/  1.229543, val:  67.92%, val_best:  68.33%, tr:  77.83%, tr_best:  77.83%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  0.677533/  1.248734, val:  65.42%, val_best:  68.33%, tr:  79.26%, tr_best:  79.26%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  0.657243/  1.267270, val:  66.67%, val_best:  68.33%, tr:  81.72%, tr_best:  81.72%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  0.666860/  1.254025, val:  69.17%, val_best:  69.17%, tr:  79.98%, tr_best:  81.72%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  0.634238/  1.274167, val:  71.25%, val_best:  71.25%, tr:  80.80%, tr_best:  81.72%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.619831/  1.271013, val:  70.42%, val_best:  71.25%, tr:  82.43%, tr_best:  82.43%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.588500/  1.331590, val:  63.33%, val_best:  71.25%, tr:  83.86%, tr_best:  83.86%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.590381/  1.332719, val:  69.58%, val_best:  71.25%, tr:  83.66%, tr_best:  83.86%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.549044/  1.332709, val:  73.33%, val_best:  73.33%, tr:  85.29%, tr_best:  85.29%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.533472/  1.349681, val:  66.25%, val_best:  73.33%, tr:  87.74%, tr_best:  87.74%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.526832/  1.340285, val:  70.42%, val_best:  73.33%, tr:  89.79%, tr_best:  89.79%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.496993/  1.367188, val:  68.75%, val_best:  73.33%, tr:  89.58%, tr_best:  89.79%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.493672/  1.380331, val:  71.25%, val_best:  73.33%, tr:  89.38%, tr_best:  89.79%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.472591/  1.397772, val:  74.58%, val_best:  74.58%, tr:  92.13%, tr_best:  92.13%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.450528/  1.385290, val:  71.25%, val_best:  74.58%, tr:  93.05%, tr_best:  93.05%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.439216/  1.421231, val:  66.25%, val_best:  74.58%, tr:  93.05%, tr_best:  93.05%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.416161/  1.420546, val:  73.75%, val_best:  74.58%, tr:  92.24%, tr_best:  93.05%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.392336/  1.464856, val:  71.67%, val_best:  74.58%, tr:  95.40%, tr_best:  95.40%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.384954/  1.436348, val:  75.83%, val_best:  75.83%, tr:  94.18%, tr_best:  95.40%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.353460/  1.455724, val:  75.83%, val_best:  75.83%, tr:  96.53%, tr_best:  96.53%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.356795/  1.484855, val:  74.58%, val_best:  75.83%, tr:  97.04%, tr_best:  97.04%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.330123/  1.504325, val:  76.67%, val_best:  76.67%, tr:  96.63%, tr_best:  97.04%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.319138/  1.546091, val:  71.67%, val_best:  76.67%, tr:  96.73%, tr_best:  97.04%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.318731/  1.532827, val:  77.92%, val_best:  77.92%, tr:  94.89%, tr_best:  97.04%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.269814/  1.574439, val:  77.08%, val_best:  77.92%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.266094/  1.605502, val:  77.92%, val_best:  77.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.250625/  1.578598, val:  76.25%, val_best:  77.92%, tr:  99.39%, tr_best:  99.80%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.233013/  1.585233, val:  78.33%, val_best:  78.33%, tr:  98.98%, tr_best:  99.80%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.219908/  1.688964, val:  76.25%, val_best:  78.33%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.228005/  1.644969, val:  82.08%, val_best:  82.08%, tr:  99.49%, tr_best:  99.80%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.208463/  1.661799, val:  78.33%, val_best:  82.08%, tr:  99.18%, tr_best:  99.80%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.188297/  1.684814, val:  81.25%, val_best:  82.08%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.170815/  1.685946, val:  81.25%, val_best:  82.08%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.165446/  1.775911, val:  74.58%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.165310/  1.745244, val:  78.75%, val_best:  82.08%, tr:  99.49%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.149854/  1.771176, val:  80.00%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.145126/  1.810258, val:  77.50%, val_best:  82.08%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.129749/  1.834355, val:  80.42%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.120706/  1.830001, val:  80.83%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.108348/  1.841014, val:  81.25%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.099440/  1.905927, val:  78.33%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.099957/  1.900790, val:  81.67%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.086384/  1.888153, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.085189/  1.931135, val:  79.17%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.078439/  1.966672, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.079402/  1.957464, val:  79.17%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.064609/  1.975993, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.060676/  1.986471, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.054418/  2.014648, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.058219/  2.041732, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.050464/  2.045059, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.046637/  2.088002, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.042637/  2.108545, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.039717/  2.124796, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.038635/  2.118150, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.033855/  2.130954, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.032493/  2.136976, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.033907/  2.172480, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.030498/  2.186117, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.028408/  2.202749, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.024459/  2.197562, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.022689/  2.202621, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.023749/  2.225277, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.023496/  2.253830, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.023652/  2.240291, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.024252/  2.252703, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.019831/  2.274104, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.017723/  2.308514, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.018786/  2.321036, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.016733/  2.283014, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.015494/  2.316970, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.016366/  2.339766, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b62c3b6236ce4de7ba5b627bd608d1ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▂▁▃▃▆▆▃▅▇▅▇▅▆▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▁▁▄▅▆▅▆▆▆▆▆▆▇▇▆▇▇▇▇▇▇▇██▇██████████████</td></tr><tr><td>tr_acc</td><td>▁▁▁▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇█████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>███▆▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▁▄▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▁▄▅▆▅▆▆▆▆▆▆▇▇▆▇▇▇▇▇▇▇██▇██████████████</td></tr><tr><td>val_loss</td><td>███▄▂▂▂▁▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.01637</td></tr><tr><td>val_acc_best</td><td>0.84167</td></tr><tr><td>val_acc_now</td><td>0.80833</td></tr><tr><td>val_loss</td><td>2.33977</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">daily-sweep-7</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m4r1856k' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m4r1856k</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_043552-m4r1856k/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5our9w03 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_051139-5our9w03</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5our9w03' target=\"_blank\">true-sweep-9</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5our9w03' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5our9w03</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = da7e4b41c275f954fb82e708e43d2b96\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=2560, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 554,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.297347/  2.200394, val:  14.58%, val_best:  14.58%, tr:   9.60%, tr_best:   9.60%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  1.753792/  1.463457, val:  54.17%, val_best:  54.17%, tr:  38.30%, tr_best:  38.30%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  1.290566/  1.345544, val:  55.00%, val_best:  55.00%, tr:  58.22%, tr_best:  58.22%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  1.090595/  1.281512, val:  55.42%, val_best:  55.42%, tr:  64.25%, tr_best:  64.25%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  0.989281/  1.261300, val:  61.67%, val_best:  61.67%, tr:  65.99%, tr_best:  65.99%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  0.908837/  1.167969, val:  62.92%, val_best:  62.92%, tr:  67.93%, tr_best:  67.93%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  0.829371/  1.155438, val:  67.50%, val_best:  67.50%, tr:  74.67%, tr_best:  74.67%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  0.769384/  1.100554, val:  70.83%, val_best:  70.83%, tr:  75.79%, tr_best:  75.79%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  0.693127/  1.171511, val:  71.25%, val_best:  71.25%, tr:  82.02%, tr_best:  82.02%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  0.607933/  1.159266, val:  71.25%, val_best:  71.25%, tr:  87.44%, tr_best:  87.44%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  0.546275/  1.135611, val:  75.00%, val_best:  75.00%, tr:  91.83%, tr_best:  91.83%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  0.448728/  1.137201, val:  75.42%, val_best:  75.42%, tr:  94.79%, tr_best:  94.79%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  0.407191/  1.165704, val:  77.92%, val_best:  77.92%, tr:  96.12%, tr_best:  96.12%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  0.344206/  1.197114, val:  77.08%, val_best:  77.92%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  0.282647/  1.331175, val:  74.58%, val_best:  77.92%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  0.237623/  1.219626, val:  83.33%, val_best:  83.33%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  0.208824/  1.263592, val:  78.75%, val_best:  83.33%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  0.174433/  1.279334, val:  80.42%, val_best:  83.33%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  0.153633/  1.361949, val:  80.83%, val_best:  83.33%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  0.123260/  1.364817, val:  79.17%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  0.107230/  1.382386, val:  82.08%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  0.087108/  1.426531, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  0.083343/  1.437676, val:  81.25%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  0.064482/  1.452685, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  0.052491/  1.486972, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  0.049065/  1.519263, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  0.045900/  1.555053, val:  78.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  0.039933/  1.569014, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  0.036844/  1.565424, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  0.031046/  1.546176, val:  82.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  0.029240/  1.605433, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  0.027056/  1.609102, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  0.025493/  1.617859, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.024640/  1.645767, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.022028/  1.646116, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.019611/  1.680136, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.018606/  1.666793, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.017660/  1.686714, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.019456/  1.724918, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.017562/  1.713892, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.013976/  1.727956, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.013603/  1.707710, val:  82.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.013444/  1.740513, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.012125/  1.743163, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.012082/  1.735510, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.011062/  1.748385, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.010164/  1.763194, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.010530/  1.772074, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.010006/  1.774401, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.009134/  1.775405, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.008964/  1.787462, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.009231/  1.801993, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.008456/  1.811542, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.008658/  1.796954, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.007878/  1.802229, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.007380/  1.821385, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.006872/  1.822895, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.007051/  1.821501, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.006727/  1.859581, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.006595/  1.868246, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.006191/  1.870070, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.006127/  1.879023, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.006350/  1.882325, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.005972/  1.879648, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.005907/  1.886286, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.005788/  1.898047, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.005754/  1.902717, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.005851/  1.888144, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.005377/  1.899630, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.004923/  1.919511, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.004967/  1.926730, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.005157/  1.931147, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.004886/  1.934512, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.004871/  1.915190, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.004463/  1.927093, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.004450/  1.946408, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.004254/  1.955460, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.004301/  1.954308, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.004224/  1.946011, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.004106/  1.957363, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.003863/  1.955914, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.003793/  1.956195, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.004081/  1.970017, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.003617/  1.958207, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.003577/  1.969593, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.003534/  1.969680, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.003560/  1.982789, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.003369/  1.974929, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.003348/  1.979147, val:  79.58%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.003361/  1.985722, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.003643/  1.987596, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.003283/  1.988184, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.003291/  1.994377, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.003330/  2.001630, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.003306/  1.995371, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.003188/  1.995144, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.003029/  2.002917, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.003058/  2.018274, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.002935/  2.014421, val:  79.58%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.002819/  2.021044, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c332e7248e854344a37d477ae3c31198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▃▆▆█▇█████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▆▇▇▇▇█████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▇███████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▆▇▇▇▇█████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▆▇▇▇▇█████████████████████████████████</td></tr><tr><td>val_loss</td><td>█▃▂▁▁▁▂▂▃▃▃▄▄▄▅▅▅▅▅▅▅▆▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00282</td></tr><tr><td>val_acc_best</td><td>0.83333</td></tr><tr><td>val_acc_now</td><td>0.8125</td></tr><tr><td>val_loss</td><td>2.02104</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">true-sweep-9</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5our9w03' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5our9w03</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_051139-5our9w03/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: e9rozidf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_054629-e9rozidf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e9rozidf' target=\"_blank\">easy-sweep-11</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e9rozidf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e9rozidf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ddd2d600e882f330b6acfa4755a9bed1\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=2560, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 554,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.306398/  2.302840, val:   3.33%, val_best:   3.33%, tr:   9.50%, tr_best:   9.50%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  2.073859/  1.773902, val:  44.17%, val_best:  44.17%, tr:  22.06%, tr_best:  22.06%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  1.474289/  1.401239, val:  57.92%, val_best:  57.92%, tr:  55.06%, tr_best:  55.06%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  1.184120/  1.284097, val:  62.92%, val_best:  62.92%, tr:  65.78%, tr_best:  65.78%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  1.064103/  1.219913, val:  62.92%, val_best:  62.92%, tr:  65.07%, tr_best:  65.78%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  0.993184/  1.142863, val:  65.00%, val_best:  65.00%, tr:  66.50%, tr_best:  66.50%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  0.920340/  1.105686, val:  69.17%, val_best:  69.17%, tr:  71.91%, tr_best:  71.91%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  0.875716/  1.056235, val:  70.42%, val_best:  70.42%, tr:  73.75%, tr_best:  73.75%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  0.808678/  1.026520, val:  77.50%, val_best:  77.50%, tr:  79.16%, tr_best:  79.16%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  0.735443/  1.019951, val:  82.08%, val_best:  82.08%, tr:  84.07%, tr_best:  84.07%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  0.695533/  0.997725, val:  81.25%, val_best:  82.08%, tr:  87.44%, tr_best:  87.44%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  0.625975/  0.941684, val:  83.33%, val_best:  83.33%, tr:  90.81%, tr_best:  90.81%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  0.581963/  0.933737, val:  83.75%, val_best:  83.75%, tr:  92.44%, tr_best:  92.44%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  0.542447/  0.921479, val:  82.92%, val_best:  83.75%, tr:  93.36%, tr_best:  93.36%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  0.488047/  0.995694, val:  78.75%, val_best:  83.75%, tr:  94.08%, tr_best:  94.08%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  0.456646/  0.896984, val:  86.25%, val_best:  86.25%, tr:  95.40%, tr_best:  95.40%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  0.448744/  0.872167, val:  87.50%, val_best:  87.50%, tr:  93.77%, tr_best:  95.40%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  0.401059/  0.874191, val:  85.83%, val_best:  87.50%, tr:  95.51%, tr_best:  95.51%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  0.375370/  0.959538, val:  83.33%, val_best:  87.50%, tr:  96.32%, tr_best:  96.32%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  0.361743/  0.917714, val:  86.25%, val_best:  87.50%, tr:  96.42%, tr_best:  96.42%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  0.327961/  0.917427, val:  85.83%, val_best:  87.50%, tr:  97.04%, tr_best:  97.04%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  0.306003/  0.878719, val:  87.92%, val_best:  87.92%, tr:  97.14%, tr_best:  97.14%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  0.315552/  0.889637, val:  89.17%, val_best:  89.17%, tr:  97.24%, tr_best:  97.24%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  0.284739/  0.921240, val:  86.25%, val_best:  89.17%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  0.267623/  0.946418, val:  85.00%, val_best:  89.17%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  0.250738/  0.927273, val:  87.08%, val_best:  89.17%, tr:  97.85%, tr_best:  98.06%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  0.235635/  0.937318, val:  87.08%, val_best:  89.17%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  0.224553/  0.977761, val:  86.25%, val_best:  89.17%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  0.214328/  0.946430, val:  89.17%, val_best:  89.17%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  0.197967/  0.998638, val:  87.50%, val_best:  89.17%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  0.189038/  0.994792, val:  85.83%, val_best:  89.17%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  0.181963/  1.003291, val:  86.67%, val_best:  89.17%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  0.170018/  0.999197, val:  87.92%, val_best:  89.17%, tr:  99.39%, tr_best:  99.59%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.164433/  1.016887, val:  86.67%, val_best:  89.17%, tr:  99.39%, tr_best:  99.59%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.157535/  1.026902, val:  87.92%, val_best:  89.17%, tr:  99.49%, tr_best:  99.59%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.143840/  1.052865, val:  86.67%, val_best:  89.17%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.138432/  1.034423, val:  87.08%, val_best:  89.17%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.130654/  1.076909, val:  87.92%, val_best:  89.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.127581/  1.054189, val:  87.50%, val_best:  89.17%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.120083/  1.079453, val:  86.25%, val_best:  89.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.112713/  1.080508, val:  86.25%, val_best:  89.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.108622/  1.086646, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.104532/  1.086915, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.094918/  1.116160, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.093021/  1.103403, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.088566/  1.122646, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.086553/  1.138621, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.079878/  1.164623, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.076848/  1.164271, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.075476/  1.176156, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.070267/  1.182141, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.067401/  1.176626, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.064313/  1.201241, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.062051/  1.208952, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.057949/  1.219162, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.056577/  1.201086, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.054774/  1.220927, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.052969/  1.248779, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.048562/  1.271835, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.047127/  1.260327, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.046281/  1.257441, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.044808/  1.289461, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.044717/  1.311375, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.043064/  1.271581, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.038752/  1.303090, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.038004/  1.308159, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.035059/  1.328021, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.036594/  1.329255, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.038488/  1.324584, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.033233/  1.331306, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.033779/  1.338697, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.035769/  1.342397, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.031793/  1.348627, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.031988/  1.341498, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.028739/  1.355876, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.028053/  1.369593, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.027807/  1.365263, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.025951/  1.384896, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.025566/  1.379163, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.026027/  1.371166, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.025961/  1.385246, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.024260/  1.387716, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.023454/  1.395077, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.023331/  1.396080, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.022680/  1.401537, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.022111/  1.413654, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.021772/  1.404848, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.021193/  1.414484, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.022062/  1.429250, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.021431/  1.429715, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.020346/  1.423326, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.019700/  1.426515, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.019242/  1.447881, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.019588/  1.434807, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.018663/  1.434922, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.019040/  1.443252, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.018322/  1.452324, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.017498/  1.460889, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.016438/  1.450360, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.017060/  1.473676, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e2b2ce684df4882ac9058d97f8f2ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▇▅▆▇▇▇▇▇███████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▆▇██▇█████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▇▇██████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▆▆▇███████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▆▇██▇█████████████████████████████████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▁▂▁▁▁▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▃▄▄▄▄▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.01706</td></tr><tr><td>val_acc_best</td><td>0.89167</td></tr><tr><td>val_acc_now</td><td>0.8625</td></tr><tr><td>val_loss</td><td>1.47368</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">easy-sweep-11</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e9rozidf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e9rozidf</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_054629-e9rozidf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6qn94m30 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_062200-6qn94m30</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6qn94m30' target=\"_blank\">youthful-sweep-13</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6qn94m30' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6qn94m30</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ddd2d600e882f330b6acfa4755a9bed1\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=2560, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 554,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.305372/  2.302864, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  2.305082/  2.302657, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  10.01%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  2.305162/  2.302672, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:  10.01%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  2.304701/  2.302695, val:  10.00%, val_best:  10.00%, tr:   7.97%, tr_best:  10.01%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  2.304796/  2.302665, val:  10.00%, val_best:  10.00%, tr:   7.46%, tr_best:  10.01%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  2.304565/  2.302659, val:  10.00%, val_best:  10.00%, tr:   7.97%, tr_best:  10.01%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  2.305082/  2.302690, val:  10.00%, val_best:  10.00%, tr:   9.70%, tr_best:  10.01%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  2.304632/  2.302694, val:  10.00%, val_best:  10.00%, tr:   7.46%, tr_best:  10.01%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  2.304545/  2.302612, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:  10.01%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  2.305081/  2.302782, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.01%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  2.304688/  2.302772, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  2.304833/  2.302622, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:  10.01%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  2.304284/  2.302666, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  10.01%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  2.304814/  2.302634, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.01%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  2.305231/  2.302722, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  10.01%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  2.305301/  2.302723, val:  10.00%, val_best:  10.00%, tr:   8.38%, tr_best:  10.01%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  2.304772/  2.302636, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  10.01%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  2.303716/  2.302659, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  2.304624/  2.302669, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  10.01%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  2.305879/  2.302768, val:  10.00%, val_best:  10.00%, tr:   8.48%, tr_best:  10.01%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  2.305089/  2.302728, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  10.01%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  2.304662/  2.302646, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  10.01%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  2.305019/  2.302714, val:  10.00%, val_best:  10.00%, tr:  10.32%, tr_best:  10.32%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  2.304242/  2.302678, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  10.32%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  2.303980/  2.302631, val:  10.00%, val_best:  10.00%, tr:   7.66%, tr_best:  10.32%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  2.304494/  2.302676, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:  10.32%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  2.304318/  2.302692, val:  10.00%, val_best:  10.00%, tr:   8.38%, tr_best:  10.32%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  2.304585/  2.302743, val:  10.00%, val_best:  10.00%, tr:   7.97%, tr_best:  10.32%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  2.304604/  2.302687, val:  10.00%, val_best:  10.00%, tr:   7.46%, tr_best:  10.32%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  2.304065/  2.302622, val:  10.00%, val_best:  10.00%, tr:   9.70%, tr_best:  10.32%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  2.304437/  2.302634, val:  10.00%, val_best:  10.00%, tr:   7.87%, tr_best:  10.32%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  2.304662/  2.302706, val:  10.00%, val_best:  10.00%, tr:   7.25%, tr_best:  10.32%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  2.305145/  2.302731, val:  10.00%, val_best:  10.00%, tr:   7.87%, tr_best:  10.32%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  2.304735/  2.302727, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  10.32%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  2.304953/  2.302632, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  10.32%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  2.304517/  2.302745, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  10.32%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  2.304946/  2.302701, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  10.32%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  2.305197/  2.302687, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  10.32%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  2.304105/  2.302715, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  10.32%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  2.304535/  2.302683, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  10.32%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  2.304653/  2.302674, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  10.32%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  2.304348/  2.302677, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  10.32%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  2.304514/  2.302729, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  10.32%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  2.304611/  2.302673, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  10.32%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  2.304545/  2.302760, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:  10.32%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  2.304274/  2.302822, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  10.32%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  2.304114/  2.302626, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.32%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  2.304720/  2.302717, val:  10.00%, val_best:  10.00%, tr:   8.48%, tr_best:  10.32%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  2.304052/  2.302806, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:  10.32%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  2.304631/  2.302734, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  10.32%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  2.304856/  2.302790, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  10.32%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  2.303980/  2.302723, val:  10.00%, val_best:  10.00%, tr:   8.38%, tr_best:  10.32%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  2.305447/  2.302761, val:  10.00%, val_best:  10.00%, tr:   7.87%, tr_best:  10.32%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  2.304740/  2.302716, val:  10.00%, val_best:  10.00%, tr:   8.48%, tr_best:  10.32%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  2.304604/  2.302668, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  10.32%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  2.304554/  2.302632, val:  10.00%, val_best:  10.00%, tr:   7.35%, tr_best:  10.32%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  2.305324/  2.302709, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.32%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  2.304349/  2.302676, val:  10.00%, val_best:  10.00%, tr:   8.07%, tr_best:  10.32%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  2.304797/  2.302742, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  10.32%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  2.305519/  2.302842, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  10.32%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  2.304214/  2.302650, val:  10.00%, val_best:  10.00%, tr:   7.87%, tr_best:  10.32%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  2.304307/  2.302675, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.32%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  2.304764/  2.302663, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  10.32%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  2.304623/  2.302703, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  10.32%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  2.303885/  2.302848, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.32%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  2.304417/  2.302755, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  10.32%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  2.303841/  2.302698, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.32%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  2.305081/  2.302692, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.32%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  2.305073/  2.302701, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  10.32%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  2.304717/  2.302645, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  10.32%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  2.305171/  2.302620, val:  10.00%, val_best:  10.00%, tr:   8.38%, tr_best:  10.32%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  2.305242/  2.302688, val:  10.00%, val_best:  10.00%, tr:   8.48%, tr_best:  10.32%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  2.304678/  2.302624, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  10.32%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  2.304676/  2.302652, val:  10.00%, val_best:  10.00%, tr:   8.48%, tr_best:  10.32%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  2.304883/  2.302660, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  10.32%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  2.305337/  2.302700, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  10.32%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  2.305019/  2.302656, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:  10.32%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  2.305215/  2.302639, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  10.32%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  2.304676/  2.302660, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.32%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  2.304999/  2.302665, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:  10.32%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  2.304617/  2.302732, val:  10.00%, val_best:  10.00%, tr:   7.35%, tr_best:  10.32%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  2.304430/  2.302707, val:  10.00%, val_best:  10.00%, tr:   7.66%, tr_best:  10.32%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  2.304436/  2.302796, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  10.32%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  2.305023/  2.302669, val:  10.00%, val_best:  10.00%, tr:   7.66%, tr_best:  10.32%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  2.304577/  2.302758, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  10.32%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  2.304359/  2.302626, val:  10.00%, val_best:  10.00%, tr:   8.07%, tr_best:  10.32%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  2.305443/  2.302771, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:  10.32%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  2.304418/  2.302739, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  10.32%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  2.305364/  2.302662, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  10.32%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  2.304665/  2.302694, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  10.32%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  2.305019/  2.302693, val:  10.00%, val_best:  10.00%, tr:   8.38%, tr_best:  10.32%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  2.304266/  2.302669, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  10.32%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  2.304705/  2.302684, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  10.32%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  2.304860/  2.302634, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  10.32%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  2.304419/  2.302675, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  10.32%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  2.304308/  2.302676, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:  10.32%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  2.304734/  2.302685, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  10.32%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  2.304735/  2.302724, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.32%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  2.305681/  2.302760, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  10.32%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  2.304706/  2.302780, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  10.32%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438308d71ff6410fad4e770817290441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▆▁▃▃▃▃▃▃▃▃▁▃▆▃▆▆▃██▃▃▁▃▁▃▁▃▃▁▃▆▃▁▁▁█▃▁▃</td></tr><tr><td>summary_val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>tr_acc</td><td>█▃▁▁▅▇▅█▄▆▂▄▇▂▆▅▆▆▃▄▇▂▆▃▇▅▆█▄▄▆▅▇▆▅▅▅▆▃▅</td></tr><tr><td>tr_epoch_loss</td><td>▆▆▄▄▅▃▆▁█▄▂▃▂▆▄▆▄▄▄▄▄▇▄▃▇▃▃▅▆▄▆▆▅▃▄▃▄▄▃▄</td></tr><tr><td>val_acc_best</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_now</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▃▆▂▄▂▅▂▁▃▁▄▅▃▃▄▅▄▄▅▂▃▇▃▅▃▁▁▃▂▂▆▅▄▃▃▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>0.0</td></tr><tr><td>tr_acc</td><td>0.09499</td></tr><tr><td>tr_epoch_loss</td><td>2.30471</td></tr><tr><td>val_acc_best</td><td>0.1</td></tr><tr><td>val_acc_now</td><td>0.1</td></tr><tr><td>val_loss</td><td>2.30278</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">youthful-sweep-13</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6qn94m30' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6qn94m30</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_062200-6qn94m30/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xz0pzut3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_065550-xz0pzut3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xz0pzut3' target=\"_blank\">azure-sweep-15</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xz0pzut3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xz0pzut3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ddd2d600e882f330b6acfa4755a9bed1\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=2560, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 554,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.018679/  1.896689, val:  47.08%, val_best:  47.08%, tr:  28.19%, tr_best:  28.19%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.508167/  1.726644, val:  52.08%, val_best:  52.08%, tr:  54.95%, tr_best:  54.95%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.489147/  1.854082, val:  56.25%, val_best:  56.25%, tr:  60.16%, tr_best:  60.16%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.271696/  1.700332, val:  54.58%, val_best:  56.25%, tr:  66.80%, tr_best:  66.80%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.117811/  1.225388, val:  76.67%, val_best:  76.67%, tr:  68.95%, tr_best:  68.95%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  0.849512/  1.395129, val:  74.17%, val_best:  76.67%, tr:  82.53%, tr_best:  82.53%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.726975/  1.310852, val:  80.00%, val_best:  80.00%, tr:  87.64%, tr_best:  87.64%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.570135/  1.280079, val:  85.42%, val_best:  85.42%, tr:  91.52%, tr_best:  91.52%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.477925/  1.323187, val:  83.33%, val_best:  85.42%, tr:  94.18%, tr_best:  94.18%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.391380/  1.444022, val:  83.33%, val_best:  85.42%, tr:  96.12%, tr_best:  96.12%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.357424/  1.409811, val:  84.58%, val_best:  85.42%, tr:  96.02%, tr_best:  96.12%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.266785/  1.393384, val:  87.92%, val_best:  87.92%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.240825/  1.401601, val:  87.92%, val_best:  87.92%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.204727/  1.452520, val:  86.25%, val_best:  87.92%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.160289/  1.633341, val:  86.25%, val_best:  87.92%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.116908/  1.645421, val:  86.25%, val_best:  87.92%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.118734/  1.645166, val:  85.42%, val_best:  87.92%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.085892/  1.597750, val:  88.33%, val_best:  88.33%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.059507/  1.734836, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.059693/  1.711416, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.047211/  1.816071, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.042075/  1.784255, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.034428/  1.838683, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.030232/  1.889038, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.023584/  1.926530, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.017225/  1.968586, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.017639/  1.976396, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.015374/  2.008937, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.012646/  2.010812, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.011132/  2.067674, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.009053/  2.015531, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.009127/  2.039982, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.008653/  2.053784, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.007121/  2.060977, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.007480/  2.050128, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.006680/  2.103112, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.006464/  2.094790, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.006574/  2.109694, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.006352/  2.082543, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.006033/  2.114070, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.004446/  2.163562, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.004262/  2.183207, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.003659/  2.185058, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.003851/  2.189637, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.004021/  2.177495, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.003827/  2.181949, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.003043/  2.183743, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.003176/  2.189110, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.002841/  2.235699, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.002959/  2.217263, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.002622/  2.239961, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.002726/  2.251560, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.002604/  2.266371, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.002542/  2.265639, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.001777/  2.288312, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.001987/  2.293679, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.001737/  2.307978, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.001804/  2.309664, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.001670/  2.296714, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.001680/  2.302336, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.001587/  2.314028, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.001558/  2.314139, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.001533/  2.307209, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.002086/  2.336030, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.001531/  2.304687, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.001559/  2.318560, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.001421/  2.316717, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.001638/  2.335976, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.001373/  2.336837, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.001235/  2.349164, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.001280/  2.336759, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.001547/  2.342261, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.001588/  2.341779, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.001223/  2.343366, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.001298/  2.335327, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.001143/  2.338958, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.000979/  2.346117, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.001142/  2.342373, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.001174/  2.346258, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.001152/  2.354104, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.001082/  2.354894, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.000977/  2.373911, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.000884/  2.358593, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.000897/  2.375360, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.000919/  2.386170, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.001072/  2.337557, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.000954/  2.370546, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.001463/  2.350097, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.001211/  2.326054, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.001129/  2.335670, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.001006/  2.346079, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.000881/  2.368436, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.000767/  2.372458, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.000881/  2.382793, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.000877/  2.384401, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.000715/  2.402555, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.000703/  2.402024, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.000698/  2.428355, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.000706/  2.417186, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.000663/  2.427002, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f04a9d2de74cc4b9f66a898d43e228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▇▇███████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▆█▇████████████████▇██████████████████</td></tr><tr><td>tr_acc</td><td>▁▄▅▇████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▆█████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▆█▇████████████████▇██████████████████</td></tr><tr><td>val_loss</td><td>▅▅▁▁▂▂▃▃▄▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00066</td></tr><tr><td>val_acc_best</td><td>0.88333</td></tr><tr><td>val_acc_now</td><td>0.85833</td></tr><tr><td>val_loss</td><td>2.427</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">azure-sweep-15</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xz0pzut3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xz0pzut3</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_065550-xz0pzut3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ijtd2500 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_073017-ijtd2500</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ijtd2500' target=\"_blank\">avid-sweep-17</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ijtd2500' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ijtd2500</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = da7e4b41c275f954fb82e708e43d2b96\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=2560, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 554,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.484389/  3.019206, val:  41.25%, val_best:  41.25%, tr:  32.07%, tr_best:  32.07%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.693431/  2.947211, val:  47.08%, val_best:  47.08%, tr:  44.64%, tr_best:  44.64%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  3.031558/  3.207776, val:  38.33%, val_best:  47.08%, tr:  51.58%, tr_best:  51.58%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  2.520675/  2.387831, val:  51.25%, val_best:  51.25%, tr:  56.69%, tr_best:  56.69%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  2.381484/  4.024237, val:  53.75%, val_best:  53.75%, tr:  58.12%, tr_best:  58.12%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.971360/  3.335500, val:  49.58%, val_best:  53.75%, tr:  63.94%, tr_best:  63.94%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.508027/  3.029517, val:  55.83%, val_best:  55.83%, tr:  70.58%, tr_best:  70.58%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.791242/  3.966787, val:  48.33%, val_best:  55.83%, tr:  71.71%, tr_best:  71.71%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.304600/  2.674857, val:  70.42%, val_best:  70.42%, tr:  77.32%, tr_best:  77.32%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.243277/  2.702357, val:  70.00%, val_best:  70.42%, tr:  82.64%, tr_best:  82.64%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.701325/  2.498919, val:  73.33%, val_best:  73.33%, tr:  90.91%, tr_best:  90.91%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.622966/  2.625027, val:  76.25%, val_best:  76.25%, tr:  92.54%, tr_best:  92.54%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.440228/  2.784808, val:  76.67%, val_best:  76.67%, tr:  96.22%, tr_best:  96.22%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.326537/  2.683970, val:  78.75%, val_best:  78.75%, tr:  97.45%, tr_best:  97.45%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.202437/  2.872350, val:  74.58%, val_best:  78.75%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.117144/  2.727440, val:  82.08%, val_best:  82.08%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.107551/  3.041938, val:  80.42%, val_best:  82.08%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.082365/  2.883193, val:  80.00%, val_best:  82.08%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.073886/  2.952217, val:  80.00%, val_best:  82.08%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.037836/  2.921362, val:  81.67%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.024971/  2.980722, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.018474/  3.110010, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.019094/  3.131807, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.010215/  3.238869, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.007429/  3.175275, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.008031/  3.188707, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.007445/  3.154888, val:  82.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.005182/  3.204719, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.004671/  3.202014, val:  82.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.003108/  3.189867, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.003181/  3.242683, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.003578/  3.232615, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.002419/  3.229367, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.002522/  3.233749, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.002311/  3.274171, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.002047/  3.239594, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.001894/  3.260161, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.001918/  3.301568, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.002520/  3.299049, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.001721/  3.293425, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.002201/  3.323694, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.001647/  3.347682, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.001571/  3.345605, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.001228/  3.367152, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.001766/  3.342015, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.001212/  3.356995, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.001100/  3.356936, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.001263/  3.347494, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.001170/  3.372662, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.001142/  3.354705, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.001048/  3.375388, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.000924/  3.377366, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.000890/  3.361294, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.000824/  3.379454, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.000724/  3.385482, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.000726/  3.398305, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.000761/  3.399866, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.000681/  3.402680, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.000811/  3.382866, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.000695/  3.386509, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.000673/  3.376041, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.000613/  3.401115, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.000933/  3.379969, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.000958/  3.392008, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.000874/  3.453165, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.000737/  3.440958, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.000928/  3.452188, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.000982/  3.466651, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.000964/  3.478141, val:  80.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.000658/  3.463021, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.000633/  3.442437, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.000571/  3.460687, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.000655/  3.441788, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.000548/  3.423218, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.000565/  3.442493, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.000604/  3.424325, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.000624/  3.449900, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.000510/  3.406161, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.000501/  3.438486, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.000620/  3.460938, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.000560/  3.478878, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.000483/  3.471608, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.000452/  3.462632, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.000468/  3.466594, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.000539/  3.460889, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.000474/  3.465091, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.000427/  3.456977, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.000389/  3.474613, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.000425/  3.480803, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.000370/  3.488059, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.000380/  3.453430, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.000410/  3.471523, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.000421/  3.482735, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.000536/  3.455275, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.000542/  3.463892, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.000476/  3.475573, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.000434/  3.492147, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.000444/  3.516654, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.000415/  3.514364, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.000454/  3.518810, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd37810afaa045a4ad1df373ce93185f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▅▅▇▇██████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▁▃▃▆▇▇▇█▇██████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▃▄▅▆███████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▇█▆▅▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▃▃▆▇▇█████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▃▃▆▇▇▇█▇██████████████████████████████</td></tr><tr><td>val_loss</td><td>▃▄██▁▁▂▂▂▃▄▃▄▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00045</td></tr><tr><td>val_acc_best</td><td>0.8375</td></tr><tr><td>val_acc_now</td><td>0.82083</td></tr><tr><td>val_loss</td><td>3.51881</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">avid-sweep-17</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ijtd2500' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ijtd2500</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_073017-ijtd2500/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jqromcqr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_080435-jqromcqr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jqromcqr' target=\"_blank\">polar-sweep-19</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jqromcqr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jqromcqr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = da7e4b41c275f954fb82e708e43d2b96\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=2560, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 554,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.305372/  2.302864, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  2.305309/  2.300885, val:  10.83%, val_best:  10.83%, tr:   8.38%, tr_best:  10.01%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  2.231461/  2.109869, val:  31.67%, val_best:  31.67%, tr:  21.45%, tr_best:  21.45%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  1.906212/  1.757536, val:  48.75%, val_best:  48.75%, tr:  39.12%, tr_best:  39.12%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  1.580045/  1.553783, val:  49.58%, val_best:  49.58%, tr:  55.36%, tr_best:  55.36%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  1.365030/  1.390936, val:  61.25%, val_best:  61.25%, tr:  59.35%, tr_best:  59.35%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  1.236948/  1.310430, val:  57.92%, val_best:  61.25%, tr:  63.94%, tr_best:  63.94%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  1.117349/  1.227471, val:  59.58%, val_best:  61.25%, tr:  65.17%, tr_best:  65.17%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  1.020162/  1.232748, val:  60.83%, val_best:  61.25%, tr:  69.77%, tr_best:  69.77%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  0.958448/  1.225021, val:  59.17%, val_best:  61.25%, tr:  70.79%, tr_best:  70.79%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  0.907675/  1.210445, val:  60.00%, val_best:  61.25%, tr:  73.24%, tr_best:  73.24%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  0.846681/  1.197652, val:  63.33%, val_best:  63.33%, tr:  76.10%, tr_best:  76.10%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  0.806981/  1.199188, val:  67.08%, val_best:  67.08%, tr:  79.16%, tr_best:  79.16%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  0.762445/  1.231458, val:  62.08%, val_best:  67.08%, tr:  84.17%, tr_best:  84.17%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  0.697234/  1.256491, val:  65.00%, val_best:  67.08%, tr:  84.58%, tr_best:  84.58%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  0.651596/  1.199190, val:  75.83%, val_best:  75.83%, tr:  89.58%, tr_best:  89.58%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  0.617336/  1.192132, val:  74.58%, val_best:  75.83%, tr:  90.81%, tr_best:  90.81%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  0.546447/  1.206781, val:  74.17%, val_best:  75.83%, tr:  94.38%, tr_best:  94.38%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  0.496562/  1.267158, val:  74.58%, val_best:  75.83%, tr:  95.61%, tr_best:  95.61%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  0.450820/  1.251657, val:  75.42%, val_best:  75.83%, tr:  95.81%, tr_best:  95.81%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  0.406618/  1.299137, val:  75.83%, val_best:  75.83%, tr:  96.83%, tr_best:  96.83%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  0.365339/  1.283093, val:  77.08%, val_best:  77.08%, tr:  97.96%, tr_best:  97.96%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  0.322588/  1.299084, val:  78.75%, val_best:  78.75%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  0.293088/  1.312129, val:  81.25%, val_best:  81.25%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  0.266891/  1.385482, val:  78.75%, val_best:  81.25%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  0.243276/  1.360729, val:  80.00%, val_best:  81.25%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  0.219011/  1.394461, val:  79.58%, val_best:  81.25%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  0.202960/  1.414138, val:  79.58%, val_best:  81.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  0.188154/  1.416995, val:  81.25%, val_best:  81.25%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  0.165845/  1.433663, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  0.150821/  1.483973, val:  80.00%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  0.139890/  1.497198, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  0.119860/  1.528072, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.115501/  1.564604, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.104083/  1.567222, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.094769/  1.602778, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.089270/  1.611237, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.082481/  1.632880, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.079172/  1.652678, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.073599/  1.685451, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.062452/  1.687946, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.058966/  1.658087, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.055954/  1.692992, val:  79.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.050165/  1.695411, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.047493/  1.728368, val:  78.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.044892/  1.750279, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.042958/  1.748015, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.039705/  1.745008, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.038449/  1.770085, val:  79.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.037129/  1.784738, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.034066/  1.797185, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.033114/  1.807173, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.030686/  1.821312, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.031907/  1.825407, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.027869/  1.851235, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.027975/  1.840826, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.026121/  1.849381, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.024815/  1.868480, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.024101/  1.862207, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.023745/  1.882383, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.021569/  1.878266, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.020541/  1.891964, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.021022/  1.888400, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.018828/  1.894126, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.018462/  1.933270, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.017932/  1.936781, val:  78.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.016268/  1.946102, val:  78.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.017706/  1.930967, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.016695/  1.942664, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.015156/  1.951798, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.015204/  1.943158, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.014938/  1.958498, val:  78.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.014061/  1.981066, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.013087/  1.984402, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.013206/  1.990916, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.013177/  1.995728, val:  79.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.013387/  1.990111, val:  79.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.012480/  1.992781, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.011489/  2.002399, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.012199/  2.010689, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.012109/  2.002271, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.011575/  2.018835, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.010558/  1.998124, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.010663/  2.025841, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.010154/  1.991738, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.010200/  1.995742, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.009444/  2.023510, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.010172/  2.014545, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.010541/  2.023132, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.009196/  2.016959, val:  79.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.010270/  2.027669, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.009075/  2.022144, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.008710/  2.045156, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.008138/  2.027623, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.008567/  2.057381, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.008392/  2.071193, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.008246/  2.060391, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.008253/  2.073847, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.007852/  2.068277, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.007632/  2.076268, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3021f1f91644730ad8e60d048b8e0c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▄▅▆▇▇▇████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▅▆▆▇▆▇▇▇██████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▂▅▅▆▆▇█████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>██▆▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▅▆▆▇▇▇▇▇██████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▅▆▆▇▆▇▇▇██████████████████████████████</td></tr><tr><td>val_loss</td><td>█▇▃▁▁▁▁▁▁▂▂▂▂▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00763</td></tr><tr><td>val_acc_best</td><td>0.825</td></tr><tr><td>val_acc_now</td><td>0.82083</td></tr><tr><td>val_loss</td><td>2.07627</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">polar-sweep-19</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jqromcqr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jqromcqr</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_080435-jqromcqr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5apn2caz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "856b14a5ccdb4812a1f33794d8be5dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113289676399695, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_083853-5apn2caz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5apn2caz' target=\"_blank\">zany-sweep-21</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5apn2caz' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5apn2caz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ddd2d600e882f330b6acfa4755a9bed1\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=2560, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 554,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  1.907157/  1.537248, val:  45.42%, val_best:  45.42%, tr:  27.07%, tr_best:  27.07%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  1.235514/  1.270691, val:  57.50%, val_best:  57.50%, tr:  58.12%, tr_best:  58.12%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  1.077250/  1.219006, val:  62.08%, val_best:  62.08%, tr:  63.23%, tr_best:  63.23%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  0.940997/  1.267527, val:  59.58%, val_best:  62.08%, tr:  69.25%, tr_best:  69.25%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  0.881984/  1.070683, val:  74.58%, val_best:  74.58%, tr:  69.46%, tr_best:  69.46%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  0.790815/  1.081401, val:  67.08%, val_best:  74.58%, tr:  75.38%, tr_best:  75.38%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  0.709684/  1.036577, val:  72.92%, val_best:  74.58%, tr:  80.69%, tr_best:  80.69%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  0.628672/  0.963667, val:  78.33%, val_best:  78.33%, tr:  88.87%, tr_best:  88.87%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  0.556145/  0.989008, val:  82.08%, val_best:  82.08%, tr:  91.11%, tr_best:  91.11%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  0.473494/  1.025262, val:  77.50%, val_best:  82.08%, tr:  93.56%, tr_best:  93.56%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  0.446833/  1.000843, val:  82.92%, val_best:  82.92%, tr:  93.77%, tr_best:  93.77%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  0.404481/  0.922034, val:  86.25%, val_best:  86.25%, tr:  95.10%, tr_best:  95.10%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  0.376491/  0.941789, val:  85.83%, val_best:  86.25%, tr:  95.91%, tr_best:  95.91%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  0.348902/  0.954048, val:  84.17%, val_best:  86.25%, tr:  96.22%, tr_best:  96.22%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  0.313275/  1.074524, val:  83.33%, val_best:  86.25%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  0.281158/  0.999420, val:  85.83%, val_best:  86.25%, tr:  97.65%, tr_best:  97.65%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  0.269466/  1.021690, val:  84.58%, val_best:  86.25%, tr:  97.45%, tr_best:  97.65%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  0.246820/  1.054438, val:  83.33%, val_best:  86.25%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  0.219450/  1.128045, val:  83.75%, val_best:  86.25%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  0.201458/  1.093922, val:  86.25%, val_best:  86.25%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  0.178908/  1.096217, val:  85.00%, val_best:  86.25%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  0.172152/  1.114624, val:  85.00%, val_best:  86.25%, tr:  98.77%, tr_best:  98.98%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  0.166903/  1.126595, val:  84.58%, val_best:  86.25%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  0.164523/  1.120685, val:  87.92%, val_best:  87.92%, tr:  98.47%, tr_best:  99.59%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  0.125876/  1.165005, val:  85.83%, val_best:  87.92%, tr:  99.49%, tr_best:  99.59%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  0.119540/  1.174114, val:  85.83%, val_best:  87.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  0.113066/  1.184885, val:  85.83%, val_best:  87.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  0.106845/  1.248633, val:  85.00%, val_best:  87.92%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  0.094833/  1.231719, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  0.090544/  1.283226, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  0.079059/  1.301368, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  0.081013/  1.283244, val:  85.83%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  0.069879/  1.297273, val:  87.08%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.068205/  1.356895, val:  85.00%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.063105/  1.358221, val:  85.42%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.053390/  1.375870, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.050957/  1.398561, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.046718/  1.440619, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.051234/  1.386583, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.043972/  1.430497, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.039188/  1.442192, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.035928/  1.453018, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.036954/  1.452834, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.029902/  1.474000, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.029760/  1.480254, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.027048/  1.499752, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.026632/  1.490817, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.024748/  1.528660, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.025309/  1.536746, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.023058/  1.523615, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.022822/  1.544585, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.021315/  1.538035, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.019247/  1.565351, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.019817/  1.572942, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.018228/  1.581061, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.018273/  1.601433, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.015788/  1.600340, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.016358/  1.607823, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.014753/  1.632127, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.014211/  1.627899, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.014600/  1.627836, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.014536/  1.636531, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.013869/  1.651988, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.013750/  1.651608, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.011972/  1.662463, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.012184/  1.666651, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.012958/  1.680237, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.012581/  1.679512, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.010617/  1.670195, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.010026/  1.684797, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.009595/  1.686287, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.010231/  1.693262, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.009720/  1.700145, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.009933/  1.699604, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.009061/  1.711403, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.009745/  1.718045, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.008796/  1.718948, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.008227/  1.734446, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.007957/  1.711610, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.007872/  1.721682, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.007899/  1.726805, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.007878/  1.737020, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.007517/  1.751635, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.007554/  1.754979, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.007726/  1.753906, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.007253/  1.770636, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.007947/  1.765736, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.007805/  1.781480, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.007663/  1.796769, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.007526/  1.790794, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.007320/  1.796650, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.007209/  1.802131, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.007006/  1.799318, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.006776/  1.810242, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.006405/  1.799048, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.006874/  1.811025, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.006524/  1.805945, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.006261/  1.810651, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.006017/  1.839554, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.006291/  1.837435, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2254c16f200244b0a925a5ff52de3d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▆▅▆▆▇▇▇▇███████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▄▆▆▆█▇▇█▇████▇█▇███████████████████████</td></tr><tr><td>tr_acc</td><td>▁▄▅▇▇███████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▆▆▇███████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▄▆▆▆█▇▇█▇████▇█▇███████████████████████</td></tr><tr><td>val_loss</td><td>▆▃▂▁▂▁▂▂▂▂▃▃▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇███████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00629</td></tr><tr><td>val_acc_best</td><td>0.8875</td></tr><tr><td>val_acc_now</td><td>0.875</td></tr><tr><td>val_loss</td><td>1.83743</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zany-sweep-21</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5apn2caz' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5apn2caz</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_083853-5apn2caz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qcrrhgtc with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_091455-qcrrhgtc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/qcrrhgtc' target=\"_blank\">ruby-sweep-23</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/qcrrhgtc' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/qcrrhgtc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.0001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ddd2d600e882f330b6acfa4755a9bed1\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=2560, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 554,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0001000'], tr/val_loss:  2.303292/  2.303262, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-1   lr=['0.0001000'], tr/val_loss:  2.303515/  2.303170, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-2   lr=['0.0001000'], tr/val_loss:  2.303898/  2.303107, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-3   lr=['0.0001000'], tr/val_loss:  2.303458/  2.303050, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-4   lr=['0.0001000'], tr/val_loss:  2.303237/  2.302970, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-5   lr=['0.0001000'], tr/val_loss:  2.303206/  2.302919, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-6   lr=['0.0001000'], tr/val_loss:  2.302941/  2.302869, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-7   lr=['0.0001000'], tr/val_loss:  2.303220/  2.302852, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-8   lr=['0.0001000'], tr/val_loss:  2.302954/  2.302815, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-9   lr=['0.0001000'], tr/val_loss:  2.302957/  2.302783, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-10  lr=['0.0001000'], tr/val_loss:  2.302881/  2.302774, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-11  lr=['0.0001000'], tr/val_loss:  2.302969/  2.302757, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-12  lr=['0.0001000'], tr/val_loss:  2.302869/  2.302741, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-13  lr=['0.0001000'], tr/val_loss:  2.302783/  2.302730, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-14  lr=['0.0001000'], tr/val_loss:  2.303017/  2.302724, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-15  lr=['0.0001000'], tr/val_loss:  2.302970/  2.302698, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-16  lr=['0.0001000'], tr/val_loss:  2.302890/  2.302687, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-17  lr=['0.0001000'], tr/val_loss:  2.302625/  2.302683, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-18  lr=['0.0001000'], tr/val_loss:  2.302800/  2.302681, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-19  lr=['0.0001000'], tr/val_loss:  2.303032/  2.302667, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-20  lr=['0.0001000'], tr/val_loss:  2.302866/  2.302667, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-21  lr=['0.0001000'], tr/val_loss:  2.302974/  2.302660, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-22  lr=['0.0001000'], tr/val_loss:  2.302978/  2.302637, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-23  lr=['0.0001000'], tr/val_loss:  2.302720/  2.302633, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-24  lr=['0.0001000'], tr/val_loss:  2.302808/  2.302633, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-25  lr=['0.0001000'], tr/val_loss:  2.302777/  2.302623, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-26  lr=['0.0001000'], tr/val_loss:  2.302785/  2.302622, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-27  lr=['0.0001000'], tr/val_loss:  2.302819/  2.302618, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-28  lr=['0.0001000'], tr/val_loss:  2.302804/  2.302614, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-29  lr=['0.0001000'], tr/val_loss:  2.302664/  2.302612, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-30  lr=['0.0001000'], tr/val_loss:  2.302889/  2.302610, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-31  lr=['0.0001000'], tr/val_loss:  2.302769/  2.302607, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-32  lr=['0.0001000'], tr/val_loss:  2.302866/  2.302616, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-33  lr=['0.0001000'], tr/val_loss:  2.302862/  2.302607, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-34  lr=['0.0001000'], tr/val_loss:  2.302861/  2.302608, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-35  lr=['0.0001000'], tr/val_loss:  2.302841/  2.302605, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-36  lr=['0.0001000'], tr/val_loss:  2.302814/  2.302599, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-37  lr=['0.0001000'], tr/val_loss:  2.302878/  2.302601, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-38  lr=['0.0001000'], tr/val_loss:  2.302762/  2.302597, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-39  lr=['0.0001000'], tr/val_loss:  2.302747/  2.302595, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-40  lr=['0.0001000'], tr/val_loss:  2.302816/  2.302597, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-41  lr=['0.0001000'], tr/val_loss:  2.302771/  2.302594, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  10.01%\n",
      "epoch-42  lr=['0.0001000'], tr/val_loss:  2.302784/  2.302598, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-43  lr=['0.0001000'], tr/val_loss:  2.302796/  2.302596, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  10.01%\n",
      "epoch-44  lr=['0.0001000'], tr/val_loss:  2.302749/  2.302593, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  10.01%\n",
      "epoch-45  lr=['0.0001000'], tr/val_loss:  2.302786/  2.302602, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  10.01%\n",
      "epoch-46  lr=['0.0001000'], tr/val_loss:  2.302707/  2.302591, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  10.01%\n",
      "epoch-47  lr=['0.0001000'], tr/val_loss:  2.302829/  2.302595, val:  10.00%, val_best:  10.00%, tr:   8.38%, tr_best:  10.01%\n",
      "epoch-48  lr=['0.0001000'], tr/val_loss:  2.302746/  2.302597, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-49  lr=['0.0001000'], tr/val_loss:  2.302768/  2.302600, val:  10.00%, val_best:  10.00%, tr:   7.97%, tr_best:  10.01%\n",
      "epoch-50  lr=['0.0001000'], tr/val_loss:  2.302785/  2.302603, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  10.01%\n",
      "epoch-51  lr=['0.0001000'], tr/val_loss:  2.302707/  2.302599, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  10.01%\n",
      "epoch-52  lr=['0.0001000'], tr/val_loss:  2.302884/  2.302603, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  10.01%\n",
      "epoch-53  lr=['0.0001000'], tr/val_loss:  2.302829/  2.302600, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  10.01%\n",
      "epoch-54  lr=['0.0001000'], tr/val_loss:  2.302757/  2.302598, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  10.01%\n",
      "epoch-55  lr=['0.0001000'], tr/val_loss:  2.302783/  2.302598, val:  10.00%, val_best:  10.00%, tr:   9.70%, tr_best:  10.01%\n",
      "epoch-56  lr=['0.0001000'], tr/val_loss:  2.302867/  2.302595, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  10.01%\n",
      "epoch-57  lr=['0.0001000'], tr/val_loss:  2.302805/  2.302598, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  10.01%\n",
      "epoch-58  lr=['0.0001000'], tr/val_loss:  2.302791/  2.302598, val:  10.00%, val_best:  10.00%, tr:   7.35%, tr_best:  10.01%\n",
      "epoch-59  lr=['0.0001000'], tr/val_loss:  2.302880/  2.302601, val:  10.00%, val_best:  10.00%, tr:   9.70%, tr_best:  10.01%\n",
      "epoch-60  lr=['0.0001000'], tr/val_loss:  2.302737/  2.302597, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  10.01%\n",
      "epoch-61  lr=['0.0001000'], tr/val_loss:  2.302770/  2.302598, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  10.01%\n",
      "epoch-62  lr=['0.0001000'], tr/val_loss:  2.302841/  2.302596, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.01%\n",
      "epoch-63  lr=['0.0001000'], tr/val_loss:  2.302749/  2.302595, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  10.11%\n",
      "epoch-64  lr=['0.0001000'], tr/val_loss:  2.302692/  2.302601, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.11%\n",
      "epoch-65  lr=['0.0001000'], tr/val_loss:  2.302779/  2.302593, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.11%\n",
      "epoch-66  lr=['0.0001000'], tr/val_loss:  2.302690/  2.302593, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.11%\n",
      "epoch-67  lr=['0.0001000'], tr/val_loss:  2.302907/  2.302598, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  10.11%\n",
      "epoch-68  lr=['0.0001000'], tr/val_loss:  2.302797/  2.302569, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  10.11%\n",
      "epoch-69  lr=['0.0001000'], tr/val_loss:  2.302759/  2.302575, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.11%\n",
      "epoch-70  lr=['0.0001000'], tr/val_loss:  2.302850/  2.302558, val:  10.42%, val_best:  10.42%, tr:   9.50%, tr_best:  10.11%\n",
      "epoch-71  lr=['0.0001000'], tr/val_loss:  2.302831/  2.302575, val:  10.00%, val_best:  10.42%, tr:   8.78%, tr_best:  10.11%\n",
      "epoch-72  lr=['0.0001000'], tr/val_loss:  2.302699/  2.302568, val:  10.42%, val_best:  10.42%, tr:  10.11%, tr_best:  10.11%\n",
      "epoch-73  lr=['0.0001000'], tr/val_loss:  2.302369/  2.302403, val:  10.42%, val_best:  10.42%, tr:   9.50%, tr_best:  10.11%\n",
      "epoch-74  lr=['0.0001000'], tr/val_loss:  2.301823/  2.301929, val:  12.08%, val_best:  12.08%, tr:  10.93%, tr_best:  10.93%\n",
      "epoch-75  lr=['0.0001000'], tr/val_loss:  2.300739/  2.300790, val:  12.50%, val_best:  12.50%, tr:  12.97%, tr_best:  12.97%\n",
      "epoch-76  lr=['0.0001000'], tr/val_loss:  2.298545/  2.298713, val:  12.50%, val_best:  12.50%, tr:  13.38%, tr_best:  13.38%\n",
      "epoch-77  lr=['0.0001000'], tr/val_loss:  2.294322/  2.295177, val:  10.00%, val_best:  12.50%, tr:  12.46%, tr_best:  13.38%\n",
      "epoch-78  lr=['0.0001000'], tr/val_loss:  2.286684/  2.288589, val:  12.92%, val_best:  12.92%, tr:  13.59%, tr_best:  13.59%\n",
      "epoch-79  lr=['0.0001000'], tr/val_loss:  2.276584/  2.278686, val:  19.17%, val_best:  19.17%, tr:  13.99%, tr_best:  13.99%\n",
      "epoch-80  lr=['0.0001000'], tr/val_loss:  2.260212/  2.266805, val:  18.75%, val_best:  19.17%, tr:  17.77%, tr_best:  17.77%\n",
      "epoch-81  lr=['0.0001000'], tr/val_loss:  2.240157/  2.250382, val:  18.33%, val_best:  19.17%, tr:  19.51%, tr_best:  19.51%\n",
      "epoch-82  lr=['0.0001000'], tr/val_loss:  2.218201/  2.233154, val:  17.92%, val_best:  19.17%, tr:  19.82%, tr_best:  19.82%\n",
      "epoch-83  lr=['0.0001000'], tr/val_loss:  2.199690/  2.220592, val:  18.75%, val_best:  19.17%, tr:  19.41%, tr_best:  19.82%\n",
      "epoch-84  lr=['0.0001000'], tr/val_loss:  2.183240/  2.207582, val:  18.33%, val_best:  19.17%, tr:  20.84%, tr_best:  20.84%\n",
      "epoch-85  lr=['0.0001000'], tr/val_loss:  2.174379/  2.199583, val:  18.33%, val_best:  19.17%, tr:  20.33%, tr_best:  20.84%\n",
      "epoch-86  lr=['0.0001000'], tr/val_loss:  2.162827/  2.192324, val:  19.58%, val_best:  19.58%, tr:  20.63%, tr_best:  20.84%\n",
      "epoch-87  lr=['0.0001000'], tr/val_loss:  2.151804/  2.186733, val:  20.83%, val_best:  20.83%, tr:  21.76%, tr_best:  21.76%\n",
      "epoch-88  lr=['0.0001000'], tr/val_loss:  2.144945/  2.180414, val:  21.25%, val_best:  21.25%, tr:  21.45%, tr_best:  21.76%\n",
      "epoch-89  lr=['0.0001000'], tr/val_loss:  2.141766/  2.176071, val:  20.83%, val_best:  21.25%, tr:  21.65%, tr_best:  21.76%\n",
      "epoch-90  lr=['0.0001000'], tr/val_loss:  2.134284/  2.171399, val:  21.25%, val_best:  21.25%, tr:  21.45%, tr_best:  21.76%\n",
      "epoch-91  lr=['0.0001000'], tr/val_loss:  2.126521/  2.164967, val:  22.08%, val_best:  22.08%, tr:  23.39%, tr_best:  23.39%\n",
      "epoch-92  lr=['0.0001000'], tr/val_loss:  2.114603/  2.161172, val:  21.25%, val_best:  22.08%, tr:  24.00%, tr_best:  24.00%\n",
      "epoch-93  lr=['0.0001000'], tr/val_loss:  2.114713/  2.154737, val:  25.00%, val_best:  25.00%, tr:  23.70%, tr_best:  24.00%\n",
      "epoch-94  lr=['0.0001000'], tr/val_loss:  2.106384/  2.149231, val:  24.17%, val_best:  25.00%, tr:  24.51%, tr_best:  24.51%\n",
      "epoch-95  lr=['0.0001000'], tr/val_loss:  2.101066/  2.144198, val:  26.67%, val_best:  26.67%, tr:  25.33%, tr_best:  25.33%\n",
      "epoch-96  lr=['0.0001000'], tr/val_loss:  2.089336/  2.139413, val:  26.67%, val_best:  26.67%, tr:  25.33%, tr_best:  25.33%\n",
      "epoch-97  lr=['0.0001000'], tr/val_loss:  2.074255/  2.132831, val:  27.92%, val_best:  27.92%, tr:  25.03%, tr_best:  25.33%\n",
      "epoch-98  lr=['0.0001000'], tr/val_loss:  2.075950/  2.126524, val:  27.08%, val_best:  27.92%, tr:  26.15%, tr_best:  26.15%\n",
      "epoch-99  lr=['0.0001000'], tr/val_loss:  2.067735/  2.118268, val:  30.00%, val_best:  30.00%, tr:  28.70%, tr_best:  28.70%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c9ab45004b4bc2ae9858009b499675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▂▃▂▂▃▃▃▅▅▁▆▅▇▃▁▃▅▂▅▁▂▂▆▂▁▃▅▂▁▁▂▃▂▃▅█▇▇▁▇</td></tr><tr><td>summary_val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▅▄▄▅▅▅██</td></tr><tr><td>tr_acc</td><td>▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▁▂▁▂▂▃▃▃▆▆▇▇▇██</td></tr><tr><td>tr_epoch_loss</td><td>████████████████████████████████▇▅▄▃▃▂▂▁</td></tr><tr><td>val_acc_best</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▅▅▅▅▅▆██</td></tr><tr><td>val_acc_now</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▅▄▄▅▅▅██</td></tr><tr><td>val_loss</td><td>████████████████████████████████▇▅▄▃▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>0.66667</td></tr><tr><td>tr_acc</td><td>0.28703</td></tr><tr><td>tr_epoch_loss</td><td>2.06774</td></tr><tr><td>val_acc_best</td><td>0.3</td></tr><tr><td>val_acc_now</td><td>0.3</td></tr><tr><td>val_loss</td><td>2.11827</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ruby-sweep-23</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/qcrrhgtc' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/qcrrhgtc</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_091455-qcrrhgtc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6pzh4vs6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ea7e1f7efd4b58aee6924cfb0e7e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113844910222622, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_095012-6pzh4vs6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6pzh4vs6' target=\"_blank\">revived-sweep-26</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6pzh4vs6' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6pzh4vs6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ddd2d600e882f330b6acfa4755a9bed1\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=2560, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 554,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  2.519556/  2.623683, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  10.11%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  2.493291/  2.401671, val:  10.00%, val_best:  10.00%, tr:   7.76%, tr_best:  10.11%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  2.471028/  2.441473, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  10.11%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  2.438038/  2.464993, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.11%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  2.435606/  2.453039, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  10.11%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  2.405079/  2.926988, val:  20.00%, val_best:  20.00%, tr:  12.16%, tr_best:  12.16%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  2.429451/  4.455843, val:  19.58%, val_best:  20.00%, tr:  30.64%, tr_best:  30.64%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  3.675644/  4.661559, val:  21.25%, val_best:  21.25%, tr:  29.83%, tr_best:  30.64%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  4.706301/  8.438703, val:  20.00%, val_best:  21.25%, tr:  24.41%, tr_best:  30.64%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  5.339550/ 10.367605, val:  12.50%, val_best:  21.25%, tr:  22.47%, tr_best:  30.64%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  8.790326/ 15.052926, val:  21.25%, val_best:  21.25%, tr:  19.92%, tr_best:  30.64%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss: 12.480469/  7.199643, val:  22.92%, val_best:  22.92%, tr:  19.82%, tr_best:  30.64%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss: 13.462415/ 17.280764, val:  10.00%, val_best:  22.92%, tr:  15.12%, tr_best:  30.64%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss: 18.117949/ 25.574879, val:  10.42%, val_best:  22.92%, tr:  16.75%, tr_best:  30.64%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss: 22.279383/ 31.532837, val:  10.00%, val_best:  22.92%, tr:  14.10%, tr_best:  30.64%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss: 23.399763/ 31.280569, val:  10.00%, val_best:  22.92%, tr:  13.18%, tr_best:  30.64%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss: 23.025785/ 31.555527, val:  10.00%, val_best:  22.92%, tr:   9.70%, tr_best:  30.64%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss: 27.646118/ 21.672724, val:  10.00%, val_best:  22.92%, tr:   9.19%, tr_best:  30.64%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss: 27.818853/ 30.671732, val:  10.00%, val_best:  22.92%, tr:   9.91%, tr_best:  30.64%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss: 27.005402/ 26.420069, val:  10.00%, val_best:  22.92%, tr:  11.85%, tr_best:  30.64%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss: 32.828495/ 38.140537, val:  10.00%, val_best:  22.92%, tr:   9.81%, tr_best:  30.64%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss: 28.620714/ 24.641888, val:  10.00%, val_best:  22.92%, tr:   9.91%, tr_best:  30.64%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss: 28.869673/ 22.588007, val:  10.00%, val_best:  22.92%, tr:  10.93%, tr_best:  30.64%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss: 29.252193/ 24.623919, val:  10.00%, val_best:  22.92%, tr:  10.01%, tr_best:  30.64%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss: 28.709742/ 28.618591, val:  10.00%, val_best:  22.92%, tr:  10.52%, tr_best:  30.64%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss: 26.950928/ 24.450872, val:  10.42%, val_best:  22.92%, tr:  10.32%, tr_best:  30.64%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss: 28.770687/ 25.214272, val:  10.00%, val_best:  22.92%, tr:   9.91%, tr_best:  30.64%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss: 25.584074/ 27.544630, val:  10.00%, val_best:  22.92%, tr:  10.01%, tr_best:  30.64%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss: 31.725374/ 28.552179, val:  10.00%, val_best:  22.92%, tr:   8.99%, tr_best:  30.64%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss: 28.410904/ 21.268902, val:  10.00%, val_best:  22.92%, tr:   9.30%, tr_best:  30.64%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss: 28.943981/ 28.205290, val:  10.00%, val_best:  22.92%, tr:   8.78%, tr_best:  30.64%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss: 27.468290/ 29.937063, val:  10.00%, val_best:  22.92%, tr:   9.60%, tr_best:  30.64%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss: 31.664621/ 20.560905, val:  10.00%, val_best:  22.92%, tr:   8.17%, tr_best:  30.64%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss: 26.113966/ 31.664207, val:  10.00%, val_best:  22.92%, tr:   8.89%, tr_best:  30.64%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss: 35.250282/ 27.622435, val:  10.00%, val_best:  22.92%, tr:   9.60%, tr_best:  30.64%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss: 28.731979/ 56.048992, val:  10.00%, val_best:  22.92%, tr:   9.70%, tr_best:  30.64%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss: 32.883991/ 25.089457, val:  10.00%, val_best:  22.92%, tr:  11.34%, tr_best:  30.64%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss: 32.360413/ 34.787350, val:  10.00%, val_best:  22.92%, tr:   8.07%, tr_best:  30.64%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss: 26.664419/ 27.676067, val:  10.00%, val_best:  22.92%, tr:  10.93%, tr_best:  30.64%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss: 25.892845/ 34.554443, val:  10.00%, val_best:  22.92%, tr:  10.62%, tr_best:  30.64%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss: 28.485209/ 29.928341, val:  10.00%, val_best:  22.92%, tr:   8.58%, tr_best:  30.64%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss: 29.667402/ 13.437392, val:  10.00%, val_best:  22.92%, tr:   9.19%, tr_best:  30.64%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss: 30.947290/ 12.924273, val:  10.42%, val_best:  22.92%, tr:   8.58%, tr_best:  30.64%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss: 23.652403/ 24.973000, val:  10.00%, val_best:  22.92%, tr:   9.30%, tr_best:  30.64%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss: 33.835823/ 28.644436, val:  10.00%, val_best:  22.92%, tr:  10.62%, tr_best:  30.64%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss: 23.767849/ 16.200369, val:  10.00%, val_best:  22.92%, tr:   9.30%, tr_best:  30.64%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss: 25.329531/ 27.481600, val:  10.00%, val_best:  22.92%, tr:  10.62%, tr_best:  30.64%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss: 31.381447/ 35.716335, val:  10.00%, val_best:  22.92%, tr:   8.58%, tr_best:  30.64%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss: 30.053200/ 38.698845, val:  10.00%, val_best:  22.92%, tr:   9.19%, tr_best:  30.64%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss: 31.818300/ 34.689693, val:  10.00%, val_best:  22.92%, tr:  10.32%, tr_best:  30.64%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss: 30.796820/ 10.906704, val:  10.00%, val_best:  22.92%, tr:  12.16%, tr_best:  30.64%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss: 23.261253/ 34.625359, val:  10.00%, val_best:  22.92%, tr:  10.32%, tr_best:  30.64%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss: 34.099712/ 18.232357, val:  10.00%, val_best:  22.92%, tr:   9.30%, tr_best:  30.64%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss: 33.616344/ 19.173805, val:  10.00%, val_best:  22.92%, tr:  10.11%, tr_best:  30.64%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss: 24.998758/ 26.521729, val:  10.00%, val_best:  22.92%, tr:  11.85%, tr_best:  30.64%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss: 31.859024/ 34.620209, val:  10.00%, val_best:  22.92%, tr:   8.89%, tr_best:  30.64%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss: 30.200272/ 24.913456, val:  10.00%, val_best:  22.92%, tr:   9.60%, tr_best:  30.64%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss: 35.181793/ 22.564287, val:  10.00%, val_best:  22.92%, tr:   7.56%, tr_best:  30.64%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss: 34.957466/ 40.887836, val:  10.00%, val_best:  22.92%, tr:  10.11%, tr_best:  30.64%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss: 36.691338/ 23.010004, val:  10.00%, val_best:  22.92%, tr:  10.01%, tr_best:  30.64%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss: 33.721336/ 31.904604, val:  10.00%, val_best:  22.92%, tr:   9.81%, tr_best:  30.64%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss: 32.251549/ 29.443119, val:  10.00%, val_best:  22.92%, tr:   9.81%, tr_best:  30.64%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss: 30.488043/ 25.208609, val:  10.00%, val_best:  22.92%, tr:   9.81%, tr_best:  30.64%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss: 30.102859/ 48.023075, val:  10.00%, val_best:  22.92%, tr:   9.19%, tr_best:  30.64%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss: 33.813030/ 31.058369, val:  10.00%, val_best:  22.92%, tr:   8.99%, tr_best:  30.64%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss: 28.213194/ 28.080887, val:  10.00%, val_best:  22.92%, tr:   9.09%, tr_best:  30.64%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss: 34.399662/ 37.752525, val:  10.00%, val_best:  22.92%, tr:   9.91%, tr_best:  30.64%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss: 34.307964/ 37.657593, val:  10.00%, val_best:  22.92%, tr:  11.13%, tr_best:  30.64%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss: 31.328907/ 22.754576, val:  10.00%, val_best:  22.92%, tr:  11.75%, tr_best:  30.64%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss: 27.259594/ 32.082905, val:  10.00%, val_best:  22.92%, tr:   9.40%, tr_best:  30.64%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss: 31.275507/ 33.680744, val:  10.00%, val_best:  22.92%, tr:   9.19%, tr_best:  30.64%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss: 30.341515/ 25.512367, val:  10.00%, val_best:  22.92%, tr:  10.52%, tr_best:  30.64%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss: 26.059261/ 21.078377, val:  10.00%, val_best:  22.92%, tr:  10.01%, tr_best:  30.64%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss: 27.874521/ 31.650448, val:  10.42%, val_best:  22.92%, tr:  11.44%, tr_best:  30.64%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss: 23.380737/ 23.852600, val:  10.00%, val_best:  22.92%, tr:  10.62%, tr_best:  30.64%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss: 33.207489/ 35.599030, val:  10.00%, val_best:  22.92%, tr:  10.83%, tr_best:  30.64%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss: 30.598228/ 37.297920, val:  10.00%, val_best:  22.92%, tr:  10.42%, tr_best:  30.64%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss: 32.253418/ 26.883923, val:  10.00%, val_best:  22.92%, tr:  11.64%, tr_best:  30.64%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss: 32.385899/ 34.864227, val:  10.00%, val_best:  22.92%, tr:   8.78%, tr_best:  30.64%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss: 26.964651/ 17.764158, val:  10.00%, val_best:  22.92%, tr:  11.13%, tr_best:  30.64%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss: 22.760351/ 29.490427, val:  10.00%, val_best:  22.92%, tr:  11.13%, tr_best:  30.64%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss: 27.146704/ 23.206608, val:  10.00%, val_best:  22.92%, tr:   9.40%, tr_best:  30.64%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss: 26.172237/ 34.996239, val:  10.00%, val_best:  22.92%, tr:   9.60%, tr_best:  30.64%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss: 25.813110/ 42.558575, val:  10.00%, val_best:  22.92%, tr:   9.81%, tr_best:  30.64%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss: 31.775494/ 40.659203, val:  10.00%, val_best:  22.92%, tr:   9.91%, tr_best:  30.64%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss: 33.275860/ 28.469936, val:  10.00%, val_best:  22.92%, tr:  10.01%, tr_best:  30.64%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss: 29.290541/ 25.623831, val:  10.00%, val_best:  22.92%, tr:  10.21%, tr_best:  30.64%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss: 20.695702/ 28.019638, val:  10.00%, val_best:  22.92%, tr:  10.11%, tr_best:  30.64%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss: 26.605164/ 50.829708, val:  10.00%, val_best:  22.92%, tr:  10.93%, tr_best:  30.64%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss: 34.031551/ 22.278759, val:  10.00%, val_best:  22.92%, tr:  11.64%, tr_best:  30.64%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss: 27.674196/ 31.138977, val:  10.00%, val_best:  22.92%, tr:   9.60%, tr_best:  30.64%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss: 29.744797/ 25.945333, val:  10.00%, val_best:  22.92%, tr:   9.81%, tr_best:  30.64%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss: 28.574057/ 22.228756, val:  10.42%, val_best:  22.92%, tr:   8.38%, tr_best:  30.64%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss: 28.966801/ 31.736418, val:  10.00%, val_best:  22.92%, tr:  10.62%, tr_best:  30.64%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss: 31.047771/ 25.270962, val:  10.42%, val_best:  22.92%, tr:  10.52%, tr_best:  30.64%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss: 30.942297/ 38.733562, val:  10.00%, val_best:  22.92%, tr:   9.70%, tr_best:  30.64%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss: 33.925896/ 24.697121, val:  10.00%, val_best:  22.92%, tr:   9.09%, tr_best:  30.64%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss: 31.515308/ 39.437992, val:  10.00%, val_best:  22.92%, tr:  10.42%, tr_best:  30.64%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss: 31.505663/ 24.958797, val:  10.00%, val_best:  22.92%, tr:  11.54%, tr_best:  30.64%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss: 30.410807/ 23.322666, val:  10.00%, val_best:  22.92%, tr:  10.32%, tr_best:  30.64%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "810004458d3e4a538ca8cd372cbecaaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▅▅▂▅▂█▂▂▂▄▂▆▄▂▄▂▂▁▂▄▁▅▁▂▂▂▁▄▂▄▅▄▂▂▁▁▅▁█▂</td></tr><tr><td>summary_val_acc</td><td>▁▁▁█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>tr_acc</td><td>▂▁▂█▆▃▃▂▂▂▂▂▂▁▂▁▂▁▂▁▂▂▂▁▂▂▁▂▂▂▂▂▂▂▂▂▂▁▂▂</td></tr><tr><td>tr_epoch_loss</td><td>▁▁▁▁▂▃▅▆▆▆▆▆▆▇▆▇▆▇▇▇▇▇▆██▇▆█▇▆▇▇▆▆▇▅▇▆▇▇</td></tr><tr><td>val_acc_best</td><td>▁▁▁▇▇███████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▁█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▁▁▁▁▂▃▅▄▄▄▄▄▃▃█▅▅▂▄▅▅▃▄▄▄▅▄▆▅▃▅▄▃▅▆▄▄▄▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>0.0</td></tr><tr><td>tr_acc</td><td>0.10317</td></tr><tr><td>tr_epoch_loss</td><td>30.41081</td></tr><tr><td>val_acc_best</td><td>0.22917</td></tr><tr><td>val_acc_now</td><td>0.1</td></tr><tr><td>val_loss</td><td>23.32267</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">revived-sweep-26</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6pzh4vs6' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6pzh4vs6</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_095012-6pzh4vs6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9xqt2ra2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85344aa977df42abb809addfc80535f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113464625345336, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_102505-9xqt2ra2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9xqt2ra2' target=\"_blank\">dulcet-sweep-27</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9xqt2ra2' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9xqt2ra2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ddd2d600e882f330b6acfa4755a9bed1\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=2560, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 554,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.305372/  2.302864, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  2.305082/  2.302657, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  10.01%\n"
     ]
    }
   ],
   "source": [
    "# sweep 하는 코드, 위 셀 주석처리 해야 됨.\n",
    "\n",
    "# 이런 워닝 뜨는 거는 걍 너가 main 안에서  wandb.config.update(hyperparameters)할 때 물려서임. 어차피 근데 sweep에서 지정한 걸로 덮어짐 \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [16]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [128]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.25]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.25, 0.5, 0.75, 1.0]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0, 0.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [1.0,2.0,3.0,4.0,5.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0, 0.25]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"net_save/save_now_net_weights_{unique_name}.pth\"]},\n",
    "        \"learning_rate\": {\"values\": [0.001,0.01,0.1,0.0001]}, \n",
    "        \"epoch_num\": {\"values\": [100]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [2]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True, False]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [True]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [False]},\n",
    "        \"denoise_on\": {\"values\": [True, False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [0]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [True]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [8]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"3\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  unique_name_hyper,\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "                        ) \n",
    "    # sigmoid와 BN이 있어야 잘된다.\n",
    "    # average pooling\n",
    "    # 이 낫다. \n",
    "    \n",
    "    # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# sweep_id = '6pj3lh8j'\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
