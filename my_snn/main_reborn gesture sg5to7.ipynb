{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24220/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "                \n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"5\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 20664,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.5,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 6.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 1/512, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 300,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = 9, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[-10,-10],[-10,-10],[-9,-9]], \n",
    "# # 1w -11~-9\n",
    "# # 1b -11~ -7\n",
    "# # 2w -10~-8\n",
    "# # 2b -10~-8\n",
    "# # 3w -10\n",
    "# # 3b -10\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: somm1xs3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 3012\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251020_211613-somm1xs3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/somm1xs3' target=\"_blank\">grateful-sweep-112</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/somm1xs3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/somm1xs3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251020_211620_671', 'my_seed': 3012, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6.5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0df5ce43f802d21fe74cde54437db10b\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 977 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = f205136b2771111650a88c4e480cfe73\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 963 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 391e4997dc3a746988cd0e9dceb2d42e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 816 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = bb0ac3251c9e44bfe72bcb8b2e969f0d\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 448 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = c796a451486ae8cd6d0dd9bd02a9e235\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 149 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = a6e81fbc907b11cedc166a7f5b843582\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 61 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = d4ded3e2b3703cdb1192f3d689158f82\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 26 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 602987c624e8b98603f8b906841eadb1\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 13 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 2d3185edb0c7b53adc6375ce1392ad59\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 4 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 9e9960951042c2f18fd3576739597330\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 4436 BATCH: 1 train_data_count: 4436\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6.5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6.5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 409.0\n",
      "lif layer 1 self.abs_max_v: 409.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 544.0\n",
      "lif layer 1 self.abs_max_v: 671.0\n",
      "fc layer 2 self.abs_max_out: 72.0\n",
      "lif layer 2 self.abs_max_v: 72.0\n",
      "lif layer 1 self.abs_max_v: 681.5\n",
      "fc layer 2 self.abs_max_out: 137.0\n",
      "lif layer 2 self.abs_max_v: 170.0\n",
      "lif layer 1 self.abs_max_v: 770.0\n",
      "fc layer 2 self.abs_max_out: 237.0\n",
      "lif layer 2 self.abs_max_v: 237.5\n",
      "fc layer 1 self.abs_max_out: 816.0\n",
      "lif layer 1 self.abs_max_v: 964.5\n",
      "lif layer 2 self.abs_max_v: 266.0\n",
      "fc layer 2 self.abs_max_out: 290.0\n",
      "lif layer 2 self.abs_max_v: 372.0\n",
      "fc layer 1 self.abs_max_out: 951.0\n",
      "fc layer 2 self.abs_max_out: 304.0\n",
      "lif layer 2 self.abs_max_v: 393.0\n",
      "lif layer 1 self.abs_max_v: 996.0\n",
      "fc layer 2 self.abs_max_out: 385.0\n",
      "lif layer 2 self.abs_max_v: 457.5\n",
      "lif layer 1 self.abs_max_v: 1004.0\n",
      "lif layer 1 self.abs_max_v: 1145.0\n",
      "fc layer 2 self.abs_max_out: 493.0\n",
      "lif layer 2 self.abs_max_v: 621.0\n",
      "fc layer 3 self.abs_max_out: 33.0\n",
      "lif layer 2 self.abs_max_v: 652.5\n",
      "fc layer 2 self.abs_max_out: 547.0\n",
      "lif layer 2 self.abs_max_v: 685.0\n",
      "fc layer 1 self.abs_max_out: 1236.0\n",
      "lif layer 1 self.abs_max_v: 1236.0\n",
      "lif layer 2 self.abs_max_v: 717.0\n",
      "fc layer 3 self.abs_max_out: 34.0\n",
      "lif layer 2 self.abs_max_v: 754.5\n",
      "fc layer 3 self.abs_max_out: 53.0\n",
      "lif layer 2 self.abs_max_v: 770.5\n",
      "fc layer 3 self.abs_max_out: 61.0\n",
      "fc layer 3 self.abs_max_out: 64.0\n",
      "fc layer 2 self.abs_max_out: 560.0\n",
      "fc layer 3 self.abs_max_out: 74.0\n",
      "lif layer 1 self.abs_max_v: 1261.0\n",
      "fc layer 3 self.abs_max_out: 126.0\n",
      "fc layer 3 self.abs_max_out: 129.0\n",
      "fc layer 2 self.abs_max_out: 637.0\n",
      "lif layer 2 self.abs_max_v: 872.0\n",
      "fc layer 2 self.abs_max_out: 726.0\n",
      "lif layer 2 self.abs_max_v: 880.0\n",
      "fc layer 2 self.abs_max_out: 817.0\n",
      "lif layer 2 self.abs_max_v: 977.5\n",
      "fc layer 3 self.abs_max_out: 214.0\n",
      "fc layer 2 self.abs_max_out: 835.0\n",
      "lif layer 2 self.abs_max_v: 982.5\n",
      "lif layer 2 self.abs_max_v: 1004.5\n",
      "fc layer 1 self.abs_max_out: 1303.0\n",
      "lif layer 1 self.abs_max_v: 1303.0\n",
      "fc layer 1 self.abs_max_out: 1509.0\n",
      "lif layer 1 self.abs_max_v: 1509.0\n",
      "fc layer 2 self.abs_max_out: 851.0\n",
      "fc layer 2 self.abs_max_out: 965.0\n",
      "lif layer 2 self.abs_max_v: 1220.0\n",
      "fc layer 2 self.abs_max_out: 1033.0\n",
      "fc layer 3 self.abs_max_out: 216.0\n",
      "lif layer 2 self.abs_max_v: 1222.0\n",
      "fc layer 1 self.abs_max_out: 1642.0\n",
      "lif layer 1 self.abs_max_v: 1642.0\n",
      "fc layer 1 self.abs_max_out: 1702.0\n",
      "lif layer 1 self.abs_max_v: 1702.0\n",
      "fc layer 2 self.abs_max_out: 1169.0\n",
      "fc layer 3 self.abs_max_out: 231.0\n",
      "lif layer 2 self.abs_max_v: 1314.0\n",
      "fc layer 1 self.abs_max_out: 1875.0\n",
      "lif layer 1 self.abs_max_v: 1875.0\n",
      "lif layer 2 self.abs_max_v: 1708.5\n",
      "fc layer 3 self.abs_max_out: 237.0\n",
      "fc layer 1 self.abs_max_out: 1970.0\n",
      "lif layer 1 self.abs_max_v: 1970.0\n",
      "fc layer 2 self.abs_max_out: 1223.0\n",
      "fc layer 3 self.abs_max_out: 238.0\n",
      "fc layer 3 self.abs_max_out: 260.0\n",
      "fc layer 3 self.abs_max_out: 270.0\n",
      "fc layer 1 self.abs_max_out: 2011.0\n",
      "lif layer 1 self.abs_max_v: 2011.0\n",
      "fc layer 1 self.abs_max_out: 2054.0\n",
      "lif layer 1 self.abs_max_v: 2054.0\n",
      "fc layer 2 self.abs_max_out: 1325.0\n",
      "fc layer 1 self.abs_max_out: 2296.0\n",
      "lif layer 1 self.abs_max_v: 2296.0\n",
      "fc layer 2 self.abs_max_out: 1562.0\n",
      "fc layer 1 self.abs_max_out: 2330.0\n",
      "lif layer 1 self.abs_max_v: 2330.0\n",
      "fc layer 1 self.abs_max_out: 2796.0\n",
      "lif layer 1 self.abs_max_v: 2796.0\n",
      "lif layer 2 self.abs_max_v: 1781.0\n",
      "lif layer 2 self.abs_max_v: 1827.5\n",
      "fc layer 3 self.abs_max_out: 271.0\n",
      "fc layer 3 self.abs_max_out: 297.0\n",
      "fc layer 3 self.abs_max_out: 318.0\n",
      "fc layer 2 self.abs_max_out: 1578.0\n",
      "fc layer 2 self.abs_max_out: 1666.0\n",
      "fc layer 1 self.abs_max_out: 2900.0\n",
      "lif layer 1 self.abs_max_v: 2900.0\n",
      "fc layer 1 self.abs_max_out: 2908.0\n",
      "lif layer 1 self.abs_max_v: 2908.0\n",
      "fc layer 1 self.abs_max_out: 3434.0\n",
      "lif layer 1 self.abs_max_v: 3434.0\n",
      "fc layer 2 self.abs_max_out: 1761.0\n",
      "fc layer 3 self.abs_max_out: 325.0\n",
      "fc layer 3 self.abs_max_out: 338.0\n",
      "fc layer 2 self.abs_max_out: 1877.0\n",
      "lif layer 2 self.abs_max_v: 1877.0\n",
      "fc layer 1 self.abs_max_out: 3709.0\n",
      "lif layer 1 self.abs_max_v: 3709.0\n",
      "lif layer 2 self.abs_max_v: 1934.0\n",
      "fc layer 3 self.abs_max_out: 371.0\n",
      "fc layer 2 self.abs_max_out: 1956.0\n",
      "lif layer 2 self.abs_max_v: 1956.0\n",
      "fc layer 2 self.abs_max_out: 1960.0\n",
      "lif layer 2 self.abs_max_v: 1960.0\n",
      "fc layer 2 self.abs_max_out: 2117.0\n",
      "lif layer 2 self.abs_max_v: 2117.0\n",
      "fc layer 1 self.abs_max_out: 3984.0\n",
      "lif layer 1 self.abs_max_v: 3984.0\n",
      "fc layer 2 self.abs_max_out: 2212.0\n",
      "lif layer 2 self.abs_max_v: 2212.0\n",
      "fc layer 1 self.abs_max_out: 4072.0\n",
      "lif layer 1 self.abs_max_v: 4072.0\n",
      "fc layer 1 self.abs_max_out: 4156.0\n",
      "lif layer 1 self.abs_max_v: 4156.0\n",
      "fc layer 2 self.abs_max_out: 2279.0\n",
      "lif layer 2 self.abs_max_v: 2279.0\n",
      "fc layer 2 self.abs_max_out: 2462.0\n",
      "lif layer 2 self.abs_max_v: 2462.0\n",
      "fc layer 2 self.abs_max_out: 2558.0\n",
      "lif layer 2 self.abs_max_v: 2558.0\n",
      "fc layer 1 self.abs_max_out: 4239.0\n",
      "lif layer 1 self.abs_max_v: 4239.0\n",
      "fc layer 2 self.abs_max_out: 2568.0\n",
      "lif layer 2 self.abs_max_v: 2568.0\n",
      "fc layer 1 self.abs_max_out: 4246.0\n",
      "lif layer 1 self.abs_max_v: 4246.0\n",
      "fc layer 1 self.abs_max_out: 4422.0\n",
      "lif layer 1 self.abs_max_v: 4422.0\n",
      "fc layer 1 self.abs_max_out: 4466.0\n",
      "lif layer 1 self.abs_max_v: 4466.0\n",
      "fc layer 1 self.abs_max_out: 4499.0\n",
      "lif layer 1 self.abs_max_v: 4499.0\n",
      "fc layer 1 self.abs_max_out: 4735.0\n",
      "lif layer 1 self.abs_max_v: 4735.0\n",
      "fc layer 3 self.abs_max_out: 381.0\n",
      "fc layer 1 self.abs_max_out: 5051.0\n",
      "lif layer 1 self.abs_max_v: 5051.0\n",
      "fc layer 1 self.abs_max_out: 5349.0\n",
      "lif layer 1 self.abs_max_v: 5349.0\n",
      "fc layer 2 self.abs_max_out: 2668.0\n",
      "lif layer 2 self.abs_max_v: 2668.0\n",
      "fc layer 1 self.abs_max_out: 5462.0\n",
      "lif layer 1 self.abs_max_v: 5462.0\n",
      "fc layer 2 self.abs_max_out: 2762.0\n",
      "lif layer 2 self.abs_max_v: 2762.0\n",
      "fc layer 2 self.abs_max_out: 2806.0\n",
      "lif layer 2 self.abs_max_v: 2806.0\n",
      "fc layer 1 self.abs_max_out: 5592.0\n",
      "lif layer 1 self.abs_max_v: 5592.0\n",
      "fc layer 2 self.abs_max_out: 2847.0\n",
      "lif layer 2 self.abs_max_v: 2847.0\n",
      "fc layer 2 self.abs_max_out: 2896.0\n",
      "lif layer 2 self.abs_max_v: 2896.0\n",
      "fc layer 2 self.abs_max_out: 2902.0\n",
      "lif layer 2 self.abs_max_v: 2902.0\n",
      "fc layer 2 self.abs_max_out: 3004.0\n",
      "lif layer 2 self.abs_max_v: 3004.0\n",
      "fc layer 1 self.abs_max_out: 5664.0\n",
      "lif layer 1 self.abs_max_v: 5664.0\n",
      "fc layer 2 self.abs_max_out: 3011.0\n",
      "lif layer 2 self.abs_max_v: 3011.0\n",
      "fc layer 1 self.abs_max_out: 5963.0\n",
      "lif layer 1 self.abs_max_v: 5963.0\n",
      "fc layer 1 self.abs_max_out: 6154.0\n",
      "lif layer 1 self.abs_max_v: 6154.0\n",
      "fc layer 1 self.abs_max_out: 6279.0\n",
      "lif layer 1 self.abs_max_v: 6279.0\n",
      "lif layer 1 self.abs_max_v: 7120.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  2.070545/  2.129270, val:  39.17%, val_best:  39.17%, tr:  94.39%, tr_best:  94.39%, epoch time: 286.05 seconds, 4.77 minutes\n",
      "total_backward_count 44360 real_backward_count 11250  25.361%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 6550.0\n",
      "fc layer 3 self.abs_max_out: 416.0\n",
      "fc layer 2 self.abs_max_out: 3032.0\n",
      "lif layer 2 self.abs_max_v: 3032.0\n",
      "fc layer 2 self.abs_max_out: 3053.0\n",
      "lif layer 2 self.abs_max_v: 3053.0\n",
      "fc layer 1 self.abs_max_out: 6649.0\n",
      "fc layer 1 self.abs_max_out: 6678.0\n",
      "fc layer 2 self.abs_max_out: 3185.0\n",
      "lif layer 2 self.abs_max_v: 3185.0\n",
      "fc layer 1 self.abs_max_out: 6718.0\n",
      "fc layer 2 self.abs_max_out: 3239.0\n",
      "lif layer 2 self.abs_max_v: 3239.0\n",
      "fc layer 1 self.abs_max_out: 6878.0\n",
      "fc layer 2 self.abs_max_out: 3288.0\n",
      "lif layer 2 self.abs_max_v: 3288.0\n",
      "lif layer 1 self.abs_max_v: 7238.5\n",
      "lif layer 1 self.abs_max_v: 7258.0\n",
      "lif layer 1 self.abs_max_v: 7362.0\n",
      "lif layer 1 self.abs_max_v: 9018.0\n",
      "fc layer 1 self.abs_max_out: 7217.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  2.046742/  2.123590, val:  43.33%, val_best:  43.33%, tr:  99.30%, tr_best:  99.30%, epoch time: 286.30 seconds, 4.77 minutes\n",
      "total_backward_count 88720 real_backward_count 18789  21.178%\n",
      "lif layer 1 self.abs_max_v: 9193.0\n",
      "lif layer 1 self.abs_max_v: 9353.5\n",
      "lif layer 1 self.abs_max_v: 10228.0\n",
      "fc layer 1 self.abs_max_out: 7784.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  2.049663/  2.129553, val:  58.75%, val_best:  58.75%, tr:  99.57%, tr_best:  99.57%, epoch time: 286.27 seconds, 4.77 minutes\n",
      "total_backward_count 133080 real_backward_count 25587  19.227%\n",
      "fc layer 1 self.abs_max_out: 7943.0\n",
      "lif layer 1 self.abs_max_v: 10363.0\n",
      "fc layer 1 self.abs_max_out: 8075.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  2.048586/  2.112665, val:  56.25%, val_best:  58.75%, tr:  99.59%, tr_best:  99.59%, epoch time: 285.35 seconds, 4.76 minutes\n",
      "total_backward_count 177440 real_backward_count 32323  18.216%\n",
      "fc layer 2 self.abs_max_out: 3296.0\n",
      "lif layer 2 self.abs_max_v: 3296.0\n",
      "fc layer 2 self.abs_max_out: 3375.0\n",
      "lif layer 2 self.abs_max_v: 3375.0\n",
      "lif layer 1 self.abs_max_v: 10844.5\n",
      "fc layer 1 self.abs_max_out: 8192.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  2.040243/  2.108734, val:  67.92%, val_best:  67.92%, tr:  99.75%, tr_best:  99.75%, epoch time: 284.96 seconds, 4.75 minutes\n",
      "total_backward_count 221800 real_backward_count 38868  17.524%\n",
      "lif layer 2 self.abs_max_v: 3403.0\n",
      "lif layer 2 self.abs_max_v: 3453.5\n",
      "lif layer 2 self.abs_max_v: 3540.0\n",
      "lif layer 2 self.abs_max_v: 3568.0\n",
      "fc layer 2 self.abs_max_out: 3403.0\n",
      "fc layer 2 self.abs_max_out: 3494.0\n",
      "lif layer 1 self.abs_max_v: 11149.0\n",
      "fc layer 1 self.abs_max_out: 8448.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  2.044613/  2.116434, val:  43.33%, val_best:  67.92%, tr:  99.73%, tr_best:  99.75%, epoch time: 286.35 seconds, 4.77 minutes\n",
      "total_backward_count 266160 real_backward_count 45302  17.021%\n",
      "lif layer 2 self.abs_max_v: 3651.5\n",
      "lif layer 2 self.abs_max_v: 3668.0\n",
      "lif layer 2 self.abs_max_v: 3831.0\n",
      "lif layer 2 self.abs_max_v: 3910.0\n",
      "fc layer 1 self.abs_max_out: 8464.0\n",
      "lif layer 1 self.abs_max_v: 11161.5\n",
      "lif layer 2 self.abs_max_v: 4113.0\n",
      "lif layer 1 self.abs_max_v: 11860.5\n",
      "fc layer 1 self.abs_max_out: 8689.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  2.043806/  2.103792, val:  71.25%, val_best:  71.25%, tr:  99.68%, tr_best:  99.75%, epoch time: 285.80 seconds, 4.76 minutes\n",
      "total_backward_count 310520 real_backward_count 51483  16.580%\n",
      "fc layer 1 self.abs_max_out: 8803.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  2.034196/  2.102506, val:  69.17%, val_best:  71.25%, tr:  99.82%, tr_best:  99.82%, epoch time: 285.82 seconds, 4.76 minutes\n",
      "total_backward_count 354880 real_backward_count 57210  16.121%\n",
      "lif layer 1 self.abs_max_v: 11971.5\n",
      "fc layer 1 self.abs_max_out: 8980.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  2.019699/  2.088976, val:  69.17%, val_best:  71.25%, tr:  99.82%, tr_best:  99.82%, epoch time: 286.84 seconds, 4.78 minutes\n",
      "total_backward_count 399240 real_backward_count 62801  15.730%\n",
      "lif layer 1 self.abs_max_v: 12131.5\n",
      "fc layer 3 self.abs_max_out: 418.0\n",
      "fc layer 1 self.abs_max_out: 9111.0\n",
      "fc layer 2 self.abs_max_out: 3563.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  2.013729/  2.083155, val:  69.58%, val_best:  71.25%, tr:  99.89%, tr_best:  99.89%, epoch time: 287.96 seconds, 4.80 minutes\n",
      "total_backward_count 443600 real_backward_count 68114  15.355%\n",
      "lif layer 1 self.abs_max_v: 12318.5\n",
      "lif layer 2 self.abs_max_v: 4204.5\n",
      "fc layer 2 self.abs_max_out: 3676.0\n",
      "fc layer 1 self.abs_max_out: 9210.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  2.002524/  2.059447, val:  73.75%, val_best:  73.75%, tr:  99.82%, tr_best:  99.89%, epoch time: 286.01 seconds, 4.77 minutes\n",
      "total_backward_count 487960 real_backward_count 73296  15.021%\n",
      "lif layer 1 self.abs_max_v: 12581.5\n",
      "fc layer 1 self.abs_max_out: 9237.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  2.002361/  2.074542, val:  66.67%, val_best:  73.75%, tr:  99.93%, tr_best:  99.93%, epoch time: 286.19 seconds, 4.77 minutes\n",
      "total_backward_count 532320 real_backward_count 78315  14.712%\n",
      "fc layer 2 self.abs_max_out: 3712.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.984806/  2.052067, val:  82.08%, val_best:  82.08%, tr:  99.89%, tr_best:  99.93%, epoch time: 288.09 seconds, 4.80 minutes\n",
      "total_backward_count 576680 real_backward_count 83080  14.407%\n",
      "fc layer 2 self.abs_max_out: 3783.0\n",
      "lif layer 2 self.abs_max_v: 4334.0\n",
      "fc layer 1 self.abs_max_out: 9281.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.978733/  2.068787, val:  72.92%, val_best:  82.08%, tr:  99.93%, tr_best:  99.93%, epoch time: 289.27 seconds, 4.82 minutes\n",
      "total_backward_count 621040 real_backward_count 87729  14.126%\n",
      "fc layer 1 self.abs_max_out: 9310.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.977437/  2.047645, val:  60.42%, val_best:  82.08%, tr:  99.89%, tr_best:  99.93%, epoch time: 287.54 seconds, 4.79 minutes\n",
      "total_backward_count 665400 real_backward_count 92176  13.853%\n",
      "fc layer 1 self.abs_max_out: 9360.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.968138/  2.041715, val:  82.50%, val_best:  82.50%, tr:  99.95%, tr_best:  99.95%, epoch time: 288.75 seconds, 4.81 minutes\n",
      "total_backward_count 709760 real_backward_count 96715  13.626%\n",
      "lif layer 1 self.abs_max_v: 12790.0\n",
      "fc layer 3 self.abs_max_out: 429.0\n",
      "fc layer 1 self.abs_max_out: 9530.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.969293/  2.049222, val:  77.92%, val_best:  82.50%, tr:  99.95%, tr_best:  99.95%, epoch time: 288.51 seconds, 4.81 minutes\n",
      "total_backward_count 754120 real_backward_count 100926  13.383%\n",
      "fc layer 1 self.abs_max_out: 9644.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.969826/  2.050923, val:  67.50%, val_best:  82.50%, tr:  99.91%, tr_best:  99.95%, epoch time: 288.97 seconds, 4.82 minutes\n",
      "total_backward_count 798480 real_backward_count 105107  13.163%\n",
      "lif layer 1 self.abs_max_v: 12820.5\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.963546/  2.033221, val:  79.17%, val_best:  82.50%, tr:  99.95%, tr_best:  99.95%, epoch time: 289.59 seconds, 4.83 minutes\n",
      "total_backward_count 842840 real_backward_count 109178  12.954%\n",
      "fc layer 1 self.abs_max_out: 9733.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.965545/  2.043742, val:  78.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 288.83 seconds, 4.81 minutes\n",
      "total_backward_count 887200 real_backward_count 113084  12.746%\n",
      "fc layer 1 self.abs_max_out: 9832.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.966482/  2.042194, val:  80.00%, val_best:  82.50%, tr:  99.98%, tr_best: 100.00%, epoch time: 290.58 seconds, 4.84 minutes\n",
      "total_backward_count 931560 real_backward_count 116895  12.548%\n",
      "fc layer 1 self.abs_max_out: 9886.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.955070/  2.029696, val:  75.42%, val_best:  82.50%, tr:  99.95%, tr_best: 100.00%, epoch time: 287.85 seconds, 4.80 minutes\n",
      "total_backward_count 975920 real_backward_count 120719  12.370%\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.954971/  2.022425, val:  85.00%, val_best:  85.00%, tr:  99.98%, tr_best: 100.00%, epoch time: 290.09 seconds, 4.83 minutes\n",
      "total_backward_count 1020280 real_backward_count 124500  12.203%\n",
      "lif layer 1 self.abs_max_v: 12837.0\n",
      "fc layer 1 self.abs_max_out: 9891.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.956496/  2.043325, val:  85.00%, val_best:  85.00%, tr:  99.98%, tr_best: 100.00%, epoch time: 289.01 seconds, 4.82 minutes\n",
      "total_backward_count 1064640 real_backward_count 128061  12.029%\n",
      "fc layer 1 self.abs_max_out: 9941.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.959815/  2.022847, val:  87.50%, val_best:  87.50%, tr:  99.98%, tr_best: 100.00%, epoch time: 287.73 seconds, 4.80 minutes\n",
      "total_backward_count 1109000 real_backward_count 131603  11.867%\n",
      "fc layer 3 self.abs_max_out: 448.0\n",
      "fc layer 3 self.abs_max_out: 454.0\n",
      "fc layer 3 self.abs_max_out: 455.0\n",
      "fc layer 1 self.abs_max_out: 10067.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.947272/  2.013908, val:  73.75%, val_best:  87.50%, tr:  99.98%, tr_best: 100.00%, epoch time: 287.14 seconds, 4.79 minutes\n",
      "total_backward_count 1153360 real_backward_count 135085  11.712%\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.937547/  2.022053, val:  88.75%, val_best:  88.75%, tr:  99.98%, tr_best: 100.00%, epoch time: 287.10 seconds, 4.79 minutes\n",
      "total_backward_count 1197720 real_backward_count 138523  11.566%\n",
      "lif layer 2 self.abs_max_v: 4396.5\n",
      "lif layer 2 self.abs_max_v: 4400.5\n",
      "fc layer 1 self.abs_max_out: 10096.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.932486/  2.003498, val:  81.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.65 seconds, 4.78 minutes\n",
      "total_backward_count 1242080 real_backward_count 141773  11.414%\n",
      "lif layer 2 self.abs_max_v: 4569.5\n",
      "fc layer 1 self.abs_max_out: 10105.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.930574/  2.009741, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.35 seconds, 4.74 minutes\n",
      "total_backward_count 1286440 real_backward_count 145031  11.274%\n",
      "fc layer 3 self.abs_max_out: 460.0\n",
      "fc layer 3 self.abs_max_out: 463.0\n",
      "fc layer 1 self.abs_max_out: 10208.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.920755/  1.994488, val:  84.17%, val_best:  88.75%, tr:  99.98%, tr_best: 100.00%, epoch time: 286.25 seconds, 4.77 minutes\n",
      "total_backward_count 1330800 real_backward_count 148174  11.134%\n",
      "fc layer 3 self.abs_max_out: 473.0\n",
      "fc layer 1 self.abs_max_out: 10215.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.917909/  2.005500, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.62 seconds, 4.74 minutes\n",
      "total_backward_count 1375160 real_backward_count 151239  10.998%\n",
      "fc layer 1 self.abs_max_out: 10299.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.916211/  1.998367, val:  81.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 283.85 seconds, 4.73 minutes\n",
      "total_backward_count 1419520 real_backward_count 154293  10.869%\n",
      "fc layer 3 self.abs_max_out: 484.0\n",
      "fc layer 1 self.abs_max_out: 10300.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.921742/  1.999400, val:  82.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.58 seconds, 4.76 minutes\n",
      "total_backward_count 1463880 real_backward_count 157299  10.745%\n",
      "lif layer 2 self.abs_max_v: 4666.0\n",
      "lif layer 2 self.abs_max_v: 4714.5\n",
      "fc layer 3 self.abs_max_out: 487.0\n",
      "fc layer 1 self.abs_max_out: 10353.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.914409/  1.988306, val:  84.58%, val_best:  88.75%, tr:  99.98%, tr_best: 100.00%, epoch time: 285.63 seconds, 4.76 minutes\n",
      "total_backward_count 1508240 real_backward_count 160204  10.622%\n",
      "fc layer 1 self.abs_max_out: 10435.0\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.909288/  1.994487, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.89 seconds, 4.76 minutes\n",
      "total_backward_count 1552600 real_backward_count 163021  10.500%\n",
      "fc layer 1 self.abs_max_out: 10490.0\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.904052/  1.988899, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 283.59 seconds, 4.73 minutes\n",
      "total_backward_count 1596960 real_backward_count 165779  10.381%\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.904847/  1.986998, val:  87.08%, val_best:  90.00%, tr:  99.98%, tr_best: 100.00%, epoch time: 286.50 seconds, 4.77 minutes\n",
      "total_backward_count 1641320 real_backward_count 168515  10.267%\n",
      "fc layer 3 self.abs_max_out: 511.0\n",
      "fc layer 1 self.abs_max_out: 10500.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.902009/  1.976304, val:  87.08%, val_best:  90.00%, tr:  99.98%, tr_best: 100.00%, epoch time: 284.71 seconds, 4.75 minutes\n",
      "total_backward_count 1685680 real_backward_count 171196  10.156%\n",
      "fc layer 1 self.abs_max_out: 10526.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.893325/  1.973087, val:  84.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.89 seconds, 4.76 minutes\n",
      "total_backward_count 1730040 real_backward_count 173724  10.042%\n",
      "lif layer 1 self.abs_max_v: 13068.5\n",
      "fc layer 1 self.abs_max_out: 10559.0\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.894314/  1.968682, val:  73.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 287.32 seconds, 4.79 minutes\n",
      "total_backward_count 1774400 real_backward_count 176364   9.939%\n",
      "lif layer 1 self.abs_max_v: 13081.5\n",
      "fc layer 1 self.abs_max_out: 10566.0\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.881529/  1.978899, val:  80.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.90 seconds, 4.76 minutes\n",
      "total_backward_count 1818760 real_backward_count 178971   9.840%\n",
      "lif layer 1 self.abs_max_v: 13155.5\n",
      "fc layer 1 self.abs_max_out: 10571.0\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.884607/  1.976883, val:  83.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 287.05 seconds, 4.78 minutes\n",
      "total_backward_count 1863120 real_backward_count 181493   9.741%\n",
      "fc layer 1 self.abs_max_out: 10606.0\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.883766/  1.975437, val:  86.25%, val_best:  90.00%, tr:  99.98%, tr_best: 100.00%, epoch time: 287.27 seconds, 4.79 minutes\n",
      "total_backward_count 1907480 real_backward_count 183886   9.640%\n",
      "fc layer 3 self.abs_max_out: 519.0\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.879768/  1.966141, val:  82.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 287.69 seconds, 4.79 minutes\n",
      "total_backward_count 1951840 real_backward_count 186222   9.541%\n",
      "fc layer 1 self.abs_max_out: 10624.0\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.871385/  1.952583, val:  79.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 287.61 seconds, 4.79 minutes\n",
      "total_backward_count 1996200 real_backward_count 188624   9.449%\n",
      "fc layer 1 self.abs_max_out: 10708.0\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.868487/  1.972239, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.21 seconds, 4.77 minutes\n",
      "total_backward_count 2040560 real_backward_count 191045   9.362%\n",
      "fc layer 1 self.abs_max_out: 10725.0\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.877526/  1.968177, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.80 seconds, 4.78 minutes\n",
      "total_backward_count 2084920 real_backward_count 193352   9.274%\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.866861/  1.960689, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 283.15 seconds, 4.72 minutes\n",
      "total_backward_count 2129280 real_backward_count 195625   9.187%\n",
      "lif layer 1 self.abs_max_v: 13178.5\n",
      "fc layer 1 self.abs_max_out: 10729.0\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.871728/  1.960476, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.85 seconds, 4.76 minutes\n",
      "total_backward_count 2173640 real_backward_count 197870   9.103%\n",
      "lif layer 1 self.abs_max_v: 13301.5\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.859513/  1.951522, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.64 seconds, 4.76 minutes\n",
      "total_backward_count 2218000 real_backward_count 200116   9.022%\n",
      "fc layer 1 self.abs_max_out: 10745.0\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.863845/  1.947389, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.06 seconds, 4.73 minutes\n",
      "total_backward_count 2262360 real_backward_count 202303   8.942%\n",
      "lif layer 2 self.abs_max_v: 4770.0\n",
      "fc layer 1 self.abs_max_out: 10767.0\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.857444/  1.943753, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 287.79 seconds, 4.80 minutes\n",
      "total_backward_count 2306720 real_backward_count 204353   8.859%\n",
      "lif layer 1 self.abs_max_v: 13318.0\n",
      "fc layer 3 self.abs_max_out: 541.0\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.862050/  1.949760, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.68 seconds, 4.76 minutes\n",
      "total_backward_count 2351080 real_backward_count 206405   8.779%\n",
      "lif layer 2 self.abs_max_v: 4784.5\n",
      "lif layer 1 self.abs_max_v: 13638.5\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.853193/  1.950047, val:  84.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.22 seconds, 4.75 minutes\n",
      "total_backward_count 2395440 real_backward_count 208418   8.701%\n",
      "fc layer 1 self.abs_max_out: 10772.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.850042/  1.954057, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.24 seconds, 4.74 minutes\n",
      "total_backward_count 2439800 real_backward_count 210454   8.626%\n",
      "fc layer 1 self.abs_max_out: 10807.0\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.855906/  1.951850, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 287.15 seconds, 4.79 minutes\n",
      "total_backward_count 2484160 real_backward_count 212417   8.551%\n",
      "fc layer 1 self.abs_max_out: 10833.0\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.845567/  1.935938, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.75 seconds, 4.76 minutes\n",
      "total_backward_count 2528520 real_backward_count 214365   8.478%\n",
      "fc layer 1 self.abs_max_out: 10855.0\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.849165/  1.940139, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.71 seconds, 4.75 minutes\n",
      "total_backward_count 2572880 real_backward_count 216253   8.405%\n",
      "lif layer 2 self.abs_max_v: 4807.5\n",
      "fc layer 1 self.abs_max_out: 10879.0\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.839580/  1.935079, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.60 seconds, 4.78 minutes\n",
      "total_backward_count 2617240 real_backward_count 218215   8.338%\n",
      "fc layer 1 self.abs_max_out: 10885.0\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.838899/  1.937003, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.04 seconds, 4.77 minutes\n",
      "total_backward_count 2661600 real_backward_count 220068   8.268%\n",
      "fc layer 1 self.abs_max_out: 10896.0\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.825433/  1.917834, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 287.79 seconds, 4.80 minutes\n",
      "total_backward_count 2705960 real_backward_count 221853   8.199%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.823361/  1.914765, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.99 seconds, 4.75 minutes\n",
      "total_backward_count 2750320 real_backward_count 223640   8.131%\n",
      "fc layer 1 self.abs_max_out: 10956.0\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.819884/  1.926816, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.73 seconds, 4.78 minutes\n",
      "total_backward_count 2794680 real_backward_count 225415   8.066%\n",
      "fc layer 1 self.abs_max_out: 10985.0\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.823776/  1.909750, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.40 seconds, 4.74 minutes\n",
      "total_backward_count 2839040 real_backward_count 227196   8.003%\n",
      "fc layer 1 self.abs_max_out: 10993.0\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.821364/  1.925209, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.18 seconds, 4.75 minutes\n",
      "total_backward_count 2883400 real_backward_count 228937   7.940%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.826272/  1.940012, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.74 seconds, 4.78 minutes\n",
      "total_backward_count 2927760 real_backward_count 230616   7.877%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.829045/  1.917491, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 288.77 seconds, 4.81 minutes\n",
      "total_backward_count 2972120 real_backward_count 232281   7.815%\n",
      "fc layer 3 self.abs_max_out: 547.0\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.805861/  1.914550, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.10 seconds, 4.75 minutes\n",
      "total_backward_count 3016480 real_backward_count 233957   7.756%\n",
      "fc layer 3 self.abs_max_out: 560.0\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.805382/  1.913808, val:  89.58%, val_best:  90.42%, tr:  99.98%, tr_best: 100.00%, epoch time: 284.69 seconds, 4.74 minutes\n",
      "total_backward_count 3060840 real_backward_count 235521   7.695%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.815757/  1.918360, val:  88.75%, val_best:  90.42%, tr:  99.98%, tr_best: 100.00%, epoch time: 286.41 seconds, 4.77 minutes\n",
      "total_backward_count 3105200 real_backward_count 237142   7.637%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.814242/  1.910990, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.18 seconds, 4.77 minutes\n",
      "total_backward_count 3149560 real_backward_count 238676   7.578%\n",
      "fc layer 1 self.abs_max_out: 11033.0\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.807436/  1.916724, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.08 seconds, 4.77 minutes\n",
      "total_backward_count 3193920 real_backward_count 240321   7.524%\n",
      "fc layer 1 self.abs_max_out: 11036.0\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.807568/  1.910785, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.13 seconds, 4.77 minutes\n",
      "total_backward_count 3238280 real_backward_count 241836   7.468%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.803795/  1.910527, val:  87.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.64 seconds, 4.78 minutes\n",
      "total_backward_count 3282640 real_backward_count 243415   7.415%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.804213/  1.915821, val:  86.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.20 seconds, 4.77 minutes\n",
      "total_backward_count 3327000 real_backward_count 244902   7.361%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.805838/  1.899109, val:  82.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 287.85 seconds, 4.80 minutes\n",
      "total_backward_count 3371360 real_backward_count 246335   7.307%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.798235/  1.905973, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.71 seconds, 4.78 minutes\n",
      "total_backward_count 3415720 real_backward_count 247841   7.256%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.800891/  1.900572, val:  85.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.56 seconds, 4.78 minutes\n",
      "total_backward_count 3460080 real_backward_count 249343   7.206%\n",
      "fc layer 3 self.abs_max_out: 587.0\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.789954/  1.895962, val:  87.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.07 seconds, 4.75 minutes\n",
      "total_backward_count 3504440 real_backward_count 250702   7.154%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.779929/  1.895909, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.22 seconds, 4.77 minutes\n",
      "total_backward_count 3548800 real_backward_count 252096   7.104%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.785853/  1.894533, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.11 seconds, 4.77 minutes\n",
      "total_backward_count 3593160 real_backward_count 253448   7.054%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.789956/  1.895124, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.24 seconds, 4.77 minutes\n",
      "total_backward_count 3637520 real_backward_count 254806   7.005%\n",
      "lif layer 2 self.abs_max_v: 4825.5\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.792849/  1.913643, val:  85.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.35 seconds, 4.77 minutes\n",
      "total_backward_count 3681880 real_backward_count 256170   6.958%\n",
      "fc layer 1 self.abs_max_out: 11050.0\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.792343/  1.899660, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 288.73 seconds, 4.81 minutes\n",
      "total_backward_count 3726240 real_backward_count 257498   6.910%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.787262/  1.895790, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.45 seconds, 4.76 minutes\n",
      "total_backward_count 3770600 real_backward_count 258828   6.864%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.777517/  1.892428, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.85 seconds, 4.76 minutes\n",
      "total_backward_count 3814960 real_backward_count 260092   6.818%\n",
      "lif layer 1 self.abs_max_v: 13724.0\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.782844/  1.893996, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.16 seconds, 4.77 minutes\n",
      "total_backward_count 3859320 real_backward_count 261384   6.773%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.780870/  1.886725, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.13 seconds, 4.77 minutes\n",
      "total_backward_count 3903680 real_backward_count 262639   6.728%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.780044/  1.893749, val:  85.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.34 seconds, 4.76 minutes\n",
      "total_backward_count 3948040 real_backward_count 263992   6.687%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.777595/  1.898576, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.05 seconds, 4.77 minutes\n",
      "total_backward_count 3992400 real_backward_count 265258   6.644%\n",
      "fc layer 3 self.abs_max_out: 588.0\n",
      "lif layer 1 self.abs_max_v: 13825.5\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.785424/  1.892694, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.79 seconds, 4.76 minutes\n",
      "total_backward_count 4036760 real_backward_count 266578   6.604%\n",
      "lif layer 2 self.abs_max_v: 4834.0\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.792805/  1.902645, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.67 seconds, 4.76 minutes\n",
      "total_backward_count 4081120 real_backward_count 267825   6.563%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.781869/  1.886573, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.50 seconds, 4.76 minutes\n",
      "total_backward_count 4125480 real_backward_count 269002   6.521%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.777319/  1.884692, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.08 seconds, 4.77 minutes\n",
      "total_backward_count 4169840 real_backward_count 270169   6.479%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.782041/  1.893570, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.53 seconds, 4.78 minutes\n",
      "total_backward_count 4214200 real_backward_count 271376   6.440%\n",
      "fc layer 1 self.abs_max_out: 11061.0\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.780767/  1.895182, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.44 seconds, 4.76 minutes\n",
      "total_backward_count 4258560 real_backward_count 272479   6.398%\n",
      "lif layer 2 self.abs_max_v: 4959.5\n",
      "fc layer 1 self.abs_max_out: 11100.0\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.781864/  1.896910, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.09 seconds, 4.75 minutes\n",
      "total_backward_count 4302920 real_backward_count 273637   6.359%\n",
      "fc layer 3 self.abs_max_out: 591.0\n",
      "fc layer 1 self.abs_max_out: 11139.0\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.771504/  1.893822, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.09 seconds, 4.73 minutes\n",
      "total_backward_count 4347280 real_backward_count 274837   6.322%\n",
      "fc layer 1 self.abs_max_out: 11175.0\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.770886/  1.895439, val:  85.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.24 seconds, 4.77 minutes\n",
      "total_backward_count 4391640 real_backward_count 276027   6.285%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.777761/  1.891042, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.25 seconds, 4.77 minutes\n",
      "total_backward_count 4436000 real_backward_count 277181   6.248%\n",
      "lif layer 2 self.abs_max_v: 5069.5\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.782715/  1.894058, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.62 seconds, 4.74 minutes\n",
      "total_backward_count 4480360 real_backward_count 278256   6.211%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.782888/  1.901647, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.64 seconds, 4.76 minutes\n",
      "total_backward_count 4524720 real_backward_count 279345   6.174%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.772439/  1.887708, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.09 seconds, 4.75 minutes\n",
      "total_backward_count 4569080 real_backward_count 280453   6.138%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.766279/  1.883864, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.09 seconds, 4.75 minutes\n",
      "total_backward_count 4613440 real_backward_count 281540   6.103%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.768983/  1.881503, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.19 seconds, 4.77 minutes\n",
      "total_backward_count 4657800 real_backward_count 282551   6.066%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.767478/  1.878245, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 287.25 seconds, 4.79 minutes\n",
      "total_backward_count 4702160 real_backward_count 283613   6.032%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.760622/  1.878399, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.14 seconds, 4.75 minutes\n",
      "total_backward_count 4746520 real_backward_count 284681   5.998%\n",
      "lif layer 1 self.abs_max_v: 14021.0\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.768589/  1.888303, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.11 seconds, 4.77 minutes\n",
      "total_backward_count 4790880 real_backward_count 285717   5.964%\n",
      "fc layer 3 self.abs_max_out: 606.0\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.770167/  1.881446, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.50 seconds, 4.78 minutes\n",
      "total_backward_count 4835240 real_backward_count 286736   5.930%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.764216/  1.877039, val:  85.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.18 seconds, 4.77 minutes\n",
      "total_backward_count 4879600 real_backward_count 287795   5.898%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.767005/  1.884444, val:  87.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 287.34 seconds, 4.79 minutes\n",
      "total_backward_count 4923960 real_backward_count 288835   5.866%\n",
      "lif layer 1 self.abs_max_v: 14037.5\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.759078/  1.866332, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.45 seconds, 4.76 minutes\n",
      "total_backward_count 4968320 real_backward_count 289836   5.834%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.748308/  1.871946, val:  87.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.23 seconds, 4.75 minutes\n",
      "total_backward_count 5012680 real_backward_count 290836   5.802%\n",
      "lif layer 1 self.abs_max_v: 14160.5\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.749151/  1.869017, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.29 seconds, 4.77 minutes\n",
      "total_backward_count 5057040 real_backward_count 291874   5.772%\n",
      "lif layer 1 self.abs_max_v: 14161.0\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.745519/  1.873387, val:  87.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.06 seconds, 4.77 minutes\n",
      "total_backward_count 5101400 real_backward_count 292874   5.741%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.743318/  1.858256, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.10 seconds, 4.75 minutes\n",
      "total_backward_count 5145760 real_backward_count 293809   5.710%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.730419/  1.861449, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.22 seconds, 4.77 minutes\n",
      "total_backward_count 5190120 real_backward_count 294737   5.679%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.739627/  1.865336, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.88 seconds, 4.78 minutes\n",
      "total_backward_count 5234480 real_backward_count 295721   5.649%\n",
      "lif layer 2 self.abs_max_v: 5332.5\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.742551/  1.858455, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.48 seconds, 4.77 minutes\n",
      "total_backward_count 5278840 real_backward_count 296714   5.621%\n",
      "fc layer 3 self.abs_max_out: 612.0\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.737426/  1.857934, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 287.48 seconds, 4.79 minutes\n",
      "total_backward_count 5323200 real_backward_count 297646   5.591%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.738183/  1.850945, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.65 seconds, 4.76 minutes\n",
      "total_backward_count 5367560 real_backward_count 298576   5.563%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.740285/  1.869172, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.42 seconds, 4.76 minutes\n",
      "total_backward_count 5411920 real_backward_count 299497   5.534%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.742589/  1.860776, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.90 seconds, 4.78 minutes\n",
      "total_backward_count 5456280 real_backward_count 300461   5.507%\n",
      "lif layer 1 self.abs_max_v: 14188.0\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.741115/  1.869728, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.80 seconds, 4.75 minutes\n",
      "total_backward_count 5500640 real_backward_count 301346   5.478%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.741634/  1.868640, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 288.53 seconds, 4.81 minutes\n",
      "total_backward_count 5545000 real_backward_count 302224   5.450%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.738513/  1.869874, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 288.56 seconds, 4.81 minutes\n",
      "total_backward_count 5589360 real_backward_count 303115   5.423%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.745617/  1.862983, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.80 seconds, 4.78 minutes\n",
      "total_backward_count 5633720 real_backward_count 304080   5.397%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.739709/  1.862915, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 287.91 seconds, 4.80 minutes\n",
      "total_backward_count 5678080 real_backward_count 304968   5.371%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.741478/  1.860445, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.73 seconds, 4.76 minutes\n",
      "total_backward_count 5722440 real_backward_count 305927   5.346%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.738589/  1.854775, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.53 seconds, 4.78 minutes\n",
      "total_backward_count 5766800 real_backward_count 306841   5.321%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.730272/  1.849736, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.81 seconds, 4.76 minutes\n",
      "total_backward_count 5811160 real_backward_count 307734   5.296%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.734537/  1.858931, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.82 seconds, 4.76 minutes\n",
      "total_backward_count 5855520 real_backward_count 308574   5.270%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.734976/  1.861156, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.80 seconds, 4.78 minutes\n",
      "total_backward_count 5899880 real_backward_count 309406   5.244%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.734834/  1.857163, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.53 seconds, 4.76 minutes\n",
      "total_backward_count 5944240 real_backward_count 310244   5.219%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.730185/  1.851944, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 287.47 seconds, 4.79 minutes\n",
      "total_backward_count 5988600 real_backward_count 311075   5.194%\n",
      "fc layer 3 self.abs_max_out: 624.0\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.730157/  1.855749, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.38 seconds, 4.76 minutes\n",
      "total_backward_count 6032960 real_backward_count 311979   5.171%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.723607/  1.860382, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.64 seconds, 4.74 minutes\n",
      "total_backward_count 6077320 real_backward_count 312779   5.147%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.723897/  1.850346, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.21 seconds, 4.75 minutes\n",
      "total_backward_count 6121680 real_backward_count 313628   5.123%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.719617/  1.851123, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.34 seconds, 4.74 minutes\n",
      "total_backward_count 6166040 real_backward_count 314447   5.100%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.718458/  1.846056, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 287.00 seconds, 4.78 minutes\n",
      "total_backward_count 6210400 real_backward_count 315265   5.076%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.720697/  1.852824, val:  87.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.51 seconds, 4.78 minutes\n",
      "total_backward_count 6254760 real_backward_count 316049   5.053%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.715131/  1.846213, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.56 seconds, 4.74 minutes\n",
      "total_backward_count 6299120 real_backward_count 316839   5.030%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.719865/  1.851831, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.72 seconds, 4.76 minutes\n",
      "total_backward_count 6343480 real_backward_count 317631   5.007%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.728810/  1.863245, val:  86.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.98 seconds, 4.77 minutes\n",
      "total_backward_count 6387840 real_backward_count 318459   4.985%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.732384/  1.857254, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 287.01 seconds, 4.78 minutes\n",
      "total_backward_count 6432200 real_backward_count 319230   4.963%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.727492/  1.859451, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 287.38 seconds, 4.79 minutes\n",
      "total_backward_count 6476560 real_backward_count 320092   4.942%\n",
      "fc layer 3 self.abs_max_out: 625.0\n",
      "lif layer 1 self.abs_max_v: 14400.0\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.729372/  1.857772, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 283.08 seconds, 4.72 minutes\n",
      "total_backward_count 6520920 real_backward_count 320838   4.920%\n",
      "lif layer 1 self.abs_max_v: 14490.5\n",
      "lif layer 2 self.abs_max_v: 5413.0\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.729865/  1.864023, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.25 seconds, 4.75 minutes\n",
      "total_backward_count 6565280 real_backward_count 321631   4.899%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.731521/  1.860821, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.94 seconds, 4.77 minutes\n",
      "total_backward_count 6609640 real_backward_count 322491   4.879%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.727382/  1.862134, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.94 seconds, 4.77 minutes\n",
      "total_backward_count 6654000 real_backward_count 323308   4.859%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.726636/  1.858781, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.64 seconds, 4.76 minutes\n",
      "total_backward_count 6698360 real_backward_count 324111   4.839%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.732194/  1.857389, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.26 seconds, 4.75 minutes\n",
      "total_backward_count 6742720 real_backward_count 324883   4.818%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.728820/  1.860050, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.75 seconds, 4.78 minutes\n",
      "total_backward_count 6787080 real_backward_count 325716   4.799%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.720913/  1.853229, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.59 seconds, 4.76 minutes\n",
      "total_backward_count 6831440 real_backward_count 326437   4.778%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.722414/  1.857112, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.27 seconds, 4.75 minutes\n",
      "total_backward_count 6875800 real_backward_count 327187   4.759%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.725096/  1.858857, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.52 seconds, 4.74 minutes\n",
      "total_backward_count 6920160 real_backward_count 327935   4.739%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.716998/  1.851341, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.16 seconds, 4.75 minutes\n",
      "total_backward_count 6964520 real_backward_count 328638   4.719%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.721079/  1.855981, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.07 seconds, 4.77 minutes\n",
      "total_backward_count 7008880 real_backward_count 329332   4.699%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.715443/  1.838634, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.21 seconds, 4.75 minutes\n",
      "total_backward_count 7053240 real_backward_count 330091   4.680%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.714152/  1.845768, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.67 seconds, 4.74 minutes\n",
      "total_backward_count 7097600 real_backward_count 330755   4.660%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.716504/  1.852346, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.80 seconds, 4.76 minutes\n",
      "total_backward_count 7141960 real_backward_count 331465   4.641%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.725500/  1.855537, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.23 seconds, 4.77 minutes\n",
      "total_backward_count 7186320 real_backward_count 332234   4.623%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.725397/  1.854662, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.65 seconds, 4.78 minutes\n",
      "total_backward_count 7230680 real_backward_count 332926   4.604%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.724918/  1.858254, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.16 seconds, 4.77 minutes\n",
      "total_backward_count 7275040 real_backward_count 333602   4.586%\n",
      "fc layer 3 self.abs_max_out: 632.0\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.722823/  1.844458, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.65 seconds, 4.76 minutes\n",
      "total_backward_count 7319400 real_backward_count 334307   4.567%\n",
      "fc layer 2 self.abs_max_out: 3837.0\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.710616/  1.837642, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 287.30 seconds, 4.79 minutes\n",
      "total_backward_count 7363760 real_backward_count 335011   4.549%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.713237/  1.838345, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.51 seconds, 4.78 minutes\n",
      "total_backward_count 7408120 real_backward_count 335746   4.532%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.710749/  1.842275, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.06 seconds, 4.77 minutes\n",
      "total_backward_count 7452480 real_backward_count 336400   4.514%\n",
      "lif layer 2 self.abs_max_v: 5424.5\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.710685/  1.837393, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 287.62 seconds, 4.79 minutes\n",
      "total_backward_count 7496840 real_backward_count 337086   4.496%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.711238/  1.841869, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.97 seconds, 4.77 minutes\n",
      "total_backward_count 7541200 real_backward_count 337756   4.479%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.715239/  1.846086, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.00 seconds, 4.77 minutes\n",
      "total_backward_count 7585560 real_backward_count 338446   4.462%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.717206/  1.842041, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.42 seconds, 4.77 minutes\n",
      "total_backward_count 7629920 real_backward_count 339130   4.445%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.710114/  1.844816, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.24 seconds, 4.77 minutes\n",
      "total_backward_count 7674280 real_backward_count 339760   4.427%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.714647/  1.847329, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.94 seconds, 4.77 minutes\n",
      "total_backward_count 7718640 real_backward_count 340399   4.410%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.713143/  1.839870, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.10 seconds, 4.75 minutes\n",
      "total_backward_count 7763000 real_backward_count 341022   4.393%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.706432/  1.837412, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.58 seconds, 4.74 minutes\n",
      "total_backward_count 7807360 real_backward_count 341659   4.376%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.706167/  1.831955, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.49 seconds, 4.77 minutes\n",
      "total_backward_count 7851720 real_backward_count 342267   4.359%\n",
      "lif layer 2 self.abs_max_v: 5493.0\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.699484/  1.826116, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.59 seconds, 4.74 minutes\n",
      "total_backward_count 7896080 real_backward_count 342926   4.343%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.699192/  1.840959, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 283.86 seconds, 4.73 minutes\n",
      "total_backward_count 7940440 real_backward_count 343491   4.326%\n",
      "fc layer 3 self.abs_max_out: 643.0\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.708169/  1.853083, val:  86.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.52 seconds, 4.74 minutes\n",
      "total_backward_count 7984800 real_backward_count 344188   4.311%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.706666/  1.838004, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.93 seconds, 4.75 minutes\n",
      "total_backward_count 8029160 real_backward_count 344881   4.295%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.698831/  1.827392, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.53 seconds, 4.74 minutes\n",
      "total_backward_count 8073520 real_backward_count 345500   4.279%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.693193/  1.826812, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.50 seconds, 4.74 minutes\n",
      "total_backward_count 8117880 real_backward_count 346094   4.263%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.694665/  1.826303, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.47 seconds, 4.76 minutes\n",
      "total_backward_count 8162240 real_backward_count 346686   4.247%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.694279/  1.827406, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.87 seconds, 4.75 minutes\n",
      "total_backward_count 8206600 real_backward_count 347308   4.232%\n",
      "fc layer 3 self.abs_max_out: 649.0\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.683268/  1.827973, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.63 seconds, 4.78 minutes\n",
      "total_backward_count 8250960 real_backward_count 347943   4.217%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.684572/  1.814984, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.62 seconds, 4.76 minutes\n",
      "total_backward_count 8295320 real_backward_count 348522   4.201%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.685429/  1.830879, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.51 seconds, 4.74 minutes\n",
      "total_backward_count 8339680 real_backward_count 349159   4.187%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.681118/  1.814328, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.69 seconds, 4.76 minutes\n",
      "total_backward_count 8384040 real_backward_count 349765   4.172%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.678416/  1.812728, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.10 seconds, 4.77 minutes\n",
      "total_backward_count 8428400 real_backward_count 350394   4.157%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.676066/  1.818782, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.23 seconds, 4.77 minutes\n",
      "total_backward_count 8472760 real_backward_count 350946   4.142%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.675729/  1.815526, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.11 seconds, 4.75 minutes\n",
      "total_backward_count 8517120 real_backward_count 351547   4.128%\n",
      "lif layer 2 self.abs_max_v: 5533.5\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.681669/  1.816230, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.69 seconds, 4.74 minutes\n",
      "total_backward_count 8561480 real_backward_count 352127   4.113%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.681046/  1.811178, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.55 seconds, 4.74 minutes\n",
      "total_backward_count 8605840 real_backward_count 352648   4.098%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.670358/  1.804261, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.07 seconds, 4.75 minutes\n",
      "total_backward_count 8650200 real_backward_count 353157   4.083%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.674337/  1.812189, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 283.93 seconds, 4.73 minutes\n",
      "total_backward_count 8694560 real_backward_count 353741   4.069%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.672340/  1.811823, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.34 seconds, 4.76 minutes\n",
      "total_backward_count 8738920 real_backward_count 354331   4.055%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.675150/  1.819002, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.37 seconds, 4.76 minutes\n",
      "total_backward_count 8783280 real_backward_count 354855   4.040%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.676248/  1.807061, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.53 seconds, 4.78 minutes\n",
      "total_backward_count 8827640 real_backward_count 355452   4.027%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.667465/  1.810594, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.96 seconds, 4.77 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6701c5f241ad485cbabcb8f6c63ce5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÑ‚ñá‚ñÜ‚ñÖ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÑ‚ñá‚ñÜ‚ñÖ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.66747</td></tr><tr><td>val_acc_best</td><td>0.92083</td></tr><tr><td>val_acc_now</td><td>0.90833</td></tr><tr><td>val_loss</td><td>1.81059</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">grateful-sweep-112</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/somm1xs3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/somm1xs3</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251020_211613-somm1xs3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 060owsk3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 10272\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251021_131131-060owsk3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/060owsk3' target=\"_blank\">generous-sweep-114</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/060owsk3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/060owsk3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251021_131140_344', 'my_seed': 10272, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 7, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0df5ce43f802d21fe74cde54437db10b\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 977 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = f205136b2771111650a88c4e480cfe73\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 963 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 391e4997dc3a746988cd0e9dceb2d42e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 816 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = bb0ac3251c9e44bfe72bcb8b2e969f0d\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 448 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = c796a451486ae8cd6d0dd9bd02a9e235\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 149 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = a6e81fbc907b11cedc166a7f5b843582\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 61 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = d4ded3e2b3703cdb1192f3d689158f82\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 26 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 602987c624e8b98603f8b906841eadb1\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 13 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 2d3185edb0c7b53adc6375ce1392ad59\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 4 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 9e9960951042c2f18fd3576739597330\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 4436 BATCH: 1 train_data_count: 4436\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=7, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=7, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 393.0\n",
      "lif layer 1 self.abs_max_v: 393.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 624.0\n",
      "lif layer 1 self.abs_max_v: 757.5\n",
      "fc layer 2 self.abs_max_out: 72.0\n",
      "lif layer 2 self.abs_max_v: 72.0\n",
      "lif layer 1 self.abs_max_v: 817.5\n",
      "lif layer 2 self.abs_max_v: 105.0\n",
      "lif layer 1 self.abs_max_v: 838.0\n",
      "fc layer 2 self.abs_max_out: 135.0\n",
      "lif layer 2 self.abs_max_v: 179.0\n",
      "fc layer 2 self.abs_max_out: 230.0\n",
      "lif layer 2 self.abs_max_v: 254.0\n",
      "fc layer 1 self.abs_max_out: 930.0\n",
      "lif layer 1 self.abs_max_v: 1086.5\n",
      "fc layer 2 self.abs_max_out: 240.0\n",
      "lif layer 2 self.abs_max_v: 284.5\n",
      "lif layer 2 self.abs_max_v: 311.5\n",
      "fc layer 2 self.abs_max_out: 295.0\n",
      "lif layer 2 self.abs_max_v: 418.0\n",
      "fc layer 2 self.abs_max_out: 450.0\n",
      "lif layer 2 self.abs_max_v: 572.5\n",
      "fc layer 1 self.abs_max_out: 1027.0\n",
      "lif layer 1 self.abs_max_v: 1198.5\n",
      "fc layer 3 self.abs_max_out: 34.0\n",
      "fc layer 1 self.abs_max_out: 1333.0\n",
      "lif layer 1 self.abs_max_v: 1333.0\n",
      "fc layer 2 self.abs_max_out: 533.0\n",
      "lif layer 2 self.abs_max_v: 698.0\n",
      "fc layer 3 self.abs_max_out: 77.0\n",
      "lif layer 2 self.abs_max_v: 735.0\n",
      "fc layer 2 self.abs_max_out: 583.0\n",
      "lif layer 2 self.abs_max_v: 838.0\n",
      "fc layer 2 self.abs_max_out: 656.0\n",
      "fc layer 3 self.abs_max_out: 79.0\n",
      "fc layer 3 self.abs_max_out: 86.0\n",
      "fc layer 1 self.abs_max_out: 1349.0\n",
      "lif layer 1 self.abs_max_v: 1349.0\n",
      "fc layer 3 self.abs_max_out: 96.0\n",
      "fc layer 2 self.abs_max_out: 726.0\n",
      "lif layer 2 self.abs_max_v: 907.5\n",
      "fc layer 3 self.abs_max_out: 119.0\n",
      "lif layer 1 self.abs_max_v: 1379.0\n",
      "fc layer 3 self.abs_max_out: 163.0\n",
      "fc layer 1 self.abs_max_out: 1444.0\n",
      "lif layer 1 self.abs_max_v: 1444.0\n",
      "fc layer 2 self.abs_max_out: 742.0\n",
      "fc layer 1 self.abs_max_out: 1454.0\n",
      "lif layer 1 self.abs_max_v: 1454.0\n",
      "fc layer 1 self.abs_max_out: 1514.0\n",
      "lif layer 1 self.abs_max_v: 1514.0\n",
      "fc layer 1 self.abs_max_out: 1568.0\n",
      "lif layer 1 self.abs_max_v: 1568.0\n",
      "lif layer 2 self.abs_max_v: 937.0\n",
      "fc layer 1 self.abs_max_out: 1802.0\n",
      "lif layer 1 self.abs_max_v: 1802.0\n",
      "fc layer 1 self.abs_max_out: 1943.0\n",
      "lif layer 1 self.abs_max_v: 1943.0\n",
      "fc layer 2 self.abs_max_out: 773.0\n",
      "lif layer 2 self.abs_max_v: 1001.5\n",
      "fc layer 2 self.abs_max_out: 778.0\n",
      "lif layer 2 self.abs_max_v: 1095.0\n",
      "lif layer 2 self.abs_max_v: 1183.0\n",
      "lif layer 2 self.abs_max_v: 1210.5\n",
      "fc layer 2 self.abs_max_out: 823.0\n",
      "fc layer 3 self.abs_max_out: 208.0\n",
      "fc layer 3 self.abs_max_out: 242.0\n",
      "fc layer 2 self.abs_max_out: 998.0\n",
      "fc layer 2 self.abs_max_out: 1122.0\n",
      "lif layer 2 self.abs_max_v: 1226.5\n",
      "lif layer 2 self.abs_max_v: 1337.0\n",
      "lif layer 2 self.abs_max_v: 1517.0\n",
      "fc layer 2 self.abs_max_out: 1184.0\n",
      "fc layer 2 self.abs_max_out: 1197.0\n",
      "lif layer 2 self.abs_max_v: 1523.5\n",
      "lif layer 2 self.abs_max_v: 1657.0\n",
      "fc layer 3 self.abs_max_out: 259.0\n",
      "fc layer 2 self.abs_max_out: 1212.0\n",
      "fc layer 1 self.abs_max_out: 2150.0\n",
      "lif layer 1 self.abs_max_v: 2150.0\n",
      "fc layer 3 self.abs_max_out: 264.0\n",
      "fc layer 1 self.abs_max_out: 2267.0\n",
      "lif layer 1 self.abs_max_v: 2267.0\n",
      "fc layer 2 self.abs_max_out: 1242.0\n",
      "fc layer 2 self.abs_max_out: 1490.0\n",
      "fc layer 1 self.abs_max_out: 2370.0\n",
      "lif layer 1 self.abs_max_v: 2370.0\n",
      "lif layer 2 self.abs_max_v: 1688.0\n",
      "lif layer 2 self.abs_max_v: 1699.0\n",
      "fc layer 3 self.abs_max_out: 280.0\n",
      "fc layer 2 self.abs_max_out: 1507.0\n",
      "fc layer 3 self.abs_max_out: 303.0\n",
      "lif layer 2 self.abs_max_v: 1717.5\n",
      "fc layer 1 self.abs_max_out: 2579.0\n",
      "lif layer 1 self.abs_max_v: 2579.0\n",
      "lif layer 2 self.abs_max_v: 1763.5\n",
      "fc layer 1 self.abs_max_out: 2682.0\n",
      "lif layer 1 self.abs_max_v: 2682.0\n",
      "fc layer 2 self.abs_max_out: 1578.0\n",
      "lif layer 2 self.abs_max_v: 1938.0\n",
      "fc layer 3 self.abs_max_out: 304.0\n",
      "fc layer 2 self.abs_max_out: 1617.0\n",
      "fc layer 3 self.abs_max_out: 349.0\n",
      "fc layer 2 self.abs_max_out: 1763.0\n",
      "fc layer 1 self.abs_max_out: 2742.0\n",
      "lif layer 1 self.abs_max_v: 2742.0\n",
      "lif layer 2 self.abs_max_v: 1962.5\n",
      "lif layer 2 self.abs_max_v: 2169.0\n",
      "fc layer 3 self.abs_max_out: 391.0\n",
      "fc layer 3 self.abs_max_out: 452.0\n",
      "fc layer 1 self.abs_max_out: 2792.0\n",
      "lif layer 1 self.abs_max_v: 2792.0\n",
      "fc layer 2 self.abs_max_out: 1774.0\n",
      "fc layer 2 self.abs_max_out: 1884.0\n",
      "fc layer 1 self.abs_max_out: 2891.0\n",
      "lif layer 1 self.abs_max_v: 2891.0\n",
      "fc layer 1 self.abs_max_out: 2911.0\n",
      "lif layer 1 self.abs_max_v: 2911.0\n",
      "fc layer 2 self.abs_max_out: 1923.0\n",
      "fc layer 1 self.abs_max_out: 2963.0\n",
      "lif layer 1 self.abs_max_v: 2963.0\n",
      "fc layer 1 self.abs_max_out: 3208.0\n",
      "lif layer 1 self.abs_max_v: 3208.0\n",
      "fc layer 2 self.abs_max_out: 2004.0\n",
      "fc layer 2 self.abs_max_out: 2008.0\n",
      "fc layer 2 self.abs_max_out: 2017.0\n",
      "fc layer 1 self.abs_max_out: 3830.0\n",
      "lif layer 1 self.abs_max_v: 3830.0\n",
      "fc layer 2 self.abs_max_out: 2026.0\n",
      "fc layer 2 self.abs_max_out: 2062.0\n",
      "fc layer 2 self.abs_max_out: 2155.0\n",
      "fc layer 2 self.abs_max_out: 2172.0\n",
      "lif layer 2 self.abs_max_v: 2172.0\n",
      "fc layer 2 self.abs_max_out: 2184.0\n",
      "lif layer 2 self.abs_max_v: 2184.0\n",
      "fc layer 2 self.abs_max_out: 2272.0\n",
      "lif layer 2 self.abs_max_v: 2272.0\n",
      "fc layer 1 self.abs_max_out: 3951.0\n",
      "lif layer 1 self.abs_max_v: 3951.0\n",
      "fc layer 1 self.abs_max_out: 3962.0\n",
      "lif layer 1 self.abs_max_v: 3962.0\n",
      "fc layer 1 self.abs_max_out: 4063.0\n",
      "lif layer 1 self.abs_max_v: 4063.0\n",
      "fc layer 1 self.abs_max_out: 4088.0\n",
      "lif layer 1 self.abs_max_v: 4088.0\n",
      "fc layer 2 self.abs_max_out: 2304.0\n",
      "lif layer 2 self.abs_max_v: 2304.0\n",
      "lif layer 1 self.abs_max_v: 4123.0\n",
      "fc layer 2 self.abs_max_out: 2365.0\n",
      "lif layer 2 self.abs_max_v: 2365.0\n",
      "fc layer 1 self.abs_max_out: 4091.0\n",
      "fc layer 1 self.abs_max_out: 4203.0\n",
      "lif layer 1 self.abs_max_v: 4203.0\n",
      "fc layer 1 self.abs_max_out: 4404.0\n",
      "lif layer 1 self.abs_max_v: 4404.0\n",
      "fc layer 2 self.abs_max_out: 2411.0\n",
      "lif layer 2 self.abs_max_v: 2411.0\n",
      "lif layer 1 self.abs_max_v: 4432.5\n",
      "fc layer 1 self.abs_max_out: 4451.0\n",
      "lif layer 1 self.abs_max_v: 4451.0\n",
      "lif layer 2 self.abs_max_v: 2436.5\n",
      "lif layer 1 self.abs_max_v: 4896.5\n",
      "fc layer 1 self.abs_max_out: 4570.0\n",
      "fc layer 1 self.abs_max_out: 4763.0\n",
      "lif layer 1 self.abs_max_v: 4955.0\n",
      "lif layer 1 self.abs_max_v: 5519.5\n",
      "fc layer 1 self.abs_max_out: 5096.0\n",
      "fc layer 2 self.abs_max_out: 2478.0\n",
      "lif layer 2 self.abs_max_v: 2478.0\n",
      "fc layer 2 self.abs_max_out: 2560.0\n",
      "lif layer 2 self.abs_max_v: 2560.0\n",
      "lif layer 1 self.abs_max_v: 5602.0\n",
      "fc layer 1 self.abs_max_out: 5144.0\n",
      "fc layer 1 self.abs_max_out: 5324.0\n",
      "fc layer 1 self.abs_max_out: 5362.0\n",
      "lif layer 1 self.abs_max_v: 5622.5\n",
      "lif layer 1 self.abs_max_v: 5984.5\n",
      "fc layer 1 self.abs_max_out: 5456.0\n",
      "fc layer 1 self.abs_max_out: 5689.0\n",
      "fc layer 1 self.abs_max_out: 5761.0\n",
      "lif layer 1 self.abs_max_v: 6172.5\n",
      "lif layer 1 self.abs_max_v: 6408.5\n",
      "lif layer 1 self.abs_max_v: 6944.5\n",
      "lif layer 2 self.abs_max_v: 2595.5\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  2.084421/  2.143443, val:  45.42%, val_best:  45.42%, tr:  94.21%, tr_best:  94.21%, epoch time: 281.18 seconds, 4.69 minutes\n",
      "total_backward_count 44360 real_backward_count 11422  25.748%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 2581.0\n",
      "fc layer 1 self.abs_max_out: 5986.0\n",
      "lif layer 2 self.abs_max_v: 2620.5\n",
      "fc layer 2 self.abs_max_out: 2609.0\n",
      "fc layer 2 self.abs_max_out: 2705.0\n",
      "lif layer 2 self.abs_max_v: 2705.0\n",
      "fc layer 2 self.abs_max_out: 2809.0\n",
      "lif layer 2 self.abs_max_v: 2809.0\n",
      "fc layer 2 self.abs_max_out: 2852.0\n",
      "lif layer 2 self.abs_max_v: 2852.0\n",
      "fc layer 1 self.abs_max_out: 6026.0\n",
      "lif layer 2 self.abs_max_v: 2900.5\n",
      "fc layer 1 self.abs_max_out: 6202.0\n",
      "fc layer 1 self.abs_max_out: 6459.0\n",
      "lif layer 2 self.abs_max_v: 3001.0\n",
      "lif layer 2 self.abs_max_v: 3035.0\n",
      "lif layer 2 self.abs_max_v: 3127.5\n",
      "lif layer 2 self.abs_max_v: 3426.0\n",
      "lif layer 2 self.abs_max_v: 3445.0\n",
      "lif layer 1 self.abs_max_v: 7244.5\n",
      "lif layer 1 self.abs_max_v: 8194.5\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  2.086918/  2.143834, val:  48.33%, val_best:  48.33%, tr:  99.32%, tr_best:  99.32%, epoch time: 283.27 seconds, 4.72 minutes\n",
      "total_backward_count 88720 real_backward_count 19103  21.532%\n",
      "fc layer 1 self.abs_max_out: 6552.0\n",
      "lif layer 2 self.abs_max_v: 3480.5\n",
      "fc layer 1 self.abs_max_out: 6571.0\n",
      "fc layer 1 self.abs_max_out: 6696.0\n",
      "fc layer 2 self.abs_max_out: 2926.0\n",
      "fc layer 2 self.abs_max_out: 3035.0\n",
      "fc layer 2 self.abs_max_out: 3057.0\n",
      "fc layer 2 self.abs_max_out: 3123.0\n",
      "lif layer 2 self.abs_max_v: 3529.5\n",
      "fc layer 1 self.abs_max_out: 6888.0\n",
      "fc layer 2 self.abs_max_out: 3131.0\n",
      "lif layer 2 self.abs_max_v: 3677.0\n",
      "fc layer 1 self.abs_max_out: 7039.0\n",
      "lif layer 2 self.abs_max_v: 3682.0\n",
      "lif layer 1 self.abs_max_v: 8491.5\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  2.075938/  2.134702, val:  50.00%, val_best:  50.00%, tr:  99.44%, tr_best:  99.44%, epoch time: 284.07 seconds, 4.73 minutes\n",
      "total_backward_count 133080 real_backward_count 26211  19.696%\n",
      "fc layer 2 self.abs_max_out: 3184.0\n",
      "fc layer 1 self.abs_max_out: 7133.0\n",
      "fc layer 2 self.abs_max_out: 3257.0\n",
      "fc layer 1 self.abs_max_out: 7228.0\n",
      "fc layer 2 self.abs_max_out: 3272.0\n",
      "fc layer 2 self.abs_max_out: 3353.0\n",
      "fc layer 2 self.abs_max_out: 3500.0\n",
      "fc layer 1 self.abs_max_out: 7252.0\n",
      "fc layer 2 self.abs_max_out: 3518.0\n",
      "fc layer 2 self.abs_max_out: 3621.0\n",
      "lif layer 2 self.abs_max_v: 3725.5\n",
      "lif layer 2 self.abs_max_v: 3796.0\n",
      "lif layer 2 self.abs_max_v: 3812.0\n",
      "lif layer 1 self.abs_max_v: 8953.0\n",
      "fc layer 1 self.abs_max_out: 7255.0\n",
      "fc layer 1 self.abs_max_out: 7286.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  2.066492/  2.133393, val:  58.75%, val_best:  58.75%, tr:  99.71%, tr_best:  99.71%, epoch time: 283.41 seconds, 4.72 minutes\n",
      "total_backward_count 177440 real_backward_count 33081  18.643%\n",
      "lif layer 2 self.abs_max_v: 3982.5\n",
      "lif layer 2 self.abs_max_v: 4113.5\n",
      "lif layer 2 self.abs_max_v: 4160.0\n",
      "fc layer 1 self.abs_max_out: 7291.0\n",
      "fc layer 1 self.abs_max_out: 7547.0\n",
      "fc layer 2 self.abs_max_out: 3656.0\n",
      "lif layer 2 self.abs_max_v: 4247.5\n",
      "lif layer 2 self.abs_max_v: 4306.5\n",
      "lif layer 2 self.abs_max_v: 4386.5\n",
      "fc layer 1 self.abs_max_out: 7755.0\n",
      "lif layer 1 self.abs_max_v: 9668.5\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  2.054665/  2.124222, val:  67.08%, val_best:  67.08%, tr:  99.57%, tr_best:  99.71%, epoch time: 281.88 seconds, 4.70 minutes\n",
      "total_backward_count 221800 real_backward_count 39634  17.869%\n",
      "fc layer 1 self.abs_max_out: 7855.0\n",
      "fc layer 1 self.abs_max_out: 7907.0\n",
      "fc layer 1 self.abs_max_out: 7929.0\n",
      "lif layer 2 self.abs_max_v: 4501.0\n",
      "lif layer 1 self.abs_max_v: 10118.5\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  2.054872/  2.109893, val:  64.17%, val_best:  67.08%, tr:  99.71%, tr_best:  99.71%, epoch time: 283.51 seconds, 4.73 minutes\n",
      "total_backward_count 266160 real_backward_count 45921  17.253%\n",
      "lif layer 2 self.abs_max_v: 4575.5\n",
      "fc layer 1 self.abs_max_out: 8096.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  2.047552/  2.114363, val:  58.75%, val_best:  67.08%, tr:  99.77%, tr_best:  99.77%, epoch time: 283.08 seconds, 4.72 minutes\n",
      "total_backward_count 310520 real_backward_count 51948  16.729%\n",
      "lif layer 2 self.abs_max_v: 4636.5\n",
      "fc layer 1 self.abs_max_out: 8145.0\n",
      "fc layer 1 self.abs_max_out: 8393.0\n",
      "lif layer 1 self.abs_max_v: 10149.5\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  2.045697/  2.111214, val:  60.83%, val_best:  67.08%, tr:  99.84%, tr_best:  99.84%, epoch time: 284.60 seconds, 4.74 minutes\n",
      "total_backward_count 354880 real_backward_count 57745  16.272%\n",
      "fc layer 1 self.abs_max_out: 8458.0\n",
      "lif layer 1 self.abs_max_v: 10683.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  2.036097/  2.103538, val:  74.17%, val_best:  74.17%, tr:  99.82%, tr_best:  99.84%, epoch time: 284.82 seconds, 4.75 minutes\n",
      "total_backward_count 399240 real_backward_count 63231  15.838%\n",
      "lif layer 1 self.abs_max_v: 10779.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  2.033444/  2.112139, val:  67.92%, val_best:  74.17%, tr:  99.89%, tr_best:  99.89%, epoch time: 284.70 seconds, 4.75 minutes\n",
      "total_backward_count 443600 real_backward_count 68498  15.441%\n",
      "fc layer 1 self.abs_max_out: 8501.0\n",
      "fc layer 1 self.abs_max_out: 8522.0\n",
      "lif layer 1 self.abs_max_v: 11123.5\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  2.033402/  2.111601, val:  77.50%, val_best:  77.50%, tr:  99.91%, tr_best:  99.91%, epoch time: 285.16 seconds, 4.75 minutes\n",
      "total_backward_count 487960 real_backward_count 73568  15.077%\n",
      "fc layer 1 self.abs_max_out: 8555.0\n",
      "lif layer 1 self.abs_max_v: 11549.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  2.029285/  2.091201, val:  66.67%, val_best:  77.50%, tr:  99.86%, tr_best:  99.91%, epoch time: 285.54 seconds, 4.76 minutes\n",
      "total_backward_count 532320 real_backward_count 78474  14.742%\n",
      "fc layer 1 self.abs_max_out: 8663.0\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'random', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        # \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [5.0, 5.5, 6.0, 6.5, 7.0]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "        \"learning_rate\": {\"values\": [1/512]}, \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [14]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [9]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        \"scale_exp_2w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        \"scale_exp_3w\": {\"values\": [-9]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"5\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_2w,wandb.config.scale_exp_2w],[wandb.config.scale_exp_3w,wandb.config.scale_exp_3w]],\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 't0h80lho'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
