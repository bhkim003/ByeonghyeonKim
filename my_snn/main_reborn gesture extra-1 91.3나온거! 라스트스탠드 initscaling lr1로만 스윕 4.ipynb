{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34047/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAIhCAYAAACfVbSSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA7/0lEQVR4nO3deXRU9f3/8dckmAlLEjYTgoQQtxpBDSYubB5ciFJArAsUlUXAgmGRpQopVhAqEbRIKybKLrKIFBBUiqZSBStIjCxWVFSQBCVGEAkgJGTm/v6g5PcdEjAZZz6XmXk+zrnnNJ/cufc9U9C3r8/nfsZhWZYlAAAA+F2Y3QUAAACEChovAAAAQ2i8AAAADKHxAgAAMITGCwAAwBAaLwAAAENovAAAAAyh8QIAADCExgsAAMAQGi/AC/Pnz5fD4ag4atWqpfj4eP3+97/Xl19+aVtdEyZMkMPhsO3+p8vPz9eQIUN0xRVXKCoqSnFxcbrlllu0bt26Suf269fP4zOtW7euWrRoodtvv13z5s1TaWlpje8/atQoORwOde3a1RdvBwB+NRov4FeYN2+eNm7cqH/9618aOnSoVq9erfbt2+vgwYN2l3ZOWLJkiTZv3qz+/ftr1apVmj17tpxOp26++WYtWLCg0vm1a9fWxo0btXHjRr3xxhuaOHGi6tatqwcffFCpqanau3dvte994sQJLVy4UJK0du1affvttz57XwDgNQtAjc2bN8+SZOXl5XmMP/HEE5Yka+7cubbUNX78eOtc+mv9/fffVxorLy+3rrzySuuiiy7yGO/bt69Vt27dKq/z1ltvWeedd5513XXXVfvey5YtsyRZXbp0sSRZTz75ZLVeV1ZWZp04caLK3x09erTa9weAqpB4AT6UlpYmSfr+++8rxo4fP67Ro0crJSVFMTExatiwodq0aaNVq1ZVer3D4dDQoUP18ssvKzk5WXXq1NFVV12lN954o9K5b775plJSUuR0OpWUlKRnnnmmypqOHz+uzMxMJSUlKSIiQhdccIGGDBmin376yeO8Fi1aqGvXrnrjjTfUunVr1a5dW8nJyRX3nj9/vpKTk1W3bl1de+21+uijj37x84iNja00Fh4ertTUVBUWFv7i609JT0/Xgw8+qA8//FDr16+v1mvmzJmjiIgIzZs3TwkJCZo3b54sy/I4591335XD4dDLL7+s0aNH64ILLpDT6dRXX32lfv36qV69evrkk0+Unp6uqKgo3XzzzZKk3Nxcde/eXc2aNVNkZKQuvvhiDRo0SPv376+49oYNG+RwOLRkyZJKtS1YsEAOh0N5eXnV/gwABAcaL8CHdu/eLUm69NJLK8ZKS0v1448/6o9//KNee+01LVmyRO3bt9edd95Z5XTbm2++qRkzZmjixIlavny5GjZsqN/97nfatWtXxTnvvPOOunfvrqioKL3yyit6+umn9eqrr2revHke17IsS3fccYeeeeYZ9e7dW2+++aZGjRqll156STfddFOldVPbtm1TZmamxowZoxUrVigmJkZ33nmnxo8fr9mzZ2vy5MlatGiRDh06pK5du+rYsWM1/ozKy8u1YcMGtWzZskavu/322yWpWo3X3r179fbbb6t79+46//zz1bdvX3311VdnfG1mZqYKCgr0wgsv6PXXX69oGMvKynT77bfrpptu0qpVq/TEE09Ikr7++mu1adNGOTk5evvtt/X444/rww8/VPv27XXixAlJUocOHdS6dWs9//zzle43Y8YMXXPNNbrmmmtq9BkACAJ2R25AIDo11bhp0ybrxIkT1uHDh621a9daTZo0sW644YYzTlVZ1smpthMnTlgDBgywWrdu7fE7SVZcXJxVUlJSMVZUVGSFhYVZWVlZFWPXXXed1bRpU+vYsWMVYyUlJVbDhg09phrXrl1rSbKmTp3qcZ+lS5dakqyZM2dWjCUmJlq1a9e29u7dWzG2detWS5IVHx/vMc322muvWZKs1atXV+fj8jBu3DhLkvXaa695jJ9tqtGyLOuzzz6zJFkPPfTQL95j4sSJliRr7dq1lmVZ1q5duyyHw2H17t3b47x///vfliTrhhtuqHSNvn37Vmva2O12WydOnLD27NljSbJWrVpV8btTf062bNlSMbZ582ZLkvXSSy/94vsAEHxIvIBf4frrr9d5552nqKgo3XbbbWrQoIFWrVqlWrVqeZy3bNkytWvXTvXq1VOtWrV03nnnac6cOfrss88qXfPGG29UVFRUxc9xcXGKjY3Vnj17JElHjx5VXl6e7rzzTkVGRlacFxUVpW7dunlc69TTg/369fMYv+eee1S3bl298847HuMpKSm64IILKn5OTk6WJHXs2FF16tSpNH6qpuqaPXu2nnzySY0ePVrdu3ev0Wut06YJz3beqenFTp06SZKSkpLUsWNHLV++XCUlJZVec9ddd53xelX9rri4WIMHD1ZCQkLF/5+JiYmS5PH/aa9evRQbG+uRej333HM6//zz1bNnz2q9HwDBhcYL+BUWLFigvLw8rVu3ToMGDdJnn32mXr16eZyzYsUK9ejRQxdccIEWLlyojRs3Ki8vT/3799fx48crXbNRo0aVxpxOZ8W03sGDB+V2u9WkSZNK550+duDAAdWqVUvnn3++x7jD4VCTJk104MABj/GGDRt6/BwREXHW8arqP5N58+Zp0KBB+sMf/qCnn3662q875VST17Rp07Oet27dOu3evVv33HOPSkpK9NNPP+mnn35Sjx499PPPP1e55io+Pr7Ka9WpU0fR0dEeY263W+np6VqxYoUeffRRvfPOO9q8ebM2bdokSR7Tr06nU4MGDdLixYv1008/6YcfftCrr76qgQMHyul01uj9AwgOtX75FABnkpycXLGg/sYbb5TL5dLs2bP1j3/8Q3fffbckaeHChUpKStLSpUs99tjyZl8qSWrQoIEcDoeKiooq/e70sUaNGqm8vFw//PCDR/NlWZaKioqMrTGaN2+eBg4cqL59++qFF17waq+x1atXSzqZvp3NnDlzJEnTpk3TtGnTqvz9oEGDPMbOVE9V4//973+1bds2zZ8/X3379q0Y/+qrr6q8xkMPPaSnnnpKc+fO1fHjx1VeXq7Bgwef9T0ACF4kXoAPTZ06VQ0aNNDjjz8ut9st6eS/vCMiIjz+JV5UVFTlU43VceqpwhUrVngkTocPH9brr7/uce6pp/BO7Wd1yvLly3X06NGK3/vT/PnzNXDgQN1///2aPXu2V01Xbm6uZs+erbZt26p9+/ZnPO/gwYNauXKl2rVrp3//+9+Vjvvuu095eXn673//6/X7OVX/6YnViy++WOX58fHxuueee5Sdna0XXnhB3bp1U/Pmzb2+P4DARuIF+FCDBg2UmZmpRx99VIsXL9b999+vrl27asWKFcrIyNDdd9+twsJCTZo0SfHx8V7vcj9p0iTddttt6tSpk0aPHi2Xy6UpU6aobt26+vHHHyvO69Spk2699VaNGTNGJSUlateunbZv367x48erdevW6t27t6/eepWWLVumAQMGKCUlRYMGDdLmzZs9ft+6dWuPBsbtdldM2ZWWlqqgoED//Oc/9eqrryo5OVmvvvrqWe+3aNEiHT9+XMOHD68yGWvUqJEWLVqkOXPm6Nlnn/XqPV122WW66KKLNHbsWFmWpYYNG+r1119Xbm7uGV/z8MMP67rrrpOkSk+eAggx9q7tBwLTmTZQtSzLOnbsmNW8eXPrkksuscrLyy3LsqynnnrKatGiheV0Oq3k5GRr1qxZVW52KskaMmRIpWsmJiZaffv29RhbvXq1deWVV1oRERFW8+bNraeeeqrKax47dswaM2aMlZiYaJ133nlWfHy89dBDD1kHDx6sdI8uXbpUundVNe3evduSZD399NNn/Iws6/8/GXimY/fu3Wc8t3bt2lbz5s2tbt26WXPnzrVKS0vPei/LsqyUlBQrNjb2rOdef/31VuPGja3S0tKKpxqXLVtWZe1nespyx44dVqdOnayoqCirQYMG1j333GMVFBRYkqzx48dX+ZoWLVpYycnJv/geAAQ3h2VV81EhAIBXtm/frquuukrPP/+8MjIy7C4HgI1ovADAT77++mvt2bNHf/rTn1RQUKCvvvrKY1sOAKGHxfUA4CeTJk1Sp06ddOTIES1btoymCwCJFwAAgCkkXgAAAIbQeAEAABhC4wUAAGBIQG+g6na79d133ykqKsqr3bABAAgllmXp8OHDatq0qcLCzGcvx48fV1lZmV+uHRERocjISL9c25cCuvH67rvvlJCQYHcZAAAElMLCQjVr1szoPY8fP66kxHoqKnb55fpNmjTR7t27z/nmK6Abr6ioKElSs78/qrDazl84+9xilUTYXYJXVtw2w+4SvJbZ+S67S/BKndnH7C7BK18tv8TuErzW4Cv//Be5v0V+c9DuErzz9+O/fM456svvYu0uoUbcx0q1d/jUin9/mlRWVqaiYpf25LdQdJRv07aSw24lpn6jsrIyGi9/OjW9GFbbqbA65/YHfTrrRGA2XvV8/JfFpFphgdWcn3JeXf/816G/hTsD6+/k/1WrVmD+Oa8VHph/xlU3cHc1CrR/95xi5/KcelEO1Yvy7f3dCpzlRgHdeAEAgMDistxy+bjXdllu317QjwLzP+sAAAACEIkXAAAwxi1Lbvk28vL19fyJxAsAAMAQEi8AAGCMW275ekWW76/oPyReAAAAhpB4AQAAY1yWJZfl2zVZvr6eP5F4AQAAGELiBQAAjAn1pxppvAAAgDFuWXKFcOPFVCMAAIAhJF4AAMCYUJ9qJPECAAAwhMQLAAAYw3YSAAAAMILECwAAGOP+3+HrawYK2xOv7OxsJSUlKTIyUqmpqdqwYYPdJQEAAPiFrY3X0qVLNWLECI0bN05btmxRhw4d1LlzZxUUFNhZFgAA8BPX//bx8vURKGxtvKZNm6YBAwZo4MCBSk5O1vTp05WQkKCcnBw7ywIAAH7isvxzBArbGq+ysjLl5+crPT3dYzw9PV0ffPBBla8pLS1VSUmJxwEAABAobGu89u/fL5fLpbi4OI/xuLg4FRUVVfmarKwsxcTEVBwJCQkmSgUAAD7i9tMRKGxfXO9wODx+tiyr0tgpmZmZOnToUMVRWFhookQAAACfsG07icaNGys8PLxSulVcXFwpBTvF6XTK6XSaKA8AAPiBWw65VHXA8muuGShsS7wiIiKUmpqq3Nxcj/Hc3Fy1bdvWpqoAAAD8x9YNVEeNGqXevXsrLS1Nbdq00cyZM1VQUKDBgwfbWRYAAPATt3Xy8PU1A4WtjVfPnj114MABTZw4Ufv27VOrVq20Zs0aJSYm2lkWAACAX9j+lUEZGRnKyMiwuwwAAGCAyw9rvHx9PX+yvfECAAChI9QbL9u3kwAAAAgVJF4AAMAYt+WQ2/LxdhI+vp4/kXgBAAAYQuIFAACMYY0XAAAAjCDxAgAAxrgUJpePcx+XT6/mXyReAAAAhpB4AQAAYyw/PNVoBdBTjTReAADAGBbXAwAAwAgSLwAAYIzLCpPL8vHiesunl/MrEi8AAABDSLwAAIAxbjnk9nHu41bgRF4kXgAAAIYEReJl/RQhqzTC7jJq5JJhH9pdgleGLh5idwle+3LceXaX4JX76m2yuwSvHJ5fZncJXrOOHbO7BK+k5AXmZ/5DWZTdJXhtf26i3SXUiKtMKrC7Bp5qBAAAgAlBkXgBAIDA4J+nGgNnjReNFwAAMObk4nrfTg36+nr+xFQjAACAISReAADAGLfC5GI7CQAAAPgbiRcAADAm1BfXk3gBAAAYQuIFAACMcSuMrwwCAACA/5F4AQAAY1yWQy7Lx18Z5OPr+RONFwAAMMblh+0kXEw1AgAA4HQkXgAAwBi3FSa3j7eTcLOdBAAAAE5H4gUAAIxhjRcAAACMIPECAADGuOX77R/cPr2af5F4AQAAGELiBQAAjPHPVwYFTo5E4wUAAIxxWWFy+Xg7CV9fz58Cp1IAAIAAR+IFAACMccsht3y9uD5wvquRxAsAAMAQEi8AAGAMa7wAAABgBIkXAAAwxj9fGRQ4OVLgVAoAABDgSLwAAIAxbssht6+/MsjH1/MnEi8AAABDSLwAAIAxbj+s8eIrgwAAAKrgtsLk9vH2D76+nj8FTqUAAAABjsQLAAAY45JDLh9/xY+vr+dPJF4AAACGkHgBAABjWOMFAAAAI0i8AACAMS75fk2Wy6dX8y8SLwAAAENIvAAAgDGhvsaLxgsAABjjssLk8nGj5Ovr+VPgVAoAABDgSLwAAIAxlhxy+3hxvcUGqgAAAOe27OxsJSUlKTIyUqmpqdqwYcNZz1+0aJGuuuoq1alTR/Hx8XrggQd04MCBGt2TxgsAABhzao2Xr4+aWrp0qUaMGKFx48Zpy5Yt6tChgzp37qyCgoIqz3///ffVp08fDRgwQJ9++qmWLVumvLw8DRw4sEb3pfECAABBoaSkxOMoLS0947nTpk3TgAEDNHDgQCUnJ2v69OlKSEhQTk5Oledv2rRJLVq00PDhw5WUlKT27dtr0KBB+uijj2pUY1Cs8bLqlcuqXW53GTUSXj/G7hK8smbZfLtL8Nrw766xuwSvlLoD86/pgbta2V2C1/anuu0uwSuOdp/YXYJXCh6+yu4SvHbb8E12l1AjpUdOaOtie2twWw65Ld+uyTp1vYSEBI/x8ePHa8KECZXOLysrU35+vsaOHesxnp6erg8++KDKe7Rt21bjxo3TmjVr1LlzZxUXF+sf//iHunTpUqNaA/Of6AAAAKcpLCxUdHR0xc9Op7PK8/bv3y+Xy6W4uDiP8bi4OBUVFVX5mrZt22rRokXq2bOnjh8/rvLyct1+++167rnnalQjU40AAMAYl8L8ckhSdHS0x3GmxusUh8MzebMsq9LYKTt27NDw4cP1+OOPKz8/X2vXrtXu3bs1ePDgGr1/Ei8AAGCMP6caq6tx48YKDw+vlG4VFxdXSsFOycrKUrt27fTII49Ikq688krVrVtXHTp00F/+8hfFx8dX694kXgAAIKREREQoNTVVubm5HuO5ublq27Ztla/5+eefFRbm2TaFh4dLOpmUVReJFwAAMMatMLl9nPt4c71Ro0apd+/eSktLU5s2bTRz5kwVFBRUTB1mZmbq22+/1YIFCyRJ3bp104MPPqicnBzdeuut2rdvn0aMGKFrr71WTZs2rfZ9abwAAEDI6dmzpw4cOKCJEydq3759atWqldasWaPExERJ0r59+zz29OrXr58OHz6sGTNmaPTo0apfv75uuukmTZkypUb3pfECAADGuCyHXD5e4+Xt9TIyMpSRkVHl7+bPn19pbNiwYRo2bJhX9zqFNV4AAACGkHgBAABjzoWnGu1E4gUAAGAIiRcAADDGssLk9uJLrX/pmoGCxgsAABjjkkMu+XhxvY+v50+B0yICAAAEOBIvAABgjNvy/WJ4d/U3jrcdiRcAAIAhJF4AAMAYtx8W1/v6ev4UOJUCAAAEOBIvAABgjFsOuX38FKKvr+dPtiZeWVlZuuaaaxQVFaXY2Fjdcccd+uKLL+wsCQAAwG9sbbzee+89DRkyRJs2bVJubq7Ky8uVnp6uo0eP2lkWAADwk1Nfku3rI1DYOtW4du1aj5/nzZun2NhY5efn64YbbrCpKgAA4C+hvrj+nFrjdejQIUlSw4YNq/x9aWmpSktLK34uKSkxUhcAAIAvnDMtomVZGjVqlNq3b69WrVpVeU5WVpZiYmIqjoSEBMNVAgCAX8Mth9yWjw8W19fc0KFDtX37di1ZsuSM52RmZurQoUMVR2FhocEKAQAAfp1zYqpx2LBhWr16tdavX69mzZqd8Tyn0ymn02mwMgAA4EuWH7aTsAIo8bK18bIsS8OGDdPKlSv17rvvKikpyc5yAAAA/MrWxmvIkCFavHixVq1apaioKBUVFUmSYmJiVLt2bTtLAwAAfnBqXZavrxkobF3jlZOTo0OHDqljx46Kj4+vOJYuXWpnWQAAAH5h+1QjAAAIHezjBQAAYAhTjQAAADCCxAsAABjj9sN2EmygCgAAgEpIvAAAgDGs8QIAAIARJF4AAMAYEi8AAAAYQeIFAACMCfXEi8YLAAAYE+qNF1ONAAAAhpB4AQAAYyz5fsPTQPrmZxIvAAAAQ0i8AACAMazxAgAAgBEkXgAAwJhQT7yCovGa1e4l1Y0KrPBu0KChdpfglesfTba7BK/V//yI3SV4ZecDde0uwSuXLvnY7hK8djQ+1e4SvPLbvG/tLsErLyxKsbsEr61//jq7S6gRV9lxSSvtLiOkBUXjBQAAAgOJFwAAgCGh3ngF1vwcAABAACPxAgAAxliWQ5aPEypfX8+fSLwAAAAMIfECAADGuOXw+VcG+fp6/kTiBQAAYAiJFwAAMIanGgEAAGAEiRcAADCGpxoBAABgBIkXAAAwJtTXeNF4AQAAY5hqBAAAgBEkXgAAwBjLD1ONJF4AAACohMQLAAAYY0myLN9fM1CQeAEAABhC4gUAAIxxyyEHX5INAAAAfyPxAgAAxoT6Pl40XgAAwBi35ZAjhHeuZ6oRAADAEBIvAABgjGX5YTuJANpPgsQLAADAEBIvAABgTKgvrifxAgAAMITECwAAGEPiBQAAACNIvAAAgDGhvo8XjRcAADCG7SQAAABgBIkXAAAw5mTi5evF9T69nF+ReAEAABhC4gUAAIxhOwkAAAAYQeIFAACMsf53+PqagYLECwAAwBASLwAAYEyor/Gi8QIAAOaE+FwjU40AAACGkHgBAABz/DDVqACaaiTxAgAAMITGCwAAGHPqS7J9fXgjOztbSUlJioyMVGpqqjZs2HDW80tLSzVu3DglJibK6XTqoosu0ty5c2t0T6YaAQBAyFm6dKlGjBih7OxstWvXTi+++KI6d+6sHTt2qHnz5lW+pkePHvr+++81Z84cXXzxxSouLlZ5eXmN7hsUjdeI2X9QuDPS7jJq5ILOBXaX4JWi1VX/YQwEt8/Ns7sEr3Sr9bPdJXjltzs/s7sErx1yr7e7BK+Mu+Euu0vwynl32F2B9xp8cczuEmqkvPy43SWcM9tJTJs2TQMGDNDAgQMlSdOnT9dbb72lnJwcZWVlVTp/7dq1eu+997Rr1y41bNhQktSiRYsa35epRgAAEBRKSko8jtLS0irPKysrU35+vtLT0z3G09PT9cEHH1T5mtWrVystLU1Tp07VBRdcoEsvvVR//OMfdexYzZrvoEi8AABAgLAcvn8K8X/XS0hI8BgeP368JkyYUOn0/fv3y+VyKS4uzmM8Li5ORUVFVd5i165dev/99xUZGamVK1dq//79ysjI0I8//lijdV40XgAAwJhfsxj+bNeUpMLCQkVHR1eMO53Os77O4fBsAC3LqjR2itvtlsPh0KJFixQTEyPp5HTl3Xffreeff161a9euVq1MNQIAgKAQHR3tcZyp8WrcuLHCw8MrpVvFxcWVUrBT4uPjdcEFF1Q0XZKUnJwsy7K0d+/eatdI4wUAAMyx/HTUQEREhFJTU5Wbm+sxnpubq7Zt21b5mnbt2um7777TkSNHKsZ27typsLAwNWvWrNr3pvECAAAhZ9SoUZo9e7bmzp2rzz77TCNHjlRBQYEGDx4sScrMzFSfPn0qzr/33nvVqFEjPfDAA9qxY4fWr1+vRx55RP3796/2NKPEGi8AAGDQubKdRM+ePXXgwAFNnDhR+/btU6tWrbRmzRolJiZKkvbt26eCgv+/9VO9evWUm5urYcOGKS0tTY0aNVKPHj30l7/8pUb3pfECAAAhKSMjQxkZGVX+bv78+ZXGLrvsskrTkzVF4wUAAMzy8VONgYQ1XgAAAIaQeAEAAGPOlTVedqHxAgAA5nix/UO1rhkgmGoEAAAwhMQLAAAY5Pjf4etrBgYSLwAAAENIvAAAgDms8QIAAIAJJF4AAMAcEi8AAACYcM40XllZWXI4HBoxYoTdpQAAAH+xHP45AsQ5MdWYl5enmTNn6sorr7S7FAAA4EeWdfLw9TUDhe2J15EjR3Tfffdp1qxZatCggd3lAAAA+I3tjdeQIUPUpUsX3XLLLb94bmlpqUpKSjwOAAAQQCw/HQHC1qnGV155RR9//LHy8vKqdX5WVpaeeOIJP1cFAADgH7YlXoWFhXr44Ye1cOFCRUZGVus1mZmZOnToUMVRWFjo5yoBAIBPsbjeHvn5+SouLlZqamrFmMvl0vr16zVjxgyVlpYqPDzc4zVOp1NOp9N0qQAAAD5hW+N1880365NPPvEYe+CBB3TZZZdpzJgxlZouAAAQ+BzWycPX1wwUtjVeUVFRatWqlcdY3bp11ahRo0rjAAAAwaDGa7xeeuklvfnmmxU/P/roo6pfv77atm2rPXv2+LQ4AAAQZEL8qcYaN16TJ09W7dq1JUkbN27UjBkzNHXqVDVu3FgjR478VcW8++67mj59+q+6BgAAOIexuL5mCgsLdfHFF0uSXnvtNd199936wx/+oHbt2qljx46+rg8AACBo1Djxqlevng4cOCBJevvttys2Po2MjNSxY8d8Wx0AAAguIT7VWOPEq1OnTho4cKBat26tnTt3qkuXLpKkTz/9VC1atPB1fQAAAEGjxonX888/rzZt2uiHH37Q8uXL1ahRI0kn9+Xq1auXzwsEAABBhMSrZurXr68ZM2ZUGuerfAAAAM6uWo3X9u3b1apVK4WFhWn79u1nPffKK6/0SWEAACAI+SOhCrbEKyUlRUVFRYqNjVVKSoocDocs6/+/y1M/OxwOuVwuvxULAAAQyKrVeO3evVvnn39+xf8GAADwij/23Qq2fbwSExOr/N+n+78pGAAAADzV+KnG3r1768iRI5XGv/nmG91www0+KQoAAASnU1+S7esjUNS48dqxY4euuOIK/ec//6kYe+mll3TVVVcpLi7Op8UBAIAgw3YSNfPhhx/qscce00033aTRo0fryy+/1Nq1a/W3v/1N/fv390eNAAAAQaHGjVetWrX01FNPyel0atKkSapVq5bee+89tWnTxh/1AQAABI0aTzWeOHFCo0eP1pQpU5SZmak2bdrod7/7ndasWeOP+gAAAIJGjROvtLQ0/fzzz3r33Xd1/fXXy7IsTZ06VXfeeaf69++v7Oxsf9QJAACCgEO+XwwfOJtJeNl4/f3vf1fdunUlndw8dcyYMbr11lt1//33+7zA6nj8gUWqExVuy729NXpFX7tL8EqL2wvtLsFrB8vr2F2CV17+4lq7S/BK8aXRdpfgtcENPrS7BK8ceDHS7hK8EpNTbncJXtt7Y2D9c8VVGiZtsruK0FbjxmvOnDlVjqekpCg/P/9XFwQAAIIYG6h679ixYzpx4oTHmNPp/FUFAQAABKsaL64/evSohg4dqtjYWNWrV08NGjTwOAAAAM4oxPfxqnHj9eijj2rdunXKzs6W0+nU7Nmz9cQTT6hp06ZasGCBP2oEAADBIsQbrxpPNb7++utasGCBOnbsqP79+6tDhw66+OKLlZiYqEWLFum+++7zR50AAAABr8aJ148//qikpCRJUnR0tH788UdJUvv27bV+/XrfVgcAAIIK39VYQxdeeKG++eYbSdLll1+uV199VdLJJKx+/fq+rA0AACCo1LjxeuCBB7Rt2zZJUmZmZsVar5EjR+qRRx7xeYEAACCIsMarZkaOHFnxv2+88UZ9/vnn+uijj3TRRRfpqquu8mlxAAAAweRX7eMlSc2bN1fz5s19UQsAAAh2/kioAijxqvFUIwAAALzzqxMvAACA6vLHU4hB+VTj3r17/VkHAAAIBae+q9HXR4CoduPVqlUrvfzyy/6sBQAAIKhVu/GaPHmyhgwZorvuuksHDhzwZ00AACBYhfh2EtVuvDIyMrRt2zYdPHhQLVu21OrVq/1ZFwAAQNCp0eL6pKQkrVu3TjNmzNBdd92l5ORk1arleYmPP/7YpwUCAIDgEeqL62v8VOOePXu0fPlyNWzYUN27d6/UeAEAAKBqNeqaZs2apdGjR+uWW27Rf//7X51//vn+qgsAAASjEN9AtdqN12233abNmzdrxowZ6tOnjz9rAgAACErVbrxcLpe2b9+uZs2a+bMeAAAQzPywxisoE6/c3Fx/1gEAAEJBiE818l2NAAAAhvBIIgAAMIfECwAAACaQeAEAAGNCfQNVEi8AAABDaLwAAAAMofECAAAwhDVeAADAnBB/qpHGCwAAGMPiegAAABhB4gUAAMwKoITK10i8AAAADCHxAgAA5oT44noSLwAAAENIvAAAgDE81QgAAAAjSLwAAIA5Ib7Gi8YLAAAYw1QjAAAAjCDxAgAA5oT4VCOJFwAAgCEkXgAAwBwSLwAAAJhA4wUAAIw59VSjrw9vZGdnKykpSZGRkUpNTdWGDRuq9br//Oc/qlWrllJSUmp8z6CYasyaca/CIyLtLqNGnOkldpfglW/ymtldgtf2KDBrD7/oiN0leOXl9e3tLsFrdTqW2V2CV77/PsbuEryS6AqgeaLTzOo/w+4SauToYbc6T7W7inPD0qVLNWLECGVnZ6tdu3Z68cUX1blzZ+3YsUPNmzc/4+sOHTqkPn366Oabb9b3339f4/uSeAEAAHMsPx2SSkpKPI7S0tIzljFt2jQNGDBAAwcOVHJysqZPn66EhATl5OSctfxBgwbp3nvvVZs2bbx6+zReAADAHD82XgkJCYqJiak4srKyqiyhrKxM+fn5Sk9P9xhPT0/XBx98cMbS582bp6+//lrjx4/35p1LCpKpRgAAgMLCQkVHR1f87HQ6qzxv//79crlciouL8xiPi4tTUVFRla/58ssvNXbsWG3YsEG1annfPtF4AQAAY/z5lUHR0dEejdcvvs7h8PjZsqxKY5Lkcrl077336oknntCll176q2ql8QIAACGlcePGCg8Pr5RuFRcXV0rBJOnw4cP66KOPtGXLFg0dOlSS5Ha7ZVmWatWqpbfffls33XRTte5N4wUAAMw5BzZQjYiIUGpqqnJzc/W73/2uYjw3N1fdu3evdH50dLQ++eQTj7Hs7GytW7dO//jHP5SUlFTte9N4AQCAkDNq1Cj17t1baWlpatOmjWbOnKmCggINHjxYkpSZmalvv/1WCxYsUFhYmFq1auXx+tjYWEVGRlYa/yU0XgAAwBh/rvGqiZ49e+rAgQOaOHGi9u3bp1atWmnNmjVKTEyUJO3bt08FBQW+LVQ0XgAAIERlZGQoIyOjyt/Nnz//rK+dMGGCJkyYUON70ngBAABzzoE1Xnai8QIAAOaEeOPFzvUAAACGkHgBAABjHP87fH3NQEHiBQAAYAiJFwAAMIc1XgAAADCBxAsAABhzrmygahcSLwAAAENsb7y+/fZb3X///WrUqJHq1KmjlJQU5efn210WAADwB8tPR4Cwdarx4MGDateunW688Ub985//VGxsrL7++mvVr1/fzrIAAIA/BVCj5Gu2Nl5TpkxRQkKC5s2bVzHWokUL+woCAADwI1unGlevXq20tDTdc889io2NVevWrTVr1qwznl9aWqqSkhKPAwAABI5Ti+t9fQQKWxuvXbt2KScnR5dcconeeustDR48WMOHD9eCBQuqPD8rK0sxMTEVR0JCguGKAQAAvGdr4+V2u3X11Vdr8uTJat26tQYNGqQHH3xQOTk5VZ6fmZmpQ4cOVRyFhYWGKwYAAL9KiC+ut7Xxio+P1+WXX+4xlpycrIKCgirPdzqdio6O9jgAAAACha2L69u1a6cvvvjCY2znzp1KTEy0qSIAAOBPbKBqo5EjR2rTpk2aPHmyvvrqKy1evFgzZ87UkCFD7CwLAADAL2xtvK655hqtXLlSS5YsUatWrTRp0iRNnz5d9913n51lAQAAfwnxNV62f1dj165d1bVrV7vLAAAA8DvbGy8AABA6Qn2NF40XAAAwxx9TgwHUeNn+JdkAAAChgsQLAACYQ+IFAAAAE0i8AACAMaG+uJ7ECwAAwBASLwAAYA5rvAAAAGACiRcAADDGYVlyWL6NqHx9PX+i8QIAAOYw1QgAAAATSLwAAIAxbCcBAAAAI0i8AACAOazxAgAAgAlBkXjVOi6Fu+2uombq1Dlmdwle+S7eaXcJXtt003N2l+CVNUeT7C7BK3858Vu7S/DazK3t7S7BK+cVRdhdglcc7nK7S/DaD65ou0uokZ9dLrtLYI2X3QUAAACEiqBIvAAAQIAI8TVeNF4AAMAYphoBAABgBIkXAAAwJ8SnGkm8AAAADCHxAgAARgXSmixfI/ECAAAwhMQLAACYY1knD19fM0CQeAEAABhC4gUAAIwJ9X28aLwAAIA5bCcBAAAAE0i8AACAMQ73ycPX1wwUJF4AAACGkHgBAABzWOMFAAAAE0i8AACAMaG+nQSJFwAAgCEkXgAAwJwQ/8ogGi8AAGAMU40AAAAwgsQLAACYw3YSAAAAMIHECwAAGMMaLwAAABhB4gUAAMwJ8e0kSLwAAAAMIfECAADGhPoaLxovAABgDttJAAAAwAQSLwAAYEyoTzWSeAEAABhC4gUAAMxxWycPX18zQJB4AQAAGELiBQAAzOGpRgAAAJhA4gUAAIxxyA9PNfr2cn5F4wUAAMzhuxoBAABgAokXAAAwhg1UAQAAYASJFwAAMIftJAAAAGACiRcAADDGYVly+PgpRF9fz5+CovFq8I9tquU4z+4yauSCwYFV7ynRwwNptxRP/R79nd0leKdhjN0VeOXiei67S/BacVo9u0vwSkS3H+wuwSu/7bbF7hK8tvnIhXaXUCOlR09I+sTuMs4Z2dnZevrpp7Vv3z61bNlS06dPV4cOHao8d8WKFcrJydHWrVtVWlqqli1basKECbr11ltrdE+mGgEAgDluPx01tHTpUo0YMULjxo3Tli1b1KFDB3Xu3FkFBQVVnr9+/Xp16tRJa9asUX5+vm688UZ169ZNW7bU7D8cgiLxAgAAgeFcmWqcNm2aBgwYoIEDB0qSpk+frrfeeks5OTnKysqqdP706dM9fp48ebJWrVql119/Xa1bt672fUm8AABAUCgpKfE4SktLqzyvrKxM+fn5Sk9P9xhPT0/XBx98UK17ud1uHT58WA0bNqxRjTReAADAHMtPh6SEhATFxMRUHFUlV5K0f/9+uVwuxcXFeYzHxcWpqKioWm/jr3/9q44ePaoePXpU951LYqoRAAAEicLCQkVHR1f87HQ6z3q+w+H5wJhlWZXGqrJkyRJNmDBBq1atUmxsbI1qpPECAADm+PFLsqOjoz0arzNp3LixwsPDK6VbxcXFlVKw0y1dulQDBgzQsmXLdMstt9S4VKYaAQBASImIiFBqaqpyc3M9xnNzc9W2bdszvm7JkiXq16+fFi9erC5dunh1bxIvAABgzLnyJdmjRo1S7969lZaWpjZt2mjmzJkqKCjQ4MGDJUmZmZn69ttvtWDBAkknm64+ffrob3/7m66//vqKtKx27dqKian+fos0XgAAIOT07NlTBw4c0MSJE7Vv3z61atVKa9asUWJioiRp3759Hnt6vfjiiyovL9eQIUM0ZMiQivG+fftq/vz51b4vjRcAADDHj2u8aiojI0MZGRlV/u70Zurdd9/16h6nY40XAACAISReAADAGIf75OHrawYKGi8AAGDOOTTVaAemGgEAAAwh8QIAAOb8n6/48ek1AwSJFwAAgCEkXgAAwBiHZcnh4zVZvr6eP5F4AQAAGELiBQAAzOGpRvuUl5frscceU1JSkmrXrq0LL7xQEydOlNsdQBtyAAAAVJOtideUKVP0wgsv6KWXXlLLli310Ucf6YEHHlBMTIwefvhhO0sDAAD+YEnydb4SOIGXvY3Xxo0b1b17d3Xp0kWS1KJFCy1ZskQfffRRleeXlpaqtLS04ueSkhIjdQIAAN9gcb2N2rdvr3feeUc7d+6UJG3btk3vv/++fvvb31Z5flZWlmJiYiqOhIQEk+UCAAD8KrYmXmPGjNGhQ4d02WWXKTw8XC6XS08++aR69epV5fmZmZkaNWpUxc8lJSU0XwAABBJLflhc79vL+ZOtjdfSpUu1cOFCLV68WC1bttTWrVs1YsQINW3aVH379q10vtPplNPptKFSAACAX8/WxuuRRx7R2LFj9fvf/16SdMUVV2jPnj3KysqqsvECAAABju0k7PPzzz8rLMyzhPDwcLaTAAAAQcnWxKtbt2568skn1bx5c7Vs2VJbtmzRtGnT1L9/fzvLAgAA/uKW5PDDNQOErY3Xc889pz//+c/KyMhQcXGxmjZtqkGDBunxxx+3sywAAAC/sLXxioqK0vTp0zV9+nQ7ywAAAIaE+j5efFcjAAAwh8X1AAAAMIHECwAAmEPiBQAAABNIvAAAgDkkXgAAADCBxAsAAJgT4huokngBAAAYQuIFAACMYQNVAAAAU1hcDwAAABNIvAAAgDluS3L4OKFyk3gBAADgNCReAADAHNZ4AQAAwAQSLwAAYJAfEi8FTuIVFI3XgZ5XKTwi0u4yamT/HLsr8E6s4we7S/DariEX2V2CVzrcut3uErzy7X2xdpfgtVmPBuZf0CGPDbe7BK+sdHWyuwSvjZ642O4SauRnt8vuEkJeUDReAAAgQIT4Gi8aLwAAYI7bks+nBtlOAgAAAKcj8QIAAOZY7pOHr68ZIEi8AAAADCHxAgAA5oT44noSLwAAAENIvAAAgDk81QgAAAATSLwAAIA5Ib7Gi8YLAACYY8kPjZdvL+dPTDUCAAAYQuIFAADMCfGpRhIvAAAAQ0i8AACAOW63JB9/xY+brwwCAADAaUi8AACAOazxAgAAgAkkXgAAwJwQT7xovAAAgDl8VyMAAABMIPECAADGWJZbluXb7R98fT1/IvECAAAwhMQLAACYY1m+X5MVQIvrSbwAAAAMIfECAADmWH54qpHECwAAAKcj8QIAAOa43ZLDx08hBtBTjTReAADAHKYaAQAAYAKJFwAAMMZyu2X5eKqRDVQBAABQCYkXAAAwhzVeAAAAMIHECwAAmOO2JAeJFwAAAPyMxAsAAJhjWZJ8vYEqiRcAAABOQ+IFAACMsdyWLB+v8bICKPGi8QIAAOZYbvl+qpENVAEAAHAaEi8AAGBMqE81kngBAAAYQuIFAADMCfE1XgHdeJ2KFl1lx22uxAsBmjWWu0rtLsFrruMB+OdEUtmRMrtL8Eog/1k5cjhw/iH+fwXkPwsluQPz45Yk/XzYZXcJNXLsyMl67ZyaK9cJn39VY7lO+PaCfuSwAmli9DR79+5VQkKC3WUAABBQCgsL1axZM6P3PH78uJKSklRUVOSX6zdp0kS7d+9WZGSkX67vKwHdeLndbn333XeKioqSw+Hw6bVLSkqUkJCgwsJCRUdH+/TaqBqfuVl83mbxeZvHZ16ZZVk6fPiwmjZtqrAw81Mvx48fV1mZf1L8iIiIc77pkgJ8qjEsLMzvHXt0dDR/YQ3jMzeLz9ssPm/z+Mw9xcTE2HbvyMjIgGiO/ClAVxoBAAAEHhovAAAAQ2i8zsDpdGr8+PFyOp12lxIy+MzN4vM2i8/bPD5znIsCenE9AABAICHxAgAAMITGCwAAwBAaLwAAAENovAAAAAyh8TqD7OxsJSUlKTIyUqmpqdqwYYPdJQWlrKwsXXPNNYqKilJsbKzuuOMOffHFF3aXFTKysrLkcDg0YsQIu0sJat9++63uv/9+NWrUSHXq1FFKSory8/PtLisolZeX67HHHlNSUpJq166tCy+8UBMnTpQ7kL8QEkGFxqsKS5cu1YgRIzRu3Dht2bJFHTp0UOfOnVVQUGB3aUHnvffe05AhQ7Rp0ybl5uaqvLxc6enpOnr0qN2lBb28vDzNnDlTV155pd2lBLWDBw+qXbt2Ou+88/TPf/5TO3bs0F//+lfVr1/f7tKC0pQpU/TCCy9oxowZ+uyzzzR16lQ9/fTTeu655+wuDZDEdhJVuu6663T11VcrJyenYiw5OVl33HGHsrKybKws+P3www+KjY3Ve++9pxtuuMHucoLWkSNHdPXVVys7O1t/+ctflJKSounTp9tdVlAaO3as/vOf/5CaG9K1a1fFxcVpzpw5FWN33XWX6tSpo5dfftnGyoCTSLxOU1ZWpvz8fKWnp3uMp6en64MPPrCpqtBx6NAhSVLDhg1triS4DRkyRF26dNEtt9xidylBb/Xq1UpLS9M999yj2NhYtW7dWrNmzbK7rKDVvn17vfPOO9q5c6ckadu2bXr//ff129/+1ubKgJMC+kuy/WH//v1yuVyKi4vzGI+Li1NRUZFNVYUGy7I0atQotW/fXq1atbK7nKD1yiuv6OOPP1ZeXp7dpYSEXbt2KScnR6NGjdKf/vQnbd68WcOHD5fT6VSfPn3sLi/ojBkzRocOHdJll12m8PBwuVwuPfnkk+rVq5fdpQGSaLzOyOFwePxsWValMfjW0KFDtX37dr3//vt2lxK0CgsL9fDDD+vtt99WZGSk3eWEBLfbrbS0NE2ePFmS1Lp1a3366afKycmh8fKDpUuXauHChVq8eLFatmyprVu3asSIEWratKn69u1rd3kAjdfpGjdurPDw8ErpVnFxcaUUDL4zbNgwrV69WuvXr1ezZs3sLido5efnq7i4WKmpqRVjLpdL69ev14wZM1RaWqrw8HAbKww+8fHxuvzyyz3GkpOTtXz5cpsqCm6PPPKIxo4dq9///veSpCuuuEJ79uxRVlYWjRfOCazxOk1ERIRSU1OVm5vrMZ6bm6u2bdvaVFXwsixLQ4cO1YoVK7Ru3TolJSXZXVJQu/nmm/XJJ59o69atFUdaWpruu+8+bd26labLD9q1a1dpi5SdO3cqMTHRpoqC288//6ywMM9/tYWHh7OdBM4ZJF5VGDVqlHr37q20tDS1adNGM2fOVEFBgQYPHmx3aUFnyJAhWrx4sVatWqWoqKiKpDEmJka1a9e2ubrgExUVVWn9XN26ddWoUSPW1fnJyJEj1bZtW02ePFk9evTQ5s2bNXPmTM2cOdPu0oJSt27d9OSTT6p58+Zq2bKltmzZomnTpql///52lwZIYjuJM8rOztbUqVO1b98+tWrVSs8++yzbG/jBmdbNzZs3T/369TNbTIjq2LEj20n42RtvvKHMzEx9+eWXSkpK0qhRo/Tggw/aXVZQOnz4sP785z9r5cqVKi4uVtOmTdWrVy89/vjjioiIsLs8gMYLAADAFNZ4AQAAGELjBQAAYAiNFwAAgCE0XgAAAIbQeAEAABhC4wUAAGAIjRcAAIAhNF4AAACG0HgBsJ3D4dBrr71mdxkA4Hc0XgDkcrnUtm1b3XXXXR7jhw4dUkJCgh577DG/3n/fvn3q3LmzX+8BAOcCvjIIgCTpyy+/VEpKimbOnKn77rtPktSnTx9t27ZNeXl5fM8dAPgAiRcASdIll1yirKwsDRs2TN99951WrVqlV155RS+99NJZm66FCxcqLS1NUVFRatKkie69914VFxdX/H7ixIlq2rSpDhw4UDF2++2364YbbpDb7ZbkOdVYVlamoUOHKj4+XpGRkWrRooWysrL886YBwDASLwAVLMvSTTfdpPDwcH3yyScaNmzYL04zzp07V/Hx8frNb36j4uJijRw5Ug0aNNCaNWsknZzG7NChg+Li4rRy5Uq98MILGjt2rLZt26bExERJJxuvlStX6o477tAzzzyjv//971q0aJGaN2+uwsJCFRYWqlevXn5//wDgbzReADx8/vnnSk5O1hVXXKGPP/5YtWrVqtHr8/LydO211+rw4cOqV6+eJGnXrl1KSUlRRkaGnnvuOY/pTMmz8Ro+fLg+/fRT/etf/5LD4fDpewMAuzHVCMDD3LlzVadOHe3evVt79+79xfO3bNmi7t27KzExUVFRUerYsaMkqaCgoOKcCy+8UM8884ymTJmibt26eTRdp+vXr5+2bt2q3/zmNxo+fLjefvvtX/2eAOBcQeMFoMLGjRv17LPPatWqVWrTpo0GDBigs4XiR48eVXp6uurVq6eFCxcqLy9PK1eulHRyrdb/tX79eoWHh+ubb75ReXn5Ga959dVXa/fu3Zo0aZKOHTumHj166O677/bNGwQAm9F4AZAkHTt2TH379tWgQYN0yy23aPbs2crLy9OLL754xtd8/vnn2r9/v5566il16NBBl112mcfC+lOWLl2qFStW6N1331VhYaEmTZp01lqio6PVs2dPzZo1S0uXLtXy5cv1448//ur3CAB2o/ECIEkaO3as3G63pkyZIklq3ry5/vrXv+qRRx7RN998U+VrmjdvroiICD333HPatWuXVq9eXamp2rt3rx566CFNmTJF7du31/z585WVlaVNmzZVec1nn31Wr7zyij7//HPt3LlTy5YtU5MmTVS/fn1fvl0AsAWNFwC99957ev755zV//nzVrVu3YvzBBx9U27ZtzzjleP7552v+/PlatmyZLr/8cj311FN65plnKn5vWZb69euna6+9VkOHDpUkderUSUOHDtX999+vI0eOVLpmvXr1NGXKFKWlpemaa67RN998ozVr1igsjH9cAQh8PNUIAABgCP8JCQAAYAiNFwAAgCE0XgAAAIbQeAEAABhC4wUAAGAIjRcAAIAhNF4AAACG0HgBAAAYQuMFAABgCI0XAACAITReAAAAhvw/3ovle6iDgAwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    lif_layer_sg_width2 = None,\n",
    "                    lif_layer_v_threshold2 = None,\n",
    "                    learning_rate2 = None,\n",
    "                    init_scaling = None,\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp,\n",
    "                    ANPI_MODE=False,\n",
    "                    lif_layer_sg_width2=lif_layer_sg_width2,\n",
    "                    lif_layer_v_threshold2=lif_layer_v_threshold2,\n",
    "                    init_scaling=init_scaling).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                # lr = group['lr']\n",
    "\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        lr = learning_rate\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        lr = learning_rate2\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        lr = 1.0\n",
    "\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "                now_T = inputs.shape[1]\n",
    "                now_time_steps = temporal_filter*TIME\n",
    "                # start_idx = random.randint(0, now_T - now_time_steps)\n",
    "                start_idx = random.choice(range(0, now_T - now_time_steps + 1, now_time_steps))\n",
    "                # start_idx = random.choice([i for i in range(0, now_T - now_time_steps + 1, now_time_steps)])\n",
    "                inputs = inputs[:, start_idx : start_idx + now_time_steps]\n",
    "                if dvs_clipping != 0:\n",
    "                    inputs[inputs<dvs_clipping] = 0.0\n",
    "                    inputs[inputs>=dvs_clipping] = 1.0\n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            now_T = inputs_val.shape[1]\n",
    "                            now_time_steps = temporal_filter*TIME\n",
    "                            start_idx = 0\n",
    "                            inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if dvs_clipping != 0:\n",
    "                                inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "\n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    assert val_acc_best > 0.2\n",
    "                elif epoch > 10:\n",
    "                    assert val_acc_best > 0.4\n",
    "                elif epoch > 30:\n",
    "                    assert val_acc_best > 0.5\n",
    "                elif epoch > 100:\n",
    "                    assert val_acc_best > 0.6\n",
    "                    \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"5\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 2871,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 8, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 200,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = -1, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[0,0],[0,0],[0,0]], \n",
    "#                 lif_layer_sg_width2 = 4.0,\n",
    "#                 lif_layer_v_threshold2 = 8,\n",
    "#                 learning_rate2 = 8,\n",
    "#                 init_scaling = [1/2,1/2,1/2],\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6h2rplmw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.046875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0234375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_125806-6h2rplmw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6h2rplmw' target=\"_blank\">stoic-sweep-15</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6h2rplmw' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6h2rplmw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251214_125815_382', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 16, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 16, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 32, 'lif_layer_v_threshold2': 128, 'init_scaling': [0.03125, 0.046875, 0.0234375], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 16, self.v_threshold 16\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 32, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=16, v_reset=10000, sg_width=16, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=32, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 46.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 41.0\n",
      "lif layer 2 self.abs_max_v: 41.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 51.0\n",
      "fc layer 2 self.abs_max_out: 80.0\n",
      "lif layer 2 self.abs_max_v: 94.5\n",
      "fc layer 1 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 69.0\n",
      "lif layer 2 self.abs_max_v: 107.5\n",
      "fc layer 1 self.abs_max_out: 62.0\n",
      "lif layer 1 self.abs_max_v: 90.0\n",
      "fc layer 1 self.abs_max_out: 101.0\n",
      "lif layer 1 self.abs_max_v: 138.0\n",
      "fc layer 1 self.abs_max_out: 147.0\n",
      "lif layer 1 self.abs_max_v: 210.0\n",
      "fc layer 1 self.abs_max_out: 200.0\n",
      "lif layer 1 self.abs_max_v: 270.0\n",
      "lif layer 2 self.abs_max_v: 123.5\n",
      "fc layer 1 self.abs_max_out: 212.0\n",
      "lif layer 1 self.abs_max_v: 330.0\n",
      "fc layer 2 self.abs_max_out: 97.0\n",
      "lif layer 2 self.abs_max_v: 124.0\n",
      "fc layer 1 self.abs_max_out: 248.0\n",
      "lif layer 1 self.abs_max_v: 362.0\n",
      "fc layer 2 self.abs_max_out: 109.0\n",
      "lif layer 2 self.abs_max_v: 158.0\n",
      "fc layer 3 self.abs_max_out: 3.0\n",
      "fc layer 1 self.abs_max_out: 259.0\n",
      "fc layer 2 self.abs_max_out: 139.0\n",
      "fc layer 3 self.abs_max_out: 4.0\n",
      "fc layer 2 self.abs_max_out: 160.0\n",
      "lif layer 2 self.abs_max_v: 196.0\n",
      "fc layer 2 self.abs_max_out: 176.0\n",
      "fc layer 1 self.abs_max_out: 270.0\n",
      "fc layer 2 self.abs_max_out: 201.0\n",
      "lif layer 2 self.abs_max_v: 201.0\n",
      "fc layer 3 self.abs_max_out: 6.0\n",
      "fc layer 1 self.abs_max_out: 347.0\n",
      "lif layer 1 self.abs_max_v: 398.5\n",
      "lif layer 1 self.abs_max_v: 430.5\n",
      "fc layer 2 self.abs_max_out: 212.0\n",
      "lif layer 2 self.abs_max_v: 212.0\n",
      "fc layer 2 self.abs_max_out: 235.0\n",
      "lif layer 2 self.abs_max_v: 235.0\n",
      "fc layer 1 self.abs_max_out: 359.0\n",
      "lif layer 2 self.abs_max_v: 247.5\n",
      "fc layer 2 self.abs_max_out: 265.0\n",
      "lif layer 2 self.abs_max_v: 265.0\n",
      "fc layer 1 self.abs_max_out: 425.0\n",
      "fc layer 1 self.abs_max_out: 428.0\n",
      "fc layer 2 self.abs_max_out: 275.0\n",
      "lif layer 2 self.abs_max_v: 275.0\n",
      "fc layer 2 self.abs_max_out: 276.0\n",
      "lif layer 2 self.abs_max_v: 276.0\n",
      "fc layer 1 self.abs_max_out: 453.0\n",
      "lif layer 1 self.abs_max_v: 453.0\n",
      "fc layer 3 self.abs_max_out: 7.0\n",
      "fc layer 2 self.abs_max_out: 280.0\n",
      "lif layer 2 self.abs_max_v: 280.0\n",
      "fc layer 3 self.abs_max_out: 9.0\n",
      "lif layer 2 self.abs_max_v: 299.5\n",
      "lif layer 1 self.abs_max_v: 463.5\n",
      "fc layer 2 self.abs_max_out: 283.0\n",
      "fc layer 2 self.abs_max_out: 295.0\n",
      "lif layer 1 self.abs_max_v: 480.0\n",
      "fc layer 1 self.abs_max_out: 524.0\n",
      "lif layer 1 self.abs_max_v: 524.0\n",
      "fc layer 1 self.abs_max_out: 587.0\n",
      "lif layer 1 self.abs_max_v: 630.5\n",
      "lif layer 1 self.abs_max_v: 744.5\n",
      "lif layer 1 self.abs_max_v: 762.5\n",
      "fc layer 2 self.abs_max_out: 303.0\n",
      "lif layer 2 self.abs_max_v: 303.0\n",
      "lif layer 1 self.abs_max_v: 772.5\n",
      "fc layer 2 self.abs_max_out: 310.0\n",
      "lif layer 2 self.abs_max_v: 310.0\n",
      "lif layer 1 self.abs_max_v: 855.5\n",
      "fc layer 2 self.abs_max_out: 318.0\n",
      "lif layer 2 self.abs_max_v: 318.0\n",
      "fc layer 2 self.abs_max_out: 319.0\n",
      "lif layer 2 self.abs_max_v: 319.0\n",
      "fc layer 2 self.abs_max_out: 326.0\n",
      "lif layer 2 self.abs_max_v: 326.0\n",
      "fc layer 2 self.abs_max_out: 383.0\n",
      "lif layer 2 self.abs_max_v: 383.0\n",
      "fc layer 1 self.abs_max_out: 606.0\n",
      "lif layer 2 self.abs_max_v: 393.0\n",
      "lif layer 2 self.abs_max_v: 399.5\n",
      "lif layer 2 self.abs_max_v: 422.0\n",
      "fc layer 3 self.abs_max_out: 10.0\n",
      "fc layer 3 self.abs_max_out: 11.0\n",
      "fc layer 3 self.abs_max_out: 13.0\n",
      "lif layer 1 self.abs_max_v: 856.0\n",
      "lif layer 1 self.abs_max_v: 861.0\n",
      "fc layer 2 self.abs_max_out: 387.0\n",
      "fc layer 2 self.abs_max_out: 391.0\n",
      "fc layer 2 self.abs_max_out: 399.0\n",
      "fc layer 2 self.abs_max_out: 428.0\n",
      "lif layer 2 self.abs_max_v: 428.0\n",
      "fc layer 3 self.abs_max_out: 14.0\n",
      "fc layer 1 self.abs_max_out: 699.0\n",
      "lif layer 1 self.abs_max_v: 1083.5\n",
      "lif layer 1 self.abs_max_v: 1230.0\n",
      "fc layer 3 self.abs_max_out: 16.0\n",
      "fc layer 2 self.abs_max_out: 438.0\n",
      "lif layer 2 self.abs_max_v: 438.0\n",
      "fc layer 2 self.abs_max_out: 502.0\n",
      "lif layer 2 self.abs_max_v: 502.0\n",
      "fc layer 3 self.abs_max_out: 17.0\n",
      "fc layer 3 self.abs_max_out: 21.0\n",
      "fc layer 1 self.abs_max_out: 710.0\n",
      "lif layer 1 self.abs_max_v: 1236.5\n",
      "fc layer 1 self.abs_max_out: 726.0\n",
      "lif layer 1 self.abs_max_v: 1336.5\n",
      "fc layer 1 self.abs_max_out: 775.0\n",
      "lif layer 1 self.abs_max_v: 1437.5\n",
      "lif layer 1 self.abs_max_v: 1441.0\n",
      "lif layer 1 self.abs_max_v: 1474.5\n",
      "fc layer 1 self.abs_max_out: 812.0\n",
      "fc layer 3 self.abs_max_out: 22.0\n",
      "fc layer 2 self.abs_max_out: 517.0\n",
      "lif layer 2 self.abs_max_v: 517.0\n",
      "fc layer 1 self.abs_max_out: 862.0\n",
      "lif layer 1 self.abs_max_v: 1522.0\n",
      "fc layer 1 self.abs_max_out: 909.0\n",
      "lif layer 1 self.abs_max_v: 1560.5\n",
      "fc layer 3 self.abs_max_out: 23.0\n",
      "lif layer 2 self.abs_max_v: 568.5\n",
      "lif layer 2 self.abs_max_v: 591.5\n",
      "fc layer 2 self.abs_max_out: 564.0\n",
      "fc layer 2 self.abs_max_out: 566.0\n",
      "fc layer 1 self.abs_max_out: 926.0\n",
      "lif layer 2 self.abs_max_v: 619.5\n",
      "lif layer 2 self.abs_max_v: 625.0\n",
      "fc layer 1 self.abs_max_out: 979.0\n",
      "lif layer 2 self.abs_max_v: 630.5\n",
      "lif layer 2 self.abs_max_v: 741.5\n",
      "lif layer 2 self.abs_max_v: 784.0\n",
      "lif layer 2 self.abs_max_v: 803.0\n",
      "fc layer 1 self.abs_max_out: 994.0\n",
      "lif layer 1 self.abs_max_v: 1744.5\n",
      "fc layer 1 self.abs_max_out: 1020.0\n",
      "fc layer 1 self.abs_max_out: 1102.0\n",
      "lif layer 1 self.abs_max_v: 1820.0\n",
      "lif layer 1 self.abs_max_v: 1915.0\n",
      "lif layer 1 self.abs_max_v: 1931.5\n",
      "fc layer 1 self.abs_max_out: 1137.0\n",
      "lif layer 1 self.abs_max_v: 2032.0\n",
      "fc layer 2 self.abs_max_out: 573.0\n",
      "fc layer 2 self.abs_max_out: 581.0\n",
      "fc layer 2 self.abs_max_out: 588.0\n",
      "fc layer 2 self.abs_max_out: 597.0\n",
      "fc layer 2 self.abs_max_out: 607.0\n",
      "fc layer 1 self.abs_max_out: 1207.0\n",
      "fc layer 1 self.abs_max_out: 1301.0\n",
      "lif layer 1 self.abs_max_v: 2235.0\n",
      "lif layer 1 self.abs_max_v: 2403.5\n",
      "lif layer 1 self.abs_max_v: 2482.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss:  0.895226/  4.726969, val:  29.17%, val_best:  29.17%, tr:  96.63%, tr_best:  96.63%, epoch time: 72.86 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0666%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.2150%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 1908  19.489%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 1309.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss:  0.754453/  3.211822, val:  35.83%, val_best:  35.83%, tr:  97.75%, tr_best:  97.75%, epoch time: 71.42 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6143%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.4087%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3587  18.320%\n",
      "lif layer 2 self.abs_max_v: 849.5\n",
      "fc layer 2 self.abs_max_out: 609.0\n",
      "fc layer 2 self.abs_max_out: 614.0\n",
      "fc layer 2 self.abs_max_out: 626.0\n",
      "fc layer 2 self.abs_max_out: 628.0\n",
      "fc layer 2 self.abs_max_out: 659.0\n",
      "lif layer 2 self.abs_max_v: 874.5\n",
      "fc layer 1 self.abs_max_out: 1422.0\n",
      "lif layer 1 self.abs_max_v: 2519.5\n",
      "fc layer 1 self.abs_max_out: 1532.0\n",
      "lif layer 1 self.abs_max_v: 2591.5\n",
      "lif layer 1 self.abs_max_v: 2733.0\n",
      "lif layer 1 self.abs_max_v: 2849.5\n",
      "lif layer 1 self.abs_max_v: 2855.0\n",
      "lif layer 2 self.abs_max_v: 879.0\n",
      "lif layer 2 self.abs_max_v: 939.5\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss:  0.762185/  4.190839, val:  32.08%, val_best:  35.83%, tr:  97.96%, tr_best:  97.96%, epoch time: 70.21 seconds, 1.17 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6221%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.2151%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 5253  17.886%\n",
      "lif layer 2 self.abs_max_v: 943.5\n",
      "lif layer 2 self.abs_max_v: 1001.0\n",
      "lif layer 2 self.abs_max_v: 1020.5\n",
      "fc layer 3 self.abs_max_out: 24.0\n",
      "fc layer 1 self.abs_max_out: 1572.0\n",
      "lif layer 1 self.abs_max_v: 2901.0\n",
      "fc layer 1 self.abs_max_out: 1638.0\n",
      "lif layer 1 self.abs_max_v: 2930.0\n",
      "lif layer 1 self.abs_max_v: 3056.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  0.836154/  3.543561, val:  42.50%, val_best:  42.50%, tr:  94.48%, tr_best:  97.96%, epoch time: 70.38 seconds, 1.17 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1056%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.3114%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 7222  18.442%\n",
      "fc layer 1 self.abs_max_out: 1640.0\n",
      "fc layer 3 self.abs_max_out: 25.0\n",
      "fc layer 3 self.abs_max_out: 26.0\n",
      "fc layer 3 self.abs_max_out: 27.0\n",
      "fc layer 3 self.abs_max_out: 28.0\n",
      "fc layer 2 self.abs_max_out: 663.0\n",
      "lif layer 2 self.abs_max_v: 1023.0\n",
      "lif layer 2 self.abs_max_v: 1045.0\n",
      "lif layer 2 self.abs_max_v: 1057.0\n",
      "fc layer 2 self.abs_max_out: 665.0\n",
      "fc layer 1 self.abs_max_out: 1739.0\n",
      "lif layer 1 self.abs_max_v: 3058.5\n",
      "lif layer 1 self.abs_max_v: 3197.5\n",
      "fc layer 2 self.abs_max_out: 703.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  0.763220/  3.731679, val:  42.08%, val_best:  42.50%, tr:  98.37%, tr_best:  98.37%, epoch time: 70.03 seconds, 1.17 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4884%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.9569%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 8812  18.002%\n",
      "lif layer 2 self.abs_max_v: 1077.5\n",
      "lif layer 2 self.abs_max_v: 1085.5\n",
      "lif layer 2 self.abs_max_v: 1124.5\n",
      "fc layer 2 self.abs_max_out: 717.0\n",
      "fc layer 2 self.abs_max_out: 739.0\n",
      "fc layer 1 self.abs_max_out: 1870.0\n",
      "lif layer 1 self.abs_max_v: 3235.0\n",
      "lif layer 2 self.abs_max_v: 1125.5\n",
      "fc layer 2 self.abs_max_out: 743.0\n",
      "fc layer 1 self.abs_max_out: 1919.0\n",
      "fc layer 1 self.abs_max_out: 1981.0\n",
      "lif layer 1 self.abs_max_v: 3330.5\n",
      "fc layer 1 self.abs_max_out: 2001.0\n",
      "lif layer 1 self.abs_max_v: 3635.0\n",
      "lif layer 2 self.abs_max_v: 1130.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  0.806481/  3.356676, val:  43.33%, val_best:  43.33%, tr:  95.71%, tr_best:  98.37%, epoch time: 70.50 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0874%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.1146%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 10689  18.197%\n",
      "fc layer 2 self.abs_max_out: 748.0\n",
      "lif layer 2 self.abs_max_v: 1156.5\n",
      "lif layer 2 self.abs_max_v: 1204.5\n",
      "lif layer 2 self.abs_max_v: 1219.5\n",
      "lif layer 2 self.abs_max_v: 1241.0\n",
      "fc layer 1 self.abs_max_out: 2005.0\n",
      "fc layer 1 self.abs_max_out: 2025.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  0.769439/  3.665798, val:  43.33%, val_best:  43.33%, tr:  96.12%, tr_best:  98.37%, epoch time: 70.76 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9734%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.1987%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 12456  18.176%\n",
      "fc layer 1 self.abs_max_out: 2049.0\n",
      "lif layer 1 self.abs_max_v: 3727.5\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  0.761377/  3.011942, val:  47.08%, val_best:  47.08%, tr:  96.42%, tr_best:  98.37%, epoch time: 70.79 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5882%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.1655%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 14281  18.234%\n",
      "fc layer 2 self.abs_max_out: 795.0\n",
      "lif layer 2 self.abs_max_v: 1265.0\n",
      "lif layer 2 self.abs_max_v: 1303.5\n",
      "lif layer 2 self.abs_max_v: 1309.0\n",
      "fc layer 2 self.abs_max_out: 798.0\n",
      "lif layer 2 self.abs_max_v: 1327.5\n",
      "lif layer 2 self.abs_max_v: 1339.5\n",
      "fc layer 1 self.abs_max_out: 2132.0\n",
      "fc layer 1 self.abs_max_out: 2207.0\n",
      "lif layer 1 self.abs_max_v: 3740.5\n",
      "fc layer 1 self.abs_max_out: 2277.0\n",
      "lif layer 1 self.abs_max_v: 4147.5\n",
      "fc layer 2 self.abs_max_out: 804.0\n",
      "lif layer 2 self.abs_max_v: 1437.5\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  0.774449/  2.774608, val:  45.00%, val_best:  47.08%, tr:  95.61%, tr_best:  98.37%, epoch time: 70.92 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1372%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.3810%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 16230  18.420%\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  0.782691/  2.861649, val:  43.75%, val_best:  47.08%, tr:  94.18%, tr_best:  98.37%, epoch time: 70.78 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5401%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.3971%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 18107  18.495%\n",
      "fc layer 2 self.abs_max_out: 873.0\n",
      "lif layer 2 self.abs_max_v: 1482.0\n",
      "lif layer 2 self.abs_max_v: 1518.0\n",
      "fc layer 1 self.abs_max_out: 2324.0\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  0.813743/  3.311740, val:  42.92%, val_best:  47.08%, tr:  93.46%, tr_best:  98.37%, epoch time: 70.29 seconds, 1.17 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3868%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.4868%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 20166  18.726%\n",
      "lif layer 2 self.abs_max_v: 1559.5\n",
      "fc layer 2 self.abs_max_out: 902.0\n",
      "fc layer 2 self.abs_max_out: 920.0\n",
      "fc layer 2 self.abs_max_out: 973.0\n",
      "fc layer 2 self.abs_max_out: 976.0\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  0.770629/  2.775945, val:  56.25%, val_best:  56.25%, tr:  95.20%, tr_best:  98.37%, epoch time: 71.47 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1601%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.1551%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 22079  18.794%\n",
      "fc layer 3 self.abs_max_out: 30.0\n",
      "fc layer 1 self.abs_max_out: 2525.0\n",
      "lif layer 1 self.abs_max_v: 4306.0\n",
      "lif layer 1 self.abs_max_v: 4519.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  0.752255/  3.088010, val:  44.17%, val_best:  56.25%, tr:  96.12%, tr_best:  98.37%, epoch time: 71.02 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4171%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.1972%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 23853  18.742%\n",
      "lif layer 2 self.abs_max_v: 1579.5\n",
      "lif layer 2 self.abs_max_v: 1604.0\n",
      "lif layer 2 self.abs_max_v: 1637.5\n",
      "lif layer 2 self.abs_max_v: 1679.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  0.778348/  3.152702, val:  42.50%, val_best:  56.25%, tr:  95.40%, tr_best:  98.37%, epoch time: 70.81 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.2373%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 25756  18.792%\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  0.797356/  3.663465, val:  41.67%, val_best:  56.25%, tr:  93.05%, tr_best:  98.37%, epoch time: 71.10 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3999%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.1935%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 27728  18.882%\n",
      "fc layer 2 self.abs_max_out: 988.0\n",
      "lif layer 2 self.abs_max_v: 1818.5\n",
      "lif layer 2 self.abs_max_v: 1821.5\n",
      "lif layer 1 self.abs_max_v: 4527.5\n",
      "lif layer 1 self.abs_max_v: 4543.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  0.761892/  2.757704, val:  39.58%, val_best:  56.25%, tr:  94.79%, tr_best:  98.37%, epoch time: 70.59 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3868%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.3191%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 29499  18.832%\n",
      "fc layer 2 self.abs_max_out: 1018.0\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  0.743411/  2.486986, val:  48.75%, val_best:  56.25%, tr:  95.10%, tr_best:  98.37%, epoch time: 70.51 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5659%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.4226%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 31365  18.846%\n",
      "fc layer 2 self.abs_max_out: 1127.0\n",
      "fc layer 1 self.abs_max_out: 2576.0\n",
      "lif layer 1 self.abs_max_v: 4617.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  0.755060/  2.734842, val:  52.92%, val_best:  56.25%, tr:  94.79%, tr_best:  98.37%, epoch time: 70.88 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4265%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.3140%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 33281  18.886%\n",
      "lif layer 2 self.abs_max_v: 1916.0\n",
      "lif layer 2 self.abs_max_v: 1935.0\n",
      "lif layer 2 self.abs_max_v: 2003.5\n",
      "fc layer 1 self.abs_max_out: 2695.0\n",
      "lif layer 1 self.abs_max_v: 4852.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  0.764100/  3.145887, val:  40.42%, val_best:  56.25%, tr:  94.38%, tr_best:  98.37%, epoch time: 70.79 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5226%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.3322%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 35220  18.934%\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  0.733844/  4.437584, val:  42.92%, val_best:  56.25%, tr:  94.08%, tr_best:  98.37%, epoch time: 70.86 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6009%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.2442%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 37091  18.943%\n",
      "fc layer 3 self.abs_max_out: 32.0\n",
      "fc layer 3 self.abs_max_out: 33.0\n",
      "fc layer 3 self.abs_max_out: 34.0\n",
      "fc layer 3 self.abs_max_out: 35.0\n",
      "lif layer 1 self.abs_max_v: 4907.5\n",
      "lif layer 1 self.abs_max_v: 4914.0\n",
      "lif layer 1 self.abs_max_v: 4985.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  0.753050/  3.115790, val:  37.50%, val_best:  56.25%, tr:  92.85%, tr_best:  98.37%, epoch time: 70.39 seconds, 1.17 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2334%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.3391%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 38995  18.967%\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  0.772797/  2.447091, val:  50.83%, val_best:  56.25%, tr:  94.99%, tr_best:  98.37%, epoch time: 70.93 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2671%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.3189%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 40876  18.979%\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  0.700447/  1.886519, val:  61.67%, val_best:  61.67%, tr:  95.20%, tr_best:  98.37%, epoch time: 70.91 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6730%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.2821%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 42668  18.949%\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  0.689803/  2.292391, val:  56.25%, val_best:  61.67%, tr:  94.99%, tr_best:  98.37%, epoch time: 71.31 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1756%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.2176%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 44426  18.908%\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  0.643282/  2.462636, val:  55.42%, val_best:  61.67%, tr:  97.34%, tr_best:  98.37%, epoch time: 71.23 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6446%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.1277%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 45927  18.765%\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  0.703597/  2.541960, val:  49.58%, val_best:  61.67%, tr:  94.18%, tr_best:  98.37%, epoch time: 71.23 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2918%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.2361%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 47742  18.756%\n",
      "fc layer 1 self.abs_max_out: 2782.0\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  0.588188/  2.501189, val:  59.58%, val_best:  61.67%, tr:  97.65%, tr_best:  98.37%, epoch time: 70.70 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4886%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.2243%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 49121  18.583%\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  0.645192/  2.528797, val:  62.50%, val_best:  62.50%, tr:  95.91%, tr_best:  98.37%, epoch time: 71.90 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4865%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.2606%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 50760  18.517%\n",
      "fc layer 1 self.abs_max_out: 2835.0\n",
      "lif layer 1 self.abs_max_v: 5097.5\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  0.653056/  2.855411, val:  53.33%, val_best:  62.50%, tr:  95.20%, tr_best:  98.37%, epoch time: 71.62 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1521%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.3723%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 52467  18.480%\n",
      "lif layer 2 self.abs_max_v: 2005.0\n",
      "lif layer 2 self.abs_max_v: 2011.5\n",
      "lif layer 2 self.abs_max_v: 2049.0\n",
      "lif layer 2 self.abs_max_v: 2128.5\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  0.709749/  3.336203, val:  48.33%, val_best:  62.50%, tr:  93.56%, tr_best:  98.37%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2637%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.3228%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 54319  18.495%\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  0.673364/  2.577390, val:  50.00%, val_best:  62.50%, tr:  95.20%, tr_best:  98.37%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1719%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.2082%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 56068  18.474%\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  0.688495/  3.192896, val:  51.67%, val_best:  62.50%, tr:  93.67%, tr_best:  98.37%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5961%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.2279%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 57852  18.467%\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  0.672276/  2.912286, val:  52.50%, val_best:  62.50%, tr:  94.69%, tr_best:  98.37%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6843%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.1459%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 59581  18.442%\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  0.692066/  3.013950, val:  52.92%, val_best:  62.50%, tr:  94.89%, tr_best:  98.37%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4726%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.2249%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 61368  18.437%\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  0.685751/  2.422489, val:  55.42%, val_best:  62.50%, tr:  93.26%, tr_best:  98.37%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5134%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.2894%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 63198  18.444%\n",
      "fc layer 2 self.abs_max_out: 1155.0\n",
      "fc layer 1 self.abs_max_out: 2991.0\n",
      "lif layer 1 self.abs_max_v: 5229.5\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  0.652613/  2.069449, val:  62.50%, val_best:  62.50%, tr:  94.79%, tr_best:  98.37%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6821%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.3178%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 64874  18.407%\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  0.629180/  2.538686, val:  55.42%, val_best:  62.50%, tr:  96.53%, tr_best:  98.37%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9283%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.2943%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 66469  18.350%\n",
      "lif layer 1 self.abs_max_v: 5288.5\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  0.612114/  2.083167, val:  61.67%, val_best:  62.50%, tr:  94.59%, tr_best:  98.37%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7849%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.3681%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 68122  18.311%\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  0.631872/  2.753652, val:  49.58%, val_best:  62.50%, tr:  95.91%, tr_best:  98.37%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8648%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.3885%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 69759  18.271%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  0.629503/  2.826874, val:  51.25%, val_best:  62.50%, tr:  94.28%, tr_best:  98.37%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5821%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.4957%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 71480  18.253%\n",
      "lif layer 1 self.abs_max_v: 5330.0\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  0.607297/  2.914765, val:  54.17%, val_best:  62.50%, tr:  96.12%, tr_best:  98.37%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5358%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.3461%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 72989  18.184%\n",
      "lif layer 1 self.abs_max_v: 5333.5\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  0.609823/  2.385157, val:  62.50%, val_best:  62.50%, tr:  96.53%, tr_best:  98.37%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6687%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.2231%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 74515  18.122%\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  0.629590/  2.331810, val:  64.58%, val_best:  64.58%, tr:  97.34%, tr_best:  98.37%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4488%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.0560%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 76025  18.059%\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  0.635834/  2.456984, val:  64.58%, val_best:  64.58%, tr:  95.51%, tr_best:  98.37%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9569%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.1382%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 77650  18.026%\n",
      "fc layer 1 self.abs_max_out: 3063.0\n",
      "lif layer 1 self.abs_max_v: 5479.5\n",
      "lif layer 1 self.abs_max_v: 5521.0\n",
      "lif layer 1 self.abs_max_v: 5805.5\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  0.660146/  2.038491, val:  71.25%, val_best:  71.25%, tr:  92.85%, tr_best:  98.37%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6933%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.3222%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 79442  18.032%\n",
      "fc layer 2 self.abs_max_out: 1368.0\n",
      "fc layer 1 self.abs_max_out: 3124.0\n",
      "lif layer 1 self.abs_max_v: 5953.0\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  0.623561/  1.973394, val:  71.25%, val_best:  71.25%, tr:  94.59%, tr_best:  98.37%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0848%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.3284%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 81110  18.011%\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  0.620582/  2.157206, val:  66.25%, val_best:  71.25%, tr:  94.28%, tr_best:  98.37%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0288%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.2973%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 82806  17.996%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  0.622688/  2.654208, val:  54.17%, val_best:  71.25%, tr:  93.87%, tr_best:  98.37%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2630%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.2321%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 84507  17.983%\n",
      "fc layer 1 self.abs_max_out: 3147.0\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  0.618310/  2.083867, val:  70.42%, val_best:  71.25%, tr:  95.30%, tr_best:  98.37%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0877%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.2797%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 86174  17.964%\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  0.574787/  1.913026, val:  74.58%, val_best:  74.58%, tr:  96.63%, tr_best:  98.37%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2538%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.3301%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 87685  17.913%\n",
      "lif layer 2 self.abs_max_v: 2194.5\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  0.614553/  2.078665, val:  65.83%, val_best:  74.58%, tr:  93.36%, tr_best:  98.37%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7034%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.4436%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 89418  17.909%\n",
      "lif layer 2 self.abs_max_v: 2268.5\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  0.616478/  2.242399, val:  60.42%, val_best:  74.58%, tr:  93.05%, tr_best:  98.37%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3863%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.4190%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 91164  17.908%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  0.640193/  2.137959, val:  64.17%, val_best:  74.58%, tr:  92.44%, tr_best:  98.37%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9032%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.4038%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 92989  17.921%\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  0.606941/  1.761712, val:  71.25%, val_best:  74.58%, tr:  94.38%, tr_best:  98.37%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9304%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.3842%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 94706  17.914%\n",
      "fc layer 1 self.abs_max_out: 3152.0\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  0.559788/  1.869958, val:  68.75%, val_best:  74.58%, tr:  96.63%, tr_best:  98.37%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3954%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.4164%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 96206  17.867%\n",
      "fc layer 1 self.abs_max_out: 3206.0\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  0.526842/  1.915048, val:  71.67%, val_best:  74.58%, tr:  96.94%, tr_best:  98.37%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2075%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.4773%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 97588  17.800%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  0.551770/  1.572449, val:  80.00%, val_best:  80.00%, tr:  95.30%, tr_best:  98.37%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5921%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5009%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 99123  17.763%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  0.537567/  2.049258, val:  60.00%, val_best:  80.00%, tr:  96.42%, tr_best:  98.37%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3476%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5068%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 100538  17.706%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  0.562652/  1.884948, val:  58.75%, val_best:  80.00%, tr:  95.61%, tr_best:  98.37%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4176%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.4511%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 102003  17.659%\n",
      "fc layer 1 self.abs_max_out: 3319.0\n",
      "lif layer 1 self.abs_max_v: 5953.5\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  0.552540/  1.878782, val:  65.42%, val_best:  80.00%, tr:  94.08%, tr_best:  98.37%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1470%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5144%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 103575  17.633%\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  0.579824/  2.646144, val:  50.00%, val_best:  80.00%, tr:  93.05%, tr_best:  98.37%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5733%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5116%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 105224  17.620%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  0.611173/  1.966725, val:  66.67%, val_best:  80.00%, tr:  94.48%, tr_best:  98.37%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1535%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5766%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 106895  17.611%\n",
      "fc layer 1 self.abs_max_out: 3338.0\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  0.553337/  2.112376, val:  60.83%, val_best:  80.00%, tr:  97.55%, tr_best:  98.37%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4022%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5256%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 108353  17.568%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  0.515360/  2.156648, val:  64.17%, val_best:  80.00%, tr:  96.94%, tr_best:  98.37%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5921%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.4890%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 109692  17.507%\n",
      "fc layer 1 self.abs_max_out: 3347.0\n",
      "fc layer 2 self.abs_max_out: 1400.0\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  0.523944/  1.763469, val:  67.08%, val_best:  80.00%, tr:  96.73%, tr_best:  98.37%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5842%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.4879%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 111106  17.460%\n",
      "fc layer 1 self.abs_max_out: 3426.0\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  0.547002/  1.984232, val:  65.42%, val_best:  80.00%, tr:  93.87%, tr_best:  98.37%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6619%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5082%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 112653  17.435%\n",
      "fc layer 1 self.abs_max_out: 3453.0\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  0.570520/  2.553463, val:  57.08%, val_best:  80.00%, tr:  94.38%, tr_best:  98.37%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3760%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5569%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 114262  17.420%\n",
      "fc layer 1 self.abs_max_out: 3488.0\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  0.538562/  1.673789, val:  76.25%, val_best:  80.00%, tr:  95.81%, tr_best:  98.37%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7508%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5061%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 115749  17.387%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  0.535586/  1.919257, val:  70.00%, val_best:  80.00%, tr:  95.30%, tr_best:  98.37%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6860%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.4907%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 117235  17.355%\n",
      "fc layer 1 self.abs_max_out: 3492.0\n",
      "lif layer 1 self.abs_max_v: 6126.0\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  0.514570/  2.508730, val:  51.67%, val_best:  80.00%, tr:  94.99%, tr_best:  98.37%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0381%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5142%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 118630  17.311%\n",
      "lif layer 1 self.abs_max_v: 6155.0\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  0.567562/  1.980988, val:  66.25%, val_best:  80.00%, tr:  93.46%, tr_best:  98.37%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4153%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6074%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 120272  17.303%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  0.568944/  1.991018, val:  53.75%, val_best:  80.00%, tr:  93.16%, tr_best:  98.37%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9968%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6511%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 121868  17.289%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  0.529366/  1.329536, val:  81.25%, val_best:  81.25%, tr:  96.02%, tr_best:  98.37%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0020%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6471%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 123326  17.256%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  0.437337/  1.854784, val:  71.67%, val_best:  81.25%, tr:  98.57%, tr_best:  98.57%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3984%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5533%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 124436  17.176%\n",
      "fc layer 1 self.abs_max_out: 3505.0\n",
      "lif layer 1 self.abs_max_v: 6186.0\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  0.476114/  1.974046, val:  62.92%, val_best:  81.25%, tr:  97.34%, tr_best:  98.57%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7993%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5710%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 125673  17.116%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  0.539490/  1.578166, val:  72.92%, val_best:  81.25%, tr:  95.81%, tr_best:  98.57%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8230%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6831%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 127129  17.086%\n",
      "fc layer 2 self.abs_max_out: 1441.0\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  0.524514/  1.550452, val:  73.33%, val_best:  81.25%, tr:  96.22%, tr_best:  98.57%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2367%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6028%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 128594  17.059%\n",
      "fc layer 2 self.abs_max_out: 1486.0\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  0.511731/  2.035396, val:  63.33%, val_best:  81.25%, tr:  96.32%, tr_best:  98.57%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3930%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5847%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 129998  17.024%\n",
      "fc layer 2 self.abs_max_out: 1487.0\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  0.544517/  1.900146, val:  69.17%, val_best:  81.25%, tr:  94.59%, tr_best:  98.57%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2289%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6815%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 131539  17.008%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  0.538195/  1.490316, val:  70.00%, val_best:  81.25%, tr:  93.97%, tr_best:  98.57%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1651%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6376%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 133005  16.982%\n",
      "lif layer 1 self.abs_max_v: 6210.0\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  0.534103/  2.123815, val:  62.08%, val_best:  81.25%, tr:  94.28%, tr_best:  98.57%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2717%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5851%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 134475  16.958%\n",
      "fc layer 1 self.abs_max_out: 3544.0\n",
      "lif layer 1 self.abs_max_v: 6295.5\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  0.560672/  1.632330, val:  74.58%, val_best:  81.25%, tr:  93.77%, tr_best:  98.57%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6915%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5987%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 136013  16.943%\n",
      "fc layer 1 self.abs_max_out: 3548.0\n",
      "lif layer 1 self.abs_max_v: 6301.0\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  0.551634/  1.942599, val:  65.83%, val_best:  81.25%, tr:  93.97%, tr_best:  98.57%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9535%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5502%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 137553  16.928%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  0.540958/  1.735769, val:  74.17%, val_best:  81.25%, tr:  93.56%, tr_best:  98.57%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3708%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5422%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 139091  16.914%\n",
      "fc layer 1 self.abs_max_out: 3596.0\n",
      "lif layer 2 self.abs_max_v: 2312.5\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  0.464735/  1.867032, val:  67.08%, val_best:  81.25%, tr:  98.26%, tr_best:  98.57%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4651%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.4980%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 140287  16.858%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  0.499579/  2.048299, val:  70.00%, val_best:  81.25%, tr:  96.32%, tr_best:  98.57%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9386%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.4726%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 141680  16.828%\n",
      "lif layer 1 self.abs_max_v: 6306.5\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  0.443168/  1.928140, val:  74.17%, val_best:  81.25%, tr:  98.26%, tr_best:  98.57%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3316%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.4964%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 142787  16.764%\n",
      "lif layer 1 self.abs_max_v: 6325.5\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  0.451355/  1.875214, val:  68.75%, val_best:  81.25%, tr:  97.85%, tr_best:  98.57%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9710%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5150%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 143990  16.713%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  0.471870/  1.901228, val:  71.67%, val_best:  81.25%, tr:  96.12%, tr_best:  98.57%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0402%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5181%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 145285  16.674%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  0.540446/  1.781129, val:  71.67%, val_best:  81.25%, tr:  93.36%, tr_best:  98.57%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2675%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5219%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 146857  16.667%\n",
      "lif layer 1 self.abs_max_v: 6354.5\n",
      "fc layer 2 self.abs_max_out: 1561.0\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  0.515620/  1.967213, val:  67.50%, val_best:  81.25%, tr:  94.69%, tr_best:  98.57%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4880%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.4207%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 148281  16.644%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  0.489652/  1.536942, val:  74.58%, val_best:  81.25%, tr:  93.97%, tr_best:  98.57%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2737%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5267%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 149660  16.616%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  0.496533/  1.726297, val:  79.58%, val_best:  81.25%, tr:  95.71%, tr_best:  98.57%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4606%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5241%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 150989  16.584%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  0.495172/  1.816685, val:  82.50%, val_best:  82.50%, tr:  94.79%, tr_best:  98.57%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5746%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.4625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 152391  16.560%\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  0.513451/  2.019827, val:  63.33%, val_best:  82.50%, tr:  93.05%, tr_best:  98.57%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2779%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5246%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 153859  16.543%\n",
      "lif layer 2 self.abs_max_v: 2363.0\n",
      "lif layer 2 self.abs_max_v: 2458.5\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  0.534471/  2.000672, val:  70.42%, val_best:  82.50%, tr:  93.77%, tr_best:  98.57%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5651%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5279%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 155299  16.524%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  0.516521/  1.598671, val:  77.08%, val_best:  82.50%, tr:  91.83%, tr_best:  98.57%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4744%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5265%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 156785  16.510%\n",
      "fc layer 1 self.abs_max_out: 3607.0\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  0.504918/  1.694092, val:  70.42%, val_best:  82.50%, tr:  93.67%, tr_best:  98.57%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3678%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5504%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 158201  16.489%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  0.484733/  2.058579, val:  60.83%, val_best:  82.50%, tr:  94.99%, tr_best:  98.57%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4853%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6052%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 159552  16.462%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  0.466968/  2.077850, val:  65.00%, val_best:  82.50%, tr:  95.20%, tr_best:  98.57%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7473%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5831%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 160835  16.428%\n",
      "fc layer 1 self.abs_max_out: 3653.0\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  0.524444/  2.068917, val:  65.42%, val_best:  82.50%, tr:  92.34%, tr_best:  98.57%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2163%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5795%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 162303  16.414%\n",
      "fc layer 1 self.abs_max_out: 3682.0\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  0.522889/  1.873943, val:  67.08%, val_best:  82.50%, tr:  94.48%, tr_best:  98.57%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1959%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6150%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 163719  16.395%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  0.504184/  1.559029, val:  77.50%, val_best:  82.50%, tr:  95.40%, tr_best:  98.57%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5688%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6523%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 165082  16.371%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  0.488055/  1.685734, val:  77.50%, val_best:  82.50%, tr:  94.59%, tr_best:  98.57%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5232%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6286%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 166403  16.344%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  0.497221/  1.746536, val:  74.58%, val_best:  82.50%, tr:  94.99%, tr_best:  98.57%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4659%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6247%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 167785  16.322%\n",
      "lif layer 1 self.abs_max_v: 6364.5\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  0.502224/  1.702496, val:  71.25%, val_best:  82.50%, tr:  94.28%, tr_best:  98.57%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0950%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6696%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 169202  16.305%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  0.480328/  1.720055, val:  67.08%, val_best:  82.50%, tr:  94.89%, tr_best:  98.57%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3329%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 170554  16.282%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  0.515527/  1.596186, val:  67.50%, val_best:  82.50%, tr:  92.75%, tr_best:  98.57%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4969%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6951%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 172027  16.270%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  0.496890/  1.986603, val:  64.17%, val_best:  82.50%, tr:  94.69%, tr_best:  98.57%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5464%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6691%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 173423  16.252%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  0.505367/  1.611918, val:  76.25%, val_best:  82.50%, tr:  94.79%, tr_best:  98.57%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6183%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6034%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 174846  16.236%\n",
      "fc layer 1 self.abs_max_out: 3860.0\n",
      "lif layer 2 self.abs_max_v: 2469.0\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  0.463943/  1.802713, val:  64.58%, val_best:  82.50%, tr:  95.40%, tr_best:  98.57%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7585%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6521%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 176200  16.214%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  0.486301/  1.792902, val:  70.83%, val_best:  82.50%, tr:  95.81%, tr_best:  98.57%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4295%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6575%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 177484  16.187%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  0.446860/  1.703993, val:  74.17%, val_best:  82.50%, tr:  95.61%, tr_best:  98.57%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5456%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6630%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 178711  16.154%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  0.454749/  1.711222, val:  77.50%, val_best:  82.50%, tr:  95.71%, tr_best:  98.57%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2074%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6200%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 180003  16.128%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  0.518581/  1.742211, val:  77.08%, val_best:  82.50%, tr:  92.65%, tr_best:  98.57%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3449%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6957%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 181399  16.112%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  0.534994/  1.559286, val:  73.75%, val_best:  82.50%, tr:  92.44%, tr_best:  98.57%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2116%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.7107%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 182887  16.104%\n",
      "fc layer 1 self.abs_max_out: 3896.0\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  0.496807/  1.757243, val:  72.92%, val_best:  82.50%, tr:  93.46%, tr_best:  98.57%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9350%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.7034%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 184280  16.088%\n",
      "lif layer 2 self.abs_max_v: 2564.0\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  0.533658/  1.485648, val:  75.00%, val_best:  82.50%, tr:  91.93%, tr_best:  98.57%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9144%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.7348%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 185774  16.081%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  0.555370/  1.602771, val:  67.92%, val_best:  82.50%, tr:  92.75%, tr_best:  98.57%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1123%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.7320%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 187222  16.070%\n",
      "fc layer 1 self.abs_max_out: 3932.0\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  0.515146/  1.722006, val:  70.00%, val_best:  82.50%, tr:  91.52%, tr_best:  98.57%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5101%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.7227%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 188630  16.056%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  0.487721/  1.836170, val:  74.17%, val_best:  82.50%, tr:  93.67%, tr_best:  98.57%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5881%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6610%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 189951  16.035%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  0.459110/  1.762975, val:  72.08%, val_best:  82.50%, tr:  95.71%, tr_best:  98.57%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3187%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6368%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 191235  16.011%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  0.431313/  1.637988, val:  80.00%, val_best:  82.50%, tr:  96.73%, tr_best:  98.57%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6176%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6215%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 192398  15.978%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  0.465144/  1.858362, val:  63.75%, val_best:  82.50%, tr:  95.81%, tr_best:  98.57%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8386%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6409%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 193650  15.952%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  0.446458/  1.559617, val:  77.50%, val_best:  82.50%, tr:  94.89%, tr_best:  98.57%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3312%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6671%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 194930  15.929%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  0.492589/  2.013460, val:  53.33%, val_best:  82.50%, tr:  92.44%, tr_best:  98.57%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7656%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.7068%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 196394  15.921%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  0.511787/  1.651277, val:  69.17%, val_best:  82.50%, tr:  92.85%, tr_best:  98.57%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9300%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.7180%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 197804  15.909%\n",
      "lif layer 1 self.abs_max_v: 6408.0\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  0.481723/  1.755440, val:  64.17%, val_best:  82.50%, tr:  94.48%, tr_best:  98.57%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9871%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6849%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 199132  15.891%\n",
      "fc layer 1 self.abs_max_out: 3981.0\n",
      "fc layer 2 self.abs_max_out: 1565.0\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  0.506851/  1.480189, val:  79.58%, val_best:  82.50%, tr:  92.95%, tr_best:  98.57%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3799%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6908%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 200511  15.877%\n",
      "fc layer 1 self.abs_max_out: 3984.0\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  0.447047/  1.514738, val:  81.25%, val_best:  82.50%, tr:  94.18%, tr_best:  98.57%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6091%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 201786  15.855%\n",
      "fc layer 2 self.abs_max_out: 1577.0\n",
      "fc layer 2 self.abs_max_out: 1637.0\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  0.459714/  2.032311, val:  72.08%, val_best:  82.50%, tr:  92.75%, tr_best:  98.57%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1331%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6037%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 203136  15.839%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  0.465214/  1.777705, val:  62.08%, val_best:  82.50%, tr:  93.67%, tr_best:  98.57%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5332%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6359%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 204501  15.825%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  0.460495/  1.796605, val:  66.67%, val_best:  82.50%, tr:  94.28%, tr_best:  98.57%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5126%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5464%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 205797  15.805%\n",
      "fc layer 2 self.abs_max_out: 1649.0\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  0.430342/  1.616394, val:  78.75%, val_best:  82.50%, tr:  93.56%, tr_best:  98.57%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2694%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5924%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 207088  15.786%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  0.485770/  1.574463, val:  74.58%, val_best:  82.50%, tr:  92.44%, tr_best:  98.57%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9373%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6482%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 208437  15.771%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  0.474228/  1.720519, val:  76.67%, val_best:  82.50%, tr:  94.59%, tr_best:  98.57%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8537%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6310%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 209746  15.753%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  0.416113/  1.520599, val:  77.50%, val_best:  82.50%, tr:  96.53%, tr_best:  98.57%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8916%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6399%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 210847  15.720%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  0.444978/  1.571759, val:  76.25%, val_best:  82.50%, tr:  95.20%, tr_best:  98.57%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0532%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6701%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 212073  15.697%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  0.412911/  1.723690, val:  76.67%, val_best:  82.50%, tr:  96.63%, tr_best:  98.57%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4325%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6758%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 213191  15.666%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  0.485869/  1.589560, val:  67.50%, val_best:  82.50%, tr:  95.51%, tr_best:  98.57%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1575%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.7064%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 214470  15.648%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  0.463642/  1.502834, val:  76.67%, val_best:  82.50%, tr:  93.67%, tr_best:  98.57%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7925%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6883%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 215816  15.634%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  0.466617/  1.835248, val:  64.58%, val_best:  82.50%, tr:  93.16%, tr_best:  98.57%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9507%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6942%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 217156  15.621%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  0.474418/  1.594524, val:  74.17%, val_best:  82.50%, tr:  95.40%, tr_best:  98.57%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4376%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6912%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 218440  15.603%\n",
      "lif layer 2 self.abs_max_v: 2573.5\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  0.453001/  1.714863, val:  77.08%, val_best:  82.50%, tr:  92.54%, tr_best:  98.57%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9724%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6696%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 219824  15.593%\n",
      "lif layer 2 self.abs_max_v: 2626.0\n",
      "fc layer 1 self.abs_max_out: 4156.0\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  0.469118/  1.724679, val:  76.67%, val_best:  82.50%, tr:  92.65%, tr_best:  98.57%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7850%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6237%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 221164  15.580%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  0.461360/  1.978215, val:  73.33%, val_best:  82.50%, tr:  92.75%, tr_best:  98.57%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0128%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6620%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 222484  15.566%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  0.485535/  1.550373, val:  72.92%, val_best:  82.50%, tr:  91.93%, tr_best:  98.57%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2062%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6808%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 223873  15.556%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  0.451700/  1.572626, val:  72.08%, val_best:  82.50%, tr:  94.59%, tr_best:  98.57%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0436%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6653%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 225123  15.537%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  0.491902/  1.524157, val:  75.42%, val_best:  82.50%, tr:  95.30%, tr_best:  98.57%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5430%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.7114%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 226354  15.517%\n",
      "lif layer 2 self.abs_max_v: 2687.0\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  0.459589/  1.508872, val:  77.50%, val_best:  82.50%, tr:  94.99%, tr_best:  98.57%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9678%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6879%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 227561  15.496%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  0.489494/  1.739803, val:  77.08%, val_best:  82.50%, tr:  92.13%, tr_best:  98.57%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2193%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6947%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 228855  15.481%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  0.443623/  1.675439, val:  73.33%, val_best:  82.50%, tr:  94.48%, tr_best:  98.57%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5673%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6774%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 230080  15.462%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  0.454863/  1.769711, val:  75.42%, val_best:  82.50%, tr:  93.77%, tr_best:  98.57%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1392%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6974%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 231326  15.444%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  0.509801/  1.652620, val:  71.67%, val_best:  82.50%, tr:  91.62%, tr_best:  98.57%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0352%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.7130%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 232676  15.433%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  0.480970/  1.560173, val:  77.50%, val_best:  82.50%, tr:  92.24%, tr_best:  98.57%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0865%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6746%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 234064  15.425%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  0.451504/  2.202514, val:  70.42%, val_best:  82.50%, tr:  93.05%, tr_best:  98.57%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0927%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6066%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 235418  15.415%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  0.443790/  1.696752, val:  78.75%, val_best:  82.50%, tr:  92.24%, tr_best:  98.57%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1850%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6402%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 236730  15.402%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  0.459636/  1.462568, val:  79.58%, val_best:  82.50%, tr:  92.65%, tr_best:  98.57%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4217%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.7080%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 238006  15.387%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  0.495585/  1.646703, val:  75.42%, val_best:  82.50%, tr:  91.93%, tr_best:  98.57%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8953%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6887%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 239335  15.375%\n",
      "lif layer 2 self.abs_max_v: 2844.5\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  0.478504/  1.608180, val:  80.00%, val_best:  82.50%, tr:  92.34%, tr_best:  98.57%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1331%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6995%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 240667  15.364%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  0.494873/  1.580378, val:  76.25%, val_best:  82.50%, tr:  91.52%, tr_best:  98.57%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2644%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6909%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 241988  15.353%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  0.448654/  1.477188, val:  78.75%, val_best:  82.50%, tr:  93.05%, tr_best:  98.57%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5486%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6540%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 243242  15.337%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  0.462801/  1.968703, val:  72.08%, val_best:  82.50%, tr:  90.91%, tr_best:  98.57%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2425%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6805%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 244557  15.325%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  0.469991/  1.621117, val:  78.33%, val_best:  82.50%, tr:  91.83%, tr_best:  98.57%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2346%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6539%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 245916  15.317%\n",
      "fc layer 2 self.abs_max_out: 1761.0\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  0.453936/  1.671623, val:  71.67%, val_best:  82.50%, tr:  93.05%, tr_best:  98.57%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9822%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6744%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 247214  15.304%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  0.497398/  1.567348, val:  78.33%, val_best:  82.50%, tr:  92.13%, tr_best:  98.57%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8016%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6873%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 248555  15.294%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  0.467777/  1.460945, val:  79.58%, val_best:  82.50%, tr:  92.65%, tr_best:  98.57%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8765%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6557%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 249857  15.282%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  0.483555/  2.041169, val:  68.33%, val_best:  82.50%, tr:  91.52%, tr_best:  98.57%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0657%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6208%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 251206  15.273%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  0.455433/  1.575609, val:  76.67%, val_best:  82.50%, tr:  92.65%, tr_best:  98.57%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6431%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 252474  15.260%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  0.458159/  1.501976, val:  79.17%, val_best:  82.50%, tr:  93.26%, tr_best:  98.57%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8393%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.7073%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 253696  15.243%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  0.463375/  1.628124, val:  71.25%, val_best:  82.50%, tr:  91.32%, tr_best:  98.57%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1141%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9682%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.7210%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 255001  15.232%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  0.450934/  1.604616, val:  80.83%, val_best:  82.50%, tr:  92.85%, tr_best:  98.57%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2549%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.7092%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 256272  15.219%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  0.475693/  1.542738, val:  80.42%, val_best:  82.50%, tr:  91.83%, tr_best:  98.57%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1650%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.7055%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 257558  15.207%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  0.467831/  1.717168, val:  75.00%, val_best:  82.50%, tr:  91.01%, tr_best:  98.57%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1570%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.7007%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 258856  15.196%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  0.484126/  1.675437, val:  79.58%, val_best:  82.50%, tr:  92.54%, tr_best:  98.57%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1032%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.7015%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 260112  15.182%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  0.421471/  1.784736, val:  68.75%, val_best:  82.50%, tr:  95.10%, tr_best:  98.57%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4411%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6708%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 261205  15.160%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  0.468686/  1.570333, val:  79.17%, val_best:  82.50%, tr:  93.05%, tr_best:  98.57%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9884%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6919%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 262372  15.141%\n",
      "fc layer 1 self.abs_max_out: 4271.0\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  0.494854/  1.500250, val:  78.33%, val_best:  82.50%, tr:  92.54%, tr_best:  98.57%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0312%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2156%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.7143%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 263635  15.129%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  0.472936/  1.561528, val:  70.00%, val_best:  82.50%, tr:  91.01%, tr_best:  98.57%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0376%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0175%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.7117%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 264943  15.119%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  0.439566/  1.676756, val:  71.67%, val_best:  82.50%, tr:  94.18%, tr_best:  98.57%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0366%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6898%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 266202  15.106%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  0.460816/  1.898849, val:  59.17%, val_best:  82.50%, tr:  93.26%, tr_best:  98.57%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1118%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6897%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 267453  15.093%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  0.451633/  1.657817, val:  74.17%, val_best:  82.50%, tr:  92.24%, tr_best:  98.57%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9440%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6998%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 268726  15.082%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  0.447835/  1.520067, val:  78.33%, val_best:  82.50%, tr:  93.56%, tr_best:  98.57%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7719%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6635%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 269943  15.067%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  0.430433/  1.738180, val:  74.58%, val_best:  82.50%, tr:  90.60%, tr_best:  98.57%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0426%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3162%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6759%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 271227  15.057%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  0.435696/  1.739267, val:  80.42%, val_best:  82.50%, tr:  91.01%, tr_best:  98.57%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2652%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6806%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 272517  15.047%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  0.452253/  1.804567, val:  81.25%, val_best:  82.50%, tr:  92.03%, tr_best:  98.57%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0735%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6391%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 273803  15.036%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  0.446074/  1.689114, val:  78.33%, val_best:  82.50%, tr:  93.05%, tr_best:  98.57%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2087%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6681%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 275040  15.024%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  0.447535/  1.987442, val:  61.67%, val_best:  82.50%, tr:  93.16%, tr_best:  98.57%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0088%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6657%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 276288  15.011%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  0.420495/  1.793103, val:  78.33%, val_best:  82.50%, tr:  92.34%, tr_best:  98.57%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1253%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6712%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 277536  14.999%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  0.482296/  1.605390, val:  78.75%, val_best:  82.50%, tr:  92.13%, tr_best:  98.57%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2130%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6919%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 278823  14.990%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  0.457187/  1.873840, val:  76.67%, val_best:  82.50%, tr:  91.22%, tr_best:  98.57%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0146%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6723%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 280097  14.979%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  0.455379/  1.597511, val:  77.92%, val_best:  82.50%, tr:  91.22%, tr_best:  98.57%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0571%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6652%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 281369  14.969%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  0.401779/  1.664780, val:  79.17%, val_best:  82.50%, tr:  95.30%, tr_best:  98.57%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7785%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6519%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 282498  14.951%\n",
      "lif layer 1 self.abs_max_v: 6428.5\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  0.477482/  1.682557, val:  69.17%, val_best:  82.50%, tr:  91.32%, tr_best:  98.57%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9713%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6969%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 283785  14.942%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  0.450720/  1.948220, val:  64.17%, val_best:  82.50%, tr:  91.52%, tr_best:  98.57%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9061%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6692%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 285041  14.931%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  0.429337/  1.696867, val:  78.33%, val_best:  82.50%, tr:  91.62%, tr_best:  98.57%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8306%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6769%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 286291  14.920%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  0.478856/  1.731063, val:  77.92%, val_best:  82.50%, tr:  90.91%, tr_best:  98.57%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6286%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.7427%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 287575  14.911%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  0.478851/  1.716568, val:  71.67%, val_best:  82.50%, tr:  91.62%, tr_best:  98.57%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.7224%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 288881  14.903%\n",
      "lif layer 1 self.abs_max_v: 6500.0\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  0.440759/  1.460780, val:  77.50%, val_best:  82.50%, tr:  91.32%, tr_best:  98.57%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8363%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.7232%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 290088  14.890%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  0.448119/  1.546503, val:  79.17%, val_best:  82.50%, tr:  93.05%, tr_best:  98.57%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8189%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6723%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e5e7c0c38c4a9b978a2516a03a789f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñá‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñá‚ñà‚ñá‚ñÖ‚ñÜ‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÖ‚ñÉ‚ñá‚ñÖ‚ñÜ‚ñÉ‚ñá‚ñÑ‚ñà‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÜ‚ñÇ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÉ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñá‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñá‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñá‚ñà‚ñá‚ñÖ‚ñÜ‚ñà</td></tr><tr><td>val_loss</td><td>‚ñá‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>0.0</td></tr><tr><td>tr_acc</td><td>0.93054</td></tr><tr><td>tr_epoch_loss</td><td>0.44812</td></tr><tr><td>val_acc_best</td><td>0.825</td></tr><tr><td>val_acc_now</td><td>0.79167</td></tr><tr><td>val_loss</td><td>1.5465</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stoic-sweep-15</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6h2rplmw' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6h2rplmw</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_125806-6h2rplmw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: v8tu4sgt with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.046875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0234375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_171315-v8tu4sgt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/v8tu4sgt' target=\"_blank\">different-sweep-24</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/v8tu4sgt' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/v8tu4sgt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251214_171324_176', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 32, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 32, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 0.5, 'lif_layer_v_threshold2': 32, 'init_scaling': [0.03125, 0.046875, 0.0234375], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 32, self.v_threshold 32\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 0.5, self.v_threshold 32\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=32, v_reset=10000, sg_width=32, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=32, v_reset=10000, sg_width=0.5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 46.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 6.0\n",
      "lif layer 2 self.abs_max_v: 6.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 59.5\n",
      "fc layer 2 self.abs_max_out: 36.0\n",
      "lif layer 2 self.abs_max_v: 35.5\n",
      "fc layer 3 self.abs_max_out: 2.0\n",
      "fc layer 1 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 69.0\n",
      "fc layer 2 self.abs_max_out: 48.0\n",
      "lif layer 2 self.abs_max_v: 53.0\n",
      "fc layer 3 self.abs_max_out: 10.0\n",
      "fc layer 1 self.abs_max_out: 62.0\n",
      "lif layer 1 self.abs_max_v: 90.0\n",
      "lif layer 2 self.abs_max_v: 59.5\n",
      "fc layer 1 self.abs_max_out: 101.0\n",
      "lif layer 1 self.abs_max_v: 109.0\n",
      "fc layer 2 self.abs_max_out: 50.0\n",
      "lif layer 2 self.abs_max_v: 66.5\n",
      "fc layer 3 self.abs_max_out: 16.0\n",
      "fc layer 1 self.abs_max_out: 147.0\n",
      "lif layer 1 self.abs_max_v: 147.0\n",
      "fc layer 2 self.abs_max_out: 65.0\n",
      "lif layer 2 self.abs_max_v: 97.0\n",
      "fc layer 1 self.abs_max_out: 200.0\n",
      "lif layer 1 self.abs_max_v: 200.0\n",
      "lif layer 2 self.abs_max_v: 113.5\n",
      "fc layer 3 self.abs_max_out: 18.0\n",
      "fc layer 2 self.abs_max_out: 70.0\n",
      "fc layer 2 self.abs_max_out: 75.0\n",
      "fc layer 3 self.abs_max_out: 28.0\n",
      "fc layer 2 self.abs_max_out: 89.0\n",
      "fc layer 1 self.abs_max_out: 205.0\n",
      "lif layer 1 self.abs_max_v: 205.0\n",
      "fc layer 3 self.abs_max_out: 38.0\n",
      "fc layer 1 self.abs_max_out: 271.0\n",
      "lif layer 1 self.abs_max_v: 271.0\n",
      "lif layer 2 self.abs_max_v: 121.5\n",
      "fc layer 2 self.abs_max_out: 117.0\n",
      "lif layer 2 self.abs_max_v: 124.0\n",
      "lif layer 2 self.abs_max_v: 137.0\n",
      "lif layer 1 self.abs_max_v: 281.5\n",
      "lif layer 2 self.abs_max_v: 143.5\n",
      "lif layer 2 self.abs_max_v: 159.0\n",
      "lif layer 1 self.abs_max_v: 300.5\n",
      "lif layer 2 self.abs_max_v: 159.5\n",
      "fc layer 3 self.abs_max_out: 42.0\n",
      "fc layer 2 self.abs_max_out: 149.0\n",
      "fc layer 2 self.abs_max_out: 154.0\n",
      "lif layer 2 self.abs_max_v: 184.5\n",
      "lif layer 2 self.abs_max_v: 197.5\n",
      "fc layer 3 self.abs_max_out: 44.0\n",
      "lif layer 2 self.abs_max_v: 200.5\n",
      "fc layer 3 self.abs_max_out: 49.0\n",
      "lif layer 1 self.abs_max_v: 306.5\n",
      "fc layer 2 self.abs_max_out: 159.0\n",
      "lif layer 2 self.abs_max_v: 211.5\n",
      "fc layer 3 self.abs_max_out: 53.0\n",
      "lif layer 1 self.abs_max_v: 372.5\n",
      "fc layer 2 self.abs_max_out: 202.0\n",
      "lif layer 2 self.abs_max_v: 251.0\n",
      "fc layer 1 self.abs_max_out: 274.0\n",
      "lif layer 1 self.abs_max_v: 378.5\n",
      "lif layer 2 self.abs_max_v: 292.5\n",
      "fc layer 1 self.abs_max_out: 343.0\n",
      "lif layer 1 self.abs_max_v: 416.5\n",
      "lif layer 2 self.abs_max_v: 311.5\n",
      "fc layer 3 self.abs_max_out: 65.0\n",
      "fc layer 2 self.abs_max_out: 210.0\n",
      "lif layer 2 self.abs_max_v: 331.0\n",
      "fc layer 3 self.abs_max_out: 67.0\n",
      "fc layer 3 self.abs_max_out: 78.0\n",
      "fc layer 2 self.abs_max_out: 221.0\n",
      "fc layer 3 self.abs_max_out: 89.0\n",
      "fc layer 1 self.abs_max_out: 401.0\n",
      "fc layer 3 self.abs_max_out: 94.0\n",
      "fc layer 2 self.abs_max_out: 222.0\n",
      "lif layer 1 self.abs_max_v: 499.5\n",
      "fc layer 2 self.abs_max_out: 225.0\n",
      "lif layer 2 self.abs_max_v: 331.5\n",
      "fc layer 1 self.abs_max_out: 405.0\n",
      "fc layer 2 self.abs_max_out: 252.0\n",
      "lif layer 2 self.abs_max_v: 343.0\n",
      "lif layer 2 self.abs_max_v: 368.5\n",
      "lif layer 2 self.abs_max_v: 379.5\n",
      "fc layer 3 self.abs_max_out: 107.0\n",
      "lif layer 1 self.abs_max_v: 527.0\n",
      "lif layer 2 self.abs_max_v: 381.0\n",
      "lif layer 2 self.abs_max_v: 384.5\n",
      "fc layer 3 self.abs_max_out: 108.0\n",
      "fc layer 3 self.abs_max_out: 109.0\n",
      "lif layer 1 self.abs_max_v: 558.0\n",
      "lif layer 1 self.abs_max_v: 611.0\n",
      "lif layer 1 self.abs_max_v: 695.5\n",
      "lif layer 1 self.abs_max_v: 705.0\n",
      "fc layer 2 self.abs_max_out: 329.0\n",
      "fc layer 2 self.abs_max_out: 336.0\n",
      "fc layer 3 self.abs_max_out: 125.0\n",
      "lif layer 2 self.abs_max_v: 406.5\n",
      "fc layer 3 self.abs_max_out: 127.0\n",
      "fc layer 3 self.abs_max_out: 130.0\n",
      "fc layer 3 self.abs_max_out: 147.0\n",
      "fc layer 3 self.abs_max_out: 172.0\n",
      "lif layer 2 self.abs_max_v: 420.0\n",
      "lif layer 2 self.abs_max_v: 422.0\n",
      "lif layer 2 self.abs_max_v: 430.0\n",
      "lif layer 2 self.abs_max_v: 457.0\n",
      "lif layer 2 self.abs_max_v: 473.0\n",
      "lif layer 2 self.abs_max_v: 512.0\n",
      "lif layer 2 self.abs_max_v: 521.5\n",
      "lif layer 2 self.abs_max_v: 524.0\n",
      "fc layer 3 self.abs_max_out: 185.0\n",
      "fc layer 1 self.abs_max_out: 414.0\n",
      "fc layer 1 self.abs_max_out: 420.0\n",
      "lif layer 1 self.abs_max_v: 719.5\n",
      "lif layer 2 self.abs_max_v: 529.0\n",
      "lif layer 2 self.abs_max_v: 554.0\n",
      "lif layer 2 self.abs_max_v: 563.5\n",
      "lif layer 2 self.abs_max_v: 568.5\n",
      "lif layer 1 self.abs_max_v: 764.5\n",
      "lif layer 2 self.abs_max_v: 573.5\n",
      "lif layer 2 self.abs_max_v: 576.5\n",
      "fc layer 3 self.abs_max_out: 198.0\n",
      "fc layer 1 self.abs_max_out: 504.0\n",
      "fc layer 3 self.abs_max_out: 205.0\n",
      "fc layer 3 self.abs_max_out: 240.0\n",
      "fc layer 3 self.abs_max_out: 242.0\n",
      "fc layer 3 self.abs_max_out: 248.0\n",
      "fc layer 1 self.abs_max_out: 522.0\n",
      "fc layer 2 self.abs_max_out: 347.0\n",
      "fc layer 2 self.abs_max_out: 359.0\n",
      "fc layer 2 self.abs_max_out: 366.0\n",
      "lif layer 2 self.abs_max_v: 598.0\n",
      "lif layer 2 self.abs_max_v: 622.0\n",
      "lif layer 2 self.abs_max_v: 640.0\n",
      "lif layer 2 self.abs_max_v: 652.5\n",
      "lif layer 2 self.abs_max_v: 678.5\n",
      "fc layer 2 self.abs_max_out: 370.0\n",
      "lif layer 2 self.abs_max_v: 698.5\n",
      "lif layer 2 self.abs_max_v: 704.5\n",
      "fc layer 3 self.abs_max_out: 249.0\n",
      "fc layer 3 self.abs_max_out: 254.0\n",
      "fc layer 3 self.abs_max_out: 280.0\n",
      "fc layer 3 self.abs_max_out: 292.0\n",
      "fc layer 3 self.abs_max_out: 295.0\n",
      "fc layer 2 self.abs_max_out: 371.0\n",
      "fc layer 2 self.abs_max_out: 407.0\n",
      "lif layer 2 self.abs_max_v: 725.0\n",
      "fc layer 2 self.abs_max_out: 417.0\n",
      "fc layer 2 self.abs_max_out: 438.0\n",
      "lif layer 1 self.abs_max_v: 823.5\n",
      "fc layer 2 self.abs_max_out: 453.0\n",
      "lif layer 2 self.abs_max_v: 747.0\n",
      "lif layer 1 self.abs_max_v: 913.0\n",
      "fc layer 1 self.abs_max_out: 563.0\n",
      "fc layer 2 self.abs_max_out: 460.0\n",
      "fc layer 2 self.abs_max_out: 468.0\n",
      "lif layer 2 self.abs_max_v: 770.0\n",
      "fc layer 1 self.abs_max_out: 601.0\n",
      "lif layer 1 self.abs_max_v: 973.5\n",
      "lif layer 1 self.abs_max_v: 983.0\n",
      "lif layer 2 self.abs_max_v: 806.5\n",
      "fc layer 2 self.abs_max_out: 490.0\n",
      "fc layer 3 self.abs_max_out: 313.0\n",
      "fc layer 3 self.abs_max_out: 322.0\n",
      "fc layer 3 self.abs_max_out: 324.0\n",
      "fc layer 2 self.abs_max_out: 493.0\n",
      "fc layer 2 self.abs_max_out: 495.0\n",
      "fc layer 2 self.abs_max_out: 498.0\n",
      "fc layer 2 self.abs_max_out: 503.0\n",
      "fc layer 3 self.abs_max_out: 333.0\n",
      "fc layer 1 self.abs_max_out: 681.0\n",
      "lif layer 1 self.abs_max_v: 1109.5\n",
      "lif layer 1 self.abs_max_v: 1128.0\n",
      "lif layer 2 self.abs_max_v: 830.5\n",
      "fc layer 2 self.abs_max_out: 524.0\n",
      "fc layer 3 self.abs_max_out: 370.0\n",
      "fc layer 2 self.abs_max_out: 538.0\n",
      "fc layer 2 self.abs_max_out: 539.0\n",
      "fc layer 2 self.abs_max_out: 540.0\n",
      "fc layer 2 self.abs_max_out: 570.0\n",
      "fc layer 1 self.abs_max_out: 688.0\n",
      "fc layer 2 self.abs_max_out: 588.0\n",
      "fc layer 2 self.abs_max_out: 589.0\n",
      "fc layer 2 self.abs_max_out: 607.0\n",
      "lif layer 2 self.abs_max_v: 854.0\n",
      "lif layer 2 self.abs_max_v: 901.0\n",
      "lif layer 2 self.abs_max_v: 902.0\n",
      "fc layer 2 self.abs_max_out: 608.0\n",
      "fc layer 2 self.abs_max_out: 640.0\n",
      "fc layer 2 self.abs_max_out: 650.0\n",
      "fc layer 2 self.abs_max_out: 682.0\n",
      "fc layer 2 self.abs_max_out: 687.0\n",
      "fc layer 2 self.abs_max_out: 714.0\n",
      "lif layer 2 self.abs_max_v: 904.0\n",
      "fc layer 1 self.abs_max_out: 689.0\n",
      "lif layer 1 self.abs_max_v: 1134.5\n",
      "lif layer 1 self.abs_max_v: 1144.0\n",
      "fc layer 2 self.abs_max_out: 716.0\n",
      "lif layer 1 self.abs_max_v: 1160.5\n",
      "lif layer 1 self.abs_max_v: 1165.5\n",
      "lif layer 1 self.abs_max_v: 1200.0\n",
      "lif layer 1 self.abs_max_v: 1203.0\n",
      "fc layer 2 self.abs_max_out: 728.0\n",
      "fc layer 2 self.abs_max_out: 739.0\n",
      "lif layer 2 self.abs_max_v: 983.0\n",
      "fc layer 2 self.abs_max_out: 740.0\n",
      "fc layer 2 self.abs_max_out: 815.0\n",
      "fc layer 1 self.abs_max_out: 778.0\n",
      "fc layer 1 self.abs_max_out: 795.0\n",
      "fc layer 1 self.abs_max_out: 855.0\n",
      "lif layer 2 self.abs_max_v: 1004.0\n",
      "lif layer 2 self.abs_max_v: 1050.0\n",
      "lif layer 2 self.abs_max_v: 1059.0\n",
      "lif layer 2 self.abs_max_v: 1076.5\n",
      "lif layer 2 self.abs_max_v: 1093.5\n",
      "lif layer 2 self.abs_max_v: 1098.0\n",
      "fc layer 2 self.abs_max_out: 819.0\n",
      "lif layer 2 self.abs_max_v: 1129.5\n",
      "lif layer 1 self.abs_max_v: 1257.0\n",
      "lif layer 1 self.abs_max_v: 1273.5\n",
      "lif layer 1 self.abs_max_v: 1396.0\n",
      "lif layer 1 self.abs_max_v: 1458.0\n",
      "lif layer 1 self.abs_max_v: 1467.0\n",
      "lif layer 1 self.abs_max_v: 1471.5\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss:  9.828664/ 75.036896, val:  33.33%, val_best:  33.33%, tr:  98.47%, tr_best:  98.47%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6460%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.4421%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 1458  14.893%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 848.0\n",
      "fc layer 2 self.abs_max_out: 864.0\n",
      "lif layer 2 self.abs_max_v: 1220.0\n",
      "lif layer 2 self.abs_max_v: 1228.5\n",
      "fc layer 2 self.abs_max_out: 873.0\n",
      "fc layer 2 self.abs_max_out: 903.0\n",
      "fc layer 2 self.abs_max_out: 904.0\n",
      "fc layer 2 self.abs_max_out: 922.0\n",
      "fc layer 2 self.abs_max_out: 937.0\n",
      "fc layer 2 self.abs_max_out: 951.0\n",
      "fc layer 2 self.abs_max_out: 963.0\n",
      "fc layer 2 self.abs_max_out: 972.0\n",
      "fc layer 2 self.abs_max_out: 978.0\n",
      "lif layer 2 self.abs_max_v: 1256.0\n",
      "lif layer 2 self.abs_max_v: 1262.0\n",
      "lif layer 2 self.abs_max_v: 1358.0\n",
      "lif layer 2 self.abs_max_v: 1409.5\n",
      "lif layer 2 self.abs_max_v: 1455.0\n",
      "fc layer 2 self.abs_max_out: 984.0\n",
      "fc layer 2 self.abs_max_out: 997.0\n",
      "fc layer 2 self.abs_max_out: 1010.0\n",
      "fc layer 2 self.abs_max_out: 1021.0\n",
      "fc layer 2 self.abs_max_out: 1029.0\n",
      "fc layer 2 self.abs_max_out: 1051.0\n",
      "fc layer 2 self.abs_max_out: 1071.0\n",
      "fc layer 2 self.abs_max_out: 1114.0\n",
      "fc layer 2 self.abs_max_out: 1117.0\n",
      "fc layer 2 self.abs_max_out: 1123.0\n",
      "fc layer 2 self.abs_max_out: 1144.0\n",
      "fc layer 2 self.abs_max_out: 1206.0\n",
      "fc layer 3 self.abs_max_out: 379.0\n",
      "fc layer 3 self.abs_max_out: 382.0\n",
      "fc layer 3 self.abs_max_out: 400.0\n",
      "lif layer 2 self.abs_max_v: 1462.0\n",
      "lif layer 2 self.abs_max_v: 1511.5\n",
      "lif layer 2 self.abs_max_v: 1531.5\n",
      "lif layer 2 self.abs_max_v: 1532.5\n",
      "lif layer 2 self.abs_max_v: 1600.5\n",
      "lif layer 2 self.abs_max_v: 1684.5\n",
      "lif layer 2 self.abs_max_v: 1710.5\n",
      "lif layer 2 self.abs_max_v: 1773.5\n",
      "fc layer 1 self.abs_max_out: 866.0\n",
      "lif layer 1 self.abs_max_v: 1500.5\n",
      "lif layer 1 self.abs_max_v: 1544.5\n",
      "fc layer 3 self.abs_max_out: 452.0\n",
      "lif layer 2 self.abs_max_v: 1774.5\n",
      "lif layer 1 self.abs_max_v: 1545.5\n",
      "fc layer 1 self.abs_max_out: 873.0\n",
      "fc layer 1 self.abs_max_out: 879.0\n",
      "fc layer 1 self.abs_max_out: 880.0\n",
      "lif layer 1 self.abs_max_v: 1630.5\n",
      "lif layer 1 self.abs_max_v: 1678.5\n",
      "lif layer 1 self.abs_max_v: 1689.5\n",
      "lif layer 1 self.abs_max_v: 1705.0\n",
      "fc layer 2 self.abs_max_out: 1209.0\n",
      "lif layer 2 self.abs_max_v: 1890.0\n",
      "fc layer 2 self.abs_max_out: 1234.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 10.118023/ 93.332741, val:  37.50%, val_best:  37.50%, tr:  99.39%, tr_best:  99.39%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7469%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.9097%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 2776  14.178%\n",
      "fc layer 2 self.abs_max_out: 1244.0\n",
      "fc layer 1 self.abs_max_out: 932.0\n",
      "fc layer 2 self.abs_max_out: 1265.0\n",
      "fc layer 2 self.abs_max_out: 1309.0\n",
      "fc layer 2 self.abs_max_out: 1313.0\n",
      "lif layer 2 self.abs_max_v: 1975.5\n",
      "fc layer 2 self.abs_max_out: 1350.0\n",
      "lif layer 2 self.abs_max_v: 2022.5\n",
      "fc layer 1 self.abs_max_out: 1015.0\n",
      "lif layer 1 self.abs_max_v: 1799.0\n",
      "fc layer 2 self.abs_max_out: 1363.0\n",
      "fc layer 2 self.abs_max_out: 1428.0\n",
      "lif layer 2 self.abs_max_v: 2174.0\n",
      "lif layer 2 self.abs_max_v: 2249.0\n",
      "fc layer 2 self.abs_max_out: 1450.0\n",
      "lif layer 2 self.abs_max_v: 2274.0\n",
      "fc layer 2 self.abs_max_out: 1475.0\n",
      "fc layer 1 self.abs_max_out: 1017.0\n",
      "fc layer 1 self.abs_max_out: 1104.0\n",
      "lif layer 1 self.abs_max_v: 1915.5\n",
      "lif layer 1 self.abs_max_v: 1974.5\n",
      "lif layer 1 self.abs_max_v: 2005.0\n",
      "lif layer 1 self.abs_max_v: 2105.5\n",
      "lif layer 1 self.abs_max_v: 2107.0\n",
      "fc layer 2 self.abs_max_out: 1483.0\n",
      "lif layer 2 self.abs_max_v: 2291.0\n",
      "fc layer 2 self.abs_max_out: 1509.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 10.795777/ 69.621468, val:  31.25%, val_best:  37.50%, tr:  98.57%, tr_best:  99.39%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.4996%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.2055%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 4115  14.011%\n",
      "fc layer 2 self.abs_max_out: 1540.0\n",
      "fc layer 3 self.abs_max_out: 471.0\n",
      "lif layer 2 self.abs_max_v: 2364.0\n",
      "lif layer 2 self.abs_max_v: 2464.5\n",
      "fc layer 2 self.abs_max_out: 1572.0\n",
      "lif layer 2 self.abs_max_v: 2480.0\n",
      "lif layer 2 self.abs_max_v: 2730.0\n",
      "fc layer 2 self.abs_max_out: 1581.0\n",
      "fc layer 2 self.abs_max_out: 1608.0\n",
      "fc layer 2 self.abs_max_out: 1657.0\n",
      "fc layer 2 self.abs_max_out: 1661.0\n",
      "lif layer 2 self.abs_max_v: 3001.5\n",
      "fc layer 2 self.abs_max_out: 1724.0\n",
      "fc layer 2 self.abs_max_out: 1729.0\n",
      "fc layer 2 self.abs_max_out: 1745.0\n",
      "fc layer 2 self.abs_max_out: 1794.0\n",
      "fc layer 2 self.abs_max_out: 1803.0\n",
      "fc layer 2 self.abs_max_out: 1859.0\n",
      "fc layer 2 self.abs_max_out: 1892.0\n",
      "fc layer 1 self.abs_max_out: 1124.0\n",
      "fc layer 1 self.abs_max_out: 1140.0\n",
      "fc layer 1 self.abs_max_out: 1146.0\n",
      "lif layer 1 self.abs_max_v: 2196.0\n",
      "fc layer 1 self.abs_max_out: 1223.0\n",
      "lif layer 1 self.abs_max_v: 2321.0\n",
      "fc layer 1 self.abs_max_out: 1232.0\n",
      "lif layer 1 self.abs_max_v: 2392.5\n",
      "lif layer 2 self.abs_max_v: 3053.0\n",
      "lif layer 2 self.abs_max_v: 3276.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss: 10.987269/ 88.191910, val:  32.50%, val_best:  37.50%, tr:  98.88%, tr_best:  99.39%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4801%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8112%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 5499  14.042%\n",
      "fc layer 2 self.abs_max_out: 1904.0\n",
      "fc layer 2 self.abs_max_out: 1911.0\n",
      "fc layer 2 self.abs_max_out: 1920.0\n",
      "fc layer 2 self.abs_max_out: 1937.0\n",
      "fc layer 2 self.abs_max_out: 1943.0\n",
      "fc layer 2 self.abs_max_out: 2044.0\n",
      "lif layer 2 self.abs_max_v: 3419.0\n",
      "fc layer 2 self.abs_max_out: 2050.0\n",
      "fc layer 2 self.abs_max_out: 2065.0\n",
      "fc layer 2 self.abs_max_out: 2079.0\n",
      "fc layer 2 self.abs_max_out: 2121.0\n",
      "fc layer 2 self.abs_max_out: 2148.0\n",
      "fc layer 2 self.abs_max_out: 2154.0\n",
      "fc layer 2 self.abs_max_out: 2179.0\n",
      "lif layer 2 self.abs_max_v: 3419.5\n",
      "lif layer 2 self.abs_max_v: 3436.0\n",
      "lif layer 2 self.abs_max_v: 3499.0\n",
      "fc layer 2 self.abs_max_out: 2180.0\n",
      "fc layer 2 self.abs_max_out: 2182.0\n",
      "fc layer 2 self.abs_max_out: 2199.0\n",
      "fc layer 2 self.abs_max_out: 2200.0\n",
      "fc layer 2 self.abs_max_out: 2201.0\n",
      "fc layer 2 self.abs_max_out: 2223.0\n",
      "fc layer 2 self.abs_max_out: 2265.0\n",
      "fc layer 2 self.abs_max_out: 2287.0\n",
      "fc layer 2 self.abs_max_out: 2334.0\n",
      "fc layer 2 self.abs_max_out: 2358.0\n",
      "fc layer 1 self.abs_max_out: 1255.0\n",
      "lif layer 1 self.abs_max_v: 2406.5\n",
      "fc layer 1 self.abs_max_out: 1261.0\n",
      "fc layer 2 self.abs_max_out: 2390.0\n",
      "lif layer 2 self.abs_max_v: 3548.0\n",
      "lif layer 2 self.abs_max_v: 3866.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss: 10.606242/ 52.445461, val:  37.92%, val_best:  37.92%, tr:  98.67%, tr_best:  99.39%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3975%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.3668%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 6839  13.971%\n",
      "fc layer 2 self.abs_max_out: 2412.0\n",
      "fc layer 2 self.abs_max_out: 2532.0\n",
      "fc layer 2 self.abs_max_out: 2625.0\n",
      "fc layer 2 self.abs_max_out: 2630.0\n",
      "fc layer 3 self.abs_max_out: 473.0\n",
      "fc layer 3 self.abs_max_out: 497.0\n",
      "fc layer 2 self.abs_max_out: 2645.0\n",
      "fc layer 2 self.abs_max_out: 2700.0\n",
      "fc layer 2 self.abs_max_out: 2701.0\n",
      "fc layer 2 self.abs_max_out: 2739.0\n",
      "fc layer 2 self.abs_max_out: 2740.0\n",
      "lif layer 2 self.abs_max_v: 3908.0\n",
      "fc layer 2 self.abs_max_out: 2769.0\n",
      "fc layer 2 self.abs_max_out: 2785.0\n",
      "fc layer 2 self.abs_max_out: 2848.0\n",
      "lif layer 2 self.abs_max_v: 4113.0\n",
      "lif layer 1 self.abs_max_v: 2433.0\n",
      "fc layer 1 self.abs_max_out: 1273.0\n",
      "lif layer 2 self.abs_max_v: 4260.5\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss: 10.757316/ 64.421516, val:  43.75%, val_best:  43.75%, tr:  98.26%, tr_best:  99.39%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.8530%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.7933%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 8204  13.967%\n",
      "fc layer 2 self.abs_max_out: 2884.0\n",
      "fc layer 2 self.abs_max_out: 2900.0\n",
      "fc layer 2 self.abs_max_out: 2902.0\n",
      "fc layer 2 self.abs_max_out: 2959.0\n",
      "fc layer 2 self.abs_max_out: 2982.0\n",
      "fc layer 2 self.abs_max_out: 3013.0\n",
      "fc layer 2 self.abs_max_out: 3093.0\n",
      "fc layer 2 self.abs_max_out: 3130.0\n",
      "fc layer 1 self.abs_max_out: 1334.0\n",
      "fc layer 2 self.abs_max_out: 3162.0\n",
      "lif layer 2 self.abs_max_v: 4292.5\n",
      "fc layer 1 self.abs_max_out: 1338.0\n",
      "fc layer 2 self.abs_max_out: 3361.0\n",
      "lif layer 2 self.abs_max_v: 4535.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss: 10.165372/ 64.115189, val:  43.33%, val_best:  43.75%, tr:  98.88%, tr_best:  99.39%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3070%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.3595%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 9519  13.890%\n",
      "lif layer 2 self.abs_max_v: 4814.5\n",
      "fc layer 2 self.abs_max_out: 3413.0\n",
      "fc layer 2 self.abs_max_out: 3455.0\n",
      "fc layer 2 self.abs_max_out: 3496.0\n",
      "fc layer 1 self.abs_max_out: 1392.0\n",
      "lif layer 2 self.abs_max_v: 4875.5\n",
      "lif layer 2 self.abs_max_v: 5288.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  9.808082/ 70.148804, val:  41.25%, val_best:  43.75%, tr:  98.88%, tr_best:  99.39%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3964%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5009%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 10801  13.791%\n",
      "fc layer 2 self.abs_max_out: 3511.0\n",
      "fc layer 2 self.abs_max_out: 3545.0\n",
      "lif layer 2 self.abs_max_v: 5805.0\n",
      "fc layer 2 self.abs_max_out: 3631.0\n",
      "fc layer 2 self.abs_max_out: 3639.0\n",
      "fc layer 2 self.abs_max_out: 3662.0\n",
      "fc layer 1 self.abs_max_out: 1439.0\n",
      "lif layer 1 self.abs_max_v: 2462.5\n",
      "fc layer 2 self.abs_max_out: 3668.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  9.702655/ 39.310589, val:  42.08%, val_best:  43.75%, tr:  98.67%, tr_best:  99.39%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.8718%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0824%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 12151  13.791%\n",
      "fc layer 2 self.abs_max_out: 3726.0\n",
      "fc layer 2 self.abs_max_out: 3733.0\n",
      "fc layer 1 self.abs_max_out: 1442.0\n",
      "fc layer 2 self.abs_max_out: 3735.0\n",
      "fc layer 2 self.abs_max_out: 3774.0\n",
      "lif layer 1 self.abs_max_v: 2465.5\n",
      "lif layer 1 self.abs_max_v: 2472.0\n",
      "fc layer 1 self.abs_max_out: 1524.0\n",
      "lif layer 1 self.abs_max_v: 2640.0\n",
      "lif layer 1 self.abs_max_v: 2677.0\n",
      "lif layer 1 self.abs_max_v: 2708.5\n",
      "fc layer 1 self.abs_max_out: 1538.0\n",
      "lif layer 1 self.abs_max_v: 2892.5\n",
      "lif layer 1 self.abs_max_v: 2911.5\n",
      "lif layer 2 self.abs_max_v: 6123.5\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  8.848047/ 86.442627, val:  32.92%, val_best:  43.75%, tr:  98.98%, tr_best:  99.39%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6995%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7451%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 13419  13.707%\n",
      "fc layer 2 self.abs_max_out: 3788.0\n",
      "fc layer 1 self.abs_max_out: 1550.0\n",
      "fc layer 2 self.abs_max_out: 3839.0\n",
      "fc layer 2 self.abs_max_out: 3947.0\n",
      "fc layer 1 self.abs_max_out: 1555.0\n",
      "fc layer 1 self.abs_max_out: 1703.0\n",
      "lif layer 1 self.abs_max_v: 3053.0\n",
      "lif layer 1 self.abs_max_v: 3114.5\n",
      "lif layer 1 self.abs_max_v: 3159.5\n",
      "fc layer 1 self.abs_max_out: 1776.0\n",
      "lif layer 1 self.abs_max_v: 3356.0\n",
      "lif layer 1 self.abs_max_v: 3381.0\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  8.716879/ 67.829269, val:  35.42%, val_best:  43.75%, tr:  98.67%, tr_best:  99.39%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5308%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 14720  13.669%\n",
      "fc layer 1 self.abs_max_out: 1810.0\n",
      "lif layer 1 self.abs_max_v: 3418.0\n",
      "lif layer 1 self.abs_max_v: 3449.0\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  8.106845/ 47.853470, val:  46.67%, val_best:  46.67%, tr:  98.88%, tr_best:  99.39%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1978%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6069%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 16008  13.626%\n",
      "lif layer 2 self.abs_max_v: 6602.5\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  7.347066/ 35.148636, val:  42.92%, val_best:  46.67%, tr:  99.18%, tr_best:  99.39%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6153%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.6559%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 17206  13.519%\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  7.153273/ 64.041840, val:  35.83%, val_best:  46.67%, tr:  98.88%, tr_best:  99.39%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6770%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.9046%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 18420  13.439%\n",
      "lif layer 2 self.abs_max_v: 6663.5\n",
      "lif layer 2 self.abs_max_v: 6779.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  6.730568/ 34.046272, val:  51.25%, val_best:  51.25%, tr:  98.77%, tr_best:  99.39%, epoch time: 74.13 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.8213%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.2886%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 19605  13.350%\n",
      "lif layer 2 self.abs_max_v: 6875.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  6.524112/ 39.406490, val:  37.92%, val_best:  51.25%, tr:  98.16%, tr_best:  99.39%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.7488%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.5142%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 20818  13.290%\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  5.653433/ 26.236515, val:  52.08%, val_best:  52.08%, tr:  98.57%, tr_best:  99.39%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1915%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.3671%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 21940  13.183%\n",
      "lif layer 2 self.abs_max_v: 6970.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  5.722009/ 23.781338, val:  56.25%, val_best:  56.25%, tr:  98.47%, tr_best:  99.39%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4190%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.2245%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 23124  13.122%\n",
      "fc layer 2 self.abs_max_out: 3998.0\n",
      "lif layer 2 self.abs_max_v: 7040.5\n",
      "fc layer 2 self.abs_max_out: 4013.0\n",
      "fc layer 1 self.abs_max_out: 1863.0\n",
      "fc layer 1 self.abs_max_out: 1866.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  5.727414/ 35.283363, val:  49.58%, val_best:  56.25%, tr:  98.37%, tr_best:  99.39%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.8564%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.8489%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 24339  13.085%\n",
      "fc layer 2 self.abs_max_out: 4061.0\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  5.349601/ 36.993462, val:  45.83%, val_best:  56.25%, tr:  98.77%, tr_best:  99.39%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.2698%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.7054%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 25512  13.030%\n",
      "fc layer 1 self.abs_max_out: 1868.0\n",
      "lif layer 2 self.abs_max_v: 7068.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  5.004944/ 45.078716, val:  36.25%, val_best:  56.25%, tr:  98.47%, tr_best:  99.39%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.9053%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.3640%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 26699  12.987%\n",
      "lif layer 2 self.abs_max_v: 7236.5\n",
      "fc layer 2 self.abs_max_out: 4537.0\n",
      "fc layer 1 self.abs_max_out: 2006.0\n",
      "fc layer 1 self.abs_max_out: 2058.0\n",
      "lif layer 1 self.abs_max_v: 3475.0\n",
      "lif layer 1 self.abs_max_v: 3578.5\n",
      "lif layer 1 self.abs_max_v: 3741.5\n",
      "lif layer 1 self.abs_max_v: 3859.0\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  5.050834/ 30.052708, val:  33.33%, val_best:  56.25%, tr:  98.16%, tr_best:  99.39%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3913%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.3966%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 27985  12.993%\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  4.782674/ 28.270798, val:  55.00%, val_best:  56.25%, tr:  97.45%, tr_best:  99.39%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4772%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.2509%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 29285  13.006%\n",
      "lif layer 2 self.abs_max_v: 7290.0\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  4.615485/ 22.406702, val:  57.08%, val_best:  57.08%, tr:  96.32%, tr_best:  99.39%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.7728%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.2528%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 30533  12.995%\n",
      "lif layer 2 self.abs_max_v: 7392.0\n",
      "fc layer 1 self.abs_max_out: 2072.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  4.641124/ 25.500097, val:  43.33%, val_best:  57.08%, tr:  96.94%, tr_best:  99.39%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.7873%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.4811%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 31762  12.977%\n",
      "fc layer 1 self.abs_max_out: 2106.0\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  4.842330/ 28.554068, val:  44.17%, val_best:  57.08%, tr:  96.22%, tr_best:  99.39%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3346%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.6213%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 33079  12.996%\n",
      "lif layer 2 self.abs_max_v: 7458.5\n",
      "fc layer 1 self.abs_max_out: 2142.0\n",
      "lif layer 1 self.abs_max_v: 3945.5\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  4.624433/ 20.127062, val:  54.58%, val_best:  57.08%, tr:  95.71%, tr_best:  99.39%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3620%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.7095%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 34323  12.985%\n",
      "fc layer 1 self.abs_max_out: 2234.0\n",
      "lif layer 2 self.abs_max_v: 7602.5\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  4.791527/ 15.341259, val:  57.92%, val_best:  57.92%, tr:  97.14%, tr_best:  99.39%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4935%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.6418%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 35536  12.964%\n",
      "fc layer 2 self.abs_max_out: 5046.0\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  4.825347/ 41.600998, val:  39.17%, val_best:  57.92%, tr:  96.73%, tr_best:  99.39%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.7127%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.6823%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 36790  12.958%\n",
      "lif layer 2 self.abs_max_v: 7762.5\n",
      "fc layer 1 self.abs_max_out: 2265.0\n",
      "lif layer 1 self.abs_max_v: 3982.5\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  4.542664/ 32.183105, val:  46.67%, val_best:  57.92%, tr:  97.14%, tr_best:  99.39%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.7537%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.9407%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 37984  12.933%\n",
      "lif layer 2 self.abs_max_v: 7798.0\n",
      "lif layer 2 self.abs_max_v: 7991.0\n",
      "lif layer 2 self.abs_max_v: 8023.5\n",
      "lif layer 2 self.abs_max_v: 8102.5\n",
      "lif layer 2 self.abs_max_v: 8332.5\n",
      "lif layer 2 self.abs_max_v: 8420.0\n",
      "lif layer 2 self.abs_max_v: 8703.5\n",
      "fc layer 1 self.abs_max_out: 2380.0\n",
      "lif layer 1 self.abs_max_v: 4058.5\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  4.559924/ 23.255600, val:  45.00%, val_best:  57.92%, tr:  96.53%, tr_best:  99.39%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3166%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.2485%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 39212  12.920%\n",
      "lif layer 2 self.abs_max_v: 8886.0\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  4.665352/ 25.585470, val:  42.92%, val_best:  57.92%, tr:  96.73%, tr_best:  99.39%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5959%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.5491%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 40423  12.903%\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  4.325245/ 27.150278, val:  40.00%, val_best:  57.92%, tr:  96.63%, tr_best:  99.39%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2023%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.5561%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 41559  12.864%\n",
      "lif layer 1 self.abs_max_v: 4079.5\n",
      "lif layer 1 self.abs_max_v: 4129.5\n",
      "lif layer 1 self.abs_max_v: 4293.0\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  4.593059/ 30.182699, val:  45.00%, val_best:  57.92%, tr:  97.24%, tr_best:  99.39%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9479%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8073%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 42792  12.856%\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  4.527490/ 34.450714, val:  47.08%, val_best:  57.92%, tr:  96.12%, tr_best:  99.39%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9974%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.6572%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 44000  12.841%\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  4.467021/ 19.130507, val:  47.50%, val_best:  57.92%, tr:  96.02%, tr_best:  99.39%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3475%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.6979%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 45162  12.814%\n",
      "fc layer 1 self.abs_max_out: 2471.0\n",
      "fc layer 1 self.abs_max_out: 2610.0\n",
      "lif layer 1 self.abs_max_v: 4358.5\n",
      "lif layer 1 self.abs_max_v: 4599.5\n",
      "lif layer 1 self.abs_max_v: 4623.5\n",
      "lif layer 1 self.abs_max_v: 4717.0\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  4.390897/ 21.071253, val:  60.83%, val_best:  60.83%, tr:  96.22%, tr_best:  99.39%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.8130%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3374%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 46363  12.799%\n",
      "lif layer 2 self.abs_max_v: 8975.5\n",
      "lif layer 2 self.abs_max_v: 9224.0\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  4.278068/ 22.832926, val:  51.25%, val_best:  60.83%, tr:  96.02%, tr_best:  99.39%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3403%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2594%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 47536  12.778%\n",
      "fc layer 2 self.abs_max_out: 5119.0\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  4.588429/ 31.628420, val:  33.33%, val_best:  60.83%, tr:  96.73%, tr_best:  99.39%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3695%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3808%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 48793  12.779%\n",
      "lif layer 2 self.abs_max_v: 9352.5\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  4.295278/ 41.295788, val:  41.25%, val_best:  60.83%, tr:  96.42%, tr_best:  99.39%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.2405%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0075%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 49993  12.766%\n",
      "lif layer 2 self.abs_max_v: 9377.5\n",
      "fc layer 2 self.abs_max_out: 5151.0\n",
      "fc layer 2 self.abs_max_out: 5202.0\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  4.464865/ 16.708763, val:  57.08%, val_best:  60.83%, tr:  95.71%, tr_best:  99.39%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4123%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1113%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 51195  12.754%\n",
      "fc layer 2 self.abs_max_out: 5285.0\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  4.337729/ 22.738503, val:  47.08%, val_best:  60.83%, tr:  96.83%, tr_best:  99.39%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6872%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8506%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 52367  12.736%\n",
      "lif layer 2 self.abs_max_v: 9479.0\n",
      "fc layer 2 self.abs_max_out: 5482.0\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  4.173946/ 24.296293, val:  40.00%, val_best:  60.83%, tr:  96.73%, tr_best:  99.39%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9491%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2619%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 53494  12.707%\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  4.249998/ 17.941399, val:  50.83%, val_best:  60.83%, tr:  96.63%, tr_best:  99.39%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.7466%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8821%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 54709  12.701%\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  4.318216/ 16.499010, val:  59.58%, val_best:  60.83%, tr:  95.91%, tr_best:  99.39%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9240%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5379%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 55875  12.683%\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  4.108913/ 26.172035, val:  52.92%, val_best:  60.83%, tr:  96.42%, tr_best:  99.39%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.7280%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3598%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 57010  12.659%\n",
      "fc layer 2 self.abs_max_out: 5543.0\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  4.023989/ 23.314686, val:  50.83%, val_best:  60.83%, tr:  95.91%, tr_best:  99.39%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5683%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4330%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 58125  12.632%\n",
      "fc layer 1 self.abs_max_out: 2725.0\n",
      "lif layer 1 self.abs_max_v: 4823.5\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  4.070029/ 32.928391, val:  40.83%, val_best:  60.83%, tr:  96.63%, tr_best:  99.39%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5865%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2622%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 59230  12.604%\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  4.010019/ 26.077477, val:  50.83%, val_best:  60.83%, tr:  96.94%, tr_best:  99.39%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.8682%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3722%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 60334  12.577%\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  4.184201/ 23.732538, val:  55.83%, val_best:  60.83%, tr:  96.73%, tr_best:  99.39%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1901%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2265%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 61467  12.557%\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  4.197442/ 31.239178, val:  44.17%, val_best:  60.83%, tr:  96.32%, tr_best:  99.39%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.0838%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5036%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 62612  12.540%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  3.938364/ 26.588394, val:  51.25%, val_best:  60.83%, tr:  95.61%, tr_best:  99.39%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.8434%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4955%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 63716  12.516%\n",
      "fc layer 2 self.abs_max_out: 5817.0\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  4.129550/ 17.312328, val:  52.50%, val_best:  60.83%, tr:  96.94%, tr_best:  99.39%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1628%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5662%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 64843  12.497%\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  4.064664/ 22.089027, val:  50.83%, val_best:  60.83%, tr:  97.24%, tr_best:  99.39%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.0592%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5613%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 65956  12.476%\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  4.149229/ 18.672581, val:  54.58%, val_best:  60.83%, tr:  96.83%, tr_best:  99.39%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.0676%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3674%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 67115  12.464%\n",
      "fc layer 1 self.abs_max_out: 2773.0\n",
      "lif layer 1 self.abs_max_v: 4873.5\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  4.127841/ 20.079468, val:  53.33%, val_best:  60.83%, tr:  97.45%, tr_best:  99.39%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6268%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4147%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 68226  12.445%\n",
      "lif layer 2 self.abs_max_v: 9522.5\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  4.181421/ 28.977505, val:  43.75%, val_best:  60.83%, tr:  96.63%, tr_best:  99.39%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6338%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1749%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 69375  12.432%\n",
      "lif layer 2 self.abs_max_v: 9536.0\n",
      "lif layer 2 self.abs_max_v: 9742.0\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  3.969166/ 20.105358, val:  50.42%, val_best:  60.83%, tr:  96.63%, tr_best:  99.39%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1300%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3898%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 70450  12.407%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  4.233470/ 27.435886, val:  50.83%, val_best:  60.83%, tr:  96.22%, tr_best:  99.39%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9264%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2155%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 71564  12.390%\n",
      "lif layer 2 self.abs_max_v: 9906.0\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  4.172697/ 33.011978, val:  43.75%, val_best:  60.83%, tr:  96.53%, tr_best:  99.39%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2013%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0898%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 72688  12.375%\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  4.175079/ 25.223587, val:  44.58%, val_best:  60.83%, tr:  96.63%, tr_best:  99.39%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5810%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3578%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 73811  12.360%\n",
      "lif layer 2 self.abs_max_v: 9976.5\n",
      "fc layer 1 self.abs_max_out: 2790.0\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  4.468011/ 24.317556, val:  51.67%, val_best:  60.83%, tr:  96.53%, tr_best:  99.39%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5561%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4765%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 74991  12.355%\n",
      "fc layer 2 self.abs_max_out: 5866.0\n",
      "fc layer 1 self.abs_max_out: 2893.0\n",
      "lif layer 1 self.abs_max_v: 5071.0\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  4.106084/ 26.808008, val:  39.17%, val_best:  60.83%, tr:  96.12%, tr_best:  99.39%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5212%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3067%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 76075  12.334%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  4.300632/ 27.314764, val:  48.75%, val_best:  60.83%, tr:  96.22%, tr_best:  99.39%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6902%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1430%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 77188  12.319%\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  4.229827/ 24.503235, val:  41.67%, val_best:  60.83%, tr:  96.83%, tr_best:  99.39%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.4708%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2096%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 78274  12.300%\n",
      "fc layer 1 self.abs_max_out: 2905.0\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  4.175159/ 19.278337, val:  48.75%, val_best:  60.83%, tr:  96.22%, tr_best:  99.39%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1207%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2107%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 79378  12.285%\n",
      "lif layer 2 self.abs_max_v: 10043.0\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  4.092493/ 26.947412, val:  45.42%, val_best:  60.83%, tr:  96.73%, tr_best:  99.39%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6139%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2799%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 80444  12.264%\n",
      "fc layer 2 self.abs_max_out: 5890.0\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  4.196496/ 19.750856, val:  46.25%, val_best:  60.83%, tr:  96.53%, tr_best:  99.39%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.7604%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6136%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 81527  12.246%\n",
      "fc layer 1 self.abs_max_out: 2922.0\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  4.048450/ 18.722923, val:  66.25%, val_best:  66.25%, tr:  96.32%, tr_best:  99.39%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.0721%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7518%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 82601  12.228%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  3.751799/ 36.247604, val:  33.75%, val_best:  66.25%, tr:  96.42%, tr_best:  99.39%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6717%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0823%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 83613  12.201%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  4.102554/ 19.692186, val:  60.83%, val_best:  66.25%, tr:  96.63%, tr_best:  99.39%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3260%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8824%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 84716  12.188%\n",
      "fc layer 2 self.abs_max_out: 6637.0\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  4.051580/ 31.819160, val:  44.17%, val_best:  66.25%, tr:  96.42%, tr_best:  99.39%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6479%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8538%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 85799  12.172%\n",
      "lif layer 2 self.abs_max_v: 10102.0\n",
      "lif layer 2 self.abs_max_v: 10120.5\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  3.937773/ 19.101322, val:  62.92%, val_best:  66.25%, tr:  95.71%, tr_best:  99.39%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5863%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1561%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 86892  12.158%\n",
      "lif layer 2 self.abs_max_v: 10669.0\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  3.939588/ 33.122391, val:  54.17%, val_best:  66.25%, tr:  96.12%, tr_best:  99.39%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6817%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7669%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 87959  12.141%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  4.171460/ 21.630152, val:  55.42%, val_best:  66.25%, tr:  97.04%, tr_best:  99.39%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.0152%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9388%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 89091  12.134%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  3.847517/ 20.346296, val:  50.42%, val_best:  66.25%, tr:  96.63%, tr_best:  99.39%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.8393%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8279%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 90166  12.118%\n",
      "lif layer 2 self.abs_max_v: 10838.5\n",
      "fc layer 1 self.abs_max_out: 2951.0\n",
      "lif layer 1 self.abs_max_v: 5096.0\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  4.087523/ 19.896248, val:  52.50%, val_best:  66.25%, tr:  96.02%, tr_best:  99.39%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5764%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5164%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 91260  12.106%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  4.296561/ 26.829025, val:  47.08%, val_best:  66.25%, tr:  96.12%, tr_best:  99.39%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.4177%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1498%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 92385  12.098%\n",
      "fc layer 1 self.abs_max_out: 2956.0\n",
      "lif layer 1 self.abs_max_v: 5154.0\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  4.315938/ 28.252234, val:  52.50%, val_best:  66.25%, tr:  96.12%, tr_best:  99.39%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6246%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2482%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 93513  12.091%\n",
      "fc layer 1 self.abs_max_out: 2974.0\n",
      "lif layer 1 self.abs_max_v: 5163.0\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  3.956904/ 21.330181, val:  58.33%, val_best:  66.25%, tr:  96.73%, tr_best:  99.39%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6848%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4512%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 94544  12.072%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  3.858157/ 20.532717, val:  48.75%, val_best:  66.25%, tr:  96.73%, tr_best:  99.39%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.7025%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7902%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 95592  12.055%\n",
      "lif layer 2 self.abs_max_v: 11303.0\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  4.053582/ 29.656797, val:  41.67%, val_best:  66.25%, tr:  95.91%, tr_best:  99.39%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.4943%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4078%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 96654  12.040%\n",
      "lif layer 2 self.abs_max_v: 11565.5\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  4.348824/ 20.379587, val:  49.58%, val_best:  66.25%, tr:  96.42%, tr_best:  99.39%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5358%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5463%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 97790  12.035%\n",
      "lif layer 2 self.abs_max_v: 11574.5\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  4.094841/ 21.155445, val:  43.75%, val_best:  66.25%, tr:  96.02%, tr_best:  99.39%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.8091%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7436%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 98881  12.024%\n",
      "lif layer 1 self.abs_max_v: 5311.0\n",
      "fc layer 1 self.abs_max_out: 3048.0\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  3.816088/ 20.323786, val:  47.92%, val_best:  66.25%, tr:  96.63%, tr_best:  99.39%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.7313%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0979%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 99921  12.008%\n",
      "fc layer 1 self.abs_max_out: 3052.0\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  3.993656/ 19.347124, val:  54.58%, val_best:  66.25%, tr:  96.42%, tr_best:  99.39%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5781%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8808%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 101014  11.998%\n",
      "fc layer 1 self.abs_max_out: 3063.0\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  3.677154/ 24.632977, val:  50.83%, val_best:  66.25%, tr:  96.22%, tr_best:  99.39%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6957%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8121%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 102017  11.978%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  3.803066/ 22.990818, val:  49.17%, val_best:  66.25%, tr:  96.73%, tr_best:  99.39%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.8009%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9592%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 103072  11.964%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  3.660146/ 20.739584, val:  56.67%, val_best:  66.25%, tr:  96.22%, tr_best:  99.39%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.7859%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0724%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 104087  11.946%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  4.118038/ 19.393641, val:  51.25%, val_best:  66.25%, tr:  95.71%, tr_best:  99.39%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3596%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 105182  11.938%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  4.182650/ 23.699425, val:  48.75%, val_best:  66.25%, tr:  96.53%, tr_best:  99.39%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6137%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5662%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 106262  11.928%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  4.013803/ 19.071550, val:  67.50%, val_best:  67.50%, tr:  96.02%, tr_best:  99.39%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6501%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7085%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 107310  11.914%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  3.996710/ 21.666513, val:  40.42%, val_best:  67.50%, tr:  97.14%, tr_best:  99.39%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6123%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7242%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 108356  11.901%\n",
      "fc layer 1 self.abs_max_out: 3084.0\n",
      "lif layer 1 self.abs_max_v: 5364.5\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  3.853284/ 23.690380, val:  52.50%, val_best:  67.50%, tr:  96.83%, tr_best:  99.39%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5038%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9091%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 109387  11.887%\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  3.824398/ 21.238338, val:  53.33%, val_best:  67.50%, tr:  97.04%, tr_best:  99.39%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.8337%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8018%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 110404  11.871%\n",
      "fc layer 1 self.abs_max_out: 3180.0\n",
      "lif layer 1 self.abs_max_v: 5584.0\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  4.022222/ 22.024956, val:  60.00%, val_best:  67.50%, tr:  96.12%, tr_best:  99.39%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6407%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9580%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 111487  11.862%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  3.900912/ 24.496069, val:  57.08%, val_best:  67.50%, tr:  96.22%, tr_best:  99.39%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5145%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8885%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 112537  11.851%\n",
      "fc layer 2 self.abs_max_out: 6687.0\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  3.700660/ 18.195112, val:  56.25%, val_best:  67.50%, tr:  96.32%, tr_best:  99.39%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5697%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7044%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 113518  11.832%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  3.825107/ 28.147221, val:  39.58%, val_best:  67.50%, tr:  95.91%, tr_best:  99.39%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6872%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0028%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 114525  11.816%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  3.878483/ 20.434452, val:  49.17%, val_best:  67.50%, tr:  96.53%, tr_best:  99.39%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6458%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9417%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 115533  11.801%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  3.619325/ 31.779663, val:  44.17%, val_best:  67.50%, tr:  96.63%, tr_best:  99.39%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3523%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0610%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 116509  11.783%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  3.813457/ 17.703512, val:  59.58%, val_best:  67.50%, tr:  96.32%, tr_best:  99.39%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6296%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0700%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 117510  11.768%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  3.458545/ 21.655447, val:  47.50%, val_best:  67.50%, tr:  95.51%, tr_best:  99.39%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.8680%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3974%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 118460  11.748%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  3.646158/ 24.645157, val:  50.00%, val_best:  67.50%, tr:  96.83%, tr_best:  99.39%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.9329%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3009%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 119468  11.734%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  3.672458/ 22.365608, val:  54.17%, val_best:  67.50%, tr:  95.71%, tr_best:  99.39%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.9078%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2789%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 120463  11.719%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  3.857511/ 23.317516, val:  45.83%, val_best:  67.50%, tr:  96.32%, tr_best:  99.39%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6364%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2148%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 121498  11.708%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  3.607865/ 29.216454, val:  41.25%, val_best:  67.50%, tr:  96.53%, tr_best:  99.39%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.9014%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0642%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 122481  11.692%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  3.648794/ 21.682980, val:  60.00%, val_best:  67.50%, tr:  97.45%, tr_best:  99.39%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.8675%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2184%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 123493  11.680%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  4.045180/ 18.109671, val:  53.75%, val_best:  67.50%, tr:  94.99%, tr_best:  99.39%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.0151%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7933%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 124531  11.670%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  3.571385/ 24.975876, val:  44.58%, val_best:  67.50%, tr:  97.04%, tr_best:  99.39%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.8875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9495%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 125510  11.655%\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  3.739840/ 16.811028, val:  52.92%, val_best:  67.50%, tr:  97.55%, tr_best:  99.39%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.8741%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1291%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 126498  11.641%\n",
      "fc layer 1 self.abs_max_out: 3248.0\n",
      "lif layer 1 self.abs_max_v: 5670.5\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  3.736138/ 20.274952, val:  55.00%, val_best:  67.50%, tr:  96.63%, tr_best:  99.39%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.7831%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0275%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 127486  11.627%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  3.885954/ 26.135139, val:  53.33%, val_best:  67.50%, tr:  97.34%, tr_best:  99.39%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.9269%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0317%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 128518  11.617%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  3.573210/ 23.224974, val:  57.92%, val_best:  67.50%, tr:  96.73%, tr_best:  99.39%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.0189%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1582%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 129493  11.603%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  3.549529/ 22.183208, val:  51.25%, val_best:  67.50%, tr:  95.71%, tr_best:  99.39%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.2306%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0159%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 130477  11.589%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  3.734427/ 25.347387, val:  50.00%, val_best:  67.50%, tr:  96.63%, tr_best:  99.39%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.3764%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0107%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 131474  11.577%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  3.544930/ 29.670034, val:  50.42%, val_best:  67.50%, tr:  97.45%, tr_best:  99.39%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.0493%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2441%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 132435  11.562%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  3.588024/ 19.528938, val:  55.83%, val_best:  67.50%, tr:  96.32%, tr_best:  99.39%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.0366%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3276%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 133383  11.546%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  3.593704/ 23.728449, val:  38.75%, val_best:  67.50%, tr:  96.94%, tr_best:  99.39%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.1463%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9900%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 134338  11.531%\n",
      "fc layer 2 self.abs_max_out: 6871.0\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  3.762858/ 25.702229, val:  49.58%, val_best:  67.50%, tr:  96.73%, tr_best:  99.39%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.7747%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0574%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 135358  11.522%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  3.608807/ 16.100609, val:  58.33%, val_best:  67.50%, tr:  96.53%, tr_best:  99.39%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.8979%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9630%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 136324  11.508%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  3.826208/ 21.898100, val:  50.83%, val_best:  67.50%, tr:  95.71%, tr_best:  99.39%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.0778%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9831%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 137359  11.500%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  3.651254/ 22.610210, val:  52.08%, val_best:  67.50%, tr:  96.02%, tr_best:  99.39%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.3655%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3164%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 138361  11.490%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  3.446412/ 22.019007, val:  55.42%, val_best:  67.50%, tr:  96.73%, tr_best:  99.39%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.2451%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 139307  11.475%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  3.485411/ 13.307348, val:  62.08%, val_best:  67.50%, tr:  97.24%, tr_best:  99.39%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.2727%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2086%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 140231  11.459%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  3.525558/ 16.065052, val:  65.83%, val_best:  67.50%, tr:  97.04%, tr_best:  99.39%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.4841%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0910%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 141185  11.446%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  3.739903/ 27.309757, val:  56.67%, val_best:  67.50%, tr:  96.73%, tr_best:  99.39%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.0068%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9401%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 142190  11.436%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  3.503944/ 21.095432, val:  57.08%, val_best:  67.50%, tr:  96.22%, tr_best:  99.39%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.0185%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7233%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 143118  11.421%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  3.402705/ 18.295822, val:  63.75%, val_best:  67.50%, tr:  96.94%, tr_best:  99.39%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.9366%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7410%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 144041  11.405%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  3.688261/ 24.796953, val:  45.83%, val_best:  67.50%, tr:  96.53%, tr_best:  99.39%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.0577%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4397%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 145033  11.396%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  3.746483/ 30.177954, val:  50.00%, val_best:  67.50%, tr:  96.32%, tr_best:  99.39%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.1332%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4266%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 145996  11.384%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  3.779283/ 20.561222, val:  54.58%, val_best:  67.50%, tr:  96.12%, tr_best:  99.39%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.4084%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 146976  11.373%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  3.423851/ 25.511452, val:  47.92%, val_best:  67.50%, tr:  95.40%, tr_best:  99.39%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.2311%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0531%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 147883  11.358%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  3.732255/ 18.387770, val:  66.25%, val_best:  67.50%, tr:  96.53%, tr_best:  99.39%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.1974%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8608%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 148848  11.346%\n",
      "fc layer 1 self.abs_max_out: 3346.0\n",
      "lif layer 1 self.abs_max_v: 5699.0\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  3.520213/ 19.040531, val:  52.92%, val_best:  67.50%, tr:  97.24%, tr_best:  99.39%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.5223%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2273%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 149779  11.333%\n",
      "lif layer 1 self.abs_max_v: 5762.5\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  3.563953/ 14.490351, val:  67.92%, val_best:  67.92%, tr:  97.04%, tr_best:  99.39%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.0356%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 150738  11.321%\n",
      "fc layer 2 self.abs_max_out: 6931.0\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  3.342733/ 16.399551, val:  67.08%, val_best:  67.92%, tr:  96.63%, tr_best:  99.39%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.2266%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9001%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 151631  11.305%\n",
      "lif layer 2 self.abs_max_v: 11586.5\n",
      "lif layer 2 self.abs_max_v: 11874.0\n",
      "fc layer 2 self.abs_max_out: 7144.0\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  3.575594/ 21.239687, val:  46.67%, val_best:  67.92%, tr:  95.61%, tr_best:  99.39%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.1761%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1288%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 152593  11.295%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  3.376648/ 25.208853, val:  55.83%, val_best:  67.92%, tr:  97.24%, tr_best:  99.39%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.2501%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9915%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 153521  11.282%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  3.608102/ 20.801897, val:  66.25%, val_best:  67.92%, tr:  96.22%, tr_best:  99.39%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.2721%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2818%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 154488  11.272%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  3.631893/ 20.195068, val:  52.08%, val_best:  67.92%, tr:  96.94%, tr_best:  99.39%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.2148%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1945%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 155440  11.261%\n",
      "fc layer 1 self.abs_max_out: 3404.0\n",
      "lif layer 1 self.abs_max_v: 5810.5\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  3.388269/ 17.665712, val:  62.08%, val_best:  67.92%, tr:  97.65%, tr_best:  99.39%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.0259%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4562%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 156347  11.247%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  3.510942/ 17.635595, val:  57.50%, val_best:  67.92%, tr:  96.63%, tr_best:  99.39%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.7716%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1947%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 157302  11.236%\n",
      "lif layer 1 self.abs_max_v: 5868.0\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  3.631706/ 24.322018, val:  57.50%, val_best:  67.92%, tr:  96.02%, tr_best:  99.39%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.1178%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9430%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 158236  11.224%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  3.591248/ 25.882511, val:  52.50%, val_best:  67.92%, tr:  97.14%, tr_best:  99.39%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.9626%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0053%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 159169  11.213%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  3.582229/ 28.352863, val:  49.58%, val_best:  67.92%, tr:  96.63%, tr_best:  99.39%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.7459%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8874%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 160052  11.198%\n",
      "lif layer 1 self.abs_max_v: 5916.0\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  3.767501/ 22.182604, val:  59.17%, val_best:  67.92%, tr:  96.42%, tr_best:  99.39%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5481%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9064%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 161021  11.189%\n",
      "fc layer 1 self.abs_max_out: 3408.0\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  3.533796/ 26.620695, val:  42.50%, val_best:  67.92%, tr:  95.71%, tr_best:  99.39%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6372%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8573%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 161931  11.176%\n",
      "lif layer 1 self.abs_max_v: 5959.0\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  3.565523/ 19.590311, val:  66.25%, val_best:  67.92%, tr:  96.94%, tr_best:  99.39%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.7174%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8254%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 162853  11.164%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  3.612527/ 23.395582, val:  62.50%, val_best:  67.92%, tr:  96.53%, tr_best:  99.39%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.0153%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9085%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 163795  11.154%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  3.448930/ 29.000343, val:  44.17%, val_best:  67.92%, tr:  96.73%, tr_best:  99.39%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.7311%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6462%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 164710  11.142%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  3.461594/ 23.375788, val:  62.92%, val_best:  67.92%, tr:  95.71%, tr_best:  99.39%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6701%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9886%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 165604  11.129%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  3.632185/ 22.639744, val:  55.00%, val_best:  67.92%, tr:  96.32%, tr_best:  99.39%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6905%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8865%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 166541  11.119%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  3.644459/ 26.708179, val:  58.33%, val_best:  67.92%, tr:  96.02%, tr_best:  99.39%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6733%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0644%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 167499  11.110%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  3.679353/ 18.065987, val:  58.75%, val_best:  67.92%, tr:  96.02%, tr_best:  99.39%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.4785%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0601%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 168438  11.100%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  3.904486/ 17.989077, val:  60.83%, val_best:  67.92%, tr:  96.42%, tr_best:  99.39%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6285%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8014%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 169428  11.094%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  3.539151/ 20.717880, val:  53.33%, val_best:  67.92%, tr:  96.42%, tr_best:  99.39%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.7349%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8129%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 170358  11.084%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  3.702128/ 21.308035, val:  65.00%, val_best:  67.92%, tr:  97.85%, tr_best:  99.39%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6004%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6370%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 171305  11.075%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  3.695517/ 16.941711, val:  65.83%, val_best:  67.92%, tr:  96.53%, tr_best:  99.39%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.7574%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8452%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 172258  11.066%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  3.697039/ 19.147367, val:  64.58%, val_best:  67.92%, tr:  97.45%, tr_best:  99.39%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.7198%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9196%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 173215  11.058%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  3.409550/ 16.930716, val:  60.83%, val_best:  67.92%, tr:  97.75%, tr_best:  99.39%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.7021%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5362%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 174079  11.044%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  3.862923/ 21.616053, val:  56.25%, val_best:  67.92%, tr:  96.94%, tr_best:  99.39%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.7274%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7490%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 175062  11.038%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  3.623500/ 24.217457, val:  56.25%, val_best:  67.92%, tr:  97.34%, tr_best:  99.39%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.8845%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5724%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 175966  11.027%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  3.476270/ 23.594435, val:  52.08%, val_best:  67.92%, tr:  97.14%, tr_best:  99.39%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.7860%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5652%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 176856  11.015%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  3.687367/ 23.351393, val:  49.17%, val_best:  67.92%, tr:  96.53%, tr_best:  99.39%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5705%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 177809  11.007%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  3.729627/ 24.427616, val:  52.50%, val_best:  67.92%, tr:  96.83%, tr_best:  99.39%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.8413%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6107%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 178771  11.000%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  3.458604/ 19.656401, val:  51.25%, val_best:  67.92%, tr:  97.75%, tr_best:  99.39%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.7902%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0098%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 179652  10.988%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  3.478627/ 34.393932, val:  45.42%, val_best:  67.92%, tr:  97.45%, tr_best:  99.39%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.4381%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1164%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 180556  10.978%\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        # \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [256.0,128.0,64.0,32.0,16.0]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "\n",
    "        \"learning_rate\": {\"values\": [1.0]}, \n",
    "        # \"lr_factor\": {\"values\": [-6, -7, -8, -9]}, \n",
    "        \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [14]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [-1]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [0]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        \"scale_exp_2w\": {\"values\": [0]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        \"scale_exp_3w\": {\"values\": [0]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "        \"lif_layer_sg_width2\": {\"values\": [0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0]},\n",
    "        \"lif_layer_v_threshold2\": {\"values\": [256.0,128.0,64.0,32.0,16.0]},\n",
    "        \"learning_rate2\": {\"values\": [1.0]},\n",
    "        \"init_scaling_0\": {\"values\": [4/128]},\n",
    "        \"init_scaling_1\": {\"values\": [6/128]},\n",
    "        \"init_scaling_2\": {\"values\": [3/128]},\n",
    "        \n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"4\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_2w,wandb.config.scale_exp_2w],[wandb.config.scale_exp_3w,wandb.config.scale_exp_3w]],\n",
    "        lif_layer_sg_width2  =  wandb.config.lif_layer_sg_width2,\n",
    "        lif_layer_v_threshold2  =  wandb.config.lif_layer_v_threshold2,\n",
    "        learning_rate2  =  wandb.config.learning_rate2,\n",
    "        init_scaling = [wandb.config.init_scaling_0,wandb.config.init_scaling_1,wandb.config.init_scaling_2],\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'e1m59f1o'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
