{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7043/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        smallest_now_T = 99999\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "                now_T = inputs.shape[1]\n",
    "                if epoch == 0 and now_T < smallest_now_T:\n",
    "                    smallest_now_T = now_T\n",
    "                    print(f'smallest_now_T updated: {smallest_now_T}')\n",
    "                now_time_steps = temporal_filter*TIME\n",
    "                if now_T < now_time_steps:\n",
    "                    # Î∂ÄÏ°±Ìïú timestep Í∞úÏàò\n",
    "                    diff = now_time_steps - now_T\n",
    "\n",
    "                    # ÎßàÏßÄÎßâ timestep Î≥µÏÇ¨ (shape: [B, 1, C, H, W])\n",
    "                    last_frame = inputs[:, -1:, :, :, :]\n",
    "\n",
    "                    # diffÎßåÌÅº repeatÌïòÏó¨ Ìå®Îî© Íµ¨ÏÑ±\n",
    "                    pad_frames = last_frame.repeat(1, diff, 1, 1, 1)\n",
    "\n",
    "                    # ÏõêÎ≥∏ + Ìå®Îî© Í≤∞Ìï©\n",
    "                    inputs = torch.cat([inputs, pad_frames], dim=1)\n",
    "                else:\n",
    "                    # start_idx = random.randint(0, now_T - now_time_steps)\n",
    "                    start_idx = random.choice(range(0, now_T - now_time_steps + 1, now_time_steps))\n",
    "                    # start_idx = random.choice([i for i in range(0, now_T - now_time_steps + 1, now_time_steps)])\n",
    "                    inputs = inputs[:, start_idx : start_idx + now_time_steps]\n",
    "                if dvs_clipping != 0:\n",
    "                    inputs[inputs<dvs_clipping] = 0.0\n",
    "                    inputs[inputs>=dvs_clipping] = 1.0\n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            smallest_now_T_val = 99999\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            now_T = inputs_val.shape[1]\n",
    "                            if epoch == 0 and now_T < smallest_now_T_val:\n",
    "                                smallest_now_T_val = now_T\n",
    "                                print(f'smallest_now_T_val updated: {smallest_now_T_val}')\n",
    "                            now_time_steps = temporal_filter*TIME\n",
    "\n",
    "                            if now_T < now_time_steps:\n",
    "                                # Î∂ÄÏ°±Ìïú timestep Í∞úÏàò\n",
    "                                diff = now_time_steps - now_T\n",
    "\n",
    "                                # ÎßàÏßÄÎßâ timestep Î≥µÏÇ¨ (shape: [B, 1, C, H, W])\n",
    "                                last_frame = inputs_val[:, -1:, :, :, :]\n",
    "\n",
    "                                # diffÎßåÌÅº repeatÌïòÏó¨ Ìå®Îî© Íµ¨ÏÑ±\n",
    "                                pad_frames = last_frame.repeat(1, diff, 1, 1, 1)\n",
    "\n",
    "                                # ÏõêÎ≥∏ + Ìå®Îî© Í≤∞Ìï©\n",
    "                                inputs_val = torch.cat([inputs_val, pad_frames], dim=1)\n",
    "                            else:\n",
    "                                pass\n",
    "                            \n",
    "                            start_idx = 0\n",
    "                            inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if dvs_clipping != 0:\n",
    "                                inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "                \n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    assert val_acc_best > 0.2\n",
    "                elif epoch > 10:\n",
    "                    assert val_acc_best > 0.4\n",
    "                elif epoch > 30:\n",
    "                    assert val_acc_best > 0.5\n",
    "                elif epoch > 100:\n",
    "                    assert val_acc_best > 0.8\n",
    "                elif epoch > 150:\n",
    "                    assert val_acc_best > 0.88\n",
    "                    \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"1\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 2871,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.25,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 1/512, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 200,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = -1, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[-9,-9],[-9,-9],[-8,-8]], \n",
    "# # 1w -11~-9\n",
    "# # 1b -11~ -7\n",
    "# # 2w -10~-8\n",
    "# # 2b -10~-8\n",
    "# # 3w -10\n",
    "# # 3b -10\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lb85yhnm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 50000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_212824-lb85yhnm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/lb85yhnm' target=\"_blank\">summer-sweep-4</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/lb85yhnm' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/lb85yhnm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': '20251117_212833_863', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 20, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 50000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 575149142d3019108310063e0e922290\n",
      "cache path doesn't exist\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=20, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=20, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 139\n",
      "fc layer 1 self.abs_max_out: 721.0\n",
      "lif layer 1 self.abs_max_v: 721.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 258.0\n",
      "lif layer 2 self.abs_max_v: 258.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "lif layer 1 self.abs_max_v: 814.5\n",
      "fc layer 2 self.abs_max_out: 403.0\n",
      "lif layer 2 self.abs_max_v: 462.0\n",
      "fc layer 1 self.abs_max_out: 750.0\n",
      "lif layer 1 self.abs_max_v: 940.5\n",
      "fc layer 2 self.abs_max_out: 407.0\n",
      "lif layer 2 self.abs_max_v: 509.0\n",
      "lif layer 2 self.abs_max_v: 568.5\n",
      "fc layer 1 self.abs_max_out: 755.0\n",
      "lif layer 1 self.abs_max_v: 1065.5\n",
      "lif layer 2 self.abs_max_v: 571.0\n",
      "fc layer 2 self.abs_max_out: 460.0\n",
      "lif layer 2 self.abs_max_v: 575.5\n",
      "fc layer 2 self.abs_max_out: 511.0\n",
      "lif layer 2 self.abs_max_v: 595.0\n",
      "fc layer 3 self.abs_max_out: 25.0\n",
      "fc layer 1 self.abs_max_out: 1053.0\n",
      "lif layer 1 self.abs_max_v: 1080.0\n",
      "lif layer 2 self.abs_max_v: 600.0\n",
      "fc layer 3 self.abs_max_out: 51.0\n",
      "smallest_now_T updated: 125\n",
      "lif layer 2 self.abs_max_v: 619.0\n",
      "fc layer 3 self.abs_max_out: 74.0\n",
      "lif layer 1 self.abs_max_v: 1127.5\n",
      "fc layer 1 self.abs_max_out: 1230.0\n",
      "lif layer 1 self.abs_max_v: 1425.5\n",
      "fc layer 2 self.abs_max_out: 581.0\n",
      "lif layer 1 self.abs_max_v: 1631.0\n",
      "fc layer 2 self.abs_max_out: 837.0\n",
      "lif layer 2 self.abs_max_v: 900.0\n",
      "fc layer 1 self.abs_max_out: 1263.0\n",
      "fc layer 1 self.abs_max_out: 1408.0\n",
      "fc layer 3 self.abs_max_out: 102.0\n",
      "smallest_now_T updated: 94\n",
      "fc layer 3 self.abs_max_out: 105.0\n",
      "fc layer 1 self.abs_max_out: 1423.0\n",
      "fc layer 1 self.abs_max_out: 1478.0\n",
      "fc layer 1 self.abs_max_out: 1490.0\n",
      "fc layer 3 self.abs_max_out: 107.0\n",
      "fc layer 3 self.abs_max_out: 121.0\n",
      "fc layer 1 self.abs_max_out: 2078.0\n",
      "lif layer 1 self.abs_max_v: 2078.0\n",
      "lif layer 2 self.abs_max_v: 972.5\n",
      "lif layer 2 self.abs_max_v: 1064.0\n",
      "lif layer 2 self.abs_max_v: 1328.5\n",
      "fc layer 3 self.abs_max_out: 130.0\n",
      "fc layer 3 self.abs_max_out: 165.0\n",
      "fc layer 2 self.abs_max_out: 847.0\n",
      "fc layer 3 self.abs_max_out: 204.0\n",
      "smallest_now_T updated: 79\n",
      "fc layer 2 self.abs_max_out: 875.0\n",
      "lif layer 1 self.abs_max_v: 2115.5\n",
      "fc layer 3 self.abs_max_out: 220.0\n",
      "fc layer 3 self.abs_max_out: 230.0\n",
      "fc layer 1 self.abs_max_out: 2242.0\n",
      "lif layer 1 self.abs_max_v: 2242.0\n",
      "fc layer 3 self.abs_max_out: 244.0\n",
      "lif layer 1 self.abs_max_v: 2263.0\n",
      "fc layer 1 self.abs_max_out: 2357.0\n",
      "lif layer 1 self.abs_max_v: 2357.0\n",
      "lif layer 1 self.abs_max_v: 2419.0\n",
      "fc layer 2 self.abs_max_out: 1072.0\n",
      "lif layer 2 self.abs_max_v: 1336.0\n",
      "lif layer 2 self.abs_max_v: 1340.0\n",
      "lif layer 2 self.abs_max_v: 1348.5\n",
      "lif layer 2 self.abs_max_v: 1508.0\n",
      "smallest_now_T updated: 73\n",
      "fc layer 3 self.abs_max_out: 269.0\n",
      "lif layer 2 self.abs_max_v: 1643.0\n",
      "fc layer 2 self.abs_max_out: 1074.0\n",
      "lif layer 2 self.abs_max_v: 1738.5\n",
      "fc layer 2 self.abs_max_out: 1101.0\n",
      "smallest_now_T updated: 65\n",
      "fc layer 2 self.abs_max_out: 1147.0\n",
      "fc layer 1 self.abs_max_out: 2720.0\n",
      "lif layer 1 self.abs_max_v: 2720.0\n",
      "fc layer 2 self.abs_max_out: 1177.0\n",
      "fc layer 2 self.abs_max_out: 1236.0\n",
      "fc layer 2 self.abs_max_out: 1286.0\n",
      "lif layer 1 self.abs_max_v: 2724.0\n",
      "lif layer 1 self.abs_max_v: 2729.5\n",
      "lif layer 1 self.abs_max_v: 2757.0\n",
      "smallest_now_T updated: 56\n",
      "smallest_now_T updated: 50\n",
      "lif layer 1 self.abs_max_v: 2786.0\n",
      "lif layer 1 self.abs_max_v: 2909.0\n",
      "lif layer 1 self.abs_max_v: 3013.0\n",
      "lif layer 1 self.abs_max_v: 3018.5\n",
      "lif layer 1 self.abs_max_v: 3174.5\n",
      "fc layer 1 self.abs_max_out: 2748.0\n",
      "lif layer 1 self.abs_max_v: 3236.0\n",
      "lif layer 1 self.abs_max_v: 3279.0\n",
      "fc layer 1 self.abs_max_out: 2914.0\n",
      "lif layer 1 self.abs_max_v: 3349.5\n",
      "lif layer 1 self.abs_max_v: 3445.0\n",
      "lif layer 1 self.abs_max_v: 3586.5\n",
      "fc layer 1 self.abs_max_out: 3009.0\n",
      "fc layer 1 self.abs_max_out: 3038.0\n",
      "lif layer 1 self.abs_max_v: 3622.0\n",
      "lif layer 1 self.abs_max_v: 3693.5\n",
      "lif layer 1 self.abs_max_v: 3852.5\n",
      "lif layer 1 self.abs_max_v: 4002.5\n",
      "fc layer 1 self.abs_max_out: 3040.0\n",
      "fc layer 1 self.abs_max_out: 3088.0\n",
      "fc layer 1 self.abs_max_out: 3108.0\n",
      "lif layer 1 self.abs_max_v: 4027.5\n",
      "lif layer 1 self.abs_max_v: 4047.0\n",
      "lif layer 1 self.abs_max_v: 4210.5\n",
      "fc layer 1 self.abs_max_out: 3450.0\n",
      "lif layer 1 self.abs_max_v: 4332.5\n",
      "lif layer 1 self.abs_max_v: 4691.5\n",
      "fc layer 1 self.abs_max_out: 3605.0\n",
      "lif layer 1 self.abs_max_v: 4853.0\n",
      "lif layer 1 self.abs_max_v: 4863.5\n",
      "lif layer 1 self.abs_max_v: 4932.0\n",
      "smallest_now_T_val updated: 129\n",
      "smallest_now_T_val updated: 106\n",
      "smallest_now_T_val updated: 104\n",
      "smallest_now_T_val updated: 102\n",
      "smallest_now_T_val updated: 85\n",
      "smallest_now_T_val updated: 50\n",
      "fc layer 1 self.abs_max_out: 3666.0\n",
      "lif layer 1 self.abs_max_v: 5007.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  2.220710/  2.225088, val:  40.42%, val_best:  40.42%, tr:  64.86%, tr_best:  64.86%, epoch time: 83.83 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 82.0943%\n",
      "layer   2  Sparsity: 89.9960%\n",
      "layer   3  Sparsity: 95.5870%\n",
      "total_backward_count 9790 real_backward_count 5304  54.178%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1311.0\n",
      "fc layer 2 self.abs_max_out: 1312.0\n",
      "fc layer 2 self.abs_max_out: 1409.0\n",
      "fc layer 2 self.abs_max_out: 1538.0\n",
      "lif layer 1 self.abs_max_v: 5139.0\n",
      "lif layer 1 self.abs_max_v: 5466.5\n",
      "fc layer 1 self.abs_max_out: 3685.0\n",
      "fc layer 2 self.abs_max_out: 1545.0\n",
      "fc layer 2 self.abs_max_out: 1550.0\n",
      "fc layer 2 self.abs_max_out: 1661.0\n",
      "fc layer 2 self.abs_max_out: 1718.0\n",
      "lif layer 1 self.abs_max_v: 5541.5\n",
      "lif layer 1 self.abs_max_v: 5876.0\n",
      "lif layer 1 self.abs_max_v: 5927.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  2.192643/  2.219036, val:  42.92%, val_best:  42.92%, tr:  91.83%, tr_best:  91.83%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.1209%\n",
      "layer   2  Sparsity: 89.9252%\n",
      "layer   3  Sparsity: 94.0427%\n",
      "total_backward_count 19580 real_backward_count 8389  42.845%\n",
      "fc layer 1 self.abs_max_out: 3900.0\n",
      "fc layer 1 self.abs_max_out: 4051.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  2.202474/  2.219255, val:  45.00%, val_best:  45.00%, tr:  93.16%, tr_best:  93.16%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.0836%\n",
      "layer   2  Sparsity: 90.6436%\n",
      "layer   3  Sparsity: 93.9925%\n",
      "total_backward_count 29370 real_backward_count 11195  38.117%\n",
      "lif layer 1 self.abs_max_v: 6080.5\n",
      "lif layer 1 self.abs_max_v: 6109.5\n",
      "lif layer 1 self.abs_max_v: 6362.0\n",
      "lif layer 1 self.abs_max_v: 6430.0\n",
      "lif layer 1 self.abs_max_v: 6434.5\n",
      "lif layer 2 self.abs_max_v: 1787.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  2.195617/  2.219926, val:  41.25%, val_best:  45.00%, tr:  95.40%, tr_best:  95.40%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1065%\n",
      "layer   2  Sparsity: 91.1429%\n",
      "layer   3  Sparsity: 93.3659%\n",
      "total_backward_count 39160 real_backward_count 13852  35.373%\n",
      "lif layer 1 self.abs_max_v: 6458.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  2.182809/  2.216243, val:  45.00%, val_best:  45.00%, tr:  97.24%, tr_best:  97.24%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1033%\n",
      "layer   2  Sparsity: 90.2393%\n",
      "layer   3  Sparsity: 92.1385%\n",
      "total_backward_count 48950 real_backward_count 16146  32.985%\n",
      "fc layer 1 self.abs_max_out: 4245.0\n",
      "lif layer 2 self.abs_max_v: 1801.5\n",
      "lif layer 1 self.abs_max_v: 6527.5\n",
      "lif layer 1 self.abs_max_v: 6978.5\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  2.180732/  2.222642, val:  46.67%, val_best:  46.67%, tr:  98.26%, tr_best:  98.26%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0955%\n",
      "layer   2  Sparsity: 89.0053%\n",
      "layer   3  Sparsity: 91.5026%\n",
      "total_backward_count 58740 real_backward_count 18235  31.044%\n",
      "fc layer 2 self.abs_max_out: 1724.0\n",
      "fc layer 1 self.abs_max_out: 4383.0\n",
      "fc layer 2 self.abs_max_out: 1782.0\n",
      "lif layer 1 self.abs_max_v: 7010.5\n",
      "lif layer 1 self.abs_max_v: 7067.5\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  2.180922/  2.214268, val:  45.42%, val_best:  46.67%, tr:  97.96%, tr_best:  98.26%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1092%\n",
      "layer   2  Sparsity: 89.3895%\n",
      "layer   3  Sparsity: 91.0274%\n",
      "total_backward_count 68530 real_backward_count 20202  29.479%\n",
      "lif layer 2 self.abs_max_v: 1860.0\n",
      "fc layer 2 self.abs_max_out: 1793.0\n",
      "fc layer 2 self.abs_max_out: 1801.0\n",
      "fc layer 1 self.abs_max_out: 4636.0\n",
      "fc layer 2 self.abs_max_out: 1809.0\n",
      "fc layer 2 self.abs_max_out: 1888.0\n",
      "lif layer 2 self.abs_max_v: 1888.0\n",
      "fc layer 2 self.abs_max_out: 1901.0\n",
      "lif layer 2 self.abs_max_v: 1901.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  2.166714/  2.194292, val:  47.08%, val_best:  47.08%, tr:  99.18%, tr_best:  99.18%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1030%\n",
      "layer   2  Sparsity: 88.3012%\n",
      "layer   3  Sparsity: 90.0333%\n",
      "total_backward_count 78320 real_backward_count 22069  28.178%\n",
      "fc layer 1 self.abs_max_out: 4656.0\n",
      "fc layer 1 self.abs_max_out: 4774.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  2.155503/  2.189987, val:  50.83%, val_best:  50.83%, tr:  98.37%, tr_best:  99.18%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.1011%\n",
      "layer   2  Sparsity: 88.7482%\n",
      "layer   3  Sparsity: 90.1784%\n",
      "total_backward_count 88110 real_backward_count 24036  27.280%\n",
      "fc layer 1 self.abs_max_out: 4926.0\n",
      "fc layer 1 self.abs_max_out: 5210.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  2.153287/  2.207782, val:  47.50%, val_best:  50.83%, tr:  98.67%, tr_best:  99.18%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.0770%\n",
      "layer   2  Sparsity: 88.8449%\n",
      "layer   3  Sparsity: 90.2980%\n",
      "total_backward_count 97900 real_backward_count 25873  26.428%\n",
      "fc layer 3 self.abs_max_out: 281.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  2.157523/  2.192951, val:  44.58%, val_best:  50.83%, tr:  98.57%, tr_best:  99.18%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.0151%\n",
      "layer   2  Sparsity: 88.9164%\n",
      "layer   3  Sparsity: 89.8117%\n",
      "total_backward_count 107690 real_backward_count 27751  25.769%\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  2.147741/  2.181532, val:  52.50%, val_best:  52.50%, tr:  98.88%, tr_best:  99.18%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1661%\n",
      "layer   2  Sparsity: 88.5864%\n",
      "layer   3  Sparsity: 89.1011%\n",
      "total_backward_count 117480 real_backward_count 29563  25.164%\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  2.134002/  2.193965, val:  46.67%, val_best:  52.50%, tr:  99.08%, tr_best:  99.18%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1374%\n",
      "layer   2  Sparsity: 87.7413%\n",
      "layer   3  Sparsity: 88.6963%\n",
      "total_backward_count 127270 real_backward_count 31246  24.551%\n",
      "lif layer 1 self.abs_max_v: 7541.5\n",
      "lif layer 1 self.abs_max_v: 7627.0\n",
      "lif layer 1 self.abs_max_v: 7995.5\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  2.147262/  2.191733, val:  42.08%, val_best:  52.50%, tr:  99.39%, tr_best:  99.39%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.1023%\n",
      "layer   2  Sparsity: 87.9699%\n",
      "layer   3  Sparsity: 89.3116%\n",
      "total_backward_count 137060 real_backward_count 32942  24.035%\n",
      "fc layer 2 self.abs_max_out: 1937.0\n",
      "lif layer 2 self.abs_max_v: 1937.0\n",
      "fc layer 1 self.abs_max_out: 5284.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  2.139155/  2.185246, val:  46.67%, val_best:  52.50%, tr:  99.18%, tr_best:  99.39%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.0505%\n",
      "layer   2  Sparsity: 87.1294%\n",
      "layer   3  Sparsity: 88.5779%\n",
      "total_backward_count 146850 real_backward_count 34629  23.581%\n",
      "fc layer 1 self.abs_max_out: 5660.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  2.144060/  2.188635, val:  53.33%, val_best:  53.33%, tr:  99.49%, tr_best:  99.49%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.0685%\n",
      "layer   2  Sparsity: 87.1606%\n",
      "layer   3  Sparsity: 88.7685%\n",
      "total_backward_count 156640 real_backward_count 36309  23.180%\n",
      "lif layer 2 self.abs_max_v: 1949.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  2.146853/  2.182986, val:  52.50%, val_best:  53.33%, tr:  99.08%, tr_best:  99.49%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0614%\n",
      "layer   2  Sparsity: 87.4149%\n",
      "layer   3  Sparsity: 88.8436%\n",
      "total_backward_count 166430 real_backward_count 37915  22.781%\n",
      "fc layer 2 self.abs_max_out: 2006.0\n",
      "lif layer 2 self.abs_max_v: 2006.0\n",
      "fc layer 2 self.abs_max_out: 2032.0\n",
      "lif layer 2 self.abs_max_v: 2032.0\n",
      "fc layer 2 self.abs_max_out: 2049.0\n",
      "lif layer 2 self.abs_max_v: 2049.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  2.136256/  2.186635, val:  57.92%, val_best:  57.92%, tr:  99.28%, tr_best:  99.49%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0918%\n",
      "layer   2  Sparsity: 88.0353%\n",
      "layer   3  Sparsity: 88.7735%\n",
      "total_backward_count 176220 real_backward_count 39580  22.461%\n",
      "lif layer 1 self.abs_max_v: 8113.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  2.141882/  2.184419, val:  50.83%, val_best:  57.92%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0559%\n",
      "layer   2  Sparsity: 87.8313%\n",
      "layer   3  Sparsity: 88.7116%\n",
      "total_backward_count 186010 real_backward_count 41279  22.192%\n",
      "lif layer 2 self.abs_max_v: 2090.5\n",
      "lif layer 1 self.abs_max_v: 8317.5\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  2.140800/  2.183280, val:  44.17%, val_best:  57.92%, tr:  99.08%, tr_best:  99.59%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.0924%\n",
      "layer   2  Sparsity: 87.7978%\n",
      "layer   3  Sparsity: 88.4156%\n",
      "total_backward_count 195800 real_backward_count 42812  21.865%\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  2.133869/  2.187747, val:  46.67%, val_best:  57.92%, tr:  99.28%, tr_best:  99.59%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.0562%\n",
      "layer   2  Sparsity: 87.5368%\n",
      "layer   3  Sparsity: 88.3156%\n",
      "total_backward_count 205590 real_backward_count 44356  21.575%\n",
      "lif layer 2 self.abs_max_v: 2131.5\n",
      "lif layer 2 self.abs_max_v: 2206.0\n",
      "lif layer 2 self.abs_max_v: 2220.0\n",
      "lif layer 1 self.abs_max_v: 8438.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  2.133617/  2.182143, val:  54.58%, val_best:  57.92%, tr:  99.08%, tr_best:  99.59%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 82.0634%\n",
      "layer   2  Sparsity: 87.3462%\n",
      "layer   3  Sparsity: 88.7279%\n",
      "total_backward_count 215380 real_backward_count 45991  21.353%\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  2.135969/  2.174151, val:  53.33%, val_best:  57.92%, tr:  98.88%, tr_best:  99.59%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0737%\n",
      "layer   2  Sparsity: 87.5717%\n",
      "layer   3  Sparsity: 88.8850%\n",
      "total_backward_count 225170 real_backward_count 47626  21.151%\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  2.145716/  2.183043, val:  58.33%, val_best:  58.33%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0750%\n",
      "layer   2  Sparsity: 87.8494%\n",
      "layer   3  Sparsity: 89.2702%\n",
      "total_backward_count 234960 real_backward_count 49200  20.940%\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  2.143314/  2.181847, val:  61.67%, val_best:  61.67%, tr:  99.49%, tr_best:  99.69%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0924%\n",
      "layer   2  Sparsity: 87.6543%\n",
      "layer   3  Sparsity: 88.6069%\n",
      "total_backward_count 244750 real_backward_count 50793  20.753%\n",
      "lif layer 1 self.abs_max_v: 8462.5\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  2.137988/  2.172951, val:  62.92%, val_best:  62.92%, tr:  99.59%, tr_best:  99.69%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.1084%\n",
      "layer   2  Sparsity: 87.8140%\n",
      "layer   3  Sparsity: 88.4801%\n",
      "total_backward_count 254540 real_backward_count 52380  20.578%\n",
      "lif layer 1 self.abs_max_v: 8518.0\n",
      "fc layer 2 self.abs_max_out: 2071.0\n",
      "lif layer 1 self.abs_max_v: 8522.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  2.133973/  2.174929, val:  52.08%, val_best:  62.92%, tr:  99.08%, tr_best:  99.69%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1095%\n",
      "layer   2  Sparsity: 87.8180%\n",
      "layer   3  Sparsity: 88.1428%\n",
      "total_backward_count 264330 real_backward_count 53910  20.395%\n",
      "lif layer 1 self.abs_max_v: 8892.5\n",
      "lif layer 1 self.abs_max_v: 8996.5\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  2.137139/  2.179672, val:  60.42%, val_best:  62.92%, tr:  99.28%, tr_best:  99.69%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0840%\n",
      "layer   2  Sparsity: 87.8072%\n",
      "layer   3  Sparsity: 88.4864%\n",
      "total_backward_count 274120 real_backward_count 55498  20.246%\n",
      "lif layer 2 self.abs_max_v: 2328.0\n",
      "fc layer 2 self.abs_max_out: 2102.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  2.135852/  2.181328, val:  55.00%, val_best:  62.92%, tr:  99.49%, tr_best:  99.69%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0568%\n",
      "layer   2  Sparsity: 87.4238%\n",
      "layer   3  Sparsity: 87.9534%\n",
      "total_backward_count 283910 real_backward_count 56989  20.073%\n",
      "fc layer 2 self.abs_max_out: 2146.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  2.138664/  2.178170, val:  55.42%, val_best:  62.92%, tr:  99.28%, tr_best:  99.69%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0904%\n",
      "layer   2  Sparsity: 87.7516%\n",
      "layer   3  Sparsity: 88.2733%\n",
      "total_backward_count 293700 real_backward_count 58463  19.906%\n",
      "fc layer 2 self.abs_max_out: 2182.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  2.137180/  2.178406, val:  57.50%, val_best:  62.92%, tr:  99.18%, tr_best:  99.69%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0842%\n",
      "layer   2  Sparsity: 88.0609%\n",
      "layer   3  Sparsity: 88.2746%\n",
      "total_backward_count 303490 real_backward_count 59943  19.751%\n",
      "fc layer 2 self.abs_max_out: 2250.0\n",
      "fc layer 2 self.abs_max_out: 2258.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  2.127892/  2.185692, val:  48.75%, val_best:  62.92%, tr:  99.59%, tr_best:  99.69%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1433%\n",
      "layer   2  Sparsity: 88.0928%\n",
      "layer   3  Sparsity: 87.7474%\n",
      "total_backward_count 313280 real_backward_count 61375  19.591%\n",
      "lif layer 1 self.abs_max_v: 9080.5\n",
      "lif layer 1 self.abs_max_v: 9364.5\n",
      "fc layer 1 self.abs_max_out: 5863.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  2.128139/  2.180732, val:  49.17%, val_best:  62.92%, tr:  99.80%, tr_best:  99.80%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.1230%\n",
      "layer   2  Sparsity: 87.9961%\n",
      "layer   3  Sparsity: 87.9206%\n",
      "total_backward_count 323070 real_backward_count 62752  19.424%\n",
      "fc layer 2 self.abs_max_out: 2267.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  2.123151/  2.166053, val:  53.33%, val_best:  62.92%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1204%\n",
      "layer   2  Sparsity: 88.0490%\n",
      "layer   3  Sparsity: 88.0236%\n",
      "total_backward_count 332860 real_backward_count 64214  19.292%\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  2.122082/  2.168002, val:  52.08%, val_best:  62.92%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0683%\n",
      "layer   2  Sparsity: 88.0301%\n",
      "layer   3  Sparsity: 88.0632%\n",
      "total_backward_count 342650 real_backward_count 65694  19.172%\n",
      "fc layer 2 self.abs_max_out: 2353.0\n",
      "lif layer 2 self.abs_max_v: 2353.0\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  2.129642/  2.186731, val:  54.58%, val_best:  62.92%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0871%\n",
      "layer   2  Sparsity: 88.2180%\n",
      "layer   3  Sparsity: 88.5417%\n",
      "total_backward_count 352440 real_backward_count 67129  19.047%\n",
      "fc layer 1 self.abs_max_out: 6025.0\n",
      "fc layer 1 self.abs_max_out: 6067.0\n",
      "fc layer 1 self.abs_max_out: 6093.0\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  2.141176/  2.181763, val:  59.58%, val_best:  62.92%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1474%\n",
      "layer   2  Sparsity: 88.2799%\n",
      "layer   3  Sparsity: 89.1676%\n",
      "total_backward_count 362230 real_backward_count 68514  18.915%\n",
      "fc layer 1 self.abs_max_out: 6143.0\n",
      "fc layer 1 self.abs_max_out: 6178.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  2.136036/  2.175538, val:  60.00%, val_best:  62.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.0674%\n",
      "layer   2  Sparsity: 88.1680%\n",
      "layer   3  Sparsity: 88.7926%\n",
      "total_backward_count 372020 real_backward_count 69922  18.795%\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  2.124232/  2.164511, val:  59.58%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1351%\n",
      "layer   2  Sparsity: 87.9947%\n",
      "layer   3  Sparsity: 87.6647%\n",
      "total_backward_count 381810 real_backward_count 71298  18.674%\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  2.119758/  2.171277, val:  56.25%, val_best:  62.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0811%\n",
      "layer   2  Sparsity: 87.5790%\n",
      "layer   3  Sparsity: 87.9317%\n",
      "total_backward_count 391600 real_backward_count 72648  18.552%\n",
      "fc layer 2 self.abs_max_out: 2361.0\n",
      "lif layer 2 self.abs_max_v: 2361.0\n",
      "lif layer 2 self.abs_max_v: 2460.0\n",
      "fc layer 2 self.abs_max_out: 2402.0\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  2.120805/  2.170080, val:  61.25%, val_best:  62.92%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1062%\n",
      "layer   2  Sparsity: 87.7581%\n",
      "layer   3  Sparsity: 88.3378%\n",
      "total_backward_count 401390 real_backward_count 74037  18.445%\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  2.128799/  2.169839, val:  67.08%, val_best:  67.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.1404%\n",
      "layer   2  Sparsity: 87.9210%\n",
      "layer   3  Sparsity: 88.5850%\n",
      "total_backward_count 411180 real_backward_count 75367  18.329%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  2.122565/  2.167110, val:  60.83%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1202%\n",
      "layer   2  Sparsity: 87.7681%\n",
      "layer   3  Sparsity: 88.6102%\n",
      "total_backward_count 420970 real_backward_count 76689  18.217%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  2.119140/  2.164141, val:  64.58%, val_best:  67.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0399%\n",
      "layer   2  Sparsity: 87.7538%\n",
      "layer   3  Sparsity: 88.4118%\n",
      "total_backward_count 430760 real_backward_count 78010  18.110%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  2.121293/  2.174499, val:  58.33%, val_best:  67.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.1697%\n",
      "layer   2  Sparsity: 87.5981%\n",
      "layer   3  Sparsity: 88.5646%\n",
      "total_backward_count 440550 real_backward_count 79333  18.008%\n",
      "lif layer 2 self.abs_max_v: 2474.5\n",
      "fc layer 1 self.abs_max_out: 6230.0\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  2.129866/  2.173489, val:  51.67%, val_best:  67.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1037%\n",
      "layer   2  Sparsity: 87.8532%\n",
      "layer   3  Sparsity: 89.0856%\n",
      "total_backward_count 450340 real_backward_count 80689  17.917%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  2.119875/  2.159089, val:  58.33%, val_best:  67.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0804%\n",
      "layer   2  Sparsity: 87.9315%\n",
      "layer   3  Sparsity: 88.2594%\n",
      "total_backward_count 460130 real_backward_count 82008  17.823%\n",
      "lif layer 1 self.abs_max_v: 9654.0\n",
      "fc layer 1 self.abs_max_out: 6321.0\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  2.103794/  2.160628, val:  54.58%, val_best:  67.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0792%\n",
      "layer   2  Sparsity: 87.8062%\n",
      "layer   3  Sparsity: 87.7676%\n",
      "total_backward_count 469920 real_backward_count 83296  17.726%\n",
      "fc layer 1 self.abs_max_out: 6337.0\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  2.102937/  2.167337, val:  56.25%, val_best:  67.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1298%\n",
      "layer   2  Sparsity: 87.6196%\n",
      "layer   3  Sparsity: 87.5103%\n",
      "total_backward_count 479710 real_backward_count 84506  17.616%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  2.121371/  2.167404, val:  58.33%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1280%\n",
      "layer   2  Sparsity: 87.7692%\n",
      "layer   3  Sparsity: 88.6180%\n",
      "total_backward_count 489500 real_backward_count 85760  17.520%\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  2.122238/  2.164479, val:  55.00%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0882%\n",
      "layer   2  Sparsity: 87.3769%\n",
      "layer   3  Sparsity: 88.6048%\n",
      "total_backward_count 499290 real_backward_count 87042  17.433%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  2.120889/  2.166216, val:  67.92%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1341%\n",
      "layer   2  Sparsity: 87.1717%\n",
      "layer   3  Sparsity: 88.6084%\n",
      "total_backward_count 509080 real_backward_count 88240  17.333%\n",
      "lif layer 2 self.abs_max_v: 2536.0\n",
      "lif layer 2 self.abs_max_v: 2632.5\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  2.118770/  2.158622, val:  60.83%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0713%\n",
      "layer   2  Sparsity: 87.4033%\n",
      "layer   3  Sparsity: 88.3588%\n",
      "total_backward_count 518870 real_backward_count 89581  17.265%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  2.110216/  2.156652, val:  59.58%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0958%\n",
      "layer   2  Sparsity: 87.5557%\n",
      "layer   3  Sparsity: 87.9872%\n",
      "total_backward_count 528660 real_backward_count 90824  17.180%\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  2.121677/  2.174719, val:  60.00%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0618%\n",
      "layer   2  Sparsity: 87.6438%\n",
      "layer   3  Sparsity: 88.6588%\n",
      "total_backward_count 538450 real_backward_count 92139  17.112%\n",
      "fc layer 1 self.abs_max_out: 6405.0\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  2.127252/  2.177809, val:  63.33%, val_best:  67.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1108%\n",
      "layer   2  Sparsity: 88.0281%\n",
      "layer   3  Sparsity: 89.0304%\n",
      "total_backward_count 548240 real_backward_count 93418  17.040%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  2.131187/  2.176568, val:  62.50%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0952%\n",
      "layer   2  Sparsity: 87.8993%\n",
      "layer   3  Sparsity: 89.2605%\n",
      "total_backward_count 558030 real_backward_count 94655  16.962%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  2.123836/  2.174305, val:  56.25%, val_best:  67.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0952%\n",
      "layer   2  Sparsity: 87.6861%\n",
      "layer   3  Sparsity: 88.6469%\n",
      "total_backward_count 567820 real_backward_count 95826  16.876%\n",
      "fc layer 1 self.abs_max_out: 6573.0\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  2.120883/  2.164763, val:  60.00%, val_best:  67.92%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0892%\n",
      "layer   2  Sparsity: 87.4805%\n",
      "layer   3  Sparsity: 88.6123%\n",
      "total_backward_count 577610 real_backward_count 96997  16.793%\n",
      "fc layer 1 self.abs_max_out: 6579.0\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  2.116661/  2.161769, val:  61.25%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.0589%\n",
      "layer   2  Sparsity: 87.3053%\n",
      "layer   3  Sparsity: 88.3715%\n",
      "total_backward_count 587400 real_backward_count 98197  16.717%\n",
      "lif layer 2 self.abs_max_v: 2633.0\n",
      "lif layer 2 self.abs_max_v: 2684.0\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  2.110361/  2.163659, val:  55.83%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0430%\n",
      "layer   2  Sparsity: 87.8043%\n",
      "layer   3  Sparsity: 88.5909%\n",
      "total_backward_count 597190 real_backward_count 99399  16.644%\n",
      "lif layer 1 self.abs_max_v: 10205.5\n",
      "lif layer 2 self.abs_max_v: 2775.5\n",
      "lif layer 2 self.abs_max_v: 2795.0\n",
      "lif layer 2 self.abs_max_v: 2854.5\n",
      "lif layer 2 self.abs_max_v: 2870.5\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  2.106518/  2.155538, val:  60.83%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.1087%\n",
      "layer   2  Sparsity: 87.7258%\n",
      "layer   3  Sparsity: 88.5201%\n",
      "total_backward_count 606980 real_backward_count 100636  16.580%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  2.108057/  2.172281, val:  59.58%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0857%\n",
      "layer   2  Sparsity: 87.7719%\n",
      "layer   3  Sparsity: 88.5031%\n",
      "total_backward_count 616770 real_backward_count 101839  16.512%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  2.117851/  2.164036, val:  58.33%, val_best:  67.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0884%\n",
      "layer   2  Sparsity: 88.0557%\n",
      "layer   3  Sparsity: 88.8208%\n",
      "total_backward_count 626560 real_backward_count 103035  16.445%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  2.112712/  2.157737, val:  64.17%, val_best:  67.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1111%\n",
      "layer   2  Sparsity: 88.3822%\n",
      "layer   3  Sparsity: 88.7268%\n",
      "total_backward_count 636350 real_backward_count 104268  16.385%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  2.112311/  2.168518, val:  68.75%, val_best:  68.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.0949%\n",
      "layer   2  Sparsity: 88.1612%\n",
      "layer   3  Sparsity: 88.7470%\n",
      "total_backward_count 646140 real_backward_count 105429  16.317%\n",
      "lif layer 1 self.abs_max_v: 10380.0\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  2.111119/  2.156155, val:  66.25%, val_best:  68.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.0732%\n",
      "layer   2  Sparsity: 88.0644%\n",
      "layer   3  Sparsity: 88.4855%\n",
      "total_backward_count 655930 real_backward_count 106621  16.255%\n",
      "lif layer 1 self.abs_max_v: 10752.5\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  2.102461/  2.155377, val:  62.50%, val_best:  68.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0994%\n",
      "layer   2  Sparsity: 88.0360%\n",
      "layer   3  Sparsity: 88.3705%\n",
      "total_backward_count 665720 real_backward_count 107810  16.194%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  2.104816/  2.155367, val:  69.17%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0778%\n",
      "layer   2  Sparsity: 88.0812%\n",
      "layer   3  Sparsity: 88.2467%\n",
      "total_backward_count 675510 real_backward_count 108934  16.126%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  2.110350/  2.162810, val:  57.50%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0668%\n",
      "layer   2  Sparsity: 88.1430%\n",
      "layer   3  Sparsity: 88.5249%\n",
      "total_backward_count 685300 real_backward_count 110083  16.063%\n",
      "fc layer 3 self.abs_max_out: 286.0\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  2.104151/  2.150391, val:  67.08%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 82.1177%\n",
      "layer   2  Sparsity: 88.2439%\n",
      "layer   3  Sparsity: 88.3811%\n",
      "total_backward_count 695090 real_backward_count 111286  16.010%\n",
      "fc layer 3 self.abs_max_out: 297.0\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  2.103833/  2.161089, val:  62.08%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.1033%\n",
      "layer   2  Sparsity: 88.6253%\n",
      "layer   3  Sparsity: 88.2807%\n",
      "total_backward_count 704880 real_backward_count 112466  15.955%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  2.094929/  2.136885, val:  66.25%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0735%\n",
      "layer   2  Sparsity: 89.0258%\n",
      "layer   3  Sparsity: 87.6911%\n",
      "total_backward_count 714670 real_backward_count 113610  15.897%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  2.086569/  2.148415, val:  61.67%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 82.1286%\n",
      "layer   2  Sparsity: 88.7672%\n",
      "layer   3  Sparsity: 87.7479%\n",
      "total_backward_count 724460 real_backward_count 114692  15.831%\n",
      "fc layer 2 self.abs_max_out: 2411.0\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  2.099042/  2.148262, val:  64.58%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.0596%\n",
      "layer   2  Sparsity: 88.8750%\n",
      "layer   3  Sparsity: 88.3806%\n",
      "total_backward_count 734250 real_backward_count 115900  15.785%\n",
      "lif layer 1 self.abs_max_v: 10823.0\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  2.100138/  2.150216, val:  68.75%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.0610%\n",
      "layer   2  Sparsity: 88.7281%\n",
      "layer   3  Sparsity: 88.7806%\n",
      "total_backward_count 744040 real_backward_count 117014  15.727%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  2.101340/  2.153975, val:  67.08%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.01 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 82.0766%\n",
      "layer   2  Sparsity: 88.9226%\n",
      "layer   3  Sparsity: 88.5447%\n",
      "total_backward_count 753830 real_backward_count 118186  15.678%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  2.102826/  2.163099, val:  56.25%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.0990%\n",
      "layer   2  Sparsity: 88.8527%\n",
      "layer   3  Sparsity: 88.3202%\n",
      "total_backward_count 763620 real_backward_count 119308  15.624%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  2.095278/  2.147308, val:  59.58%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.0678%\n",
      "layer   2  Sparsity: 88.8478%\n",
      "layer   3  Sparsity: 88.2671%\n",
      "total_backward_count 773410 real_backward_count 120420  15.570%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  2.094614/  2.151447, val:  69.17%, val_best:  69.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0865%\n",
      "layer   2  Sparsity: 88.6633%\n",
      "layer   3  Sparsity: 88.4102%\n",
      "total_backward_count 783200 real_backward_count 121477  15.510%\n",
      "lif layer 1 self.abs_max_v: 10858.0\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  2.100119/  2.144020, val:  67.08%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0668%\n",
      "layer   2  Sparsity: 88.5673%\n",
      "layer   3  Sparsity: 88.5171%\n",
      "total_backward_count 792990 real_backward_count 122563  15.456%\n",
      "fc layer 1 self.abs_max_out: 6706.0\n",
      "fc layer 1 self.abs_max_out: 6805.0\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  2.104969/  2.155757, val:  56.67%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1173%\n",
      "layer   2  Sparsity: 88.6975%\n",
      "layer   3  Sparsity: 88.8124%\n",
      "total_backward_count 802780 real_backward_count 123654  15.403%\n",
      "lif layer 1 self.abs_max_v: 10884.5\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  2.106576/  2.156731, val:  60.83%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.1084%\n",
      "layer   2  Sparsity: 88.8947%\n",
      "layer   3  Sparsity: 88.6850%\n",
      "total_backward_count 812570 real_backward_count 124725  15.349%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  2.105657/  2.159101, val:  58.33%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0873%\n",
      "layer   2  Sparsity: 88.6743%\n",
      "layer   3  Sparsity: 88.5349%\n",
      "total_backward_count 822360 real_backward_count 125807  15.298%\n",
      "fc layer 2 self.abs_max_out: 2420.0\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  2.105374/  2.155541, val:  65.00%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1451%\n",
      "layer   2  Sparsity: 88.6993%\n",
      "layer   3  Sparsity: 88.3751%\n",
      "total_backward_count 832150 real_backward_count 126872  15.246%\n",
      "fc layer 3 self.abs_max_out: 306.0\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  2.094836/  2.152274, val:  69.17%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0708%\n",
      "layer   2  Sparsity: 88.7594%\n",
      "layer   3  Sparsity: 87.9956%\n",
      "total_backward_count 841940 real_backward_count 127958  15.198%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  2.101122/  2.151150, val:  62.92%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.0933%\n",
      "layer   2  Sparsity: 88.8972%\n",
      "layer   3  Sparsity: 88.3858%\n",
      "total_backward_count 851730 real_backward_count 129020  15.148%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  2.101461/  2.153242, val:  63.75%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.0895%\n",
      "layer   2  Sparsity: 88.8266%\n",
      "layer   3  Sparsity: 88.7824%\n",
      "total_backward_count 861520 real_backward_count 130092  15.100%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  2.099403/  2.150347, val:  60.00%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.0999%\n",
      "layer   2  Sparsity: 88.8724%\n",
      "layer   3  Sparsity: 88.7008%\n",
      "total_backward_count 871310 real_backward_count 131170  15.054%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  2.098600/  2.151958, val:  67.92%, val_best:  69.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0986%\n",
      "layer   2  Sparsity: 89.1029%\n",
      "layer   3  Sparsity: 88.9278%\n",
      "total_backward_count 881100 real_backward_count 132271  15.012%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  2.094491/  2.154005, val:  66.67%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0674%\n",
      "layer   2  Sparsity: 89.2971%\n",
      "layer   3  Sparsity: 89.0336%\n",
      "total_backward_count 890890 real_backward_count 133345  14.968%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  2.100029/  2.146317, val:  72.08%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1089%\n",
      "layer   2  Sparsity: 89.2276%\n",
      "layer   3  Sparsity: 88.9190%\n",
      "total_backward_count 900680 real_backward_count 134343  14.916%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  2.095006/  2.147893, val:  65.83%, val_best:  72.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1276%\n",
      "layer   2  Sparsity: 89.2470%\n",
      "layer   3  Sparsity: 88.9563%\n",
      "total_backward_count 910470 real_backward_count 135337  14.865%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  2.102973/  2.159547, val:  69.17%, val_best:  72.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1168%\n",
      "layer   2  Sparsity: 89.5668%\n",
      "layer   3  Sparsity: 89.3775%\n",
      "total_backward_count 920260 real_backward_count 136366  14.818%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  2.110092/  2.153292, val:  62.92%, val_best:  72.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1201%\n",
      "layer   2  Sparsity: 89.4805%\n",
      "layer   3  Sparsity: 88.7082%\n",
      "total_backward_count 930050 real_backward_count 137438  14.777%\n",
      "lif layer 1 self.abs_max_v: 10918.5\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  2.096817/  2.141317, val:  72.08%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0652%\n",
      "layer   2  Sparsity: 89.3515%\n",
      "layer   3  Sparsity: 88.3034%\n",
      "total_backward_count 939840 real_backward_count 138475  14.734%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  2.088682/  2.138456, val:  62.08%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0786%\n",
      "layer   2  Sparsity: 89.4384%\n",
      "layer   3  Sparsity: 88.3217%\n",
      "total_backward_count 949630 real_backward_count 139502  14.690%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  2.093666/  2.152471, val:  67.50%, val_best:  72.08%, tr:  99.39%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0766%\n",
      "layer   2  Sparsity: 89.6030%\n",
      "layer   3  Sparsity: 88.7174%\n",
      "total_backward_count 959420 real_backward_count 140550  14.649%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  2.096992/  2.169455, val:  65.00%, val_best:  72.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.0844%\n",
      "layer   2  Sparsity: 89.6122%\n",
      "layer   3  Sparsity: 89.1507%\n",
      "total_backward_count 969210 real_backward_count 141619  14.612%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  2.105289/  2.159156, val:  64.58%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.0698%\n",
      "layer   2  Sparsity: 89.5224%\n",
      "layer   3  Sparsity: 88.9060%\n",
      "total_backward_count 979000 real_backward_count 142694  14.575%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  2.105901/  2.160407, val:  62.08%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0380%\n",
      "layer   2  Sparsity: 89.6501%\n",
      "layer   3  Sparsity: 88.6998%\n",
      "total_backward_count 988790 real_backward_count 143736  14.537%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  2.094890/  2.149601, val:  65.83%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0846%\n",
      "layer   2  Sparsity: 89.6465%\n",
      "layer   3  Sparsity: 88.5303%\n",
      "total_backward_count 998580 real_backward_count 144766  14.497%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  2.104201/  2.158525, val:  70.83%, val_best:  72.08%, tr:  99.39%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0833%\n",
      "layer   2  Sparsity: 89.6719%\n",
      "layer   3  Sparsity: 88.9654%\n",
      "total_backward_count 1008370 real_backward_count 145793  14.458%\n",
      "lif layer 1 self.abs_max_v: 11038.5\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  2.100837/  2.155771, val:  63.75%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0572%\n",
      "layer   2  Sparsity: 89.6487%\n",
      "layer   3  Sparsity: 88.5937%\n",
      "total_backward_count 1018160 real_backward_count 146787  14.417%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  2.109088/  2.162093, val:  66.67%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.0931%\n",
      "layer   2  Sparsity: 89.7438%\n",
      "layer   3  Sparsity: 88.8026%\n",
      "total_backward_count 1027950 real_backward_count 147798  14.378%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  2.107686/  2.154851, val:  62.92%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0881%\n",
      "layer   2  Sparsity: 89.6153%\n",
      "layer   3  Sparsity: 88.5288%\n",
      "total_backward_count 1037740 real_backward_count 148862  14.345%\n",
      "lif layer 1 self.abs_max_v: 11114.0\n",
      "fc layer 1 self.abs_max_out: 6807.0\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  2.096452/  2.148778, val:  66.25%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0969%\n",
      "layer   2  Sparsity: 89.7626%\n",
      "layer   3  Sparsity: 88.3207%\n",
      "total_backward_count 1047530 real_backward_count 149812  14.301%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  2.099030/  2.143031, val:  63.75%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0931%\n",
      "layer   2  Sparsity: 89.6329%\n",
      "layer   3  Sparsity: 88.2344%\n",
      "total_backward_count 1057320 real_backward_count 150820  14.264%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  2.091268/  2.147312, val:  73.75%, val_best:  73.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0977%\n",
      "layer   2  Sparsity: 89.4249%\n",
      "layer   3  Sparsity: 88.2401%\n",
      "total_backward_count 1067110 real_backward_count 151822  14.227%\n",
      "lif layer 1 self.abs_max_v: 11180.0\n",
      "lif layer 1 self.abs_max_v: 11363.5\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  2.090347/  2.150798, val:  68.75%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0901%\n",
      "layer   2  Sparsity: 89.5549%\n",
      "layer   3  Sparsity: 88.3116%\n",
      "total_backward_count 1076900 real_backward_count 152768  14.186%\n",
      "lif layer 1 self.abs_max_v: 11537.0\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  2.092815/  2.144099, val:  61.25%, val_best:  73.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0730%\n",
      "layer   2  Sparsity: 89.7555%\n",
      "layer   3  Sparsity: 88.3005%\n",
      "total_backward_count 1086690 real_backward_count 153769  14.150%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  2.091518/  2.150471, val:  61.25%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0674%\n",
      "layer   2  Sparsity: 89.5787%\n",
      "layer   3  Sparsity: 88.5073%\n",
      "total_backward_count 1096480 real_backward_count 154741  14.113%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  2.097346/  2.150343, val:  67.08%, val_best:  73.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1252%\n",
      "layer   2  Sparsity: 89.5578%\n",
      "layer   3  Sparsity: 88.4968%\n",
      "total_backward_count 1106270 real_backward_count 155702  14.075%\n",
      "fc layer 1 self.abs_max_out: 6999.0\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  2.095315/  2.154528, val:  67.92%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1066%\n",
      "layer   2  Sparsity: 89.6567%\n",
      "layer   3  Sparsity: 88.7151%\n",
      "total_backward_count 1116060 real_backward_count 156680  14.039%\n",
      "lif layer 1 self.abs_max_v: 11709.0\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  2.099058/  2.148152, val:  67.50%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.1259%\n",
      "layer   2  Sparsity: 89.5873%\n",
      "layer   3  Sparsity: 88.7426%\n",
      "total_backward_count 1125850 real_backward_count 157612  13.999%\n",
      "fc layer 1 self.abs_max_out: 7025.0\n",
      "lif layer 1 self.abs_max_v: 11843.5\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  2.094939/  2.153714, val:  69.58%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0523%\n",
      "layer   2  Sparsity: 89.5052%\n",
      "layer   3  Sparsity: 88.5797%\n",
      "total_backward_count 1135640 real_backward_count 158558  13.962%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  2.098826/  2.147283, val:  60.00%, val_best:  73.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0841%\n",
      "layer   2  Sparsity: 89.4302%\n",
      "layer   3  Sparsity: 88.6532%\n",
      "total_backward_count 1145430 real_backward_count 159521  13.927%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  2.095357/  2.154848, val:  74.17%, val_best:  74.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1135%\n",
      "layer   2  Sparsity: 89.5414%\n",
      "layer   3  Sparsity: 88.7400%\n",
      "total_backward_count 1155220 real_backward_count 160441  13.888%\n",
      "fc layer 1 self.abs_max_out: 7061.0\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  2.097806/  2.155767, val:  62.08%, val_best:  74.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0308%\n",
      "layer   2  Sparsity: 89.5793%\n",
      "layer   3  Sparsity: 88.6893%\n",
      "total_backward_count 1165010 real_backward_count 161369  13.851%\n",
      "fc layer 1 self.abs_max_out: 7389.0\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  2.099743/  2.157071, val:  73.33%, val_best:  74.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.0855%\n",
      "layer   2  Sparsity: 89.4122%\n",
      "layer   3  Sparsity: 88.8772%\n",
      "total_backward_count 1174800 real_backward_count 162337  13.818%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  2.099622/  2.154367, val:  68.33%, val_best:  74.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1235%\n",
      "layer   2  Sparsity: 89.6674%\n",
      "layer   3  Sparsity: 88.8052%\n",
      "total_backward_count 1184590 real_backward_count 163314  13.787%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  2.093767/  2.146799, val:  67.50%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1079%\n",
      "layer   2  Sparsity: 89.7343%\n",
      "layer   3  Sparsity: 89.1028%\n",
      "total_backward_count 1194380 real_backward_count 164272  13.754%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  2.091422/  2.157219, val:  68.75%, val_best:  74.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1304%\n",
      "layer   2  Sparsity: 89.6152%\n",
      "layer   3  Sparsity: 88.9924%\n",
      "total_backward_count 1204170 real_backward_count 165244  13.723%\n",
      "fc layer 3 self.abs_max_out: 322.0\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  2.090380/  2.149765, val:  67.92%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.0791%\n",
      "layer   2  Sparsity: 89.6460%\n",
      "layer   3  Sparsity: 88.6969%\n",
      "total_backward_count 1213960 real_backward_count 166150  13.687%\n",
      "lif layer 1 self.abs_max_v: 11897.5\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  2.087843/  2.145659, val:  63.75%, val_best:  74.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1031%\n",
      "layer   2  Sparsity: 89.4832%\n",
      "layer   3  Sparsity: 88.5847%\n",
      "total_backward_count 1223750 real_backward_count 167082  13.653%\n",
      "lif layer 1 self.abs_max_v: 12081.5\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  2.088042/  2.143312, val:  71.25%, val_best:  74.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0583%\n",
      "layer   2  Sparsity: 89.3566%\n",
      "layer   3  Sparsity: 88.5852%\n",
      "total_backward_count 1233540 real_backward_count 167991  13.619%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  2.088403/  2.151097, val:  70.83%, val_best:  74.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0566%\n",
      "layer   2  Sparsity: 89.2774%\n",
      "layer   3  Sparsity: 88.9700%\n",
      "total_backward_count 1243330 real_backward_count 168888  13.584%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  2.089354/  2.146422, val:  69.58%, val_best:  74.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0927%\n",
      "layer   2  Sparsity: 89.4104%\n",
      "layer   3  Sparsity: 88.7392%\n",
      "total_backward_count 1253120 real_backward_count 169811  13.551%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  2.089412/  2.143951, val:  69.17%, val_best:  74.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1106%\n",
      "layer   2  Sparsity: 89.5788%\n",
      "layer   3  Sparsity: 89.1839%\n",
      "total_backward_count 1262910 real_backward_count 170782  13.523%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  2.092923/  2.154408, val:  66.67%, val_best:  74.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0879%\n",
      "layer   2  Sparsity: 89.6858%\n",
      "layer   3  Sparsity: 89.2126%\n",
      "total_backward_count 1272700 real_backward_count 171663  13.488%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  2.090064/  2.142936, val:  60.42%, val_best:  74.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0996%\n",
      "layer   2  Sparsity: 89.6723%\n",
      "layer   3  Sparsity: 88.8141%\n",
      "total_backward_count 1282490 real_backward_count 172575  13.456%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  2.089087/  2.141511, val:  70.83%, val_best:  74.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1083%\n",
      "layer   2  Sparsity: 89.4865%\n",
      "layer   3  Sparsity: 89.1486%\n",
      "total_backward_count 1292280 real_backward_count 173515  13.427%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  2.084217/  2.141313, val:  64.58%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0820%\n",
      "layer   2  Sparsity: 89.3046%\n",
      "layer   3  Sparsity: 88.8098%\n",
      "total_backward_count 1302070 real_backward_count 174373  13.392%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  2.082000/  2.142180, val:  66.25%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0935%\n",
      "layer   2  Sparsity: 89.4225%\n",
      "layer   3  Sparsity: 88.9389%\n",
      "total_backward_count 1311860 real_backward_count 175286  13.362%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  2.081658/  2.133980, val:  64.17%, val_best:  74.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1080%\n",
      "layer   2  Sparsity: 89.3452%\n",
      "layer   3  Sparsity: 88.9338%\n",
      "total_backward_count 1321650 real_backward_count 176186  13.331%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  2.086693/  2.141612, val:  67.08%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0853%\n",
      "layer   2  Sparsity: 89.1247%\n",
      "layer   3  Sparsity: 88.8538%\n",
      "total_backward_count 1331440 real_backward_count 177071  13.299%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  2.083036/  2.143153, val:  67.50%, val_best:  74.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0808%\n",
      "layer   2  Sparsity: 89.1272%\n",
      "layer   3  Sparsity: 88.7676%\n",
      "total_backward_count 1341230 real_backward_count 177958  13.268%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  2.083021/  2.142013, val:  69.58%, val_best:  74.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1080%\n",
      "layer   2  Sparsity: 89.2035%\n",
      "layer   3  Sparsity: 88.9628%\n",
      "total_backward_count 1351020 real_backward_count 178825  13.236%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  2.088182/  2.152055, val:  68.33%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1011%\n",
      "layer   2  Sparsity: 89.2820%\n",
      "layer   3  Sparsity: 89.2170%\n",
      "total_backward_count 1360810 real_backward_count 179709  13.206%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  2.090776/  2.142495, val:  71.67%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1164%\n",
      "layer   2  Sparsity: 89.5099%\n",
      "layer   3  Sparsity: 89.0289%\n",
      "total_backward_count 1370600 real_backward_count 180611  13.178%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  2.080600/  2.144397, val:  65.83%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1322%\n",
      "layer   2  Sparsity: 89.3605%\n",
      "layer   3  Sparsity: 88.9665%\n",
      "total_backward_count 1380390 real_backward_count 181479  13.147%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  2.076938/  2.136103, val:  69.58%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0963%\n",
      "layer   2  Sparsity: 89.2731%\n",
      "layer   3  Sparsity: 89.0032%\n",
      "total_backward_count 1390180 real_backward_count 182399  13.121%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  2.079567/  2.137746, val:  75.00%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0832%\n",
      "layer   2  Sparsity: 89.2628%\n",
      "layer   3  Sparsity: 88.9962%\n",
      "total_backward_count 1399970 real_backward_count 183280  13.092%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  2.070033/  2.136703, val:  62.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1662%\n",
      "layer   2  Sparsity: 89.3415%\n",
      "layer   3  Sparsity: 89.0242%\n",
      "total_backward_count 1409760 real_backward_count 184149  13.062%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  2.068553/  2.134122, val:  71.67%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0467%\n",
      "layer   2  Sparsity: 89.3055%\n",
      "layer   3  Sparsity: 88.8301%\n",
      "total_backward_count 1419550 real_backward_count 185003  13.033%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  2.079209/  2.148357, val:  61.25%, val_best:  75.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1008%\n",
      "layer   2  Sparsity: 89.4855%\n",
      "layer   3  Sparsity: 89.0190%\n",
      "total_backward_count 1429340 real_backward_count 185874  13.004%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  2.084926/  2.146295, val:  63.75%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1280%\n",
      "layer   2  Sparsity: 89.6020%\n",
      "layer   3  Sparsity: 89.2848%\n",
      "total_backward_count 1439130 real_backward_count 186789  12.979%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  2.082020/  2.152321, val:  64.58%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1208%\n",
      "layer   2  Sparsity: 89.5831%\n",
      "layer   3  Sparsity: 88.9965%\n",
      "total_backward_count 1448920 real_backward_count 187614  12.949%\n",
      "fc layer 1 self.abs_max_out: 7636.0\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  2.089192/  2.151886, val:  67.92%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1336%\n",
      "layer   2  Sparsity: 89.2902%\n",
      "layer   3  Sparsity: 89.0096%\n",
      "total_backward_count 1458710 real_backward_count 188505  12.923%\n",
      "fc layer 1 self.abs_max_out: 7789.0\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  2.080885/  2.143212, val:  63.75%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0500%\n",
      "layer   2  Sparsity: 89.2199%\n",
      "layer   3  Sparsity: 89.0094%\n",
      "total_backward_count 1468500 real_backward_count 189410  12.898%\n",
      "fc layer 1 self.abs_max_out: 7821.0\n",
      "fc layer 1 self.abs_max_out: 8421.0\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  2.081435/  2.144314, val:  70.83%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0739%\n",
      "layer   2  Sparsity: 89.0849%\n",
      "layer   3  Sparsity: 88.7488%\n",
      "total_backward_count 1478290 real_backward_count 190280  12.872%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  2.086813/  2.146980, val:  63.75%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1218%\n",
      "layer   2  Sparsity: 89.0680%\n",
      "layer   3  Sparsity: 89.0940%\n",
      "total_backward_count 1488080 real_backward_count 191175  12.847%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  2.092609/  2.148705, val:  61.67%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1082%\n",
      "layer   2  Sparsity: 89.1989%\n",
      "layer   3  Sparsity: 88.9943%\n",
      "total_backward_count 1497870 real_backward_count 192010  12.819%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  2.081914/  2.137161, val:  67.08%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.0447%\n",
      "layer   2  Sparsity: 89.1617%\n",
      "layer   3  Sparsity: 89.1256%\n",
      "total_backward_count 1507660 real_backward_count 192906  12.795%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  2.081904/  2.138086, val:  65.83%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0761%\n",
      "layer   2  Sparsity: 89.3601%\n",
      "layer   3  Sparsity: 89.0285%\n",
      "total_backward_count 1517450 real_backward_count 193762  12.769%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  2.074579/  2.139705, val:  63.75%, val_best:  75.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1092%\n",
      "layer   2  Sparsity: 89.1840%\n",
      "layer   3  Sparsity: 88.5389%\n",
      "total_backward_count 1527240 real_backward_count 194632  12.744%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  2.073429/  2.141639, val:  65.83%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1177%\n",
      "layer   2  Sparsity: 89.1625%\n",
      "layer   3  Sparsity: 88.8105%\n",
      "total_backward_count 1537030 real_backward_count 195474  12.718%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  2.079501/  2.145255, val:  65.00%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0861%\n",
      "layer   2  Sparsity: 89.2395%\n",
      "layer   3  Sparsity: 89.0704%\n",
      "total_backward_count 1546820 real_backward_count 196312  12.691%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  2.083447/  2.143758, val:  70.00%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0859%\n",
      "layer   2  Sparsity: 89.2625%\n",
      "layer   3  Sparsity: 89.0568%\n",
      "total_backward_count 1556610 real_backward_count 197156  12.666%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  2.082934/  2.143614, val:  65.00%, val_best:  75.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0732%\n",
      "layer   2  Sparsity: 89.2040%\n",
      "layer   3  Sparsity: 88.9026%\n",
      "total_backward_count 1566400 real_backward_count 198011  12.641%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  2.075603/  2.136065, val:  68.75%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1161%\n",
      "layer   2  Sparsity: 89.1804%\n",
      "layer   3  Sparsity: 88.9174%\n",
      "total_backward_count 1576190 real_backward_count 198832  12.615%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  2.072971/  2.130985, val:  69.17%, val_best:  75.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1248%\n",
      "layer   2  Sparsity: 89.2573%\n",
      "layer   3  Sparsity: 88.7306%\n",
      "total_backward_count 1585980 real_backward_count 199705  12.592%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  2.070418/  2.130927, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.0405%\n",
      "layer   2  Sparsity: 89.1769%\n",
      "layer   3  Sparsity: 88.7511%\n",
      "total_backward_count 1595770 real_backward_count 200533  12.567%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  2.065698/  2.130920, val:  67.50%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1219%\n",
      "layer   2  Sparsity: 89.2469%\n",
      "layer   3  Sparsity: 88.6665%\n",
      "total_backward_count 1605560 real_backward_count 201349  12.541%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  2.062483/  2.130983, val:  70.42%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0699%\n",
      "layer   2  Sparsity: 89.2950%\n",
      "layer   3  Sparsity: 88.9364%\n",
      "total_backward_count 1615350 real_backward_count 202151  12.514%\n",
      "fc layer 1 self.abs_max_out: 8466.0\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  2.068339/  2.125349, val:  67.08%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1030%\n",
      "layer   2  Sparsity: 89.2397%\n",
      "layer   3  Sparsity: 88.7648%\n",
      "total_backward_count 1625140 real_backward_count 202988  12.490%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  2.060131/  2.130283, val:  67.50%, val_best:  75.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0590%\n",
      "layer   2  Sparsity: 89.3465%\n",
      "layer   3  Sparsity: 88.8281%\n",
      "total_backward_count 1634930 real_backward_count 203776  12.464%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  2.075028/  2.139475, val:  60.83%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 82.0751%\n",
      "layer   2  Sparsity: 89.3875%\n",
      "layer   3  Sparsity: 89.1827%\n",
      "total_backward_count 1644720 real_backward_count 204568  12.438%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  2.070149/  2.127150, val:  67.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0965%\n",
      "layer   2  Sparsity: 89.5979%\n",
      "layer   3  Sparsity: 88.9722%\n",
      "total_backward_count 1654510 real_backward_count 205366  12.412%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  2.065428/  2.134093, val:  67.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0739%\n",
      "layer   2  Sparsity: 89.4936%\n",
      "layer   3  Sparsity: 88.3470%\n",
      "total_backward_count 1664300 real_backward_count 206207  12.390%\n",
      "fc layer 1 self.abs_max_out: 8560.0\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  2.069863/  2.137609, val:  62.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.1159%\n",
      "layer   2  Sparsity: 89.4661%\n",
      "layer   3  Sparsity: 88.2184%\n",
      "total_backward_count 1674090 real_backward_count 206981  12.364%\n",
      "fc layer 1 self.abs_max_out: 8671.0\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  2.067226/  2.141358, val:  63.33%, val_best:  75.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1107%\n",
      "layer   2  Sparsity: 89.3547%\n",
      "layer   3  Sparsity: 88.6730%\n",
      "total_backward_count 1683880 real_backward_count 207775  12.339%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  2.067750/  2.129156, val:  62.50%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1317%\n",
      "layer   2  Sparsity: 89.2498%\n",
      "layer   3  Sparsity: 88.4399%\n",
      "total_backward_count 1693670 real_backward_count 208621  12.318%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  2.059865/  2.121083, val:  66.67%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0897%\n",
      "layer   2  Sparsity: 89.2889%\n",
      "layer   3  Sparsity: 87.9326%\n",
      "total_backward_count 1703460 real_backward_count 209398  12.293%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  2.055710/  2.126607, val:  65.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1280%\n",
      "layer   2  Sparsity: 89.2411%\n",
      "layer   3  Sparsity: 88.0446%\n",
      "total_backward_count 1713250 real_backward_count 210188  12.268%\n",
      "fc layer 1 self.abs_max_out: 8750.0\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  2.053463/  2.129883, val:  71.67%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1205%\n",
      "layer   2  Sparsity: 89.3236%\n",
      "layer   3  Sparsity: 87.9644%\n",
      "total_backward_count 1723040 real_backward_count 210970  12.244%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  2.060254/  2.131358, val:  69.17%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0804%\n",
      "layer   2  Sparsity: 89.3546%\n",
      "layer   3  Sparsity: 87.7559%\n",
      "total_backward_count 1732830 real_backward_count 211725  12.218%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  2.058699/  2.127100, val:  66.67%, val_best:  75.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0925%\n",
      "layer   2  Sparsity: 89.1868%\n",
      "layer   3  Sparsity: 87.9245%\n",
      "total_backward_count 1742620 real_backward_count 212500  12.194%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  2.067071/  2.135879, val:  70.42%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0955%\n",
      "layer   2  Sparsity: 89.1638%\n",
      "layer   3  Sparsity: 88.2228%\n",
      "total_backward_count 1752410 real_backward_count 213273  12.170%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  2.065441/  2.132175, val:  69.58%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0799%\n",
      "layer   2  Sparsity: 89.4136%\n",
      "layer   3  Sparsity: 88.2711%\n",
      "total_backward_count 1762200 real_backward_count 214047  12.147%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  2.059736/  2.122806, val:  66.25%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1277%\n",
      "layer   2  Sparsity: 89.3361%\n",
      "layer   3  Sparsity: 88.2789%\n",
      "total_backward_count 1771990 real_backward_count 214833  12.124%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  2.056872/  2.133660, val:  64.17%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0709%\n",
      "layer   2  Sparsity: 89.4401%\n",
      "layer   3  Sparsity: 88.7020%\n",
      "total_backward_count 1781780 real_backward_count 215568  12.098%\n",
      "fc layer 1 self.abs_max_out: 8818.0\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  2.059491/  2.127139, val:  70.42%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0652%\n",
      "layer   2  Sparsity: 89.4521%\n",
      "layer   3  Sparsity: 88.6925%\n",
      "total_backward_count 1791570 real_backward_count 216383  12.078%\n",
      "fc layer 1 self.abs_max_out: 8883.0\n",
      "fc layer 1 self.abs_max_out: 8895.0\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  2.061580/  2.139252, val:  66.67%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0702%\n",
      "layer   2  Sparsity: 89.4936%\n",
      "layer   3  Sparsity: 88.7539%\n",
      "total_backward_count 1801360 real_backward_count 217105  12.052%\n",
      "fc layer 1 self.abs_max_out: 8940.0\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  2.068936/  2.138315, val:  62.92%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1140%\n",
      "layer   2  Sparsity: 89.4607%\n",
      "layer   3  Sparsity: 88.6784%\n",
      "total_backward_count 1811150 real_backward_count 217854  12.028%\n",
      "fc layer 1 self.abs_max_out: 9012.0\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  2.065142/  2.128071, val:  62.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1510%\n",
      "layer   2  Sparsity: 89.3781%\n",
      "layer   3  Sparsity: 88.5308%\n",
      "total_backward_count 1820940 real_backward_count 218663  12.008%\n",
      "lif layer 2 self.abs_max_v: 2948.5\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  2.059160/  2.128492, val:  70.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.1361%\n",
      "layer   2  Sparsity: 89.4147%\n",
      "layer   3  Sparsity: 88.5223%\n",
      "total_backward_count 1830730 real_backward_count 219491  11.989%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  2.063859/  2.135183, val:  72.08%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.0838%\n",
      "layer   2  Sparsity: 89.5771%\n",
      "layer   3  Sparsity: 88.6305%\n",
      "total_backward_count 1840520 real_backward_count 220271  11.968%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  2.068209/  2.136081, val:  69.58%, val_best:  75.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1271%\n",
      "layer   2  Sparsity: 89.6767%\n",
      "layer   3  Sparsity: 88.4041%\n",
      "total_backward_count 1850310 real_backward_count 221026  11.945%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  2.066680/  2.129821, val:  65.83%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1191%\n",
      "layer   2  Sparsity: 89.4630%\n",
      "layer   3  Sparsity: 88.3442%\n",
      "total_backward_count 1860100 real_backward_count 221751  11.921%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  2.051062/  2.116399, val:  71.25%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.0981%\n",
      "layer   2  Sparsity: 89.2607%\n",
      "layer   3  Sparsity: 88.1907%\n",
      "total_backward_count 1869890 real_backward_count 222470  11.897%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  2.045524/  2.121862, val:  59.58%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.1115%\n",
      "layer   2  Sparsity: 89.0294%\n",
      "layer   3  Sparsity: 88.2055%\n",
      "total_backward_count 1879680 real_backward_count 223223  11.876%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  2.046956/  2.125564, val:  73.75%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1111%\n",
      "layer   2  Sparsity: 89.1376%\n",
      "layer   3  Sparsity: 88.3563%\n",
      "total_backward_count 1889470 real_backward_count 223963  11.853%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  2.053485/  2.125803, val:  70.00%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1135%\n",
      "layer   2  Sparsity: 89.2350%\n",
      "layer   3  Sparsity: 88.4306%\n",
      "total_backward_count 1899260 real_backward_count 224639  11.828%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  2.059946/  2.125586, val:  67.08%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0297%\n",
      "layer   2  Sparsity: 89.2736%\n",
      "layer   3  Sparsity: 88.3118%\n",
      "total_backward_count 1909050 real_backward_count 225406  11.807%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  2.051063/  2.121716, val:  65.42%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.1131%\n",
      "layer   2  Sparsity: 89.2212%\n",
      "layer   3  Sparsity: 88.1731%\n",
      "total_backward_count 1918840 real_backward_count 226116  11.784%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  2.044784/  2.118978, val:  65.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0913%\n",
      "layer   2  Sparsity: 89.0509%\n",
      "layer   3  Sparsity: 88.3516%\n",
      "total_backward_count 1928630 real_backward_count 226845  11.762%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  2.053452/  2.127558, val:  61.67%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0740%\n",
      "layer   2  Sparsity: 88.9904%\n",
      "layer   3  Sparsity: 88.4977%\n",
      "total_backward_count 1938420 real_backward_count 227579  11.740%\n",
      "fc layer 3 self.abs_max_out: 340.0\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  2.048409/  2.118399, val:  67.08%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1154%\n",
      "layer   2  Sparsity: 89.0821%\n",
      "layer   3  Sparsity: 88.1237%\n",
      "total_backward_count 1948210 real_backward_count 228358  11.721%\n",
      "lif layer 1 self.abs_max_v: 12096.0\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  2.042829/  2.111545, val:  67.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.1044%\n",
      "layer   2  Sparsity: 89.2943%\n",
      "layer   3  Sparsity: 88.2182%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0a5b36369d4a40b1432e35d450976d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÜ‚ñÉ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñá‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÜ‚ñÉ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñá‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>2.04283</td></tr><tr><td>val_acc_best</td><td>0.75</td></tr><tr><td>val_acc_now</td><td>0.675</td></tr><tr><td>val_loss</td><td>2.11155</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">summer-sweep-4</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/lb85yhnm' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/lb85yhnm</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_212824-lb85yhnm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: f114y14n with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 12000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_014647-f114y14n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/f114y14n' target=\"_blank\">eager-sweep-7</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/f114y14n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/f114y14n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': '20251118_014655_810', 'my_seed': 42, 'TIME': 5, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 12000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 67f09733060e9328908e01cda0ab3532\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 592\n",
      "fc layer 1 self.abs_max_out: 236.0\n",
      "lif layer 1 self.abs_max_v: 236.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 582.0\n",
      "lif layer 2 self.abs_max_v: 582.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 344.0\n",
      "fc layer 1 self.abs_max_out: 256.0\n",
      "lif layer 1 self.abs_max_v: 352.0\n",
      "lif layer 2 self.abs_max_v: 626.5\n",
      "fc layer 1 self.abs_max_out: 271.0\n",
      "fc layer 2 self.abs_max_out: 678.0\n",
      "lif layer 2 self.abs_max_v: 766.5\n",
      "fc layer 1 self.abs_max_out: 387.0\n",
      "lif layer 1 self.abs_max_v: 477.5\n",
      "fc layer 2 self.abs_max_out: 738.0\n",
      "lif layer 2 self.abs_max_v: 1015.5\n",
      "smallest_now_T updated: 534\n",
      "fc layer 1 self.abs_max_out: 408.0\n",
      "fc layer 2 self.abs_max_out: 1003.0\n",
      "fc layer 1 self.abs_max_out: 462.0\n",
      "lif layer 1 self.abs_max_v: 541.5\n",
      "lif layer 2 self.abs_max_v: 1358.5\n",
      "fc layer 3 self.abs_max_out: 407.0\n",
      "lif layer 1 self.abs_max_v: 633.0\n",
      "fc layer 1 self.abs_max_out: 698.0\n",
      "lif layer 1 self.abs_max_v: 892.5\n",
      "lif layer 2 self.abs_max_v: 1371.5\n",
      "smallest_now_T updated: 407\n",
      "fc layer 2 self.abs_max_out: 1097.0\n",
      "fc layer 2 self.abs_max_out: 1305.0\n",
      "lif layer 2 self.abs_max_v: 1648.5\n",
      "lif layer 2 self.abs_max_v: 1737.5\n",
      "fc layer 1 self.abs_max_out: 768.0\n",
      "lif layer 1 self.abs_max_v: 923.5\n",
      "lif layer 1 self.abs_max_v: 992.0\n",
      "lif layer 1 self.abs_max_v: 1073.0\n",
      "fc layer 1 self.abs_max_out: 896.0\n",
      "lif layer 1 self.abs_max_v: 1138.5\n",
      "lif layer 2 self.abs_max_v: 1790.5\n",
      "fc layer 3 self.abs_max_out: 409.0\n",
      "fc layer 3 self.abs_max_out: 473.0\n",
      "lif layer 2 self.abs_max_v: 1914.0\n",
      "smallest_now_T updated: 345\n",
      "lif layer 1 self.abs_max_v: 1163.5\n",
      "fc layer 1 self.abs_max_out: 1133.0\n",
      "lif layer 1 self.abs_max_v: 1197.5\n",
      "fc layer 3 self.abs_max_out: 480.0\n",
      "smallest_now_T updated: 317\n",
      "lif layer 1 self.abs_max_v: 1415.5\n",
      "lif layer 1 self.abs_max_v: 1589.0\n",
      "lif layer 2 self.abs_max_v: 1978.0\n",
      "fc layer 1 self.abs_max_out: 1146.0\n",
      "fc layer 1 self.abs_max_out: 1219.0\n",
      "fc layer 2 self.abs_max_out: 1307.0\n",
      "fc layer 2 self.abs_max_out: 1556.0\n",
      "lif layer 2 self.abs_max_v: 2027.5\n",
      "lif layer 1 self.abs_max_v: 1667.5\n",
      "fc layer 3 self.abs_max_out: 500.0\n",
      "smallest_now_T updated: 286\n",
      "lif layer 2 self.abs_max_v: 2277.0\n",
      "fc layer 1 self.abs_max_out: 1253.0\n",
      "fc layer 1 self.abs_max_out: 1357.0\n",
      "fc layer 3 self.abs_max_out: 534.0\n",
      "fc layer 1 self.abs_max_out: 1396.0\n",
      "fc layer 1 self.abs_max_out: 1723.0\n",
      "lif layer 1 self.abs_max_v: 1723.0\n",
      "lif layer 1 self.abs_max_v: 1746.5\n",
      "fc layer 3 self.abs_max_out: 664.0\n",
      "smallest_now_T updated: 247\n",
      "smallest_now_T updated: 192\n",
      "lif layer 1 self.abs_max_v: 1753.5\n",
      "lif layer 1 self.abs_max_v: 2093.5\n",
      "lif layer 2 self.abs_max_v: 2306.5\n",
      "lif layer 1 self.abs_max_v: 2145.0\n",
      "lif layer 1 self.abs_max_v: 2421.5\n",
      "lif layer 1 self.abs_max_v: 2600.0\n",
      "fc layer 2 self.abs_max_out: 1567.0\n",
      "fc layer 1 self.abs_max_out: 1799.0\n",
      "fc layer 2 self.abs_max_out: 1592.0\n",
      "fc layer 3 self.abs_max_out: 712.0\n",
      "lif layer 2 self.abs_max_v: 2327.0\n",
      "lif layer 2 self.abs_max_v: 2338.0\n",
      "lif layer 2 self.abs_max_v: 2369.5\n",
      "fc layer 2 self.abs_max_out: 1594.0\n",
      "lif layer 2 self.abs_max_v: 2409.0\n",
      "fc layer 1 self.abs_max_out: 2129.0\n",
      "lif layer 2 self.abs_max_v: 2484.0\n",
      "fc layer 2 self.abs_max_out: 1619.0\n",
      "lif layer 2 self.abs_max_v: 2527.5\n",
      "fc layer 2 self.abs_max_out: 1661.0\n",
      "smallest_now_T_val updated: 552\n",
      "smallest_now_T_val updated: 456\n",
      "smallest_now_T_val updated: 448\n",
      "smallest_now_T_val updated: 440\n",
      "smallest_now_T_val updated: 368\n",
      "fc layer 2 self.abs_max_out: 1693.0\n",
      "smallest_now_T_val updated: 137\n",
      "fc layer 2 self.abs_max_out: 1733.0\n",
      "fc layer 2 self.abs_max_out: 1752.0\n",
      "fc layer 2 self.abs_max_out: 1769.0\n",
      "lif layer 2 self.abs_max_v: 2621.5\n",
      "lif layer 2 self.abs_max_v: 2725.0\n",
      "fc layer 1 self.abs_max_out: 2381.0\n",
      "lif layer 1 self.abs_max_v: 2690.0\n",
      "lif layer 1 self.abs_max_v: 3055.5\n",
      "lif layer 1 self.abs_max_v: 3312.0\n",
      "fc layer 2 self.abs_max_out: 1798.0\n",
      "lif layer 1 self.abs_max_v: 3491.5\n",
      "fc layer 2 self.abs_max_out: 1799.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.972991/  2.042510, val:  32.92%, val_best:  32.92%, tr:  69.87%, tr_best:  69.87%, epoch time: 41.06 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0025%\n",
      "layer   2  Sparsity: 73.8218%\n",
      "layer   3  Sparsity: 69.3998%\n",
      "total_backward_count 4895 real_backward_count 2123  43.371%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 2788.5\n",
      "lif layer 2 self.abs_max_v: 2841.5\n",
      "fc layer 2 self.abs_max_out: 1818.0\n",
      "lif layer 2 self.abs_max_v: 2854.5\n",
      "lif layer 2 self.abs_max_v: 2958.0\n",
      "lif layer 2 self.abs_max_v: 2998.5\n",
      "fc layer 3 self.abs_max_out: 721.0\n",
      "fc layer 2 self.abs_max_out: 1851.0\n",
      "fc layer 2 self.abs_max_out: 1864.0\n",
      "fc layer 2 self.abs_max_out: 1937.0\n",
      "fc layer 1 self.abs_max_out: 2451.0\n",
      "lif layer 1 self.abs_max_v: 3645.0\n",
      "lif layer 1 self.abs_max_v: 4095.5\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.922185/  2.057730, val:  35.83%, val_best:  35.83%, tr:  80.18%, tr_best:  80.18%, epoch time: 41.10 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 92.9987%\n",
      "layer   2  Sparsity: 73.1113%\n",
      "layer   3  Sparsity: 68.1304%\n",
      "total_backward_count 9790 real_backward_count 3741  38.212%\n",
      "fc layer 1 self.abs_max_out: 2604.0\n",
      "fc layer 3 self.abs_max_out: 762.0\n",
      "fc layer 2 self.abs_max_out: 1994.0\n",
      "fc layer 1 self.abs_max_out: 2669.0\n",
      "lif layer 2 self.abs_max_v: 3059.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.910593/  2.047965, val:  34.17%, val_best:  35.83%, tr:  83.55%, tr_best:  83.55%, epoch time: 41.08 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0272%\n",
      "layer   2  Sparsity: 74.1794%\n",
      "layer   3  Sparsity: 69.1916%\n",
      "total_backward_count 14685 real_backward_count 5198  35.397%\n",
      "fc layer 2 self.abs_max_out: 2097.0\n",
      "fc layer 2 self.abs_max_out: 2171.0\n",
      "lif layer 2 self.abs_max_v: 3071.5\n",
      "lif layer 2 self.abs_max_v: 3149.0\n",
      "lif layer 2 self.abs_max_v: 3170.5\n",
      "lif layer 2 self.abs_max_v: 3196.5\n",
      "lif layer 2 self.abs_max_v: 3232.0\n",
      "lif layer 2 self.abs_max_v: 3282.5\n",
      "lif layer 2 self.abs_max_v: 3386.5\n",
      "fc layer 1 self.abs_max_out: 2875.0\n",
      "lif layer 1 self.abs_max_v: 4335.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.919515/  2.040129, val:  42.08%, val_best:  42.08%, tr:  87.44%, tr_best:  87.44%, epoch time: 41.00 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9970%\n",
      "layer   2  Sparsity: 72.5058%\n",
      "layer   3  Sparsity: 68.3450%\n",
      "total_backward_count 19580 real_backward_count 6489  33.141%\n",
      "lif layer 1 self.abs_max_v: 4341.0\n",
      "lif layer 2 self.abs_max_v: 3519.5\n",
      "fc layer 1 self.abs_max_out: 3003.0\n",
      "lif layer 1 self.abs_max_v: 4480.0\n",
      "lif layer 1 self.abs_max_v: 4756.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.898205/  2.024923, val:  33.75%, val_best:  42.08%, tr:  88.66%, tr_best:  88.66%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9968%\n",
      "layer   2  Sparsity: 73.1911%\n",
      "layer   3  Sparsity: 68.2683%\n",
      "total_backward_count 24475 real_backward_count 7756  31.689%\n",
      "fc layer 1 self.abs_max_out: 3010.0\n",
      "fc layer 1 self.abs_max_out: 3133.0\n",
      "lif layer 2 self.abs_max_v: 3642.0\n",
      "lif layer 2 self.abs_max_v: 3759.0\n",
      "fc layer 1 self.abs_max_out: 3293.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.910528/  2.043844, val:  41.67%, val_best:  42.08%, tr:  90.81%, tr_best:  90.81%, epoch time: 40.88 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9594%\n",
      "layer   2  Sparsity: 73.2886%\n",
      "layer   3  Sparsity: 68.7803%\n",
      "total_backward_count 29370 real_backward_count 8962  30.514%\n",
      "fc layer 2 self.abs_max_out: 2409.0\n",
      "lif layer 2 self.abs_max_v: 3763.5\n",
      "lif layer 2 self.abs_max_v: 3848.0\n",
      "lif layer 1 self.abs_max_v: 4811.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.921925/  2.044800, val:  42.08%, val_best:  42.08%, tr:  89.68%, tr_best:  90.81%, epoch time: 40.76 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0250%\n",
      "layer   2  Sparsity: 73.3579%\n",
      "layer   3  Sparsity: 68.7295%\n",
      "total_backward_count 34265 real_backward_count 10216  29.815%\n",
      "fc layer 3 self.abs_max_out: 789.0\n",
      "lif layer 1 self.abs_max_v: 5190.5\n",
      "fc layer 1 self.abs_max_out: 3310.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.916486/  2.061764, val:  34.58%, val_best:  42.08%, tr:  91.62%, tr_best:  91.62%, epoch time: 40.60 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0132%\n",
      "layer   2  Sparsity: 73.1296%\n",
      "layer   3  Sparsity: 68.7029%\n",
      "total_backward_count 39160 real_backward_count 11371  29.037%\n",
      "lif layer 2 self.abs_max_v: 3969.5\n",
      "fc layer 1 self.abs_max_out: 3418.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.941395/  2.047827, val:  40.00%, val_best:  42.08%, tr:  89.89%, tr_best:  91.62%, epoch time: 40.12 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0200%\n",
      "layer   2  Sparsity: 73.0220%\n",
      "layer   3  Sparsity: 69.5575%\n",
      "total_backward_count 44055 real_backward_count 12563  28.517%\n",
      "lif layer 2 self.abs_max_v: 4181.0\n",
      "fc layer 1 self.abs_max_out: 3465.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.931100/  2.056651, val:  40.00%, val_best:  42.08%, tr:  92.24%, tr_best:  92.24%, epoch time: 40.86 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9857%\n",
      "layer   2  Sparsity: 73.1632%\n",
      "layer   3  Sparsity: 69.6808%\n",
      "total_backward_count 48950 real_backward_count 13647  27.879%\n",
      "lif layer 1 self.abs_max_v: 5615.0\n",
      "fc layer 1 self.abs_max_out: 3474.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.912822/  2.043455, val:  39.17%, val_best:  42.08%, tr:  91.52%, tr_best:  92.24%, epoch time: 40.82 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9764%\n",
      "layer   2  Sparsity: 73.9016%\n",
      "layer   3  Sparsity: 69.5939%\n",
      "total_backward_count 53845 real_backward_count 14730  27.356%\n",
      "fc layer 2 self.abs_max_out: 2451.0\n",
      "lif layer 2 self.abs_max_v: 4341.5\n",
      "fc layer 1 self.abs_max_out: 3551.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.930740/  2.036216, val:  31.67%, val_best:  42.08%, tr:  92.54%, tr_best:  92.54%, epoch time: 40.62 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0328%\n",
      "layer   2  Sparsity: 73.2224%\n",
      "layer   3  Sparsity: 69.7217%\n",
      "total_backward_count 58740 real_backward_count 15824  26.939%\n",
      "fc layer 2 self.abs_max_out: 2499.0\n",
      "lif layer 1 self.abs_max_v: 6012.0\n",
      "fc layer 1 self.abs_max_out: 3647.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.919541/  2.026814, val:  38.33%, val_best:  42.08%, tr:  92.34%, tr_best:  92.54%, epoch time: 41.00 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9642%\n",
      "layer   2  Sparsity: 72.5842%\n",
      "layer   3  Sparsity: 69.8523%\n",
      "total_backward_count 63635 real_backward_count 16893  26.547%\n",
      "lif layer 2 self.abs_max_v: 4387.0\n",
      "lif layer 2 self.abs_max_v: 4394.5\n",
      "lif layer 2 self.abs_max_v: 4576.5\n",
      "fc layer 2 self.abs_max_out: 2505.0\n",
      "fc layer 2 self.abs_max_out: 2538.0\n",
      "fc layer 2 self.abs_max_out: 2705.0\n",
      "fc layer 1 self.abs_max_out: 3669.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.907626/  2.038256, val:  37.08%, val_best:  42.08%, tr:  91.52%, tr_best:  92.54%, epoch time: 40.77 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9947%\n",
      "layer   2  Sparsity: 72.9111%\n",
      "layer   3  Sparsity: 70.0617%\n",
      "total_backward_count 68530 real_backward_count 17992  26.254%\n",
      "lif layer 2 self.abs_max_v: 4633.0\n",
      "lif layer 1 self.abs_max_v: 6621.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.908887/  2.033422, val:  43.75%, val_best:  43.75%, tr:  93.26%, tr_best:  93.26%, epoch time: 40.65 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0110%\n",
      "layer   2  Sparsity: 72.8007%\n",
      "layer   3  Sparsity: 69.9573%\n",
      "total_backward_count 73425 real_backward_count 19004  25.882%\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.920487/  2.045041, val:  40.42%, val_best:  43.75%, tr:  90.81%, tr_best:  93.26%, epoch time: 40.70 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0006%\n",
      "layer   2  Sparsity: 72.6533%\n",
      "layer   3  Sparsity: 69.6996%\n",
      "total_backward_count 78320 real_backward_count 20061  25.614%\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.918132/  2.035421, val:  53.75%, val_best:  53.75%, tr:  93.77%, tr_best:  93.77%, epoch time: 41.32 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 92.9870%\n",
      "layer   2  Sparsity: 72.5176%\n",
      "layer   3  Sparsity: 69.7207%\n",
      "total_backward_count 83215 real_backward_count 21084  25.337%\n",
      "fc layer 1 self.abs_max_out: 3704.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.920516/  2.035302, val:  54.17%, val_best:  54.17%, tr:  93.26%, tr_best:  93.77%, epoch time: 40.27 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0041%\n",
      "layer   2  Sparsity: 72.1363%\n",
      "layer   3  Sparsity: 69.7927%\n",
      "total_backward_count 88110 real_backward_count 22080  25.060%\n",
      "fc layer 1 self.abs_max_out: 3763.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.913966/  2.035020, val:  45.42%, val_best:  54.17%, tr:  92.34%, tr_best:  93.77%, epoch time: 41.28 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.0129%\n",
      "layer   2  Sparsity: 71.7680%\n",
      "layer   3  Sparsity: 69.2691%\n",
      "total_backward_count 93005 real_backward_count 23119  24.858%\n",
      "fc layer 1 self.abs_max_out: 3843.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.917960/  2.035981, val:  40.83%, val_best:  54.17%, tr:  94.59%, tr_best:  94.59%, epoch time: 40.71 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9639%\n",
      "layer   2  Sparsity: 72.5320%\n",
      "layer   3  Sparsity: 69.6567%\n",
      "total_backward_count 97900 real_backward_count 24121  24.638%\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.914616/  2.022051, val:  50.42%, val_best:  54.17%, tr:  92.44%, tr_best:  94.59%, epoch time: 41.81 seconds, 0.70 minutes\n",
      "layer   1  Sparsity: 93.0093%\n",
      "layer   2  Sparsity: 72.5160%\n",
      "layer   3  Sparsity: 69.8221%\n",
      "total_backward_count 102795 real_backward_count 25136  24.453%\n",
      "fc layer 1 self.abs_max_out: 3906.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.917683/  2.022640, val:  58.33%, val_best:  58.33%, tr:  92.13%, tr_best:  94.59%, epoch time: 40.59 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0038%\n",
      "layer   2  Sparsity: 72.2209%\n",
      "layer   3  Sparsity: 69.8182%\n",
      "total_backward_count 107690 real_backward_count 26203  24.332%\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.923460/  2.030509, val:  50.00%, val_best:  58.33%, tr:  93.36%, tr_best:  94.59%, epoch time: 40.19 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0261%\n",
      "layer   2  Sparsity: 71.8698%\n",
      "layer   3  Sparsity: 69.6743%\n",
      "total_backward_count 112585 real_backward_count 27259  24.212%\n",
      "fc layer 2 self.abs_max_out: 2779.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.913840/  2.036160, val:  46.67%, val_best:  58.33%, tr:  93.36%, tr_best:  94.59%, epoch time: 40.17 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0019%\n",
      "layer   2  Sparsity: 71.9249%\n",
      "layer   3  Sparsity: 69.6017%\n",
      "total_backward_count 117480 real_backward_count 28263  24.058%\n",
      "lif layer 1 self.abs_max_v: 6710.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.917604/  2.044473, val:  44.17%, val_best:  58.33%, tr:  94.18%, tr_best:  94.59%, epoch time: 40.59 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0014%\n",
      "layer   2  Sparsity: 72.1863%\n",
      "layer   3  Sparsity: 70.2094%\n",
      "total_backward_count 122375 real_backward_count 29275  23.922%\n",
      "lif layer 1 self.abs_max_v: 6769.5\n",
      "lif layer 1 self.abs_max_v: 7055.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.939543/  2.050737, val:  50.42%, val_best:  58.33%, tr:  92.65%, tr_best:  94.59%, epoch time: 40.72 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0164%\n",
      "layer   2  Sparsity: 72.0756%\n",
      "layer   3  Sparsity: 70.1189%\n",
      "total_backward_count 127270 real_backward_count 30306  23.812%\n",
      "lif layer 2 self.abs_max_v: 4663.5\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.929798/  2.039867, val:  47.50%, val_best:  58.33%, tr:  93.36%, tr_best:  94.59%, epoch time: 40.18 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9827%\n",
      "layer   2  Sparsity: 71.7721%\n",
      "layer   3  Sparsity: 69.7361%\n",
      "total_backward_count 132165 real_backward_count 31282  23.669%\n",
      "fc layer 1 self.abs_max_out: 4003.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.920204/  2.036315, val:  52.50%, val_best:  58.33%, tr:  93.67%, tr_best:  94.59%, epoch time: 40.42 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9981%\n",
      "layer   2  Sparsity: 71.6704%\n",
      "layer   3  Sparsity: 69.3833%\n",
      "total_backward_count 137060 real_backward_count 32284  23.555%\n",
      "fc layer 1 self.abs_max_out: 4032.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.923366/  2.041173, val:  47.08%, val_best:  58.33%, tr:  93.36%, tr_best:  94.59%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0109%\n",
      "layer   2  Sparsity: 71.4073%\n",
      "layer   3  Sparsity: 69.5245%\n",
      "total_backward_count 141955 real_backward_count 33275  23.441%\n",
      "fc layer 2 self.abs_max_out: 2949.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.920098/  2.037042, val:  42.08%, val_best:  58.33%, tr:  92.44%, tr_best:  94.59%, epoch time: 40.41 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0340%\n",
      "layer   2  Sparsity: 71.6140%\n",
      "layer   3  Sparsity: 70.2449%\n",
      "total_backward_count 146850 real_backward_count 34274  23.339%\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.925991/  2.024818, val:  50.00%, val_best:  58.33%, tr:  94.08%, tr_best:  94.59%, epoch time: 40.03 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0094%\n",
      "layer   2  Sparsity: 71.4922%\n",
      "layer   3  Sparsity: 69.8975%\n",
      "total_backward_count 151745 real_backward_count 35221  23.211%\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.914716/  2.028499, val:  43.33%, val_best:  58.33%, tr:  93.26%, tr_best:  94.59%, epoch time: 40.74 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0304%\n",
      "layer   2  Sparsity: 71.7801%\n",
      "layer   3  Sparsity: 69.5933%\n",
      "total_backward_count 156640 real_backward_count 36233  23.131%\n",
      "fc layer 1 self.abs_max_out: 4084.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.912695/  2.038601, val:  56.67%, val_best:  58.33%, tr:  92.85%, tr_best:  94.59%, epoch time: 40.30 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9587%\n",
      "layer   2  Sparsity: 71.8635%\n",
      "layer   3  Sparsity: 69.6295%\n",
      "total_backward_count 161535 real_backward_count 37195  23.026%\n",
      "fc layer 1 self.abs_max_out: 4211.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.923338/  2.027642, val:  55.00%, val_best:  58.33%, tr:  93.26%, tr_best:  94.59%, epoch time: 40.66 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9879%\n",
      "layer   2  Sparsity: 71.1179%\n",
      "layer   3  Sparsity: 69.7634%\n",
      "total_backward_count 166430 real_backward_count 38155  22.926%\n",
      "fc layer 1 self.abs_max_out: 4226.0\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.904552/  2.017499, val:  50.00%, val_best:  58.33%, tr:  94.48%, tr_best:  94.59%, epoch time: 39.85 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 92.9387%\n",
      "layer   2  Sparsity: 70.6717%\n",
      "layer   3  Sparsity: 70.0391%\n",
      "total_backward_count 171325 real_backward_count 39068  22.803%\n",
      "lif layer 1 self.abs_max_v: 7231.0\n",
      "fc layer 1 self.abs_max_out: 4308.0\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.898168/  2.012034, val:  57.92%, val_best:  58.33%, tr:  93.46%, tr_best:  94.59%, epoch time: 41.39 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.0043%\n",
      "layer   2  Sparsity: 70.6663%\n",
      "layer   3  Sparsity: 69.0504%\n",
      "total_backward_count 176220 real_backward_count 40068  22.737%\n",
      "lif layer 2 self.abs_max_v: 4794.5\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.894545/  2.016234, val:  51.25%, val_best:  58.33%, tr:  93.97%, tr_best:  94.59%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0260%\n",
      "layer   2  Sparsity: 70.5349%\n",
      "layer   3  Sparsity: 69.0180%\n",
      "total_backward_count 181115 real_backward_count 41048  22.664%\n",
      "lif layer 2 self.abs_max_v: 4866.5\n",
      "fc layer 1 self.abs_max_out: 4338.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.892913/  2.010463, val:  47.92%, val_best:  58.33%, tr:  94.79%, tr_best:  94.79%, epoch time: 40.68 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9825%\n",
      "layer   2  Sparsity: 70.4922%\n",
      "layer   3  Sparsity: 69.3165%\n",
      "total_backward_count 186010 real_backward_count 41952  22.554%\n",
      "fc layer 1 self.abs_max_out: 4438.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.894873/  2.016725, val:  45.42%, val_best:  58.33%, tr:  93.05%, tr_best:  94.79%, epoch time: 40.44 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0176%\n",
      "layer   2  Sparsity: 70.7406%\n",
      "layer   3  Sparsity: 69.5095%\n",
      "total_backward_count 190905 real_backward_count 42896  22.470%\n",
      "fc layer 2 self.abs_max_out: 3121.0\n",
      "fc layer 1 self.abs_max_out: 4500.0\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.906878/  2.019912, val:  48.75%, val_best:  58.33%, tr:  94.28%, tr_best:  94.79%, epoch time: 40.99 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0359%\n",
      "layer   2  Sparsity: 71.1306%\n",
      "layer   3  Sparsity: 70.0562%\n",
      "total_backward_count 195800 real_backward_count 43831  22.386%\n",
      "fc layer 1 self.abs_max_out: 4514.0\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.899194/  1.998534, val:  54.17%, val_best:  58.33%, tr:  93.67%, tr_best:  94.79%, epoch time: 40.37 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9623%\n",
      "layer   2  Sparsity: 70.8902%\n",
      "layer   3  Sparsity: 69.7592%\n",
      "total_backward_count 200695 real_backward_count 44759  22.302%\n",
      "lif layer 1 self.abs_max_v: 7239.5\n",
      "fc layer 1 self.abs_max_out: 4588.0\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.901198/  2.012848, val:  49.58%, val_best:  58.33%, tr:  94.38%, tr_best:  94.79%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0023%\n",
      "layer   2  Sparsity: 70.5505%\n",
      "layer   3  Sparsity: 70.3398%\n",
      "total_backward_count 205590 real_backward_count 45739  22.248%\n",
      "lif layer 2 self.abs_max_v: 4919.5\n",
      "lif layer 1 self.abs_max_v: 7241.5\n",
      "lif layer 1 self.abs_max_v: 7261.5\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.891144/  2.015579, val:  53.75%, val_best:  58.33%, tr:  94.69%, tr_best:  94.79%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9644%\n",
      "layer   2  Sparsity: 70.9317%\n",
      "layer   3  Sparsity: 69.8360%\n",
      "total_backward_count 210485 real_backward_count 46595  22.137%\n",
      "lif layer 1 self.abs_max_v: 7580.0\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.907771/  2.030988, val:  55.42%, val_best:  58.33%, tr:  95.30%, tr_best:  95.30%, epoch time: 40.91 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9794%\n",
      "layer   2  Sparsity: 71.0410%\n",
      "layer   3  Sparsity: 70.0216%\n",
      "total_backward_count 215380 real_backward_count 47503  22.055%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.910667/  2.009037, val:  55.42%, val_best:  58.33%, tr:  95.20%, tr_best:  95.30%, epoch time: 40.66 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0136%\n",
      "layer   2  Sparsity: 71.1015%\n",
      "layer   3  Sparsity: 70.3546%\n",
      "total_backward_count 220275 real_backward_count 48413  21.978%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.892780/  2.016346, val:  53.33%, val_best:  58.33%, tr:  94.79%, tr_best:  95.30%, epoch time: 40.92 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9861%\n",
      "layer   2  Sparsity: 71.0226%\n",
      "layer   3  Sparsity: 70.2807%\n",
      "total_backward_count 225170 real_backward_count 49355  21.919%\n",
      "lif layer 2 self.abs_max_v: 5058.0\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.901689/  2.008736, val:  60.00%, val_best:  60.00%, tr:  93.05%, tr_best:  95.30%, epoch time: 40.33 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9775%\n",
      "layer   2  Sparsity: 71.1116%\n",
      "layer   3  Sparsity: 70.4066%\n",
      "total_backward_count 230065 real_backward_count 50270  21.850%\n",
      "fc layer 1 self.abs_max_out: 4683.0\n",
      "lif layer 2 self.abs_max_v: 5179.5\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.895602/  2.018351, val:  43.75%, val_best:  60.00%, tr:  94.28%, tr_best:  95.30%, epoch time: 41.04 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0295%\n",
      "layer   2  Sparsity: 71.1583%\n",
      "layer   3  Sparsity: 70.2957%\n",
      "total_backward_count 234960 real_backward_count 51142  21.766%\n",
      "fc layer 1 self.abs_max_out: 4723.0\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.895784/  2.016311, val:  53.33%, val_best:  60.00%, tr:  94.89%, tr_best:  95.30%, epoch time: 41.06 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0477%\n",
      "layer   2  Sparsity: 71.1803%\n",
      "layer   3  Sparsity: 69.8582%\n",
      "total_backward_count 239855 real_backward_count 52061  21.705%\n",
      "fc layer 1 self.abs_max_out: 4758.0\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.902338/  2.007235, val:  59.17%, val_best:  60.00%, tr:  94.59%, tr_best:  95.30%, epoch time: 40.93 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9770%\n",
      "layer   2  Sparsity: 71.4171%\n",
      "layer   3  Sparsity: 70.5217%\n",
      "total_backward_count 244750 real_backward_count 53016  21.661%\n",
      "fc layer 2 self.abs_max_out: 3201.0\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.896006/  2.010245, val:  44.17%, val_best:  60.00%, tr:  94.99%, tr_best:  95.30%, epoch time: 40.98 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9973%\n",
      "layer   2  Sparsity: 71.4322%\n",
      "layer   3  Sparsity: 70.3865%\n",
      "total_backward_count 249645 real_backward_count 53910  21.595%\n",
      "lif layer 1 self.abs_max_v: 7928.5\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.893069/  2.002236, val:  51.25%, val_best:  60.00%, tr:  95.51%, tr_best:  95.51%, epoch time: 40.66 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9656%\n",
      "layer   2  Sparsity: 71.4030%\n",
      "layer   3  Sparsity: 70.0191%\n",
      "total_backward_count 254540 real_backward_count 54808  21.532%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.890446/  1.996120, val:  52.08%, val_best:  60.00%, tr:  94.89%, tr_best:  95.51%, epoch time: 40.61 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0017%\n",
      "layer   2  Sparsity: 71.3696%\n",
      "layer   3  Sparsity: 70.9285%\n",
      "total_backward_count 259435 real_backward_count 55742  21.486%\n",
      "fc layer 1 self.abs_max_out: 4786.0\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.899357/  2.009344, val:  50.83%, val_best:  60.00%, tr:  94.08%, tr_best:  95.51%, epoch time: 40.27 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0022%\n",
      "layer   2  Sparsity: 71.2253%\n",
      "layer   3  Sparsity: 70.9715%\n",
      "total_backward_count 264330 real_backward_count 56653  21.433%\n",
      "lif layer 1 self.abs_max_v: 7964.5\n",
      "fc layer 1 self.abs_max_out: 4825.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.905011/  2.012341, val:  57.08%, val_best:  60.00%, tr:  94.08%, tr_best:  95.51%, epoch time: 40.64 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0037%\n",
      "layer   2  Sparsity: 71.6395%\n",
      "layer   3  Sparsity: 70.9477%\n",
      "total_backward_count 269225 real_backward_count 57571  21.384%\n",
      "fc layer 1 self.abs_max_out: 4910.0\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.900457/  2.024513, val:  45.42%, val_best:  60.00%, tr:  95.10%, tr_best:  95.51%, epoch time: 40.94 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0199%\n",
      "layer   2  Sparsity: 71.3263%\n",
      "layer   3  Sparsity: 70.3364%\n",
      "total_backward_count 274120 real_backward_count 58502  21.342%\n",
      "fc layer 1 self.abs_max_out: 4998.0\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.888748/  1.993852, val:  50.00%, val_best:  60.00%, tr:  95.20%, tr_best:  95.51%, epoch time: 40.59 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9879%\n",
      "layer   2  Sparsity: 70.9330%\n",
      "layer   3  Sparsity: 70.2235%\n",
      "total_backward_count 279015 real_backward_count 59407  21.292%\n",
      "fc layer 1 self.abs_max_out: 5055.0\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.893601/  1.995464, val:  55.83%, val_best:  60.00%, tr:  95.91%, tr_best:  95.91%, epoch time: 40.62 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0553%\n",
      "layer   2  Sparsity: 70.8195%\n",
      "layer   3  Sparsity: 69.8902%\n",
      "total_backward_count 283910 real_backward_count 60235  21.216%\n",
      "fc layer 1 self.abs_max_out: 5144.0\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.882229/  1.990799, val:  55.00%, val_best:  60.00%, tr:  94.69%, tr_best:  95.91%, epoch time: 40.49 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9975%\n",
      "layer   2  Sparsity: 71.0905%\n",
      "layer   3  Sparsity: 70.5853%\n",
      "total_backward_count 288805 real_backward_count 61132  21.167%\n",
      "fc layer 1 self.abs_max_out: 5147.0\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.891111/  2.026119, val:  45.42%, val_best:  60.00%, tr:  94.89%, tr_best:  95.91%, epoch time: 40.30 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0061%\n",
      "layer   2  Sparsity: 71.2934%\n",
      "layer   3  Sparsity: 70.4520%\n",
      "total_backward_count 293700 real_backward_count 61977  21.102%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.895248/  2.017078, val:  45.42%, val_best:  60.00%, tr:  95.91%, tr_best:  95.91%, epoch time: 40.75 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9883%\n",
      "layer   2  Sparsity: 70.9635%\n",
      "layer   3  Sparsity: 70.1594%\n",
      "total_backward_count 298595 real_backward_count 62885  21.060%\n",
      "fc layer 1 self.abs_max_out: 5175.0\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.889277/  2.001858, val:  62.08%, val_best:  62.08%, tr:  95.10%, tr_best:  95.91%, epoch time: 40.69 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9382%\n",
      "layer   2  Sparsity: 70.7874%\n",
      "layer   3  Sparsity: 70.8242%\n",
      "total_backward_count 303490 real_backward_count 63770  21.012%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.888535/  1.999587, val:  44.58%, val_best:  62.08%, tr:  95.81%, tr_best:  95.91%, epoch time: 40.49 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0030%\n",
      "layer   2  Sparsity: 70.6547%\n",
      "layer   3  Sparsity: 71.1020%\n",
      "total_backward_count 308385 real_backward_count 64646  20.963%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.887400/  2.004705, val:  42.92%, val_best:  62.08%, tr:  96.32%, tr_best:  96.32%, epoch time: 40.17 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0354%\n",
      "layer   2  Sparsity: 70.5463%\n",
      "layer   3  Sparsity: 70.6132%\n",
      "total_backward_count 313280 real_backward_count 65511  20.911%\n",
      "lif layer 1 self.abs_max_v: 8121.5\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.892021/  2.001011, val:  53.33%, val_best:  62.08%, tr:  95.71%, tr_best:  96.32%, epoch time: 40.39 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0222%\n",
      "layer   2  Sparsity: 70.6999%\n",
      "layer   3  Sparsity: 70.8925%\n",
      "total_backward_count 318175 real_backward_count 66375  20.861%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.871666/  1.999350, val:  57.08%, val_best:  62.08%, tr:  95.40%, tr_best:  96.32%, epoch time: 40.11 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0340%\n",
      "layer   2  Sparsity: 70.7647%\n",
      "layer   3  Sparsity: 71.1675%\n",
      "total_backward_count 323070 real_backward_count 67214  20.805%\n",
      "lif layer 1 self.abs_max_v: 8428.0\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.876811/  1.990115, val:  63.33%, val_best:  63.33%, tr:  95.61%, tr_best:  96.32%, epoch time: 40.98 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9940%\n",
      "layer   2  Sparsity: 70.8847%\n",
      "layer   3  Sparsity: 71.0573%\n",
      "total_backward_count 327965 real_backward_count 68057  20.751%\n",
      "lif layer 1 self.abs_max_v: 8562.0\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.895386/  2.006227, val:  54.58%, val_best:  63.33%, tr:  94.59%, tr_best:  96.32%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9682%\n",
      "layer   2  Sparsity: 70.6532%\n",
      "layer   3  Sparsity: 70.4811%\n",
      "total_backward_count 332860 real_backward_count 68956  20.716%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.895237/  1.995034, val:  61.67%, val_best:  63.33%, tr:  94.99%, tr_best:  96.32%, epoch time: 41.17 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 92.9898%\n",
      "layer   2  Sparsity: 70.6585%\n",
      "layer   3  Sparsity: 70.9579%\n",
      "total_backward_count 337755 real_backward_count 69770  20.657%\n",
      "lif layer 1 self.abs_max_v: 8705.5\n",
      "fc layer 1 self.abs_max_out: 5290.0\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.894061/  2.001445, val:  62.08%, val_best:  63.33%, tr:  95.61%, tr_best:  96.32%, epoch time: 40.41 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9748%\n",
      "layer   2  Sparsity: 70.4290%\n",
      "layer   3  Sparsity: 70.8802%\n",
      "total_backward_count 342650 real_backward_count 70620  20.610%\n",
      "fc layer 1 self.abs_max_out: 5302.0\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.872928/  1.983549, val:  63.75%, val_best:  63.75%, tr:  97.14%, tr_best:  97.14%, epoch time: 40.78 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9960%\n",
      "layer   2  Sparsity: 70.6935%\n",
      "layer   3  Sparsity: 70.6728%\n",
      "total_backward_count 347545 real_backward_count 71474  20.565%\n",
      "fc layer 1 self.abs_max_out: 5530.0\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.879887/  2.008919, val:  49.17%, val_best:  63.75%, tr:  95.51%, tr_best:  97.14%, epoch time: 40.30 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9574%\n",
      "layer   2  Sparsity: 70.8739%\n",
      "layer   3  Sparsity: 70.9052%\n",
      "total_backward_count 352440 real_backward_count 72336  20.524%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.874594/  1.977757, val:  63.33%, val_best:  63.75%, tr:  96.53%, tr_best:  97.14%, epoch time: 40.79 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9440%\n",
      "layer   2  Sparsity: 70.8326%\n",
      "layer   3  Sparsity: 71.0340%\n",
      "total_backward_count 357335 real_backward_count 73192  20.483%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.869035/  1.989406, val:  58.33%, val_best:  63.75%, tr:  96.53%, tr_best:  97.14%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0533%\n",
      "layer   2  Sparsity: 70.7057%\n",
      "layer   3  Sparsity: 71.2343%\n",
      "total_backward_count 362230 real_backward_count 74001  20.429%\n",
      "lif layer 2 self.abs_max_v: 5292.0\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.890912/  1.991418, val:  59.17%, val_best:  63.75%, tr:  95.30%, tr_best:  97.14%, epoch time: 41.11 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 92.9835%\n",
      "layer   2  Sparsity: 70.1747%\n",
      "layer   3  Sparsity: 70.9415%\n",
      "total_backward_count 367125 real_backward_count 74843  20.386%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.872717/  1.978754, val:  63.33%, val_best:  63.75%, tr:  96.83%, tr_best:  97.14%, epoch time: 40.91 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0106%\n",
      "layer   2  Sparsity: 70.3310%\n",
      "layer   3  Sparsity: 70.8021%\n",
      "total_backward_count 372020 real_backward_count 75635  20.331%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.868144/  1.990890, val:  58.75%, val_best:  63.75%, tr:  96.73%, tr_best:  97.14%, epoch time: 40.72 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9732%\n",
      "layer   2  Sparsity: 70.5024%\n",
      "layer   3  Sparsity: 71.6090%\n",
      "total_backward_count 376915 real_backward_count 76453  20.284%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.883487/  2.014723, val:  43.75%, val_best:  63.75%, tr:  96.22%, tr_best:  97.14%, epoch time: 40.40 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0117%\n",
      "layer   2  Sparsity: 69.9932%\n",
      "layer   3  Sparsity: 71.3932%\n",
      "total_backward_count 381810 real_backward_count 77247  20.232%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.879711/  1.985266, val:  62.08%, val_best:  63.75%, tr:  96.83%, tr_best:  97.14%, epoch time: 40.04 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0218%\n",
      "layer   2  Sparsity: 70.4007%\n",
      "layer   3  Sparsity: 71.6527%\n",
      "total_backward_count 386705 real_backward_count 78045  20.182%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.894921/  2.000573, val:  61.67%, val_best:  63.75%, tr:  96.22%, tr_best:  97.14%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0187%\n",
      "layer   2  Sparsity: 70.2459%\n",
      "layer   3  Sparsity: 71.9029%\n",
      "total_backward_count 391600 real_backward_count 78865  20.139%\n",
      "fc layer 1 self.abs_max_out: 5532.0\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.883577/  1.993655, val:  62.50%, val_best:  63.75%, tr:  96.63%, tr_best:  97.14%, epoch time: 40.37 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0016%\n",
      "layer   2  Sparsity: 70.3242%\n",
      "layer   3  Sparsity: 71.3623%\n",
      "total_backward_count 396495 real_backward_count 79695  20.100%\n",
      "fc layer 1 self.abs_max_out: 5578.0\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.880881/  2.000599, val:  59.58%, val_best:  63.75%, tr:  96.42%, tr_best:  97.14%, epoch time: 40.56 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0347%\n",
      "layer   2  Sparsity: 70.4798%\n",
      "layer   3  Sparsity: 71.5553%\n",
      "total_backward_count 401390 real_backward_count 80535  20.064%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.887599/  1.983364, val:  53.75%, val_best:  63.75%, tr:  96.63%, tr_best:  97.14%, epoch time: 40.52 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9960%\n",
      "layer   2  Sparsity: 70.3082%\n",
      "layer   3  Sparsity: 71.2467%\n",
      "total_backward_count 406285 real_backward_count 81347  20.022%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.875307/  1.997011, val:  62.50%, val_best:  63.75%, tr:  95.61%, tr_best:  97.14%, epoch time: 40.86 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9657%\n",
      "layer   2  Sparsity: 70.3462%\n",
      "layer   3  Sparsity: 71.2731%\n",
      "total_backward_count 411180 real_backward_count 82178  19.986%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.887298/  1.978176, val:  62.08%, val_best:  63.75%, tr:  95.71%, tr_best:  97.14%, epoch time: 40.42 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0082%\n",
      "layer   2  Sparsity: 70.4408%\n",
      "layer   3  Sparsity: 71.4340%\n",
      "total_backward_count 416075 real_backward_count 82985  19.945%\n",
      "fc layer 1 self.abs_max_out: 5713.0\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.870451/  2.018559, val:  53.75%, val_best:  63.75%, tr:  96.22%, tr_best:  97.14%, epoch time: 40.61 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0407%\n",
      "layer   2  Sparsity: 70.2600%\n",
      "layer   3  Sparsity: 71.2253%\n",
      "total_backward_count 420970 real_backward_count 83802  19.907%\n",
      "fc layer 1 self.abs_max_out: 5718.0\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.880170/  1.996048, val:  65.42%, val_best:  65.42%, tr:  96.22%, tr_best:  97.14%, epoch time: 40.22 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0661%\n",
      "layer   2  Sparsity: 70.3820%\n",
      "layer   3  Sparsity: 70.9091%\n",
      "total_backward_count 425865 real_backward_count 84609  19.868%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.890435/  1.993721, val:  53.33%, val_best:  65.42%, tr:  97.14%, tr_best:  97.14%, epoch time: 40.57 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9385%\n",
      "layer   2  Sparsity: 70.4425%\n",
      "layer   3  Sparsity: 71.0408%\n",
      "total_backward_count 430760 real_backward_count 85397  19.825%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.883447/  1.983730, val:  67.08%, val_best:  67.08%, tr:  96.94%, tr_best:  97.14%, epoch time: 40.21 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0426%\n",
      "layer   2  Sparsity: 70.4223%\n",
      "layer   3  Sparsity: 70.9340%\n",
      "total_backward_count 435655 real_backward_count 86181  19.782%\n",
      "fc layer 2 self.abs_max_out: 3211.0\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.876439/  1.972724, val:  67.50%, val_best:  67.50%, tr:  96.63%, tr_best:  97.14%, epoch time: 40.27 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0596%\n",
      "layer   2  Sparsity: 70.3297%\n",
      "layer   3  Sparsity: 70.8450%\n",
      "total_backward_count 440550 real_backward_count 86934  19.733%\n",
      "fc layer 2 self.abs_max_out: 3378.0\n",
      "fc layer 1 self.abs_max_out: 5752.0\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.863218/  2.001822, val:  61.25%, val_best:  67.50%, tr:  96.02%, tr_best:  97.14%, epoch time: 40.29 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0129%\n",
      "layer   2  Sparsity: 70.2615%\n",
      "layer   3  Sparsity: 70.6551%\n",
      "total_backward_count 445445 real_backward_count 87714  19.691%\n",
      "fc layer 1 self.abs_max_out: 5762.0\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.857903/  1.966523, val:  65.42%, val_best:  67.50%, tr:  97.96%, tr_best:  97.96%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9558%\n",
      "layer   2  Sparsity: 69.6549%\n",
      "layer   3  Sparsity: 70.6515%\n",
      "total_backward_count 450340 real_backward_count 88459  19.643%\n",
      "fc layer 2 self.abs_max_out: 3422.0\n",
      "fc layer 1 self.abs_max_out: 5859.0\n",
      "lif layer 2 self.abs_max_v: 5437.0\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.861578/  1.977640, val:  55.00%, val_best:  67.50%, tr:  96.32%, tr_best:  97.96%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0188%\n",
      "layer   2  Sparsity: 69.8331%\n",
      "layer   3  Sparsity: 70.9297%\n",
      "total_backward_count 455235 real_backward_count 89208  19.596%\n",
      "fc layer 1 self.abs_max_out: 5879.0\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.860281/  1.967489, val:  57.50%, val_best:  67.50%, tr:  96.83%, tr_best:  97.96%, epoch time: 40.57 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0094%\n",
      "layer   2  Sparsity: 70.0573%\n",
      "layer   3  Sparsity: 70.9192%\n",
      "total_backward_count 460130 real_backward_count 89985  19.556%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.862795/  1.973714, val:  67.08%, val_best:  67.50%, tr:  95.20%, tr_best:  97.96%, epoch time: 40.94 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0785%\n",
      "layer   2  Sparsity: 70.3579%\n",
      "layer   3  Sparsity: 71.0911%\n",
      "total_backward_count 465025 real_backward_count 90818  19.530%\n",
      "fc layer 1 self.abs_max_out: 5920.0\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.870132/  1.989267, val:  58.33%, val_best:  67.50%, tr:  97.24%, tr_best:  97.96%, epoch time: 40.46 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9905%\n",
      "layer   2  Sparsity: 70.1651%\n",
      "layer   3  Sparsity: 70.5687%\n",
      "total_backward_count 469920 real_backward_count 91611  19.495%\n",
      "fc layer 2 self.abs_max_out: 3435.0\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.867149/  1.990197, val:  62.08%, val_best:  67.50%, tr:  96.32%, tr_best:  97.96%, epoch time: 41.19 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.0055%\n",
      "layer   2  Sparsity: 70.4714%\n",
      "layer   3  Sparsity: 70.7443%\n",
      "total_backward_count 474815 real_backward_count 92344  19.448%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.862460/  1.964727, val:  67.92%, val_best:  67.92%, tr:  96.53%, tr_best:  97.96%, epoch time: 40.60 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0220%\n",
      "layer   2  Sparsity: 70.4378%\n",
      "layer   3  Sparsity: 71.4803%\n",
      "total_backward_count 479710 real_backward_count 93137  19.415%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.863605/  1.990690, val:  52.50%, val_best:  67.92%, tr:  96.53%, tr_best:  97.96%, epoch time: 40.56 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0702%\n",
      "layer   2  Sparsity: 70.4837%\n",
      "layer   3  Sparsity: 71.4724%\n",
      "total_backward_count 484605 real_backward_count 93885  19.374%\n",
      "lif layer 1 self.abs_max_v: 8712.5\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.856917/  1.968149, val:  70.42%, val_best:  70.42%, tr:  97.04%, tr_best:  97.96%, epoch time: 40.40 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0278%\n",
      "layer   2  Sparsity: 70.0805%\n",
      "layer   3  Sparsity: 71.5184%\n",
      "total_backward_count 489500 real_backward_count 94638  19.334%\n",
      "fc layer 2 self.abs_max_out: 3449.0\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.865305/  1.969540, val:  64.58%, val_best:  70.42%, tr:  95.91%, tr_best:  97.96%, epoch time: 40.55 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9966%\n",
      "layer   2  Sparsity: 70.0344%\n",
      "layer   3  Sparsity: 71.4162%\n",
      "total_backward_count 494395 real_backward_count 95422  19.301%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.863233/  1.974278, val:  65.83%, val_best:  70.42%, tr:  97.34%, tr_best:  97.96%, epoch time: 40.05 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9511%\n",
      "layer   2  Sparsity: 70.1089%\n",
      "layer   3  Sparsity: 71.6800%\n",
      "total_backward_count 499290 real_backward_count 96221  19.272%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.873543/  1.987016, val:  62.92%, val_best:  70.42%, tr:  97.14%, tr_best:  97.96%, epoch time: 40.77 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0052%\n",
      "layer   2  Sparsity: 70.4958%\n",
      "layer   3  Sparsity: 71.4896%\n",
      "total_backward_count 504185 real_backward_count 96971  19.233%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.867352/  1.986734, val:  62.92%, val_best:  70.42%, tr:  97.24%, tr_best:  97.96%, epoch time: 40.62 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9657%\n",
      "layer   2  Sparsity: 70.5134%\n",
      "layer   3  Sparsity: 71.0030%\n",
      "total_backward_count 509080 real_backward_count 97751  19.202%\n",
      "fc layer 2 self.abs_max_out: 3475.0\n",
      "lif layer 1 self.abs_max_v: 8866.5\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.868311/  1.990937, val:  63.75%, val_best:  70.42%, tr:  95.81%, tr_best:  97.96%, epoch time: 40.86 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9704%\n",
      "layer   2  Sparsity: 70.2669%\n",
      "layer   3  Sparsity: 71.1368%\n",
      "total_backward_count 513975 real_backward_count 98529  19.170%\n",
      "fc layer 1 self.abs_max_out: 5973.0\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.872449/  1.974908, val:  61.25%, val_best:  70.42%, tr:  96.42%, tr_best:  97.96%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0382%\n",
      "layer   2  Sparsity: 70.3621%\n",
      "layer   3  Sparsity: 70.8304%\n",
      "total_backward_count 518870 real_backward_count 99341  19.146%\n",
      "fc layer 1 self.abs_max_out: 6001.0\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.866839/  1.975081, val:  58.33%, val_best:  70.42%, tr:  97.14%, tr_best:  97.96%, epoch time: 40.80 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0546%\n",
      "layer   2  Sparsity: 70.5936%\n",
      "layer   3  Sparsity: 71.0657%\n",
      "total_backward_count 523765 real_backward_count 100079  19.108%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.870578/  1.981754, val:  58.33%, val_best:  70.42%, tr:  97.55%, tr_best:  97.96%, epoch time: 40.78 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0370%\n",
      "layer   2  Sparsity: 70.2842%\n",
      "layer   3  Sparsity: 71.2676%\n",
      "total_backward_count 528660 real_backward_count 100816  19.070%\n",
      "fc layer 1 self.abs_max_out: 6067.0\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.865038/  1.961892, val:  60.83%, val_best:  70.42%, tr:  97.04%, tr_best:  97.96%, epoch time: 40.42 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0356%\n",
      "layer   2  Sparsity: 69.8870%\n",
      "layer   3  Sparsity: 70.6937%\n",
      "total_backward_count 533555 real_backward_count 101606  19.043%\n",
      "fc layer 1 self.abs_max_out: 6165.0\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.860734/  1.967202, val:  63.33%, val_best:  70.42%, tr:  97.55%, tr_best:  97.96%, epoch time: 40.42 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0298%\n",
      "layer   2  Sparsity: 69.7722%\n",
      "layer   3  Sparsity: 70.8769%\n",
      "total_backward_count 538450 real_backward_count 102338  19.006%\n",
      "fc layer 2 self.abs_max_out: 3513.0\n",
      "fc layer 1 self.abs_max_out: 6190.0\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.865977/  1.965843, val:  62.08%, val_best:  70.42%, tr:  97.14%, tr_best:  97.96%, epoch time: 40.65 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0372%\n",
      "layer   2  Sparsity: 69.6605%\n",
      "layer   3  Sparsity: 70.7478%\n",
      "total_backward_count 543345 real_backward_count 103123  18.979%\n",
      "fc layer 2 self.abs_max_out: 3578.0\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.852366/  1.946398, val:  66.67%, val_best:  70.42%, tr:  97.75%, tr_best:  97.96%, epoch time: 40.69 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0076%\n",
      "layer   2  Sparsity: 69.7894%\n",
      "layer   3  Sparsity: 70.8097%\n",
      "total_backward_count 548240 real_backward_count 103848  18.942%\n",
      "fc layer 2 self.abs_max_out: 3677.0\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.841346/  1.961813, val:  66.25%, val_best:  70.42%, tr:  97.85%, tr_best:  97.96%, epoch time: 40.57 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0121%\n",
      "layer   2  Sparsity: 69.8250%\n",
      "layer   3  Sparsity: 70.8125%\n",
      "total_backward_count 553135 real_backward_count 104609  18.912%\n",
      "lif layer 2 self.abs_max_v: 5458.5\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.841375/  1.955029, val:  63.33%, val_best:  70.42%, tr:  97.65%, tr_best:  97.96%, epoch time: 40.43 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9874%\n",
      "layer   2  Sparsity: 69.6719%\n",
      "layer   3  Sparsity: 70.7514%\n",
      "total_backward_count 558030 real_backward_count 105326  18.875%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.847304/  1.963005, val:  55.00%, val_best:  70.42%, tr:  97.45%, tr_best:  97.96%, epoch time: 40.56 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9926%\n",
      "layer   2  Sparsity: 70.0050%\n",
      "layer   3  Sparsity: 70.5418%\n",
      "total_backward_count 562925 real_backward_count 106100  18.848%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.848533/  1.966904, val:  59.58%, val_best:  70.42%, tr:  97.04%, tr_best:  97.96%, epoch time: 40.75 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0310%\n",
      "layer   2  Sparsity: 69.8562%\n",
      "layer   3  Sparsity: 70.8934%\n",
      "total_backward_count 567820 real_backward_count 106792  18.807%\n",
      "lif layer 2 self.abs_max_v: 5461.0\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.861211/  1.976281, val:  56.25%, val_best:  70.42%, tr:  96.83%, tr_best:  97.96%, epoch time: 40.04 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9992%\n",
      "layer   2  Sparsity: 69.5936%\n",
      "layer   3  Sparsity: 71.7273%\n",
      "total_backward_count 572715 real_backward_count 107553  18.779%\n",
      "fc layer 1 self.abs_max_out: 6264.0\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.865003/  1.972270, val:  62.08%, val_best:  70.42%, tr:  97.24%, tr_best:  97.96%, epoch time: 40.76 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9969%\n",
      "layer   2  Sparsity: 69.7179%\n",
      "layer   3  Sparsity: 70.9484%\n",
      "total_backward_count 577610 real_backward_count 108324  18.754%\n",
      "fc layer 2 self.abs_max_out: 3682.0\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.864192/  1.966121, val:  65.42%, val_best:  70.42%, tr:  97.34%, tr_best:  97.96%, epoch time: 40.63 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0359%\n",
      "layer   2  Sparsity: 69.8965%\n",
      "layer   3  Sparsity: 71.0104%\n",
      "total_backward_count 582505 real_backward_count 109061  18.723%\n",
      "lif layer 2 self.abs_max_v: 5503.5\n",
      "fc layer 1 self.abs_max_out: 6324.0\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.856326/  1.963138, val:  65.00%, val_best:  70.42%, tr:  97.34%, tr_best:  97.96%, epoch time: 40.39 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9780%\n",
      "layer   2  Sparsity: 69.8247%\n",
      "layer   3  Sparsity: 70.8916%\n",
      "total_backward_count 587400 real_backward_count 109783  18.690%\n",
      "lif layer 1 self.abs_max_v: 9188.5\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.845123/  1.965659, val:  65.83%, val_best:  70.42%, tr:  96.73%, tr_best:  97.96%, epoch time: 40.49 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0012%\n",
      "layer   2  Sparsity: 69.9693%\n",
      "layer   3  Sparsity: 71.1680%\n",
      "total_backward_count 592295 real_backward_count 110529  18.661%\n",
      "fc layer 2 self.abs_max_out: 3770.0\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.853581/  1.970464, val:  63.33%, val_best:  70.42%, tr:  97.45%, tr_best:  97.96%, epoch time: 40.42 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0132%\n",
      "layer   2  Sparsity: 69.6439%\n",
      "layer   3  Sparsity: 70.8236%\n",
      "total_backward_count 597190 real_backward_count 111249  18.629%\n",
      "fc layer 2 self.abs_max_out: 3788.0\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.861148/  1.988649, val:  47.92%, val_best:  70.42%, tr:  96.83%, tr_best:  97.96%, epoch time: 40.68 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9464%\n",
      "layer   2  Sparsity: 69.3614%\n",
      "layer   3  Sparsity: 70.3140%\n",
      "total_backward_count 602085 real_backward_count 111981  18.599%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.856295/  1.974675, val:  60.83%, val_best:  70.42%, tr:  97.75%, tr_best:  97.96%, epoch time: 39.87 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 92.9919%\n",
      "layer   2  Sparsity: 69.6123%\n",
      "layer   3  Sparsity: 70.6469%\n",
      "total_backward_count 606980 real_backward_count 112698  18.567%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.848396/  1.944206, val:  70.83%, val_best:  70.83%, tr:  97.24%, tr_best:  97.96%, epoch time: 40.65 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9569%\n",
      "layer   2  Sparsity: 69.6621%\n",
      "layer   3  Sparsity: 71.0019%\n",
      "total_backward_count 611875 real_backward_count 113424  18.537%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.845128/  1.950981, val:  60.00%, val_best:  70.83%, tr:  97.24%, tr_best:  97.96%, epoch time: 40.05 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9860%\n",
      "layer   2  Sparsity: 69.6018%\n",
      "layer   3  Sparsity: 70.8977%\n",
      "total_backward_count 616770 real_backward_count 114105  18.500%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.853734/  1.963133, val:  52.50%, val_best:  70.83%, tr:  95.51%, tr_best:  97.96%, epoch time: 40.81 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9845%\n",
      "layer   2  Sparsity: 69.7384%\n",
      "layer   3  Sparsity: 70.9765%\n",
      "total_backward_count 621665 real_backward_count 114873  18.478%\n",
      "fc layer 1 self.abs_max_out: 6332.0\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.848634/  1.960562, val:  67.92%, val_best:  70.83%, tr:  97.96%, tr_best:  97.96%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9988%\n",
      "layer   2  Sparsity: 69.6035%\n",
      "layer   3  Sparsity: 71.1228%\n",
      "total_backward_count 626560 real_backward_count 115608  18.451%\n",
      "fc layer 1 self.abs_max_out: 6353.0\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.865637/  1.983938, val:  70.42%, val_best:  70.83%, tr:  97.65%, tr_best:  97.96%, epoch time: 40.95 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9982%\n",
      "layer   2  Sparsity: 69.6820%\n",
      "layer   3  Sparsity: 71.0421%\n",
      "total_backward_count 631455 real_backward_count 116340  18.424%\n",
      "fc layer 1 self.abs_max_out: 6431.0\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.866752/  1.981123, val:  57.92%, val_best:  70.83%, tr:  97.45%, tr_best:  97.96%, epoch time: 40.30 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0126%\n",
      "layer   2  Sparsity: 69.8228%\n",
      "layer   3  Sparsity: 71.1136%\n",
      "total_backward_count 636350 real_backward_count 117089  18.400%\n",
      "lif layer 2 self.abs_max_v: 5732.5\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.864634/  1.970031, val:  46.25%, val_best:  70.83%, tr:  97.96%, tr_best:  97.96%, epoch time: 40.85 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0216%\n",
      "layer   2  Sparsity: 69.5984%\n",
      "layer   3  Sparsity: 71.2943%\n",
      "total_backward_count 641245 real_backward_count 117808  18.372%\n",
      "lif layer 1 self.abs_max_v: 9335.5\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.851624/  1.953743, val:  68.33%, val_best:  70.83%, tr:  96.94%, tr_best:  97.96%, epoch time: 40.29 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0147%\n",
      "layer   2  Sparsity: 69.5720%\n",
      "layer   3  Sparsity: 71.9637%\n",
      "total_backward_count 646140 real_backward_count 118561  18.349%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.846720/  1.962208, val:  60.00%, val_best:  70.83%, tr:  96.63%, tr_best:  97.96%, epoch time: 40.89 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0071%\n",
      "layer   2  Sparsity: 69.8717%\n",
      "layer   3  Sparsity: 71.9095%\n",
      "total_backward_count 651035 real_backward_count 119252  18.317%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.847773/  1.962434, val:  57.92%, val_best:  70.83%, tr:  96.83%, tr_best:  97.96%, epoch time: 40.43 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9260%\n",
      "layer   2  Sparsity: 70.0313%\n",
      "layer   3  Sparsity: 71.3053%\n",
      "total_backward_count 655930 real_backward_count 119988  18.293%\n",
      "fc layer 1 self.abs_max_out: 6467.0\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.862585/  1.992537, val:  62.08%, val_best:  70.83%, tr:  96.63%, tr_best:  97.96%, epoch time: 40.88 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0113%\n",
      "layer   2  Sparsity: 70.0110%\n",
      "layer   3  Sparsity: 71.1217%\n",
      "total_backward_count 660825 real_backward_count 120735  18.270%\n",
      "fc layer 1 self.abs_max_out: 6501.0\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.867452/  1.958470, val:  68.75%, val_best:  70.83%, tr:  97.45%, tr_best:  97.96%, epoch time: 40.44 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0282%\n",
      "layer   2  Sparsity: 69.9816%\n",
      "layer   3  Sparsity: 71.5744%\n",
      "total_backward_count 665720 real_backward_count 121513  18.253%\n",
      "fc layer 1 self.abs_max_out: 6502.0\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.845602/  1.948756, val:  58.33%, val_best:  70.83%, tr:  97.65%, tr_best:  97.96%, epoch time: 40.26 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0203%\n",
      "layer   2  Sparsity: 69.8445%\n",
      "layer   3  Sparsity: 71.7413%\n",
      "total_backward_count 670615 real_backward_count 122243  18.228%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.830672/  1.953571, val:  65.00%, val_best:  70.83%, tr:  97.75%, tr_best:  97.96%, epoch time: 40.73 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9714%\n",
      "layer   2  Sparsity: 69.5223%\n",
      "layer   3  Sparsity: 71.9261%\n",
      "total_backward_count 675510 real_backward_count 122911  18.195%\n",
      "fc layer 1 self.abs_max_out: 6508.0\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.854305/  1.962908, val:  65.00%, val_best:  70.83%, tr:  98.16%, tr_best:  98.16%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0239%\n",
      "layer   2  Sparsity: 69.5378%\n",
      "layer   3  Sparsity: 71.7063%\n",
      "total_backward_count 680405 real_backward_count 123644  18.172%\n",
      "lif layer 1 self.abs_max_v: 9422.0\n",
      "fc layer 1 self.abs_max_out: 6543.0\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.846219/  1.962323, val:  67.92%, val_best:  70.83%, tr:  97.55%, tr_best:  98.16%, epoch time: 40.55 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9900%\n",
      "layer   2  Sparsity: 69.7402%\n",
      "layer   3  Sparsity: 72.2203%\n",
      "total_backward_count 685300 real_backward_count 124363  18.147%\n",
      "fc layer 1 self.abs_max_out: 6705.0\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.845084/  1.958024, val:  62.92%, val_best:  70.83%, tr:  97.85%, tr_best:  98.16%, epoch time: 40.56 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0207%\n",
      "layer   2  Sparsity: 70.0277%\n",
      "layer   3  Sparsity: 72.3821%\n",
      "total_backward_count 690195 real_backward_count 125068  18.121%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.845063/  1.965972, val:  68.33%, val_best:  70.83%, tr:  96.73%, tr_best:  98.16%, epoch time: 40.79 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0007%\n",
      "layer   2  Sparsity: 69.9005%\n",
      "layer   3  Sparsity: 72.3445%\n",
      "total_backward_count 695090 real_backward_count 125801  18.099%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.857741/  1.960250, val:  67.92%, val_best:  70.83%, tr:  98.06%, tr_best:  98.16%, epoch time: 39.92 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9928%\n",
      "layer   2  Sparsity: 70.0399%\n",
      "layer   3  Sparsity: 72.0740%\n",
      "total_backward_count 699985 real_backward_count 126524  18.075%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.847747/  1.961095, val:  68.33%, val_best:  70.83%, tr:  97.96%, tr_best:  98.16%, epoch time: 40.59 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9714%\n",
      "layer   2  Sparsity: 70.0139%\n",
      "layer   3  Sparsity: 72.0452%\n",
      "total_backward_count 704880 real_backward_count 127211  18.047%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.847154/  1.948256, val:  66.67%, val_best:  70.83%, tr:  97.45%, tr_best:  98.16%, epoch time: 40.18 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0035%\n",
      "layer   2  Sparsity: 69.8081%\n",
      "layer   3  Sparsity: 71.7454%\n",
      "total_backward_count 709775 real_backward_count 127945  18.026%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.843319/  1.954387, val:  62.08%, val_best:  70.83%, tr:  97.55%, tr_best:  98.16%, epoch time: 40.45 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0142%\n",
      "layer   2  Sparsity: 69.7436%\n",
      "layer   3  Sparsity: 71.5999%\n",
      "total_backward_count 714670 real_backward_count 128637  17.999%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.837660/  1.948897, val:  66.25%, val_best:  70.83%, tr:  96.83%, tr_best:  98.16%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0450%\n",
      "layer   2  Sparsity: 69.7550%\n",
      "layer   3  Sparsity: 71.2427%\n",
      "total_backward_count 719565 real_backward_count 129357  17.977%\n",
      "fc layer 1 self.abs_max_out: 6712.0\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.830214/  1.949912, val:  56.25%, val_best:  70.83%, tr:  97.65%, tr_best:  98.16%, epoch time: 40.75 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0233%\n",
      "layer   2  Sparsity: 69.7932%\n",
      "layer   3  Sparsity: 71.5958%\n",
      "total_backward_count 724460 real_backward_count 130029  17.948%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.841201/  1.951484, val:  62.92%, val_best:  70.83%, tr:  97.34%, tr_best:  98.16%, epoch time: 40.29 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9624%\n",
      "layer   2  Sparsity: 69.7211%\n",
      "layer   3  Sparsity: 71.8703%\n",
      "total_backward_count 729355 real_backward_count 130716  17.922%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.849668/  1.940736, val:  70.42%, val_best:  70.83%, tr:  97.04%, tr_best:  98.16%, epoch time: 40.65 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0320%\n",
      "layer   2  Sparsity: 69.8909%\n",
      "layer   3  Sparsity: 72.0923%\n",
      "total_backward_count 734250 real_backward_count 131469  17.905%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.844763/  1.949116, val:  63.33%, val_best:  70.83%, tr:  98.37%, tr_best:  98.37%, epoch time: 40.37 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9741%\n",
      "layer   2  Sparsity: 69.8553%\n",
      "layer   3  Sparsity: 71.6634%\n",
      "total_backward_count 739145 real_backward_count 132112  17.874%\n",
      "fc layer 1 self.abs_max_out: 6761.0\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.851040/  1.965784, val:  68.33%, val_best:  70.83%, tr:  97.04%, tr_best:  98.37%, epoch time: 40.64 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0176%\n",
      "layer   2  Sparsity: 70.0115%\n",
      "layer   3  Sparsity: 71.9243%\n",
      "total_backward_count 744040 real_backward_count 132841  17.854%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.854279/  1.964051, val:  66.67%, val_best:  70.83%, tr:  97.75%, tr_best:  98.37%, epoch time: 39.37 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 93.0216%\n",
      "layer   2  Sparsity: 69.9266%\n",
      "layer   3  Sparsity: 71.5219%\n",
      "total_backward_count 748935 real_backward_count 133485  17.823%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.843973/  1.935813, val:  72.92%, val_best:  72.92%, tr:  98.06%, tr_best:  98.37%, epoch time: 39.31 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 93.0650%\n",
      "layer   2  Sparsity: 69.8940%\n",
      "layer   3  Sparsity: 71.1148%\n",
      "total_backward_count 753830 real_backward_count 134206  17.803%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.848943/  1.949888, val:  66.25%, val_best:  72.92%, tr:  97.45%, tr_best:  98.37%, epoch time: 41.21 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.0278%\n",
      "layer   2  Sparsity: 69.8373%\n",
      "layer   3  Sparsity: 71.3465%\n",
      "total_backward_count 758725 real_backward_count 134919  17.782%\n",
      "lif layer 1 self.abs_max_v: 9752.5\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.837766/  1.945778, val:  66.25%, val_best:  72.92%, tr:  97.85%, tr_best:  98.37%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9905%\n",
      "layer   2  Sparsity: 69.8696%\n",
      "layer   3  Sparsity: 71.5613%\n",
      "total_backward_count 763620 real_backward_count 135609  17.759%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.843618/  1.972432, val:  52.08%, val_best:  72.92%, tr:  97.96%, tr_best:  98.37%, epoch time: 40.89 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9933%\n",
      "layer   2  Sparsity: 69.9518%\n",
      "layer   3  Sparsity: 71.5618%\n",
      "total_backward_count 768515 real_backward_count 136251  17.729%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.860249/  1.953665, val:  71.25%, val_best:  72.92%, tr:  97.85%, tr_best:  98.37%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0334%\n",
      "layer   2  Sparsity: 69.8607%\n",
      "layer   3  Sparsity: 71.1610%\n",
      "total_backward_count 773410 real_backward_count 136909  17.702%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.853348/  1.966977, val:  71.67%, val_best:  72.92%, tr:  97.45%, tr_best:  98.37%, epoch time: 40.43 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9966%\n",
      "layer   2  Sparsity: 69.7671%\n",
      "layer   3  Sparsity: 71.1474%\n",
      "total_backward_count 778305 real_backward_count 137595  17.679%\n",
      "fc layer 2 self.abs_max_out: 3910.0\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.851768/  1.950101, val:  67.08%, val_best:  72.92%, tr:  97.85%, tr_best:  98.37%, epoch time: 40.33 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9481%\n",
      "layer   2  Sparsity: 69.6920%\n",
      "layer   3  Sparsity: 71.6320%\n",
      "total_backward_count 783200 real_backward_count 138275  17.655%\n",
      "fc layer 2 self.abs_max_out: 3912.0\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.845072/  1.952878, val:  70.42%, val_best:  72.92%, tr:  97.65%, tr_best:  98.37%, epoch time: 40.82 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0066%\n",
      "layer   2  Sparsity: 69.6551%\n",
      "layer   3  Sparsity: 71.2286%\n",
      "total_backward_count 788095 real_backward_count 138961  17.633%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.851128/  1.958365, val:  61.67%, val_best:  72.92%, tr:  97.75%, tr_best:  98.37%, epoch time: 40.37 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0162%\n",
      "layer   2  Sparsity: 70.1295%\n",
      "layer   3  Sparsity: 70.9366%\n",
      "total_backward_count 792990 real_backward_count 139616  17.606%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.850599/  1.954308, val:  67.50%, val_best:  72.92%, tr:  97.14%, tr_best:  98.37%, epoch time: 39.70 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 93.0089%\n",
      "layer   2  Sparsity: 70.0952%\n",
      "layer   3  Sparsity: 71.3344%\n",
      "total_backward_count 797885 real_backward_count 140313  17.586%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.843772/  1.967281, val:  57.08%, val_best:  72.92%, tr:  97.65%, tr_best:  98.37%, epoch time: 40.06 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9954%\n",
      "layer   2  Sparsity: 69.8116%\n",
      "layer   3  Sparsity: 71.8410%\n",
      "total_backward_count 802780 real_backward_count 140972  17.560%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.859589/  1.979005, val:  63.75%, val_best:  72.92%, tr:  97.24%, tr_best:  98.37%, epoch time: 40.44 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9721%\n",
      "layer   2  Sparsity: 69.7008%\n",
      "layer   3  Sparsity: 71.6607%\n",
      "total_backward_count 807675 real_backward_count 141654  17.538%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.864319/  1.974303, val:  62.50%, val_best:  72.92%, tr:  97.65%, tr_best:  98.37%, epoch time: 40.93 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9978%\n",
      "layer   2  Sparsity: 69.5605%\n",
      "layer   3  Sparsity: 71.3401%\n",
      "total_backward_count 812570 real_backward_count 142308  17.513%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.854915/  1.962141, val:  72.08%, val_best:  72.92%, tr:  98.26%, tr_best:  98.37%, epoch time: 40.64 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0630%\n",
      "layer   2  Sparsity: 69.5244%\n",
      "layer   3  Sparsity: 71.5931%\n",
      "total_backward_count 817465 real_backward_count 142957  17.488%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.850910/  1.964522, val:  66.25%, val_best:  72.92%, tr:  97.85%, tr_best:  98.37%, epoch time: 40.90 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9476%\n",
      "layer   2  Sparsity: 69.5494%\n",
      "layer   3  Sparsity: 71.5698%\n",
      "total_backward_count 822360 real_backward_count 143649  17.468%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.849752/  1.961028, val:  64.58%, val_best:  72.92%, tr:  97.85%, tr_best:  98.37%, epoch time: 40.83 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0231%\n",
      "layer   2  Sparsity: 69.9672%\n",
      "layer   3  Sparsity: 70.9879%\n",
      "total_backward_count 827255 real_backward_count 144331  17.447%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.854332/  1.954622, val:  67.08%, val_best:  72.92%, tr:  97.65%, tr_best:  98.37%, epoch time: 40.97 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9788%\n",
      "layer   2  Sparsity: 69.8329%\n",
      "layer   3  Sparsity: 71.1575%\n",
      "total_backward_count 832150 real_backward_count 145026  17.428%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.852181/  1.967079, val:  58.33%, val_best:  72.92%, tr:  97.75%, tr_best:  98.37%, epoch time: 40.68 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0343%\n",
      "layer   2  Sparsity: 69.8446%\n",
      "layer   3  Sparsity: 71.2464%\n",
      "total_backward_count 837045 real_backward_count 145678  17.404%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.847533/  1.968492, val:  63.33%, val_best:  72.92%, tr:  97.34%, tr_best:  98.37%, epoch time: 40.76 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0115%\n",
      "layer   2  Sparsity: 69.7505%\n",
      "layer   3  Sparsity: 71.0025%\n",
      "total_backward_count 841940 real_backward_count 146330  17.380%\n",
      "fc layer 2 self.abs_max_out: 3947.0\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.847692/  1.951648, val:  70.83%, val_best:  72.92%, tr:  97.55%, tr_best:  98.37%, epoch time: 40.88 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0187%\n",
      "layer   2  Sparsity: 69.7483%\n",
      "layer   3  Sparsity: 70.5922%\n",
      "total_backward_count 846835 real_backward_count 147023  17.361%\n",
      "lif layer 1 self.abs_max_v: 9914.5\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.835232/  1.939552, val:  70.00%, val_best:  72.92%, tr:  97.85%, tr_best:  98.37%, epoch time: 40.05 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0227%\n",
      "layer   2  Sparsity: 69.7464%\n",
      "layer   3  Sparsity: 70.5424%\n",
      "total_backward_count 851730 real_backward_count 147694  17.340%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.830150/  1.944460, val:  70.83%, val_best:  72.92%, tr:  98.16%, tr_best:  98.37%, epoch time: 40.92 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9877%\n",
      "layer   2  Sparsity: 69.8933%\n",
      "layer   3  Sparsity: 70.9765%\n",
      "total_backward_count 856625 real_backward_count 148358  17.319%\n",
      "fc layer 2 self.abs_max_out: 3986.0\n",
      "lif layer 1 self.abs_max_v: 9994.5\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.830751/  1.942229, val:  66.25%, val_best:  72.92%, tr:  97.34%, tr_best:  98.37%, epoch time: 41.07 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9804%\n",
      "layer   2  Sparsity: 69.6432%\n",
      "layer   3  Sparsity: 70.8842%\n",
      "total_backward_count 861520 real_backward_count 149059  17.302%\n",
      "fc layer 2 self.abs_max_out: 3997.0\n",
      "fc layer 2 self.abs_max_out: 4002.0\n",
      "lif layer 2 self.abs_max_v: 5784.0\n",
      "lif layer 1 self.abs_max_v: 10111.5\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.820390/  1.917411, val:  67.50%, val_best:  72.92%, tr:  98.37%, tr_best:  98.37%, epoch time: 40.78 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0458%\n",
      "layer   2  Sparsity: 69.6265%\n",
      "layer   3  Sparsity: 70.9091%\n",
      "total_backward_count 866415 real_backward_count 149680  17.276%\n",
      "lif layer 1 self.abs_max_v: 10200.0\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.826287/  1.945794, val:  67.50%, val_best:  72.92%, tr:  96.53%, tr_best:  98.37%, epoch time: 40.76 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9741%\n",
      "layer   2  Sparsity: 69.4546%\n",
      "layer   3  Sparsity: 71.5571%\n",
      "total_backward_count 871310 real_backward_count 150380  17.259%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.835235/  1.951140, val:  67.08%, val_best:  72.92%, tr:  98.37%, tr_best:  98.37%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0334%\n",
      "layer   2  Sparsity: 69.5238%\n",
      "layer   3  Sparsity: 71.2049%\n",
      "total_backward_count 876205 real_backward_count 151048  17.239%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.830050/  1.928747, val:  74.17%, val_best:  74.17%, tr:  98.47%, tr_best:  98.47%, epoch time: 40.39 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0267%\n",
      "layer   2  Sparsity: 69.7934%\n",
      "layer   3  Sparsity: 71.0848%\n",
      "total_backward_count 881100 real_backward_count 151705  17.218%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.823085/  1.916082, val:  71.67%, val_best:  74.17%, tr:  98.47%, tr_best:  98.47%, epoch time: 40.43 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9538%\n",
      "layer   2  Sparsity: 69.5506%\n",
      "layer   3  Sparsity: 71.2113%\n",
      "total_backward_count 885995 real_backward_count 152365  17.197%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.833680/  1.944444, val:  73.75%, val_best:  74.17%, tr:  97.85%, tr_best:  98.47%, epoch time: 40.20 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9766%\n",
      "layer   2  Sparsity: 69.3434%\n",
      "layer   3  Sparsity: 71.2363%\n",
      "total_backward_count 890890 real_backward_count 153025  17.177%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.830639/  1.954615, val:  59.17%, val_best:  74.17%, tr:  98.47%, tr_best:  98.47%, epoch time: 40.19 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9770%\n",
      "layer   2  Sparsity: 69.2086%\n",
      "layer   3  Sparsity: 71.1691%\n",
      "total_backward_count 895785 real_backward_count 153649  17.152%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.839033/  1.962157, val:  59.17%, val_best:  74.17%, tr:  97.34%, tr_best:  98.47%, epoch time: 40.61 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9709%\n",
      "layer   2  Sparsity: 69.1322%\n",
      "layer   3  Sparsity: 71.3013%\n",
      "total_backward_count 900680 real_backward_count 154274  17.129%\n",
      "fc layer 1 self.abs_max_out: 6766.0\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.837873/  1.936007, val:  58.75%, val_best:  74.17%, tr:  98.06%, tr_best:  98.47%, epoch time: 40.18 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9569%\n",
      "layer   2  Sparsity: 69.3410%\n",
      "layer   3  Sparsity: 71.0410%\n",
      "total_backward_count 905575 real_backward_count 154926  17.108%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.828492/  1.936475, val:  75.42%, val_best:  75.42%, tr:  97.75%, tr_best:  98.47%, epoch time: 41.16 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.0164%\n",
      "layer   2  Sparsity: 69.5466%\n",
      "layer   3  Sparsity: 71.3815%\n",
      "total_backward_count 910470 real_backward_count 155596  17.090%\n",
      "fc layer 1 self.abs_max_out: 6822.0\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.840180/  1.962123, val:  66.67%, val_best:  75.42%, tr:  97.34%, tr_best:  98.47%, epoch time: 40.85 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0091%\n",
      "layer   2  Sparsity: 69.5422%\n",
      "layer   3  Sparsity: 71.4643%\n",
      "total_backward_count 915365 real_backward_count 156260  17.071%\n",
      "fc layer 1 self.abs_max_out: 6877.0\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.843557/  1.960812, val:  60.83%, val_best:  75.42%, tr:  97.96%, tr_best:  98.47%, epoch time: 40.80 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0299%\n",
      "layer   2  Sparsity: 69.4947%\n",
      "layer   3  Sparsity: 71.8454%\n",
      "total_backward_count 920260 real_backward_count 156919  17.052%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.835544/  1.943571, val:  74.17%, val_best:  75.42%, tr:  98.37%, tr_best:  98.47%, epoch time: 40.00 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9840%\n",
      "layer   2  Sparsity: 69.1879%\n",
      "layer   3  Sparsity: 71.0861%\n",
      "total_backward_count 925155 real_backward_count 157562  17.031%\n",
      "fc layer 1 self.abs_max_out: 6928.0\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.829600/  1.952438, val:  62.08%, val_best:  75.42%, tr:  98.37%, tr_best:  98.47%, epoch time: 41.03 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9832%\n",
      "layer   2  Sparsity: 69.3704%\n",
      "layer   3  Sparsity: 71.7502%\n",
      "total_backward_count 930050 real_backward_count 158181  17.008%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.834556/  1.945855, val:  68.75%, val_best:  75.42%, tr:  98.57%, tr_best:  98.57%, epoch time: 40.56 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0274%\n",
      "layer   2  Sparsity: 69.6272%\n",
      "layer   3  Sparsity: 72.2388%\n",
      "total_backward_count 934945 real_backward_count 158788  16.984%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.843910/  1.951563, val:  55.83%, val_best:  75.42%, tr:  98.16%, tr_best:  98.57%, epoch time: 41.33 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.0205%\n",
      "layer   2  Sparsity: 69.3851%\n",
      "layer   3  Sparsity: 71.8096%\n",
      "total_backward_count 939840 real_backward_count 159434  16.964%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.854778/  1.968371, val:  63.33%, val_best:  75.42%, tr:  98.26%, tr_best:  98.57%, epoch time: 40.31 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9971%\n",
      "layer   2  Sparsity: 69.3445%\n",
      "layer   3  Sparsity: 72.1882%\n",
      "total_backward_count 944735 real_backward_count 160069  16.943%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.842261/  1.955538, val:  54.58%, val_best:  75.42%, tr:  99.08%, tr_best:  99.08%, epoch time: 40.73 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9386%\n",
      "layer   2  Sparsity: 69.6867%\n",
      "layer   3  Sparsity: 72.4390%\n",
      "total_backward_count 949630 real_backward_count 160670  16.919%\n",
      "lif layer 1 self.abs_max_v: 10246.5\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.843640/  1.930904, val:  72.08%, val_best:  75.42%, tr:  98.06%, tr_best:  99.08%, epoch time: 40.42 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9704%\n",
      "layer   2  Sparsity: 69.8233%\n",
      "layer   3  Sparsity: 72.7512%\n",
      "total_backward_count 954525 real_backward_count 161336  16.902%\n",
      "fc layer 2 self.abs_max_out: 4082.0\n",
      "fc layer 1 self.abs_max_out: 7014.0\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.833779/  1.944661, val:  69.58%, val_best:  75.42%, tr:  97.75%, tr_best:  99.08%, epoch time: 40.44 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0241%\n",
      "layer   2  Sparsity: 69.8807%\n",
      "layer   3  Sparsity: 72.5189%\n",
      "total_backward_count 959420 real_backward_count 162009  16.886%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.830823/  1.926448, val:  77.08%, val_best:  77.08%, tr:  98.06%, tr_best:  99.08%, epoch time: 40.05 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9932%\n",
      "layer   2  Sparsity: 69.6188%\n",
      "layer   3  Sparsity: 72.4304%\n",
      "total_backward_count 964315 real_backward_count 162623  16.864%\n",
      "fc layer 1 self.abs_max_out: 7179.0\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.819949/  1.935381, val:  60.83%, val_best:  77.08%, tr:  98.16%, tr_best:  99.08%, epoch time: 39.98 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9567%\n",
      "layer   2  Sparsity: 69.5025%\n",
      "layer   3  Sparsity: 71.9875%\n",
      "total_backward_count 969210 real_backward_count 163257  16.844%\n",
      "fc layer 1 self.abs_max_out: 7206.0\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.813283/  1.953539, val:  56.25%, val_best:  77.08%, tr:  97.75%, tr_best:  99.08%, epoch time: 39.55 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 92.9683%\n",
      "layer   2  Sparsity: 69.6027%\n",
      "layer   3  Sparsity: 72.0280%\n",
      "total_backward_count 974105 real_backward_count 163910  16.827%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.823668/  1.940855, val:  65.42%, val_best:  77.08%, tr:  98.47%, tr_best:  99.08%, epoch time: 40.55 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0076%\n",
      "layer   2  Sparsity: 69.4705%\n",
      "layer   3  Sparsity: 72.6374%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6329994f01be474c879e21f5c1b3a817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñá‚ñá‚ñÖ‚ñà‚ñà‚ñÖ‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñá‚ñá‚ñÖ‚ñà‚ñà‚ñÖ‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.98468</td></tr><tr><td>tr_epoch_loss</td><td>1.82367</td></tr><tr><td>val_acc_best</td><td>0.77083</td></tr><tr><td>val_acc_now</td><td>0.65417</td></tr><tr><td>val_loss</td><td>1.94086</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">eager-sweep-7</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/f114y14n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/f114y14n</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_014647-f114y14n/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3qwdux9g with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 75000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faca3580428d47f0934728a7b101f8f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112622666405513, max=1.0‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_040245-3qwdux9g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3qwdux9g' target=\"_blank\">lilac-sweep-10</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3qwdux9g' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3qwdux9g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': '20251118_040255_459', 'my_seed': 42, 'TIME': 5, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 75000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 4761a05d236841af8daa3414213005d1\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 91\n",
      "fc layer 1 self.abs_max_out: 485.0\n",
      "lif layer 1 self.abs_max_v: 485.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 343.0\n",
      "lif layer 2 self.abs_max_v: 343.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 59.0\n",
      "lif layer 1 self.abs_max_v: 703.5\n",
      "fc layer 2 self.abs_max_out: 383.0\n",
      "lif layer 2 self.abs_max_v: 528.0\n",
      "fc layer 3 self.abs_max_out: 66.0\n",
      "lif layer 1 self.abs_max_v: 738.0\n",
      "fc layer 2 self.abs_max_out: 596.0\n",
      "lif layer 2 self.abs_max_v: 725.5\n",
      "fc layer 3 self.abs_max_out: 187.0\n",
      "fc layer 1 self.abs_max_out: 558.0\n",
      "lif layer 1 self.abs_max_v: 744.5\n",
      "fc layer 2 self.abs_max_out: 610.0\n",
      "lif layer 2 self.abs_max_v: 763.5\n",
      "smallest_now_T updated: 82\n",
      "fc layer 1 self.abs_max_out: 587.0\n",
      "fc layer 2 self.abs_max_out: 628.0\n",
      "fc layer 3 self.abs_max_out: 253.0\n",
      "fc layer 1 self.abs_max_out: 745.0\n",
      "lif layer 1 self.abs_max_v: 977.0\n",
      "fc layer 2 self.abs_max_out: 632.0\n",
      "fc layer 1 self.abs_max_out: 900.0\n",
      "lif layer 1 self.abs_max_v: 1197.0\n",
      "fc layer 2 self.abs_max_out: 796.0\n",
      "lif layer 2 self.abs_max_v: 1038.0\n",
      "fc layer 1 self.abs_max_out: 989.0\n",
      "lif layer 1 self.abs_max_v: 1233.5\n",
      "fc layer 1 self.abs_max_out: 1046.0\n",
      "fc layer 3 self.abs_max_out: 298.0\n",
      "smallest_now_T updated: 61\n",
      "fc layer 1 self.abs_max_out: 1213.0\n",
      "fc layer 3 self.abs_max_out: 340.0\n",
      "fc layer 1 self.abs_max_out: 1555.0\n",
      "lif layer 1 self.abs_max_v: 1555.0\n",
      "fc layer 2 self.abs_max_out: 840.0\n",
      "lif layer 2 self.abs_max_v: 1307.5\n",
      "fc layer 2 self.abs_max_out: 878.0\n",
      "lif layer 2 self.abs_max_v: 1346.0\n",
      "fc layer 2 self.abs_max_out: 893.0\n",
      "lif layer 2 self.abs_max_v: 1507.0\n",
      "fc layer 2 self.abs_max_out: 897.0\n",
      "fc layer 2 self.abs_max_out: 1028.0\n",
      "fc layer 3 self.abs_max_out: 391.0\n",
      "fc layer 1 self.abs_max_out: 1734.0\n",
      "lif layer 1 self.abs_max_v: 1734.0\n",
      "fc layer 2 self.abs_max_out: 1088.0\n",
      "lif layer 1 self.abs_max_v: 2080.5\n",
      "fc layer 2 self.abs_max_out: 1184.0\n",
      "fc layer 2 self.abs_max_out: 1237.0\n",
      "lif layer 2 self.abs_max_v: 1628.0\n",
      "lif layer 2 self.abs_max_v: 1654.5\n",
      "lif layer 2 self.abs_max_v: 1694.5\n",
      "lif layer 1 self.abs_max_v: 2113.5\n",
      "lif layer 2 self.abs_max_v: 1925.5\n",
      "fc layer 1 self.abs_max_out: 1938.0\n",
      "fc layer 2 self.abs_max_out: 1239.0\n",
      "fc layer 2 self.abs_max_out: 1293.0\n",
      "smallest_now_T updated: 51\n",
      "fc layer 1 self.abs_max_out: 1990.0\n",
      "fc layer 2 self.abs_max_out: 1296.0\n",
      "fc layer 2 self.abs_max_out: 1427.0\n",
      "fc layer 3 self.abs_max_out: 421.0\n",
      "lif layer 1 self.abs_max_v: 2424.5\n",
      "fc layer 1 self.abs_max_out: 2578.0\n",
      "lif layer 1 self.abs_max_v: 2690.5\n",
      "lif layer 1 self.abs_max_v: 2860.0\n",
      "lif layer 1 self.abs_max_v: 3145.0\n",
      "lif layer 2 self.abs_max_v: 2051.5\n",
      "lif layer 1 self.abs_max_v: 3487.5\n",
      "lif layer 2 self.abs_max_v: 2085.0\n",
      "lif layer 2 self.abs_max_v: 2113.5\n",
      "lif layer 2 self.abs_max_v: 2368.0\n",
      "fc layer 2 self.abs_max_out: 1471.0\n",
      "smallest_now_T updated: 47\n",
      "lif layer 2 self.abs_max_v: 2371.0\n",
      "fc layer 2 self.abs_max_out: 1532.0\n",
      "fc layer 2 self.abs_max_out: 1607.0\n",
      "fc layer 2 self.abs_max_out: 1643.0\n",
      "fc layer 2 self.abs_max_out: 1684.0\n",
      "lif layer 1 self.abs_max_v: 3672.0\n",
      "lif layer 1 self.abs_max_v: 4096.0\n",
      "lif layer 2 self.abs_max_v: 2397.0\n",
      "fc layer 3 self.abs_max_out: 489.0\n",
      "fc layer 2 self.abs_max_out: 1949.0\n",
      "lif layer 2 self.abs_max_v: 2807.5\n",
      "fc layer 3 self.abs_max_out: 501.0\n",
      "fc layer 3 self.abs_max_out: 558.0\n",
      "fc layer 1 self.abs_max_out: 2597.0\n",
      "fc layer 1 self.abs_max_out: 2728.0\n",
      "smallest_now_T updated: 42\n",
      "fc layer 1 self.abs_max_out: 2797.0\n",
      "fc layer 1 self.abs_max_out: 3476.0\n",
      "lif layer 1 self.abs_max_v: 4141.0\n",
      "lif layer 1 self.abs_max_v: 4621.5\n",
      "lif layer 2 self.abs_max_v: 2969.0\n",
      "smallest_now_T updated: 36\n",
      "smallest_now_T updated: 27\n",
      "fc layer 2 self.abs_max_out: 2000.0\n",
      "fc layer 2 self.abs_max_out: 2005.0\n",
      "fc layer 3 self.abs_max_out: 569.0\n",
      "lif layer 2 self.abs_max_v: 2989.0\n",
      "lif layer 2 self.abs_max_v: 3216.5\n",
      "fc layer 3 self.abs_max_out: 609.0\n",
      "fc layer 3 self.abs_max_out: 610.0\n",
      "fc layer 2 self.abs_max_out: 2046.0\n",
      "lif layer 1 self.abs_max_v: 4853.0\n",
      "fc layer 2 self.abs_max_out: 2050.0\n",
      "fc layer 2 self.abs_max_out: 2089.0\n",
      "lif layer 1 self.abs_max_v: 4965.5\n",
      "fc layer 2 self.abs_max_out: 2103.0\n",
      "fc layer 2 self.abs_max_out: 2214.0\n",
      "lif layer 1 self.abs_max_v: 5058.0\n",
      "fc layer 1 self.abs_max_out: 3601.0\n",
      "lif layer 1 self.abs_max_v: 5124.0\n",
      "lif layer 1 self.abs_max_v: 5222.5\n",
      "lif layer 1 self.abs_max_v: 5343.0\n",
      "fc layer 2 self.abs_max_out: 2304.0\n",
      "lif layer 1 self.abs_max_v: 5608.0\n",
      "fc layer 2 self.abs_max_out: 2318.0\n",
      "lif layer 1 self.abs_max_v: 5672.0\n",
      "lif layer 1 self.abs_max_v: 5805.0\n",
      "fc layer 3 self.abs_max_out: 618.0\n",
      "fc layer 2 self.abs_max_out: 2354.0\n",
      "fc layer 2 self.abs_max_out: 2395.0\n",
      "fc layer 1 self.abs_max_out: 3857.0\n",
      "lif layer 1 self.abs_max_v: 6531.5\n",
      "lif layer 1 self.abs_max_v: 6751.0\n",
      "lif layer 2 self.abs_max_v: 3333.5\n",
      "lif layer 2 self.abs_max_v: 3559.5\n",
      "lif layer 2 self.abs_max_v: 3809.0\n",
      "fc layer 2 self.abs_max_out: 2406.0\n",
      "fc layer 2 self.abs_max_out: 2619.0\n",
      "fc layer 2 self.abs_max_out: 2726.0\n",
      "fc layer 2 self.abs_max_out: 2830.0\n",
      "fc layer 3 self.abs_max_out: 659.0\n",
      "fc layer 1 self.abs_max_out: 4061.0\n",
      "fc layer 3 self.abs_max_out: 661.0\n",
      "fc layer 1 self.abs_max_out: 4116.0\n",
      "fc layer 3 self.abs_max_out: 669.0\n",
      "fc layer 1 self.abs_max_out: 4321.0\n",
      "fc layer 1 self.abs_max_out: 4418.0\n",
      "lif layer 1 self.abs_max_v: 7090.5\n",
      "fc layer 3 self.abs_max_out: 676.0\n",
      "fc layer 3 self.abs_max_out: 679.0\n",
      "lif layer 2 self.abs_max_v: 3825.0\n",
      "smallest_now_T_val updated: 84\n",
      "smallest_now_T_val updated: 69\n",
      "lif layer 2 self.abs_max_v: 4125.0\n",
      "smallest_now_T_val updated: 68\n",
      "smallest_now_T_val updated: 67\n",
      "smallest_now_T_val updated: 55\n",
      "smallest_now_T_val updated: 25\n",
      "fc layer 1 self.abs_max_out: 4635.0\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.854065/  1.949523, val:  46.67%, val_best:  46.67%, tr:  85.70%, tr_best:  85.70%, epoch time: 40.82 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8569%\n",
      "layer   2  Sparsity: 72.7705%\n",
      "layer   3  Sparsity: 71.9107%\n",
      "total_backward_count 4895 real_backward_count 1445  29.520%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 4147.5\n",
      "fc layer 3 self.abs_max_out: 700.0\n",
      "fc layer 1 self.abs_max_out: 4932.0\n",
      "fc layer 2 self.abs_max_out: 2901.0\n",
      "fc layer 3 self.abs_max_out: 764.0\n",
      "fc layer 3 self.abs_max_out: 788.0\n",
      "lif layer 1 self.abs_max_v: 7335.5\n",
      "lif layer 1 self.abs_max_v: 7849.0\n",
      "fc layer 2 self.abs_max_out: 3149.0\n",
      "lif layer 1 self.abs_max_v: 7864.5\n",
      "lif layer 2 self.abs_max_v: 4157.5\n",
      "lif layer 1 self.abs_max_v: 8151.5\n",
      "fc layer 1 self.abs_max_out: 5241.0\n",
      "lif layer 2 self.abs_max_v: 4450.0\n",
      "lif layer 2 self.abs_max_v: 4628.0\n",
      "lif layer 1 self.abs_max_v: 8203.5\n",
      "lif layer 1 self.abs_max_v: 8266.0\n",
      "fc layer 1 self.abs_max_out: 5261.0\n",
      "fc layer 1 self.abs_max_out: 5313.0\n",
      "lif layer 1 self.abs_max_v: 9176.0\n",
      "lif layer 1 self.abs_max_v: 9179.0\n",
      "fc layer 1 self.abs_max_out: 5388.0\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.779862/  1.945754, val:  47.50%, val_best:  47.50%, tr:  90.81%, tr_best:  90.81%, epoch time: 40.44 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8904%\n",
      "layer   2  Sparsity: 75.1521%\n",
      "layer   3  Sparsity: 74.6309%\n",
      "total_backward_count 9790 real_backward_count 2507  25.608%\n",
      "fc layer 2 self.abs_max_out: 3192.0\n",
      "fc layer 2 self.abs_max_out: 3263.0\n",
      "fc layer 1 self.abs_max_out: 5393.0\n",
      "fc layer 1 self.abs_max_out: 5443.0\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.772274/  1.893596, val:  55.00%, val_best:  55.00%, tr:  93.16%, tr_best:  93.16%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9222%\n",
      "layer   2  Sparsity: 77.0537%\n",
      "layer   3  Sparsity: 75.8573%\n",
      "total_backward_count 14685 real_backward_count 3489  23.759%\n",
      "fc layer 1 self.abs_max_out: 5482.0\n",
      "fc layer 1 self.abs_max_out: 5489.0\n",
      "fc layer 1 self.abs_max_out: 5958.0\n",
      "fc layer 2 self.abs_max_out: 3340.0\n",
      "lif layer 2 self.abs_max_v: 4721.5\n",
      "fc layer 3 self.abs_max_out: 814.0\n",
      "lif layer 1 self.abs_max_v: 9192.5\n",
      "lif layer 1 self.abs_max_v: 9617.5\n",
      "lif layer 2 self.abs_max_v: 4897.0\n",
      "lif layer 2 self.abs_max_v: 4908.0\n",
      "fc layer 1 self.abs_max_out: 6742.0\n",
      "lif layer 1 self.abs_max_v: 10846.0\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.766433/  1.897848, val:  48.75%, val_best:  55.00%, tr:  94.18%, tr_best:  94.18%, epoch time: 40.62 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8999%\n",
      "layer   2  Sparsity: 77.5815%\n",
      "layer   3  Sparsity: 76.7888%\n",
      "total_backward_count 19580 real_backward_count 4394  22.441%\n",
      "lif layer 2 self.abs_max_v: 5024.5\n",
      "lif layer 2 self.abs_max_v: 5170.5\n",
      "fc layer 1 self.abs_max_out: 7112.0\n",
      "lif layer 1 self.abs_max_v: 10882.5\n",
      "fc layer 2 self.abs_max_out: 3423.0\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.764935/  1.939882, val:  52.92%, val_best:  55.00%, tr:  94.18%, tr_best:  94.18%, epoch time: 39.84 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8483%\n",
      "layer   2  Sparsity: 77.3165%\n",
      "layer   3  Sparsity: 76.2545%\n",
      "total_backward_count 24475 real_backward_count 5263  21.504%\n",
      "lif layer 2 self.abs_max_v: 5180.0\n",
      "fc layer 1 self.abs_max_out: 7351.0\n",
      "lif layer 1 self.abs_max_v: 11449.0\n",
      "fc layer 3 self.abs_max_out: 856.0\n",
      "fc layer 1 self.abs_max_out: 7481.0\n",
      "lif layer 1 self.abs_max_v: 12069.5\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.756685/  1.906659, val:  51.25%, val_best:  55.00%, tr:  93.05%, tr_best:  94.18%, epoch time: 40.30 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8807%\n",
      "layer   2  Sparsity: 76.5827%\n",
      "layer   3  Sparsity: 76.1360%\n",
      "total_backward_count 29370 real_backward_count 6112  20.810%\n",
      "lif layer 2 self.abs_max_v: 5280.0\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.748217/  1.891128, val:  61.67%, val_best:  61.67%, tr:  94.38%, tr_best:  94.38%, epoch time: 40.49 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8934%\n",
      "layer   2  Sparsity: 76.1922%\n",
      "layer   3  Sparsity: 75.6776%\n",
      "total_backward_count 34265 real_backward_count 6955  20.298%\n",
      "lif layer 2 self.abs_max_v: 5327.5\n",
      "lif layer 2 self.abs_max_v: 5709.0\n",
      "lif layer 2 self.abs_max_v: 5746.5\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.732865/  1.895485, val:  56.67%, val_best:  61.67%, tr:  95.71%, tr_best:  95.71%, epoch time: 40.47 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8708%\n",
      "layer   2  Sparsity: 75.2450%\n",
      "layer   3  Sparsity: 75.6788%\n",
      "total_backward_count 39160 real_backward_count 7746  19.780%\n",
      "fc layer 2 self.abs_max_out: 3485.0\n",
      "fc layer 1 self.abs_max_out: 7607.0\n",
      "lif layer 1 self.abs_max_v: 12209.5\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.705221/  1.804393, val:  70.83%, val_best:  70.83%, tr:  95.61%, tr_best:  95.71%, epoch time: 40.69 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9379%\n",
      "layer   2  Sparsity: 75.8972%\n",
      "layer   3  Sparsity: 76.0550%\n",
      "total_backward_count 44055 real_backward_count 8492  19.276%\n",
      "fc layer 2 self.abs_max_out: 3544.0\n",
      "lif layer 1 self.abs_max_v: 12470.5\n",
      "fc layer 2 self.abs_max_out: 3631.0\n",
      "fc layer 1 self.abs_max_out: 8050.0\n",
      "lif layer 1 self.abs_max_v: 12551.0\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.683900/  1.873704, val:  57.92%, val_best:  70.83%, tr:  96.53%, tr_best:  96.53%, epoch time: 41.19 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.8655%\n",
      "layer   2  Sparsity: 76.3567%\n",
      "layer   3  Sparsity: 77.1632%\n",
      "total_backward_count 48950 real_backward_count 9188  18.770%\n",
      "lif layer 1 self.abs_max_v: 12619.5\n",
      "lif layer 1 self.abs_max_v: 12774.0\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.700763/  1.841072, val:  61.25%, val_best:  70.83%, tr:  97.34%, tr_best:  97.34%, epoch time: 40.64 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8814%\n",
      "layer   2  Sparsity: 75.9669%\n",
      "layer   3  Sparsity: 77.2208%\n",
      "total_backward_count 53845 real_backward_count 9839  18.273%\n",
      "lif layer 2 self.abs_max_v: 5751.5\n",
      "lif layer 1 self.abs_max_v: 13558.5\n",
      "lif layer 1 self.abs_max_v: 14321.0\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.675726/  1.835360, val:  73.33%, val_best:  73.33%, tr:  97.24%, tr_best:  97.34%, epoch time: 40.13 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8912%\n",
      "layer   2  Sparsity: 75.4395%\n",
      "layer   3  Sparsity: 76.8984%\n",
      "total_backward_count 58740 real_backward_count 10471  17.826%\n",
      "fc layer 1 self.abs_max_out: 8284.0\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.677985/  1.840139, val:  75.00%, val_best:  75.00%, tr:  97.34%, tr_best:  97.34%, epoch time: 41.38 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.8756%\n",
      "layer   2  Sparsity: 76.0490%\n",
      "layer   3  Sparsity: 77.7165%\n",
      "total_backward_count 63635 real_backward_count 11084  17.418%\n",
      "fc layer 1 self.abs_max_out: 8441.0\n",
      "fc layer 2 self.abs_max_out: 3670.0\n",
      "lif layer 2 self.abs_max_v: 5965.0\n",
      "lif layer 2 self.abs_max_v: 6301.0\n",
      "lif layer 2 self.abs_max_v: 6642.5\n",
      "fc layer 1 self.abs_max_out: 8957.0\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.665271/  1.840024, val:  54.58%, val_best:  75.00%, tr:  97.96%, tr_best:  97.96%, epoch time: 40.14 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9097%\n",
      "layer   2  Sparsity: 75.6514%\n",
      "layer   3  Sparsity: 77.7783%\n",
      "total_backward_count 68530 real_backward_count 11642  16.988%\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.666296/  1.808391, val:  66.67%, val_best:  75.00%, tr:  98.37%, tr_best:  98.37%, epoch time: 41.01 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8680%\n",
      "layer   2  Sparsity: 75.7489%\n",
      "layer   3  Sparsity: 78.0890%\n",
      "total_backward_count 73425 real_backward_count 12184  16.594%\n",
      "fc layer 1 self.abs_max_out: 9043.0\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.668989/  1.800399, val:  70.42%, val_best:  75.00%, tr:  98.37%, tr_best:  98.37%, epoch time: 40.61 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8839%\n",
      "layer   2  Sparsity: 75.6610%\n",
      "layer   3  Sparsity: 77.9240%\n",
      "total_backward_count 78320 real_backward_count 12739  16.265%\n",
      "fc layer 2 self.abs_max_out: 3711.0\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.647275/  1.768878, val:  75.00%, val_best:  75.00%, tr:  98.06%, tr_best:  98.37%, epoch time: 40.64 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8820%\n",
      "layer   2  Sparsity: 76.1498%\n",
      "layer   3  Sparsity: 78.2487%\n",
      "total_backward_count 83215 real_backward_count 13205  15.869%\n",
      "fc layer 2 self.abs_max_out: 3720.0\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.643430/  1.781963, val:  77.92%, val_best:  77.92%, tr:  98.06%, tr_best:  98.37%, epoch time: 40.08 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8365%\n",
      "layer   2  Sparsity: 75.5523%\n",
      "layer   3  Sparsity: 78.0657%\n",
      "total_backward_count 88110 real_backward_count 13717  15.568%\n",
      "fc layer 1 self.abs_max_out: 9471.0\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.648867/  1.784012, val:  75.00%, val_best:  77.92%, tr:  98.47%, tr_best:  98.47%, epoch time: 40.97 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9005%\n",
      "layer   2  Sparsity: 75.2716%\n",
      "layer   3  Sparsity: 78.1248%\n",
      "total_backward_count 93005 real_backward_count 14180  15.246%\n",
      "fc layer 1 self.abs_max_out: 9756.0\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.623191/  1.806195, val:  65.42%, val_best:  77.92%, tr:  98.57%, tr_best:  98.57%, epoch time: 40.07 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8598%\n",
      "layer   2  Sparsity: 75.6432%\n",
      "layer   3  Sparsity: 78.7604%\n",
      "total_backward_count 97900 real_backward_count 14619  14.933%\n",
      "fc layer 3 self.abs_max_out: 860.0\n",
      "fc layer 3 self.abs_max_out: 891.0\n",
      "fc layer 3 self.abs_max_out: 900.0\n",
      "fc layer 3 self.abs_max_out: 917.0\n",
      "fc layer 2 self.abs_max_out: 3851.0\n",
      "fc layer 3 self.abs_max_out: 921.0\n",
      "fc layer 3 self.abs_max_out: 946.0\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.601493/  1.766295, val:  77.92%, val_best:  77.92%, tr:  98.67%, tr_best:  98.67%, epoch time: 40.57 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8772%\n",
      "layer   2  Sparsity: 75.6322%\n",
      "layer   3  Sparsity: 78.6698%\n",
      "total_backward_count 102795 real_backward_count 15002  14.594%\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.595720/  1.721509, val:  84.17%, val_best:  84.17%, tr:  98.67%, tr_best:  98.67%, epoch time: 40.40 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8805%\n",
      "layer   2  Sparsity: 75.4481%\n",
      "layer   3  Sparsity: 78.5292%\n",
      "total_backward_count 107690 real_backward_count 15392  14.293%\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.609813/  1.724036, val:  77.92%, val_best:  84.17%, tr:  99.18%, tr_best:  99.18%, epoch time: 40.68 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8925%\n",
      "layer   2  Sparsity: 75.3878%\n",
      "layer   3  Sparsity: 78.0252%\n",
      "total_backward_count 112585 real_backward_count 15782  14.018%\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.592365/  1.759071, val:  76.67%, val_best:  84.17%, tr:  99.18%, tr_best:  99.18%, epoch time: 40.49 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8452%\n",
      "layer   2  Sparsity: 75.4751%\n",
      "layer   3  Sparsity: 78.3847%\n",
      "total_backward_count 117480 real_backward_count 16163  13.758%\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.606309/  1.719463, val:  81.25%, val_best:  84.17%, tr:  98.88%, tr_best:  99.18%, epoch time: 40.44 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8882%\n",
      "layer   2  Sparsity: 75.2499%\n",
      "layer   3  Sparsity: 79.3411%\n",
      "total_backward_count 122375 real_backward_count 16551  13.525%\n",
      "lif layer 1 self.abs_max_v: 14450.5\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.598844/  1.732787, val:  78.75%, val_best:  84.17%, tr:  98.77%, tr_best:  99.18%, epoch time: 40.37 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8798%\n",
      "layer   2  Sparsity: 75.2221%\n",
      "layer   3  Sparsity: 78.8009%\n",
      "total_backward_count 127270 real_backward_count 16922  13.296%\n",
      "lif layer 1 self.abs_max_v: 14772.5\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.567357/  1.710086, val:  85.00%, val_best:  85.00%, tr:  99.18%, tr_best:  99.18%, epoch time: 40.54 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9264%\n",
      "layer   2  Sparsity: 74.9304%\n",
      "layer   3  Sparsity: 77.9253%\n",
      "total_backward_count 132165 real_backward_count 17256  13.056%\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.545284/  1.720305, val:  66.67%, val_best:  85.00%, tr:  99.28%, tr_best:  99.28%, epoch time: 40.62 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8620%\n",
      "layer   2  Sparsity: 75.0199%\n",
      "layer   3  Sparsity: 77.8820%\n",
      "total_backward_count 137060 real_backward_count 17577  12.824%\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.557610/  1.725523, val:  83.33%, val_best:  85.00%, tr:  99.08%, tr_best:  99.28%, epoch time: 40.45 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8968%\n",
      "layer   2  Sparsity: 74.5932%\n",
      "layer   3  Sparsity: 78.1551%\n",
      "total_backward_count 141955 real_backward_count 17917  12.622%\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.572951/  1.721396, val:  84.58%, val_best:  85.00%, tr:  98.98%, tr_best:  99.28%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8611%\n",
      "layer   2  Sparsity: 74.6111%\n",
      "layer   3  Sparsity: 77.9405%\n",
      "total_backward_count 146850 real_backward_count 18233  12.416%\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.546642/  1.681868, val:  80.00%, val_best:  85.00%, tr:  99.59%, tr_best:  99.59%, epoch time: 40.46 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8918%\n",
      "layer   2  Sparsity: 74.7627%\n",
      "layer   3  Sparsity: 78.1317%\n",
      "total_backward_count 151745 real_backward_count 18562  12.232%\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.530361/  1.670798, val:  82.50%, val_best:  85.00%, tr:  99.69%, tr_best:  99.69%, epoch time: 40.50 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8686%\n",
      "layer   2  Sparsity: 75.1336%\n",
      "layer   3  Sparsity: 78.5248%\n",
      "total_backward_count 156640 real_backward_count 18828  12.020%\n",
      "fc layer 1 self.abs_max_out: 10117.0\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.548305/  1.691236, val:  80.00%, val_best:  85.00%, tr:  98.88%, tr_best:  99.69%, epoch time: 40.18 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9126%\n",
      "layer   2  Sparsity: 74.7993%\n",
      "layer   3  Sparsity: 77.9629%\n",
      "total_backward_count 161535 real_backward_count 19137  11.847%\n",
      "fc layer 1 self.abs_max_out: 10713.0\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.545291/  1.697168, val:  81.67%, val_best:  85.00%, tr:  99.39%, tr_best:  99.69%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8883%\n",
      "layer   2  Sparsity: 74.2459%\n",
      "layer   3  Sparsity: 78.3485%\n",
      "total_backward_count 166430 real_backward_count 19404  11.659%\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.534910/  1.683117, val:  81.67%, val_best:  85.00%, tr:  99.49%, tr_best:  99.69%, epoch time: 40.43 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8822%\n",
      "layer   2  Sparsity: 74.7502%\n",
      "layer   3  Sparsity: 78.5693%\n",
      "total_backward_count 171325 real_backward_count 19659  11.475%\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.516455/  1.677999, val:  79.58%, val_best:  85.00%, tr:  99.49%, tr_best:  99.69%, epoch time: 40.80 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8964%\n",
      "layer   2  Sparsity: 74.2623%\n",
      "layer   3  Sparsity: 78.4111%\n",
      "total_backward_count 176220 real_backward_count 19928  11.309%\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.543967/  1.695170, val:  80.00%, val_best:  85.00%, tr:  99.18%, tr_best:  99.69%, epoch time: 40.38 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8796%\n",
      "layer   2  Sparsity: 74.6005%\n",
      "layer   3  Sparsity: 79.1116%\n",
      "total_backward_count 181115 real_backward_count 20202  11.154%\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.522234/  1.680171, val:  80.42%, val_best:  85.00%, tr:  99.49%, tr_best:  99.69%, epoch time: 40.39 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9242%\n",
      "layer   2  Sparsity: 75.0926%\n",
      "layer   3  Sparsity: 78.5897%\n",
      "total_backward_count 186010 real_backward_count 20463  11.001%\n",
      "lif layer 1 self.abs_max_v: 15214.0\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.507291/  1.647430, val:  84.58%, val_best:  85.00%, tr:  99.59%, tr_best:  99.69%, epoch time: 40.23 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8925%\n",
      "layer   2  Sparsity: 74.8112%\n",
      "layer   3  Sparsity: 79.2147%\n",
      "total_backward_count 190905 real_backward_count 20708  10.847%\n",
      "fc layer 2 self.abs_max_out: 3953.0\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.501380/  1.651605, val:  82.08%, val_best:  85.00%, tr:  99.59%, tr_best:  99.69%, epoch time: 40.37 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8651%\n",
      "layer   2  Sparsity: 74.5179%\n",
      "layer   3  Sparsity: 79.2263%\n",
      "total_backward_count 195800 real_backward_count 20960  10.705%\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.501854/  1.647881, val:  79.58%, val_best:  85.00%, tr:  99.28%, tr_best:  99.69%, epoch time: 40.15 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8611%\n",
      "layer   2  Sparsity: 74.4838%\n",
      "layer   3  Sparsity: 79.0385%\n",
      "total_backward_count 200695 real_backward_count 21188  10.557%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.497193/  1.685556, val:  72.50%, val_best:  85.00%, tr:  98.98%, tr_best:  99.69%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8839%\n",
      "layer   2  Sparsity: 74.7062%\n",
      "layer   3  Sparsity: 78.6637%\n",
      "total_backward_count 205590 real_backward_count 21434  10.426%\n",
      "fc layer 3 self.abs_max_out: 961.0\n",
      "fc layer 2 self.abs_max_out: 3959.0\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.495454/  1.644261, val:  80.83%, val_best:  85.00%, tr:  99.59%, tr_best:  99.69%, epoch time: 40.38 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9211%\n",
      "layer   2  Sparsity: 74.7943%\n",
      "layer   3  Sparsity: 78.7981%\n",
      "total_backward_count 210485 real_backward_count 21676  10.298%\n",
      "lif layer 2 self.abs_max_v: 6785.0\n",
      "lif layer 2 self.abs_max_v: 6933.0\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.500404/  1.670984, val:  82.92%, val_best:  85.00%, tr:  99.08%, tr_best:  99.69%, epoch time: 40.38 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9014%\n",
      "layer   2  Sparsity: 74.5461%\n",
      "layer   3  Sparsity: 79.2696%\n",
      "total_backward_count 215380 real_backward_count 21919  10.177%\n",
      "lif layer 2 self.abs_max_v: 6952.0\n",
      "lif layer 2 self.abs_max_v: 7186.5\n",
      "fc layer 2 self.abs_max_out: 4002.0\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.497318/  1.637412, val:  85.00%, val_best:  85.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 40.20 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8725%\n",
      "layer   2  Sparsity: 74.8831%\n",
      "layer   3  Sparsity: 79.3069%\n",
      "total_backward_count 220275 real_backward_count 22110  10.037%\n",
      "fc layer 3 self.abs_max_out: 976.0\n",
      "lif layer 2 self.abs_max_v: 7195.5\n",
      "fc layer 2 self.abs_max_out: 4051.0\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.481848/  1.624536, val:  85.00%, val_best:  85.00%, tr:  99.39%, tr_best:  99.90%, epoch time: 40.15 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8426%\n",
      "layer   2  Sparsity: 74.8709%\n",
      "layer   3  Sparsity: 79.0330%\n",
      "total_backward_count 225170 real_backward_count 22299   9.903%\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.478466/  1.650755, val:  76.67%, val_best:  85.00%, tr:  99.49%, tr_best:  99.90%, epoch time: 40.44 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8575%\n",
      "layer   2  Sparsity: 74.8564%\n",
      "layer   3  Sparsity: 79.0517%\n",
      "total_backward_count 230065 real_backward_count 22495   9.778%\n",
      "fc layer 2 self.abs_max_out: 4104.0\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.474070/  1.632326, val:  82.50%, val_best:  85.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8927%\n",
      "layer   2  Sparsity: 74.6705%\n",
      "layer   3  Sparsity: 79.0779%\n",
      "total_backward_count 234960 real_backward_count 22703   9.662%\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.469199/  1.635632, val:  85.42%, val_best:  85.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8771%\n",
      "layer   2  Sparsity: 74.5800%\n",
      "layer   3  Sparsity: 79.2799%\n",
      "total_backward_count 239855 real_backward_count 22901   9.548%\n",
      "fc layer 3 self.abs_max_out: 977.0\n",
      "fc layer 3 self.abs_max_out: 978.0\n",
      "fc layer 3 self.abs_max_out: 987.0\n",
      "fc layer 3 self.abs_max_out: 1020.0\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.475385/  1.620002, val:  83.75%, val_best:  85.42%, tr:  99.59%, tr_best:  99.90%, epoch time: 40.10 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8863%\n",
      "layer   2  Sparsity: 74.2422%\n",
      "layer   3  Sparsity: 79.3125%\n",
      "total_backward_count 244750 real_backward_count 23094   9.436%\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.483720/  1.661427, val:  80.83%, val_best:  85.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 40.49 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8712%\n",
      "layer   2  Sparsity: 74.3911%\n",
      "layer   3  Sparsity: 78.9418%\n",
      "total_backward_count 249645 real_backward_count 23292   9.330%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.488186/  1.646304, val:  82.08%, val_best:  85.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8663%\n",
      "layer   2  Sparsity: 74.6707%\n",
      "layer   3  Sparsity: 79.6117%\n",
      "total_backward_count 254540 real_backward_count 23481   9.225%\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.474613/  1.626232, val:  84.17%, val_best:  85.42%, tr:  99.59%, tr_best:  99.90%, epoch time: 40.61 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9398%\n",
      "layer   2  Sparsity: 75.0061%\n",
      "layer   3  Sparsity: 79.5593%\n",
      "total_backward_count 259435 real_backward_count 23672   9.124%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.472362/  1.646632, val:  80.83%, val_best:  85.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8683%\n",
      "layer   2  Sparsity: 74.8939%\n",
      "layer   3  Sparsity: 79.3619%\n",
      "total_backward_count 264330 real_backward_count 23844   9.021%\n",
      "lif layer 1 self.abs_max_v: 15565.5\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.470363/  1.632699, val:  85.42%, val_best:  85.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 41.03 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8786%\n",
      "layer   2  Sparsity: 74.6728%\n",
      "layer   3  Sparsity: 79.2097%\n",
      "total_backward_count 269225 real_backward_count 24029   8.925%\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.447467/  1.593908, val:  83.75%, val_best:  85.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 39.93 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8777%\n",
      "layer   2  Sparsity: 74.7984%\n",
      "layer   3  Sparsity: 79.4922%\n",
      "total_backward_count 274120 real_backward_count 24204   8.830%\n",
      "fc layer 1 self.abs_max_out: 10777.0\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.452259/  1.617165, val:  80.42%, val_best:  85.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 41.37 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.9210%\n",
      "layer   2  Sparsity: 74.7798%\n",
      "layer   3  Sparsity: 79.7319%\n",
      "total_backward_count 279015 real_backward_count 24374   8.736%\n",
      "fc layer 1 self.abs_max_out: 11075.0\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.453581/  1.627498, val:  82.92%, val_best:  85.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 40.21 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9170%\n",
      "layer   2  Sparsity: 74.4421%\n",
      "layer   3  Sparsity: 79.9835%\n",
      "total_backward_count 283910 real_backward_count 24534   8.641%\n",
      "lif layer 1 self.abs_max_v: 15637.0\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.451257/  1.616030, val:  81.25%, val_best:  85.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 41.82 seconds, 0.70 minutes\n",
      "layer   1  Sparsity: 88.9105%\n",
      "layer   2  Sparsity: 74.5836%\n",
      "layer   3  Sparsity: 80.0721%\n",
      "total_backward_count 288805 real_backward_count 24710   8.556%\n",
      "fc layer 3 self.abs_max_out: 1025.0\n",
      "lif layer 1 self.abs_max_v: 16013.0\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.455500/  1.629192, val:  80.00%, val_best:  85.42%, tr:  99.59%, tr_best:  99.90%, epoch time: 40.57 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8795%\n",
      "layer   2  Sparsity: 74.3072%\n",
      "layer   3  Sparsity: 79.5511%\n",
      "total_backward_count 293700 real_backward_count 24872   8.469%\n",
      "lif layer 1 self.abs_max_v: 16330.5\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.447610/  1.614744, val:  85.83%, val_best:  85.83%, tr:  99.69%, tr_best:  99.90%, epoch time: 40.82 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8702%\n",
      "layer   2  Sparsity: 74.4951%\n",
      "layer   3  Sparsity: 79.7804%\n",
      "total_backward_count 298595 real_backward_count 25043   8.387%\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.428694/  1.606279, val:  83.33%, val_best:  85.83%, tr:  99.59%, tr_best:  99.90%, epoch time: 39.88 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8758%\n",
      "layer   2  Sparsity: 74.6640%\n",
      "layer   3  Sparsity: 79.6933%\n",
      "total_backward_count 303490 real_backward_count 25185   8.298%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.436183/  1.616856, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.21 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8753%\n",
      "layer   2  Sparsity: 74.9506%\n",
      "layer   3  Sparsity: 79.9234%\n",
      "total_backward_count 308385 real_backward_count 25316   8.209%\n",
      "lif layer 2 self.abs_max_v: 7330.5\n",
      "lif layer 2 self.abs_max_v: 7364.5\n",
      "fc layer 2 self.abs_max_out: 4188.0\n",
      "lif layer 2 self.abs_max_v: 7439.5\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.444029/  1.615593, val:  80.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.09 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8374%\n",
      "layer   2  Sparsity: 74.5228%\n",
      "layer   3  Sparsity: 80.0388%\n",
      "total_backward_count 313280 real_backward_count 25443   8.121%\n",
      "fc layer 2 self.abs_max_out: 4228.0\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.426479/  1.586897, val:  83.33%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.49 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8979%\n",
      "layer   2  Sparsity: 74.6084%\n",
      "layer   3  Sparsity: 79.7837%\n",
      "total_backward_count 318175 real_backward_count 25590   8.043%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.445449/  1.625425, val:  80.83%, val_best:  85.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 40.50 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9084%\n",
      "layer   2  Sparsity: 74.6696%\n",
      "layer   3  Sparsity: 80.2664%\n",
      "total_backward_count 323070 real_backward_count 25725   7.963%\n",
      "fc layer 2 self.abs_max_out: 4284.0\n",
      "fc layer 2 self.abs_max_out: 4325.0\n",
      "lif layer 2 self.abs_max_v: 7909.5\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.445055/  1.601570, val:  77.92%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.58 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8969%\n",
      "layer   2  Sparsity: 74.6738%\n",
      "layer   3  Sparsity: 80.2209%\n",
      "total_backward_count 327965 real_backward_count 25867   7.887%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.439031/  1.646162, val:  82.08%, val_best:  85.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 40.93 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8844%\n",
      "layer   2  Sparsity: 74.5414%\n",
      "layer   3  Sparsity: 80.1951%\n",
      "total_backward_count 332860 real_backward_count 26012   7.815%\n",
      "lif layer 2 self.abs_max_v: 8078.0\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.433579/  1.633909, val:  80.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.17 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9164%\n",
      "layer   2  Sparsity: 74.3930%\n",
      "layer   3  Sparsity: 80.0074%\n",
      "total_backward_count 337755 real_backward_count 26149   7.742%\n",
      "fc layer 2 self.abs_max_out: 4327.0\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.417806/  1.592849, val:  82.08%, val_best:  85.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 41.65 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.9049%\n",
      "layer   2  Sparsity: 74.3068%\n",
      "layer   3  Sparsity: 79.8019%\n",
      "total_backward_count 342650 real_backward_count 26302   7.676%\n",
      "fc layer 2 self.abs_max_out: 4478.0\n",
      "lif layer 2 self.abs_max_v: 8090.0\n",
      "lif layer 2 self.abs_max_v: 8132.0\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.419541/  1.630000, val:  83.33%, val_best:  85.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 40.19 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8943%\n",
      "layer   2  Sparsity: 74.2647%\n",
      "layer   3  Sparsity: 79.8084%\n",
      "total_backward_count 347545 real_backward_count 26442   7.608%\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.424912/  1.617220, val:  85.00%, val_best:  85.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 40.61 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9061%\n",
      "layer   2  Sparsity: 74.3165%\n",
      "layer   3  Sparsity: 79.9084%\n",
      "total_backward_count 352440 real_backward_count 26574   7.540%\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.418251/  1.622549, val:  82.50%, val_best:  85.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 40.24 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8508%\n",
      "layer   2  Sparsity: 74.4404%\n",
      "layer   3  Sparsity: 79.6967%\n",
      "total_backward_count 357335 real_backward_count 26689   7.469%\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.425973/  1.617204, val:  81.25%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.75 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8218%\n",
      "layer   2  Sparsity: 74.4353%\n",
      "layer   3  Sparsity: 79.8938%\n",
      "total_backward_count 362230 real_backward_count 26792   7.396%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.408811/  1.609855, val:  82.92%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.54 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9150%\n",
      "layer   2  Sparsity: 74.1279%\n",
      "layer   3  Sparsity: 79.6292%\n",
      "total_backward_count 367125 real_backward_count 26921   7.333%\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.408015/  1.614735, val:  82.92%, val_best:  85.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 40.02 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9128%\n",
      "layer   2  Sparsity: 74.1322%\n",
      "layer   3  Sparsity: 79.4792%\n",
      "total_backward_count 372020 real_backward_count 27034   7.267%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.398688/  1.580564, val:  83.33%, val_best:  85.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8897%\n",
      "layer   2  Sparsity: 74.1759%\n",
      "layer   3  Sparsity: 79.8094%\n",
      "total_backward_count 376915 real_backward_count 27142   7.201%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.386909/  1.596717, val:  76.25%, val_best:  85.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 40.68 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8867%\n",
      "layer   2  Sparsity: 74.0088%\n",
      "layer   3  Sparsity: 80.1815%\n",
      "total_backward_count 381810 real_backward_count 27278   7.144%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.389071/  1.592659, val:  82.08%, val_best:  85.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 40.79 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8956%\n",
      "layer   2  Sparsity: 74.1988%\n",
      "layer   3  Sparsity: 80.3739%\n",
      "total_backward_count 386705 real_backward_count 27392   7.083%\n",
      "fc layer 3 self.abs_max_out: 1040.0\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.393098/  1.570626, val:  84.58%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.49 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9138%\n",
      "layer   2  Sparsity: 74.2687%\n",
      "layer   3  Sparsity: 79.6695%\n",
      "total_backward_count 391600 real_backward_count 27510   7.025%\n",
      "fc layer 3 self.abs_max_out: 1066.0\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.367648/  1.565278, val:  84.58%, val_best:  85.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 40.44 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8594%\n",
      "layer   2  Sparsity: 74.1610%\n",
      "layer   3  Sparsity: 79.5433%\n",
      "total_backward_count 396495 real_backward_count 27611   6.964%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.373055/  1.550694, val:  82.92%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.37 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8655%\n",
      "layer   2  Sparsity: 73.9770%\n",
      "layer   3  Sparsity: 79.4404%\n",
      "total_backward_count 401390 real_backward_count 27716   6.905%\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  1.373183/  1.556859, val:  85.42%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8960%\n",
      "layer   2  Sparsity: 74.1596%\n",
      "layer   3  Sparsity: 79.1656%\n",
      "total_backward_count 406285 real_backward_count 27827   6.849%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  1.370921/  1.576864, val:  82.50%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.29 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8703%\n",
      "layer   2  Sparsity: 73.9012%\n",
      "layer   3  Sparsity: 79.6647%\n",
      "total_backward_count 411180 real_backward_count 27925   6.791%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  1.382419/  1.601261, val:  82.50%, val_best:  85.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 40.55 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8579%\n",
      "layer   2  Sparsity: 73.7803%\n",
      "layer   3  Sparsity: 79.7167%\n",
      "total_backward_count 416075 real_backward_count 28039   6.739%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  1.380116/  1.571652, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.15 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8917%\n",
      "layer   2  Sparsity: 73.4925%\n",
      "layer   3  Sparsity: 79.7238%\n",
      "total_backward_count 420970 real_backward_count 28148   6.686%\n",
      "lif layer 1 self.abs_max_v: 16674.0\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  1.391897/  1.614826, val:  80.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.70 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8808%\n",
      "layer   2  Sparsity: 73.6868%\n",
      "layer   3  Sparsity: 79.5098%\n",
      "total_backward_count 425865 real_backward_count 28265   6.637%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  1.390942/  1.573260, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.22 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9206%\n",
      "layer   2  Sparsity: 73.9045%\n",
      "layer   3  Sparsity: 79.5907%\n",
      "total_backward_count 430760 real_backward_count 28377   6.588%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  1.387301/  1.557940, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.73 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8588%\n",
      "layer   2  Sparsity: 74.1756%\n",
      "layer   3  Sparsity: 79.7786%\n",
      "total_backward_count 435655 real_backward_count 28482   6.538%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  1.375916/  1.561956, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.46 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8744%\n",
      "layer   2  Sparsity: 74.3178%\n",
      "layer   3  Sparsity: 80.0765%\n",
      "total_backward_count 440550 real_backward_count 28587   6.489%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  1.364792/  1.549853, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 41.07 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8890%\n",
      "layer   2  Sparsity: 74.3472%\n",
      "layer   3  Sparsity: 79.8578%\n",
      "total_backward_count 445445 real_backward_count 28677   6.438%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  1.358469/  1.541522, val:  82.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.77 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8802%\n",
      "layer   2  Sparsity: 74.2725%\n",
      "layer   3  Sparsity: 79.9127%\n",
      "total_backward_count 450340 real_backward_count 28768   6.388%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  1.376308/  1.572150, val:  82.92%, val_best:  85.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8949%\n",
      "layer   2  Sparsity: 74.1545%\n",
      "layer   3  Sparsity: 80.2917%\n",
      "total_backward_count 455235 real_backward_count 28860   6.340%\n",
      "fc layer 3 self.abs_max_out: 1078.0\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  1.387444/  1.594694, val:  81.25%, val_best:  85.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 41.00 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8763%\n",
      "layer   2  Sparsity: 73.9816%\n",
      "layer   3  Sparsity: 80.0640%\n",
      "total_backward_count 460130 real_backward_count 28953   6.292%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  1.385046/  1.561539, val:  85.42%, val_best:  85.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.61 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8737%\n",
      "layer   2  Sparsity: 74.1563%\n",
      "layer   3  Sparsity: 80.1642%\n",
      "total_backward_count 465025 real_backward_count 29043   6.245%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  1.362351/  1.558481, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.18 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8920%\n",
      "layer   2  Sparsity: 74.2172%\n",
      "layer   3  Sparsity: 79.8668%\n",
      "total_backward_count 469920 real_backward_count 29125   6.198%\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  1.364545/  1.550541, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8834%\n",
      "layer   2  Sparsity: 74.2116%\n",
      "layer   3  Sparsity: 79.9683%\n",
      "total_backward_count 474815 real_backward_count 29198   6.149%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  1.355470/  1.538611, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.86 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.9287%\n",
      "layer   2  Sparsity: 73.9298%\n",
      "layer   3  Sparsity: 80.2541%\n",
      "total_backward_count 479710 real_backward_count 29286   6.105%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  1.352686/  1.560170, val:  81.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.73 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8803%\n",
      "layer   2  Sparsity: 74.0002%\n",
      "layer   3  Sparsity: 80.5232%\n",
      "total_backward_count 484605 real_backward_count 29361   6.059%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  1.361650/  1.538204, val:  85.42%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.41 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8846%\n",
      "layer   2  Sparsity: 73.9756%\n",
      "layer   3  Sparsity: 81.0245%\n",
      "total_backward_count 489500 real_backward_count 29445   6.015%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  1.359875/  1.551333, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.84 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8909%\n",
      "layer   2  Sparsity: 73.9893%\n",
      "layer   3  Sparsity: 80.7345%\n",
      "total_backward_count 494395 real_backward_count 29513   5.970%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  1.371552/  1.566432, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 41.03 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9343%\n",
      "layer   2  Sparsity: 74.0244%\n",
      "layer   3  Sparsity: 80.6678%\n",
      "total_backward_count 499290 real_backward_count 29579   5.924%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  1.369091/  1.567729, val:  81.67%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.31 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8936%\n",
      "layer   2  Sparsity: 73.9756%\n",
      "layer   3  Sparsity: 80.9381%\n",
      "total_backward_count 504185 real_backward_count 29646   5.880%\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  1.356493/  1.563782, val:  82.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.71 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8903%\n",
      "layer   2  Sparsity: 73.9897%\n",
      "layer   3  Sparsity: 80.9563%\n",
      "total_backward_count 509080 real_backward_count 29711   5.836%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  1.360606/  1.557937, val:  84.58%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.59 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9151%\n",
      "layer   2  Sparsity: 73.9573%\n",
      "layer   3  Sparsity: 81.1426%\n",
      "total_backward_count 513975 real_backward_count 29784   5.795%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  1.359862/  1.558392, val:  81.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.65 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8971%\n",
      "layer   2  Sparsity: 74.0715%\n",
      "layer   3  Sparsity: 81.1751%\n",
      "total_backward_count 518870 real_backward_count 29853   5.753%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  1.355017/  1.562728, val:  82.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.38 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8772%\n",
      "layer   2  Sparsity: 73.8780%\n",
      "layer   3  Sparsity: 80.9123%\n",
      "total_backward_count 523765 real_backward_count 29918   5.712%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  1.372698/  1.559154, val:  81.25%, val_best:  86.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 40.12 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8627%\n",
      "layer   2  Sparsity: 73.8898%\n",
      "layer   3  Sparsity: 80.8162%\n",
      "total_backward_count 528660 real_backward_count 30018   5.678%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  1.361361/  1.566821, val:  82.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.10 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9022%\n",
      "layer   2  Sparsity: 74.0268%\n",
      "layer   3  Sparsity: 80.9187%\n",
      "total_backward_count 533555 real_backward_count 30070   5.636%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  1.361654/  1.552258, val:  81.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.20 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9002%\n",
      "layer   2  Sparsity: 73.9595%\n",
      "layer   3  Sparsity: 81.0794%\n",
      "total_backward_count 538450 real_backward_count 30102   5.590%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  1.361042/  1.559844, val:  82.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.00 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8926%\n",
      "layer   2  Sparsity: 73.7961%\n",
      "layer   3  Sparsity: 81.0550%\n",
      "total_backward_count 543345 real_backward_count 30148   5.549%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  1.358310/  1.545883, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.39 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8837%\n",
      "layer   2  Sparsity: 73.9025%\n",
      "layer   3  Sparsity: 80.9009%\n",
      "total_backward_count 548240 real_backward_count 30210   5.510%\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  1.354738/  1.554285, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 41.02 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8653%\n",
      "layer   2  Sparsity: 73.9439%\n",
      "layer   3  Sparsity: 80.7089%\n",
      "total_backward_count 553135 real_backward_count 30261   5.471%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  1.344404/  1.541294, val:  82.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.19 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8611%\n",
      "layer   2  Sparsity: 73.8732%\n",
      "layer   3  Sparsity: 81.0433%\n",
      "total_backward_count 558030 real_backward_count 30309   5.431%\n",
      "fc layer 1 self.abs_max_out: 11095.0\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  1.347626/  1.543743, val:  81.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.97 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8999%\n",
      "layer   2  Sparsity: 73.9979%\n",
      "layer   3  Sparsity: 81.0149%\n",
      "total_backward_count 562925 real_backward_count 30353   5.392%\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  1.348945/  1.555396, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.22 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8610%\n",
      "layer   2  Sparsity: 73.8246%\n",
      "layer   3  Sparsity: 81.0116%\n",
      "total_backward_count 567820 real_backward_count 30417   5.357%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  1.353325/  1.544203, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.22 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8681%\n",
      "layer   2  Sparsity: 73.6347%\n",
      "layer   3  Sparsity: 80.7702%\n",
      "total_backward_count 572715 real_backward_count 30470   5.320%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  1.352598/  1.550011, val:  83.75%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.10 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8787%\n",
      "layer   2  Sparsity: 73.8300%\n",
      "layer   3  Sparsity: 80.7457%\n",
      "total_backward_count 577610 real_backward_count 30524   5.285%\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  1.342979/  1.547232, val:  80.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8950%\n",
      "layer   2  Sparsity: 73.7889%\n",
      "layer   3  Sparsity: 80.4489%\n",
      "total_backward_count 582505 real_backward_count 30577   5.249%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  1.338584/  1.540629, val:  83.75%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9085%\n",
      "layer   2  Sparsity: 73.7263%\n",
      "layer   3  Sparsity: 80.4114%\n",
      "total_backward_count 587400 real_backward_count 30640   5.216%\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  1.346750/  1.547814, val:  85.42%, val_best:  86.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 40.09 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9271%\n",
      "layer   2  Sparsity: 73.7530%\n",
      "layer   3  Sparsity: 80.4775%\n",
      "total_backward_count 592295 real_backward_count 30706   5.184%\n",
      "fc layer 1 self.abs_max_out: 11105.0\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  1.350719/  1.552943, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.22 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8844%\n",
      "layer   2  Sparsity: 73.8657%\n",
      "layer   3  Sparsity: 80.3721%\n",
      "total_backward_count 597190 real_backward_count 30766   5.152%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  1.347985/  1.543851, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.00 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8978%\n",
      "layer   2  Sparsity: 74.0934%\n",
      "layer   3  Sparsity: 80.4539%\n",
      "total_backward_count 602085 real_backward_count 30828   5.120%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  1.342065/  1.540120, val:  81.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.68 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8993%\n",
      "layer   2  Sparsity: 74.1519%\n",
      "layer   3  Sparsity: 80.8628%\n",
      "total_backward_count 606980 real_backward_count 30887   5.089%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  1.327698/  1.541664, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.48 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8620%\n",
      "layer   2  Sparsity: 73.9166%\n",
      "layer   3  Sparsity: 80.9919%\n",
      "total_backward_count 611875 real_backward_count 30947   5.058%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  1.328097/  1.533131, val:  84.17%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.59 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9043%\n",
      "layer   2  Sparsity: 74.2728%\n",
      "layer   3  Sparsity: 80.9787%\n",
      "total_backward_count 616770 real_backward_count 31008   5.027%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  1.325508/  1.521198, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.09 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8599%\n",
      "layer   2  Sparsity: 74.1856%\n",
      "layer   3  Sparsity: 81.0702%\n",
      "total_backward_count 621665 real_backward_count 31059   4.996%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  1.318560/  1.524767, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.55 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8904%\n",
      "layer   2  Sparsity: 74.2162%\n",
      "layer   3  Sparsity: 80.9867%\n",
      "total_backward_count 626560 real_backward_count 31113   4.966%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  1.319869/  1.533832, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.46 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9177%\n",
      "layer   2  Sparsity: 74.4089%\n",
      "layer   3  Sparsity: 81.1194%\n",
      "total_backward_count 631455 real_backward_count 31165   4.935%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  1.331685/  1.544467, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8865%\n",
      "layer   2  Sparsity: 74.2151%\n",
      "layer   3  Sparsity: 80.9906%\n",
      "total_backward_count 636350 real_backward_count 31219   4.906%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  1.325895/  1.531524, val:  82.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.54 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8564%\n",
      "layer   2  Sparsity: 74.3101%\n",
      "layer   3  Sparsity: 80.9269%\n",
      "total_backward_count 641245 real_backward_count 31262   4.875%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  1.313863/  1.525731, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.31 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8727%\n",
      "layer   2  Sparsity: 74.1834%\n",
      "layer   3  Sparsity: 81.0342%\n",
      "total_backward_count 646140 real_backward_count 31313   4.846%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  1.316808/  1.510438, val:  81.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.32 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9219%\n",
      "layer   2  Sparsity: 74.1986%\n",
      "layer   3  Sparsity: 80.9643%\n",
      "total_backward_count 651035 real_backward_count 31359   4.817%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  1.307087/  1.502636, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.08 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8717%\n",
      "layer   2  Sparsity: 73.9551%\n",
      "layer   3  Sparsity: 80.8692%\n",
      "total_backward_count 655930 real_backward_count 31423   4.791%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  1.304421/  1.519868, val:  85.00%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8752%\n",
      "layer   2  Sparsity: 74.0325%\n",
      "layer   3  Sparsity: 80.6970%\n",
      "total_backward_count 660825 real_backward_count 31467   4.762%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  1.315964/  1.518897, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.33 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8521%\n",
      "layer   2  Sparsity: 73.9363%\n",
      "layer   3  Sparsity: 81.1005%\n",
      "total_backward_count 665720 real_backward_count 31512   4.734%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  1.311099/  1.518621, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.26 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9167%\n",
      "layer   2  Sparsity: 74.0207%\n",
      "layer   3  Sparsity: 81.1288%\n",
      "total_backward_count 670615 real_backward_count 31556   4.706%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  1.321072/  1.514777, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.63 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8719%\n",
      "layer   2  Sparsity: 74.0230%\n",
      "layer   3  Sparsity: 81.1691%\n",
      "total_backward_count 675510 real_backward_count 31629   4.682%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  1.299787/  1.523375, val:  80.42%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8877%\n",
      "layer   2  Sparsity: 73.8047%\n",
      "layer   3  Sparsity: 80.7554%\n",
      "total_backward_count 680405 real_backward_count 31655   4.652%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  1.302706/  1.512136, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.76 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8649%\n",
      "layer   2  Sparsity: 73.8106%\n",
      "layer   3  Sparsity: 80.6500%\n",
      "total_backward_count 685300 real_backward_count 31686   4.624%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  1.307920/  1.512892, val:  82.50%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.86 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8997%\n",
      "layer   2  Sparsity: 73.8365%\n",
      "layer   3  Sparsity: 80.6932%\n",
      "total_backward_count 690195 real_backward_count 31724   4.596%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  1.315971/  1.524159, val:  85.42%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.85 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8979%\n",
      "layer   2  Sparsity: 73.8415%\n",
      "layer   3  Sparsity: 81.0251%\n",
      "total_backward_count 695090 real_backward_count 31773   4.571%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  1.318172/  1.524921, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.47 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8705%\n",
      "layer   2  Sparsity: 74.0192%\n",
      "layer   3  Sparsity: 80.9948%\n",
      "total_backward_count 699985 real_backward_count 31818   4.546%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  1.312588/  1.514309, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.54 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8919%\n",
      "layer   2  Sparsity: 73.8686%\n",
      "layer   3  Sparsity: 81.0906%\n",
      "total_backward_count 704880 real_backward_count 31874   4.522%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  1.317482/  1.515908, val:  85.42%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.73 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8889%\n",
      "layer   2  Sparsity: 74.1973%\n",
      "layer   3  Sparsity: 81.1968%\n",
      "total_backward_count 709775 real_backward_count 31925   4.498%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  1.310386/  1.512901, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.60 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8747%\n",
      "layer   2  Sparsity: 74.3388%\n",
      "layer   3  Sparsity: 80.8522%\n",
      "total_backward_count 714670 real_backward_count 31963   4.472%\n",
      "fc layer 3 self.abs_max_out: 1099.0\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  1.309391/  1.522438, val:  84.58%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.52 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9019%\n",
      "layer   2  Sparsity: 74.5674%\n",
      "layer   3  Sparsity: 80.6495%\n",
      "total_backward_count 719565 real_backward_count 32002   4.447%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  1.305077/  1.507372, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.56 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9170%\n",
      "layer   2  Sparsity: 74.5795%\n",
      "layer   3  Sparsity: 81.1825%\n",
      "total_backward_count 724460 real_backward_count 32028   4.421%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  1.305767/  1.509731, val:  84.58%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.15 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8813%\n",
      "layer   2  Sparsity: 74.4779%\n",
      "layer   3  Sparsity: 80.9057%\n",
      "total_backward_count 729355 real_backward_count 32071   4.397%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  1.321097/  1.543158, val:  81.25%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.90 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9066%\n",
      "layer   2  Sparsity: 74.4130%\n",
      "layer   3  Sparsity: 80.9011%\n",
      "total_backward_count 734250 real_backward_count 32104   4.372%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  1.317709/  1.515813, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8856%\n",
      "layer   2  Sparsity: 74.5506%\n",
      "layer   3  Sparsity: 81.1171%\n",
      "total_backward_count 739145 real_backward_count 32147   4.349%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  1.315292/  1.516279, val:  82.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.93 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8957%\n",
      "layer   2  Sparsity: 74.2075%\n",
      "layer   3  Sparsity: 81.2330%\n",
      "total_backward_count 744040 real_backward_count 32189   4.326%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  1.319603/  1.521296, val:  82.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.13 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9282%\n",
      "layer   2  Sparsity: 73.9128%\n",
      "layer   3  Sparsity: 80.8881%\n",
      "total_backward_count 748935 real_backward_count 32220   4.302%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  1.311779/  1.510999, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.30 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8719%\n",
      "layer   2  Sparsity: 73.9598%\n",
      "layer   3  Sparsity: 80.8340%\n",
      "total_backward_count 753830 real_backward_count 32259   4.279%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  1.302346/  1.524090, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8910%\n",
      "layer   2  Sparsity: 73.8570%\n",
      "layer   3  Sparsity: 81.0993%\n",
      "total_backward_count 758725 real_backward_count 32282   4.255%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  1.314408/  1.530494, val:  82.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8983%\n",
      "layer   2  Sparsity: 73.7839%\n",
      "layer   3  Sparsity: 81.1512%\n",
      "total_backward_count 763620 real_backward_count 32321   4.233%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  1.307959/  1.517445, val:  82.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.26 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8959%\n",
      "layer   2  Sparsity: 73.9386%\n",
      "layer   3  Sparsity: 81.1114%\n",
      "total_backward_count 768515 real_backward_count 32362   4.211%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  1.312240/  1.525567, val:  82.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.37 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8961%\n",
      "layer   2  Sparsity: 73.8642%\n",
      "layer   3  Sparsity: 80.8819%\n",
      "total_backward_count 773410 real_backward_count 32405   4.190%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  1.306169/  1.524180, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.63 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9034%\n",
      "layer   2  Sparsity: 73.7500%\n",
      "layer   3  Sparsity: 81.0107%\n",
      "total_backward_count 778305 real_backward_count 32442   4.168%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  1.308046/  1.517813, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.68 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9190%\n",
      "layer   2  Sparsity: 73.8305%\n",
      "layer   3  Sparsity: 81.3476%\n",
      "total_backward_count 783200 real_backward_count 32484   4.148%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  1.300490/  1.519520, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.72 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8815%\n",
      "layer   2  Sparsity: 73.8356%\n",
      "layer   3  Sparsity: 81.5038%\n",
      "total_backward_count 788095 real_backward_count 32518   4.126%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  1.313164/  1.515487, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.50 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8552%\n",
      "layer   2  Sparsity: 73.8559%\n",
      "layer   3  Sparsity: 81.4924%\n",
      "total_backward_count 792990 real_backward_count 32551   4.105%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  1.309920/  1.520749, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.44 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9403%\n",
      "layer   2  Sparsity: 73.9984%\n",
      "layer   3  Sparsity: 81.3475%\n",
      "total_backward_count 797885 real_backward_count 32588   4.084%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  1.314227/  1.517710, val:  84.17%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8828%\n",
      "layer   2  Sparsity: 74.1471%\n",
      "layer   3  Sparsity: 81.0633%\n",
      "total_backward_count 802780 real_backward_count 32614   4.063%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  1.311167/  1.513482, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.68 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9227%\n",
      "layer   2  Sparsity: 74.3263%\n",
      "layer   3  Sparsity: 81.0745%\n",
      "total_backward_count 807675 real_backward_count 32633   4.040%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  1.313144/  1.509252, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.69 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8363%\n",
      "layer   2  Sparsity: 74.1438%\n",
      "layer   3  Sparsity: 81.2977%\n",
      "total_backward_count 812570 real_backward_count 32658   4.019%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  1.301539/  1.502347, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.46 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8355%\n",
      "layer   2  Sparsity: 73.9998%\n",
      "layer   3  Sparsity: 81.0335%\n",
      "total_backward_count 817465 real_backward_count 32681   3.998%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  1.288976/  1.492814, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.77 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8625%\n",
      "layer   2  Sparsity: 74.2247%\n",
      "layer   3  Sparsity: 80.9063%\n",
      "total_backward_count 822360 real_backward_count 32708   3.977%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  1.285505/  1.488203, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.22 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8694%\n",
      "layer   2  Sparsity: 74.3541%\n",
      "layer   3  Sparsity: 80.8273%\n",
      "total_backward_count 827255 real_backward_count 32721   3.955%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  1.299486/  1.510740, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.75 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8831%\n",
      "layer   2  Sparsity: 74.3107%\n",
      "layer   3  Sparsity: 80.9009%\n",
      "total_backward_count 832150 real_backward_count 32745   3.935%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  1.293980/  1.515990, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.98 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9120%\n",
      "layer   2  Sparsity: 74.1448%\n",
      "layer   3  Sparsity: 81.2166%\n",
      "total_backward_count 837045 real_backward_count 32768   3.915%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  1.300756/  1.516469, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.81 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8994%\n",
      "layer   2  Sparsity: 74.2213%\n",
      "layer   3  Sparsity: 81.2886%\n",
      "total_backward_count 841940 real_backward_count 32801   3.896%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  1.300189/  1.516405, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.09 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9289%\n",
      "layer   2  Sparsity: 74.1406%\n",
      "layer   3  Sparsity: 81.2802%\n",
      "total_backward_count 846835 real_backward_count 32836   3.877%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  1.291520/  1.505680, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.45 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8465%\n",
      "layer   2  Sparsity: 74.1081%\n",
      "layer   3  Sparsity: 81.3483%\n",
      "total_backward_count 851730 real_backward_count 32874   3.860%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  1.284684/  1.497352, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.50 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8724%\n",
      "layer   2  Sparsity: 74.2422%\n",
      "layer   3  Sparsity: 81.2911%\n",
      "total_backward_count 856625 real_backward_count 32900   3.841%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  1.287703/  1.510840, val:  84.58%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.57 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9253%\n",
      "layer   2  Sparsity: 74.2968%\n",
      "layer   3  Sparsity: 81.4797%\n",
      "total_backward_count 861520 real_backward_count 32929   3.822%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  1.283681/  1.498928, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.07 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8767%\n",
      "layer   2  Sparsity: 74.2745%\n",
      "layer   3  Sparsity: 81.4812%\n",
      "total_backward_count 866415 real_backward_count 32949   3.803%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  1.287264/  1.517912, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.44 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8597%\n",
      "layer   2  Sparsity: 74.2189%\n",
      "layer   3  Sparsity: 81.7444%\n",
      "total_backward_count 871310 real_backward_count 32971   3.784%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  1.302014/  1.511914, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.03 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8744%\n",
      "layer   2  Sparsity: 74.3674%\n",
      "layer   3  Sparsity: 81.4846%\n",
      "total_backward_count 876205 real_backward_count 32997   3.766%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  1.314869/  1.510378, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.23 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8986%\n",
      "layer   2  Sparsity: 74.3802%\n",
      "layer   3  Sparsity: 81.2315%\n",
      "total_backward_count 881100 real_backward_count 33028   3.748%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  1.300391/  1.511091, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.68 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8686%\n",
      "layer   2  Sparsity: 74.4559%\n",
      "layer   3  Sparsity: 81.2730%\n",
      "total_backward_count 885995 real_backward_count 33045   3.730%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  1.304535/  1.501704, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.33 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8429%\n",
      "layer   2  Sparsity: 74.1408%\n",
      "layer   3  Sparsity: 81.1064%\n",
      "total_backward_count 890890 real_backward_count 33066   3.712%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  1.307049/  1.511360, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.76 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9309%\n",
      "layer   2  Sparsity: 74.0550%\n",
      "layer   3  Sparsity: 81.1956%\n",
      "total_backward_count 895785 real_backward_count 33090   3.694%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  1.311082/  1.536982, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.32 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8965%\n",
      "layer   2  Sparsity: 74.1717%\n",
      "layer   3  Sparsity: 81.0282%\n",
      "total_backward_count 900680 real_backward_count 33123   3.678%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  1.312184/  1.532356, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.77 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8748%\n",
      "layer   2  Sparsity: 74.0952%\n",
      "layer   3  Sparsity: 81.5326%\n",
      "total_backward_count 905575 real_backward_count 33142   3.660%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  1.311629/  1.526666, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.15 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8840%\n",
      "layer   2  Sparsity: 73.9288%\n",
      "layer   3  Sparsity: 81.7220%\n",
      "total_backward_count 910470 real_backward_count 33170   3.643%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  1.319472/  1.533277, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.67 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8682%\n",
      "layer   2  Sparsity: 74.1778%\n",
      "layer   3  Sparsity: 81.8769%\n",
      "total_backward_count 915365 real_backward_count 33195   3.626%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  1.303813/  1.509580, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.99 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9031%\n",
      "layer   2  Sparsity: 74.4271%\n",
      "layer   3  Sparsity: 81.5114%\n",
      "total_backward_count 920260 real_backward_count 33222   3.610%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  1.292941/  1.497228, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.95 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8676%\n",
      "layer   2  Sparsity: 74.2088%\n",
      "layer   3  Sparsity: 81.2453%\n",
      "total_backward_count 925155 real_backward_count 33242   3.593%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  1.290310/  1.484771, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.12 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8915%\n",
      "layer   2  Sparsity: 74.0101%\n",
      "layer   3  Sparsity: 81.1278%\n",
      "total_backward_count 930050 real_backward_count 33267   3.577%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  1.284258/  1.513707, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.80 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8945%\n",
      "layer   2  Sparsity: 73.9697%\n",
      "layer   3  Sparsity: 81.2554%\n",
      "total_backward_count 934945 real_backward_count 33296   3.561%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  1.302795/  1.518038, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8813%\n",
      "layer   2  Sparsity: 73.8661%\n",
      "layer   3  Sparsity: 81.6632%\n",
      "total_backward_count 939840 real_backward_count 33313   3.545%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  1.295885/  1.506662, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 41.04 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9003%\n",
      "layer   2  Sparsity: 74.1019%\n",
      "layer   3  Sparsity: 81.7634%\n",
      "total_backward_count 944735 real_backward_count 33336   3.529%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  1.299310/  1.505709, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.40 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8603%\n",
      "layer   2  Sparsity: 74.1560%\n",
      "layer   3  Sparsity: 81.8806%\n",
      "total_backward_count 949630 real_backward_count 33351   3.512%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  1.298694/  1.502765, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.62 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8444%\n",
      "layer   2  Sparsity: 73.9995%\n",
      "layer   3  Sparsity: 81.9348%\n",
      "total_backward_count 954525 real_backward_count 33370   3.496%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  1.307866/  1.513258, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.83 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9026%\n",
      "layer   2  Sparsity: 74.0586%\n",
      "layer   3  Sparsity: 81.8169%\n",
      "total_backward_count 959420 real_backward_count 33396   3.481%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  1.293519/  1.504723, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.69 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8935%\n",
      "layer   2  Sparsity: 74.2351%\n",
      "layer   3  Sparsity: 81.7797%\n",
      "total_backward_count 964315 real_backward_count 33412   3.465%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  1.291449/  1.500577, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.59 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8693%\n",
      "layer   2  Sparsity: 74.2893%\n",
      "layer   3  Sparsity: 81.6850%\n",
      "total_backward_count 969210 real_backward_count 33421   3.448%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  1.281100/  1.498021, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.11 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8999%\n",
      "layer   2  Sparsity: 74.3194%\n",
      "layer   3  Sparsity: 81.8619%\n",
      "total_backward_count 974105 real_backward_count 33438   3.433%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  1.286338/  1.500703, val:  85.42%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.59 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8551%\n",
      "layer   2  Sparsity: 74.3889%\n",
      "layer   3  Sparsity: 81.9320%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee660f9eb7b49b78987408e0e1f3619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÜ‚ñÖ‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÜ‚ñÖ‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99898</td></tr><tr><td>tr_epoch_loss</td><td>1.28634</td></tr><tr><td>val_acc_best</td><td>0.87083</td></tr><tr><td>val_acc_now</td><td>0.85417</td></tr><tr><td>val_loss</td><td>1.5007</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lilac-sweep-10</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3qwdux9g' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3qwdux9g</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_040245-3qwdux9g/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: orbkz6c8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 50000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_061822-orbkz6c8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/orbkz6c8' target=\"_blank\">young-sweep-11</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/orbkz6c8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/orbkz6c8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': '20251118_061831_405', 'my_seed': 42, 'TIME': 5, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 15, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 50000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = dac77cc348b2d880ae59906e26f08f17\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=15, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=15, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 139\n",
      "fc layer 1 self.abs_max_out: 918.0\n",
      "lif layer 1 self.abs_max_v: 918.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 797.0\n",
      "lif layer 2 self.abs_max_v: 797.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 206.0\n",
      "fc layer 1 self.abs_max_out: 1040.0\n",
      "lif layer 1 self.abs_max_v: 1082.0\n",
      "fc layer 2 self.abs_max_out: 1243.0\n",
      "lif layer 2 self.abs_max_v: 1388.5\n",
      "fc layer 3 self.abs_max_out: 212.0\n",
      "fc layer 1 self.abs_max_out: 1108.0\n",
      "lif layer 1 self.abs_max_v: 1406.5\n",
      "fc layer 2 self.abs_max_out: 1512.0\n",
      "lif layer 2 self.abs_max_v: 1628.5\n",
      "fc layer 3 self.abs_max_out: 450.0\n",
      "fc layer 1 self.abs_max_out: 1149.0\n",
      "lif layer 2 self.abs_max_v: 1648.0\n",
      "fc layer 1 self.abs_max_out: 1598.0\n",
      "lif layer 1 self.abs_max_v: 1598.0\n",
      "fc layer 2 self.abs_max_out: 1765.0\n",
      "lif layer 2 self.abs_max_v: 1873.0\n",
      "smallest_now_T updated: 125\n",
      "lif layer 1 self.abs_max_v: 1722.5\n",
      "fc layer 3 self.abs_max_out: 505.0\n",
      "fc layer 1 self.abs_max_out: 2844.0\n",
      "lif layer 1 self.abs_max_v: 3009.0\n",
      "fc layer 1 self.abs_max_out: 2974.0\n",
      "lif layer 2 self.abs_max_v: 2091.5\n",
      "smallest_now_T updated: 94\n",
      "fc layer 1 self.abs_max_out: 3177.0\n",
      "lif layer 1 self.abs_max_v: 3177.0\n",
      "fc layer 3 self.abs_max_out: 570.0\n",
      "fc layer 1 self.abs_max_out: 3408.0\n",
      "lif layer 1 self.abs_max_v: 3408.0\n",
      "lif layer 2 self.abs_max_v: 2298.5\n",
      "fc layer 3 self.abs_max_out: 711.0\n",
      "lif layer 2 self.abs_max_v: 2520.5\n",
      "fc layer 2 self.abs_max_out: 1871.0\n",
      "fc layer 2 self.abs_max_out: 1955.0\n",
      "lif layer 1 self.abs_max_v: 3744.5\n",
      "fc layer 1 self.abs_max_out: 3509.0\n",
      "lif layer 1 self.abs_max_v: 5274.0\n",
      "fc layer 1 self.abs_max_out: 3676.0\n",
      "fc layer 1 self.abs_max_out: 3869.0\n",
      "lif layer 2 self.abs_max_v: 2575.0\n",
      "lif layer 1 self.abs_max_v: 5828.5\n",
      "fc layer 3 self.abs_max_out: 826.0\n",
      "fc layer 1 self.abs_max_out: 4026.0\n",
      "lif layer 1 self.abs_max_v: 5936.5\n",
      "fc layer 2 self.abs_max_out: 2057.0\n",
      "lif layer 2 self.abs_max_v: 2583.0\n",
      "fc layer 3 self.abs_max_out: 893.0\n",
      "fc layer 1 self.abs_max_out: 4321.0\n",
      "lif layer 1 self.abs_max_v: 6058.0\n",
      "lif layer 2 self.abs_max_v: 2685.5\n",
      "lif layer 1 self.abs_max_v: 6926.0\n",
      "lif layer 1 self.abs_max_v: 7073.0\n",
      "lif layer 2 self.abs_max_v: 3251.0\n",
      "smallest_now_T updated: 79\n",
      "fc layer 1 self.abs_max_out: 4344.0\n",
      "fc layer 1 self.abs_max_out: 4779.0\n",
      "lif layer 1 self.abs_max_v: 7356.5\n",
      "fc layer 2 self.abs_max_out: 2317.0\n",
      "lif layer 2 self.abs_max_v: 3364.0\n",
      "lif layer 2 self.abs_max_v: 3520.5\n",
      "smallest_now_T updated: 73\n",
      "fc layer 2 self.abs_max_out: 2338.0\n",
      "fc layer 1 self.abs_max_out: 5012.0\n",
      "fc layer 2 self.abs_max_out: 2589.0\n",
      "lif layer 2 self.abs_max_v: 3608.0\n",
      "fc layer 2 self.abs_max_out: 2601.0\n",
      "lif layer 1 self.abs_max_v: 7429.0\n",
      "lif layer 1 self.abs_max_v: 7460.5\n",
      "lif layer 2 self.abs_max_v: 3776.0\n",
      "lif layer 2 self.abs_max_v: 3925.5\n",
      "lif layer 2 self.abs_max_v: 3978.5\n",
      "fc layer 2 self.abs_max_out: 2695.0\n",
      "lif layer 2 self.abs_max_v: 4296.5\n",
      "fc layer 3 self.abs_max_out: 1043.0\n",
      "smallest_now_T updated: 65\n",
      "fc layer 2 self.abs_max_out: 2918.0\n",
      "fc layer 1 self.abs_max_out: 5476.0\n",
      "fc layer 1 self.abs_max_out: 5793.0\n",
      "fc layer 3 self.abs_max_out: 1079.0\n",
      "fc layer 3 self.abs_max_out: 1097.0\n",
      "fc layer 3 self.abs_max_out: 1144.0\n",
      "fc layer 3 self.abs_max_out: 1168.0\n",
      "fc layer 2 self.abs_max_out: 2936.0\n",
      "fc layer 3 self.abs_max_out: 1221.0\n",
      "fc layer 2 self.abs_max_out: 3103.0\n",
      "lif layer 1 self.abs_max_v: 7988.5\n",
      "lif layer 1 self.abs_max_v: 8154.5\n",
      "fc layer 2 self.abs_max_out: 3173.0\n",
      "lif layer 2 self.abs_max_v: 4383.5\n",
      "lif layer 2 self.abs_max_v: 4390.0\n",
      "lif layer 2 self.abs_max_v: 4437.5\n",
      "lif layer 2 self.abs_max_v: 4554.0\n",
      "smallest_now_T updated: 56\n",
      "fc layer 3 self.abs_max_out: 1274.0\n",
      "smallest_now_T updated: 43\n",
      "fc layer 3 self.abs_max_out: 1343.0\n",
      "fc layer 3 self.abs_max_out: 1344.0\n",
      "fc layer 3 self.abs_max_out: 1603.0\n",
      "fc layer 3 self.abs_max_out: 1645.0\n",
      "lif layer 1 self.abs_max_v: 8525.5\n",
      "lif layer 1 self.abs_max_v: 9309.0\n",
      "fc layer 1 self.abs_max_out: 6077.0\n",
      "fc layer 1 self.abs_max_out: 6206.0\n",
      "fc layer 1 self.abs_max_out: 6418.0\n",
      "lif layer 1 self.abs_max_v: 10039.0\n",
      "lif layer 1 self.abs_max_v: 10413.0\n",
      "fc layer 1 self.abs_max_out: 7084.0\n",
      "lif layer 1 self.abs_max_v: 11880.5\n",
      "lif layer 1 self.abs_max_v: 12585.5\n",
      "fc layer 1 self.abs_max_out: 7320.0\n",
      "fc layer 2 self.abs_max_out: 3239.0\n",
      "lif layer 1 self.abs_max_v: 13014.0\n",
      "lif layer 1 self.abs_max_v: 13133.0\n",
      "smallest_now_T_val updated: 129\n",
      "smallest_now_T_val updated: 106\n",
      "smallest_now_T_val updated: 104\n",
      "smallest_now_T_val updated: 102\n",
      "fc layer 1 self.abs_max_out: 8342.0\n",
      "smallest_now_T_val updated: 85\n",
      "smallest_now_T_val updated: 29\n",
      "lif layer 1 self.abs_max_v: 13617.0\n",
      "lif layer 2 self.abs_max_v: 4616.5\n",
      "lif layer 2 self.abs_max_v: 4685.5\n",
      "fc layer 2 self.abs_max_out: 3280.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.853720/  2.059502, val:  31.25%, val_best:  31.25%, tr:  89.38%, tr_best:  89.38%, epoch time: 41.06 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8491%\n",
      "layer   2  Sparsity: 79.7907%\n",
      "layer   3  Sparsity: 82.5671%\n",
      "total_backward_count 4895 real_backward_count 1507  30.787%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 3329.0\n",
      "lif layer 2 self.abs_max_v: 4726.0\n",
      "lif layer 2 self.abs_max_v: 4818.5\n",
      "lif layer 2 self.abs_max_v: 4911.5\n",
      "fc layer 2 self.abs_max_out: 3348.0\n",
      "fc layer 2 self.abs_max_out: 3498.0\n",
      "lif layer 2 self.abs_max_v: 5273.5\n",
      "fc layer 1 self.abs_max_out: 8529.0\n",
      "fc layer 1 self.abs_max_out: 8889.0\n",
      "fc layer 1 self.abs_max_out: 9148.0\n",
      "lif layer 1 self.abs_max_v: 13870.5\n",
      "lif layer 1 self.abs_max_v: 14669.0\n",
      "fc layer 1 self.abs_max_out: 9456.0\n",
      "lif layer 1 self.abs_max_v: 15014.0\n",
      "fc layer 1 self.abs_max_out: 10093.0\n",
      "lif layer 1 self.abs_max_v: 15872.0\n",
      "fc layer 2 self.abs_max_out: 3507.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.817743/  2.014261, val:  27.08%, val_best:  31.25%, tr:  92.03%, tr_best:  92.03%, epoch time: 40.01 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8379%\n",
      "layer   2  Sparsity: 80.4975%\n",
      "layer   3  Sparsity: 84.8106%\n",
      "total_backward_count 9790 real_backward_count 2738  27.967%\n",
      "fc layer 3 self.abs_max_out: 1652.0\n",
      "lif layer 2 self.abs_max_v: 5515.0\n",
      "lif layer 1 self.abs_max_v: 16907.5\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.828428/  1.960405, val:  49.58%, val_best:  49.58%, tr:  91.62%, tr_best:  92.03%, epoch time: 40.95 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9140%\n",
      "layer   2  Sparsity: 80.9288%\n",
      "layer   3  Sparsity: 87.0150%\n",
      "total_backward_count 14685 real_backward_count 3943  26.851%\n",
      "lif layer 2 self.abs_max_v: 5678.0\n",
      "lif layer 2 self.abs_max_v: 5916.0\n",
      "fc layer 1 self.abs_max_out: 10401.0\n",
      "fc layer 3 self.abs_max_out: 1760.0\n",
      "lif layer 1 self.abs_max_v: 17343.0\n",
      "lif layer 1 self.abs_max_v: 17840.5\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.800454/  1.956498, val:  42.50%, val_best:  49.58%, tr:  91.83%, tr_best:  92.03%, epoch time: 40.39 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8813%\n",
      "layer   2  Sparsity: 81.1783%\n",
      "layer   3  Sparsity: 87.1618%\n",
      "total_backward_count 19580 real_backward_count 5122  26.159%\n",
      "fc layer 2 self.abs_max_out: 3563.0\n",
      "fc layer 2 self.abs_max_out: 3830.0\n",
      "fc layer 2 self.abs_max_out: 3872.0\n",
      "fc layer 1 self.abs_max_out: 10471.0\n",
      "lif layer 1 self.abs_max_v: 18377.5\n",
      "lif layer 1 self.abs_max_v: 19124.0\n",
      "fc layer 2 self.abs_max_out: 3900.0\n",
      "fc layer 1 self.abs_max_out: 11062.0\n",
      "fc layer 1 self.abs_max_out: 11331.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.809948/  1.968614, val:  40.00%, val_best:  49.58%, tr:  92.13%, tr_best:  92.13%, epoch time: 40.90 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8118%\n",
      "layer   2  Sparsity: 82.5349%\n",
      "layer   3  Sparsity: 88.0235%\n",
      "total_backward_count 24475 real_backward_count 6245  25.516%\n",
      "lif layer 1 self.abs_max_v: 19369.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.810578/  1.958734, val:  52.50%, val_best:  52.50%, tr:  91.22%, tr_best:  92.13%, epoch time: 40.40 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8913%\n",
      "layer   2  Sparsity: 82.4533%\n",
      "layer   3  Sparsity: 87.8307%\n",
      "total_backward_count 29370 real_backward_count 7442  25.339%\n",
      "lif layer 2 self.abs_max_v: 6036.0\n",
      "lif layer 2 self.abs_max_v: 6177.0\n",
      "lif layer 2 self.abs_max_v: 6194.0\n",
      "lif layer 2 self.abs_max_v: 6526.0\n",
      "lif layer 2 self.abs_max_v: 6562.0\n",
      "fc layer 2 self.abs_max_out: 4410.0\n",
      "lif layer 1 self.abs_max_v: 19486.0\n",
      "lif layer 2 self.abs_max_v: 6678.0\n",
      "lif layer 2 self.abs_max_v: 6963.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.825514/  1.989726, val:  44.17%, val_best:  52.50%, tr:  91.11%, tr_best:  92.13%, epoch time: 40.79 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8583%\n",
      "layer   2  Sparsity: 82.9471%\n",
      "layer   3  Sparsity: 88.4803%\n",
      "total_backward_count 34265 real_backward_count 8657  25.265%\n",
      "lif layer 2 self.abs_max_v: 7126.0\n",
      "lif layer 2 self.abs_max_v: 7533.0\n",
      "fc layer 1 self.abs_max_out: 11402.0\n",
      "fc layer 1 self.abs_max_out: 12642.0\n",
      "lif layer 1 self.abs_max_v: 21099.5\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.790746/  1.913745, val:  47.50%, val_best:  52.50%, tr:  91.42%, tr_best:  92.13%, epoch time: 40.78 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8775%\n",
      "layer   2  Sparsity: 82.9091%\n",
      "layer   3  Sparsity: 87.3992%\n",
      "total_backward_count 39160 real_backward_count 9760  24.923%\n",
      "fc layer 1 self.abs_max_out: 13181.0\n",
      "lif layer 1 self.abs_max_v: 21955.0\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.778955/  1.955831, val:  53.75%, val_best:  53.75%, tr:  92.13%, tr_best:  92.13%, epoch time: 40.09 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8470%\n",
      "layer   2  Sparsity: 83.9382%\n",
      "layer   3  Sparsity: 87.5946%\n",
      "total_backward_count 44055 real_backward_count 10913  24.771%\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.816561/  1.984491, val:  43.75%, val_best:  53.75%, tr:  90.91%, tr_best:  92.13%, epoch time: 40.22 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8945%\n",
      "layer   2  Sparsity: 84.3026%\n",
      "layer   3  Sparsity: 88.9273%\n",
      "total_backward_count 48950 real_backward_count 12072  24.662%\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.825419/  1.974763, val:  51.25%, val_best:  53.75%, tr:  91.11%, tr_best:  92.13%, epoch time: 40.03 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8241%\n",
      "layer   2  Sparsity: 84.0447%\n",
      "layer   3  Sparsity: 89.0011%\n",
      "total_backward_count 53845 real_backward_count 13228  24.567%\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.822797/  1.987403, val:  47.50%, val_best:  53.75%, tr:  93.46%, tr_best:  93.46%, epoch time: 40.23 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8326%\n",
      "layer   2  Sparsity: 83.9897%\n",
      "layer   3  Sparsity: 88.6139%\n",
      "total_backward_count 58740 real_backward_count 14362  24.450%\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.876784/  2.031461, val:  41.25%, val_best:  53.75%, tr:  91.01%, tr_best:  93.46%, epoch time: 39.96 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8258%\n",
      "layer   2  Sparsity: 84.1541%\n",
      "layer   3  Sparsity: 89.6134%\n",
      "total_backward_count 63635 real_backward_count 15532  24.408%\n",
      "lif layer 1 self.abs_max_v: 22158.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.833963/  2.005502, val:  44.58%, val_best:  53.75%, tr:  91.11%, tr_best:  93.46%, epoch time: 40.22 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8427%\n",
      "layer   2  Sparsity: 85.0655%\n",
      "layer   3  Sparsity: 88.8396%\n",
      "total_backward_count 68530 real_backward_count 16653  24.300%\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.832679/  1.959620, val:  52.50%, val_best:  53.75%, tr:  92.95%, tr_best:  93.46%, epoch time: 39.93 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8979%\n",
      "layer   2  Sparsity: 85.0570%\n",
      "layer   3  Sparsity: 88.8147%\n",
      "total_backward_count 73425 real_backward_count 17698  24.104%\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.825044/  1.921304, val:  62.50%, val_best:  62.50%, tr:  92.44%, tr_best:  93.46%, epoch time: 40.58 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8514%\n",
      "layer   2  Sparsity: 84.8651%\n",
      "layer   3  Sparsity: 88.3473%\n",
      "total_backward_count 78320 real_backward_count 18847  24.064%\n",
      "lif layer 1 self.abs_max_v: 22728.5\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.786425/  1.941884, val:  56.25%, val_best:  62.50%, tr:  93.26%, tr_best:  93.46%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8359%\n",
      "layer   2  Sparsity: 84.6494%\n",
      "layer   3  Sparsity: 87.2178%\n",
      "total_backward_count 83215 real_backward_count 19904  23.919%\n",
      "fc layer 1 self.abs_max_out: 13356.0\n",
      "fc layer 1 self.abs_max_out: 13563.0\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.797225/  1.924152, val:  56.67%, val_best:  62.50%, tr:  92.03%, tr_best:  93.46%, epoch time: 40.60 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8467%\n",
      "layer   2  Sparsity: 85.6575%\n",
      "layer   3  Sparsity: 87.8465%\n",
      "total_backward_count 88110 real_backward_count 21025  23.862%\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.842548/  1.958287, val:  47.50%, val_best:  62.50%, tr:  91.42%, tr_best:  93.46%, epoch time: 40.48 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8874%\n",
      "layer   2  Sparsity: 85.9879%\n",
      "layer   3  Sparsity: 88.6349%\n",
      "total_backward_count 93005 real_backward_count 22207  23.877%\n",
      "fc layer 1 self.abs_max_out: 13709.0\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.830154/  2.008142, val:  46.25%, val_best:  62.50%, tr:  92.03%, tr_best:  93.46%, epoch time: 40.24 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8833%\n",
      "layer   2  Sparsity: 85.8148%\n",
      "layer   3  Sparsity: 89.1011%\n",
      "total_backward_count 97900 real_backward_count 23307  23.807%\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.864102/  1.950788, val:  49.17%, val_best:  62.50%, tr:  90.91%, tr_best:  93.46%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8434%\n",
      "layer   2  Sparsity: 86.4208%\n",
      "layer   3  Sparsity: 89.9907%\n",
      "total_backward_count 102795 real_backward_count 24448  23.783%\n",
      "fc layer 1 self.abs_max_out: 14565.0\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.830494/  1.991254, val:  44.17%, val_best:  62.50%, tr:  91.73%, tr_best:  93.46%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8668%\n",
      "layer   2  Sparsity: 85.7702%\n",
      "layer   3  Sparsity: 89.2879%\n",
      "total_backward_count 107690 real_backward_count 25615  23.786%\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.835651/  1.945743, val:  54.17%, val_best:  62.50%, tr:  92.24%, tr_best:  93.46%, epoch time: 40.25 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8557%\n",
      "layer   2  Sparsity: 85.5666%\n",
      "layer   3  Sparsity: 88.8787%\n",
      "total_backward_count 112585 real_backward_count 26726  23.739%\n",
      "lif layer 1 self.abs_max_v: 25012.5\n",
      "fc layer 1 self.abs_max_out: 14896.0\n",
      "fc layer 1 self.abs_max_out: 15458.0\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.821431/  1.977374, val:  58.33%, val_best:  62.50%, tr:  91.22%, tr_best:  93.46%, epoch time: 40.94 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8237%\n",
      "layer   2  Sparsity: 85.1213%\n",
      "layer   3  Sparsity: 88.5474%\n",
      "total_backward_count 117480 real_backward_count 27847  23.704%\n",
      "lif layer 1 self.abs_max_v: 25152.5\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.852100/  1.969878, val:  50.83%, val_best:  62.50%, tr:  91.52%, tr_best:  93.46%, epoch time: 40.67 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8199%\n",
      "layer   2  Sparsity: 84.8691%\n",
      "layer   3  Sparsity: 89.1713%\n",
      "total_backward_count 122375 real_backward_count 29012  23.707%\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.864766/  1.958028, val:  65.42%, val_best:  65.42%, tr:  91.22%, tr_best:  93.46%, epoch time: 40.78 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8771%\n",
      "layer   2  Sparsity: 85.0579%\n",
      "layer   3  Sparsity: 89.2473%\n",
      "total_backward_count 127270 real_backward_count 30168  23.704%\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.812773/  1.941318, val:  53.33%, val_best:  65.42%, tr:  93.05%, tr_best:  93.46%, epoch time: 40.55 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8770%\n",
      "layer   2  Sparsity: 85.3828%\n",
      "layer   3  Sparsity: 88.1998%\n",
      "total_backward_count 132165 real_backward_count 31219  23.621%\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.843763/  1.983359, val:  60.83%, val_best:  65.42%, tr:  92.24%, tr_best:  93.46%, epoch time: 40.18 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8544%\n",
      "layer   2  Sparsity: 85.2469%\n",
      "layer   3  Sparsity: 89.6543%\n",
      "total_backward_count 137060 real_backward_count 32333  23.590%\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  1.818602/  1.997466, val:  50.00%, val_best:  65.42%, tr:  93.36%, tr_best:  93.46%, epoch time: 40.19 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8781%\n",
      "layer   2  Sparsity: 84.3244%\n",
      "layer   3  Sparsity: 88.7120%\n",
      "total_backward_count 141955 real_backward_count 33375  23.511%\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.814800/  1.953278, val:  55.00%, val_best:  65.42%, tr:  91.83%, tr_best:  93.46%, epoch time: 40.05 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8778%\n",
      "layer   2  Sparsity: 84.3170%\n",
      "layer   3  Sparsity: 88.3360%\n",
      "total_backward_count 146850 real_backward_count 34464  23.469%\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.785004/  1.931645, val:  64.17%, val_best:  65.42%, tr:  93.46%, tr_best:  93.46%, epoch time: 40.77 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8190%\n",
      "layer   2  Sparsity: 84.9330%\n",
      "layer   3  Sparsity: 87.3761%\n",
      "total_backward_count 151745 real_backward_count 35502  23.396%\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.791010/  1.934393, val:  57.08%, val_best:  65.42%, tr:  92.24%, tr_best:  93.46%, epoch time: 40.31 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8897%\n",
      "layer   2  Sparsity: 84.8904%\n",
      "layer   3  Sparsity: 87.8881%\n",
      "total_backward_count 156640 real_backward_count 36520  23.315%\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  1.763716/  1.943193, val:  55.00%, val_best:  65.42%, tr:  93.36%, tr_best:  93.46%, epoch time: 40.41 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8318%\n",
      "layer   2  Sparsity: 84.8154%\n",
      "layer   3  Sparsity: 86.7036%\n",
      "total_backward_count 161535 real_backward_count 37506  23.218%\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  1.764763/  1.898976, val:  62.08%, val_best:  65.42%, tr:  93.36%, tr_best:  93.46%, epoch time: 40.40 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8477%\n",
      "layer   2  Sparsity: 83.4715%\n",
      "layer   3  Sparsity: 86.7005%\n",
      "total_backward_count 166430 real_backward_count 38541  23.157%\n",
      "lif layer 1 self.abs_max_v: 25181.5\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  1.784315/  1.928707, val:  54.58%, val_best:  65.42%, tr:  92.34%, tr_best:  93.46%, epoch time: 40.20 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9184%\n",
      "layer   2  Sparsity: 84.0194%\n",
      "layer   3  Sparsity: 87.7888%\n",
      "total_backward_count 171325 real_backward_count 39593  23.110%\n",
      "lif layer 1 self.abs_max_v: 25227.0\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  1.794641/  1.949269, val:  50.42%, val_best:  65.42%, tr:  93.67%, tr_best:  93.67%, epoch time: 40.23 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8496%\n",
      "layer   2  Sparsity: 84.1622%\n",
      "layer   3  Sparsity: 87.5186%\n",
      "total_backward_count 176220 real_backward_count 40649  23.067%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  1.783987/  1.895633, val:  67.50%, val_best:  67.50%, tr:  92.95%, tr_best:  93.67%, epoch time: 40.13 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9181%\n",
      "layer   2  Sparsity: 84.6985%\n",
      "layer   3  Sparsity: 88.5984%\n",
      "total_backward_count 181115 real_backward_count 41684  23.015%\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  1.759693/  1.938756, val:  53.75%, val_best:  67.50%, tr:  93.26%, tr_best:  93.67%, epoch time: 40.19 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8904%\n",
      "layer   2  Sparsity: 84.8999%\n",
      "layer   3  Sparsity: 88.0806%\n",
      "total_backward_count 186010 real_backward_count 42623  22.914%\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  1.805018/  1.954329, val:  61.25%, val_best:  67.50%, tr:  93.16%, tr_best:  93.67%, epoch time: 40.71 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8601%\n",
      "layer   2  Sparsity: 84.9536%\n",
      "layer   3  Sparsity: 88.7989%\n",
      "total_backward_count 190905 real_backward_count 43703  22.893%\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  1.831479/  1.940902, val:  59.17%, val_best:  67.50%, tr:  93.87%, tr_best:  93.87%, epoch time: 40.32 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8740%\n",
      "layer   2  Sparsity: 85.6958%\n",
      "layer   3  Sparsity: 89.3429%\n",
      "total_backward_count 195800 real_backward_count 44646  22.802%\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  1.749382/  1.926431, val:  51.67%, val_best:  67.50%, tr:  94.59%, tr_best:  94.59%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8879%\n",
      "layer   2  Sparsity: 85.5453%\n",
      "layer   3  Sparsity: 86.8002%\n",
      "total_backward_count 200695 real_backward_count 45666  22.754%\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  1.750636/  1.908134, val:  49.58%, val_best:  67.50%, tr:  93.56%, tr_best:  94.59%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8772%\n",
      "layer   2  Sparsity: 85.2906%\n",
      "layer   3  Sparsity: 87.2307%\n",
      "total_backward_count 205590 real_backward_count 46669  22.700%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  1.701342/  1.880262, val:  53.33%, val_best:  67.50%, tr:  93.67%, tr_best:  94.59%, epoch time: 40.63 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8626%\n",
      "layer   2  Sparsity: 85.3432%\n",
      "layer   3  Sparsity: 86.8209%\n",
      "total_backward_count 210485 real_backward_count 47648  22.637%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  1.725003/  1.918692, val:  57.92%, val_best:  67.50%, tr:  94.59%, tr_best:  94.59%, epoch time: 40.70 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8516%\n",
      "layer   2  Sparsity: 84.9054%\n",
      "layer   3  Sparsity: 87.4565%\n",
      "total_backward_count 215380 real_backward_count 48604  22.567%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  1.780670/  1.905466, val:  64.17%, val_best:  67.50%, tr:  92.24%, tr_best:  94.59%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9153%\n",
      "layer   2  Sparsity: 85.6308%\n",
      "layer   3  Sparsity: 88.2761%\n",
      "total_backward_count 220275 real_backward_count 49698  22.562%\n",
      "fc layer 1 self.abs_max_out: 17132.0\n",
      "fc layer 1 self.abs_max_out: 17414.0\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  1.742399/  1.878086, val:  61.25%, val_best:  67.50%, tr:  93.46%, tr_best:  94.59%, epoch time: 40.33 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8325%\n",
      "layer   2  Sparsity: 85.6660%\n",
      "layer   3  Sparsity: 87.2731%\n",
      "total_backward_count 225170 real_backward_count 50702  22.517%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  1.739503/  1.924119, val:  55.00%, val_best:  67.50%, tr:  93.97%, tr_best:  94.59%, epoch time: 40.23 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8490%\n",
      "layer   2  Sparsity: 84.7920%\n",
      "layer   3  Sparsity: 87.0585%\n",
      "total_backward_count 230065 real_backward_count 51696  22.470%\n",
      "fc layer 1 self.abs_max_out: 17728.0\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  1.743556/  1.904215, val:  55.00%, val_best:  67.50%, tr:  94.28%, tr_best:  94.59%, epoch time: 40.77 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8545%\n",
      "layer   2  Sparsity: 84.5861%\n",
      "layer   3  Sparsity: 86.3534%\n",
      "total_backward_count 234960 real_backward_count 52672  22.417%\n",
      "lif layer 1 self.abs_max_v: 27468.5\n",
      "lif layer 1 self.abs_max_v: 27520.5\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  1.721142/  1.898043, val:  55.00%, val_best:  67.50%, tr:  93.97%, tr_best:  94.59%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8681%\n",
      "layer   2  Sparsity: 84.3872%\n",
      "layer   3  Sparsity: 86.8648%\n",
      "total_backward_count 239855 real_backward_count 53652  22.369%\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  1.707441/  1.879297, val:  55.42%, val_best:  67.50%, tr:  94.69%, tr_best:  94.69%, epoch time: 40.62 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8446%\n",
      "layer   2  Sparsity: 84.0806%\n",
      "layer   3  Sparsity: 86.4879%\n",
      "total_backward_count 244750 real_backward_count 54705  22.351%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  1.722709/  1.886549, val:  51.25%, val_best:  67.50%, tr:  94.79%, tr_best:  94.79%, epoch time: 39.92 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9149%\n",
      "layer   2  Sparsity: 84.1567%\n",
      "layer   3  Sparsity: 87.1587%\n",
      "total_backward_count 249645 real_backward_count 55680  22.304%\n",
      "lif layer 1 self.abs_max_v: 27563.0\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  1.717134/  1.877967, val:  47.50%, val_best:  67.50%, tr:  94.28%, tr_best:  94.79%, epoch time: 40.63 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8718%\n",
      "layer   2  Sparsity: 84.0805%\n",
      "layer   3  Sparsity: 87.4002%\n",
      "total_backward_count 254540 real_backward_count 56670  22.264%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  1.737037/  1.933584, val:  52.92%, val_best:  67.50%, tr:  93.26%, tr_best:  94.79%, epoch time: 40.19 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8335%\n",
      "layer   2  Sparsity: 84.1076%\n",
      "layer   3  Sparsity: 88.2069%\n",
      "total_backward_count 259435 real_backward_count 57695  22.239%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  1.708257/  1.862536, val:  64.17%, val_best:  67.50%, tr:  95.10%, tr_best:  95.10%, epoch time: 40.93 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8604%\n",
      "layer   2  Sparsity: 84.5810%\n",
      "layer   3  Sparsity: 86.7222%\n",
      "total_backward_count 264330 real_backward_count 58633  22.182%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  1.720933/  1.856893, val:  68.33%, val_best:  68.33%, tr:  93.97%, tr_best:  95.10%, epoch time: 40.14 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8794%\n",
      "layer   2  Sparsity: 84.3724%\n",
      "layer   3  Sparsity: 86.9566%\n",
      "total_backward_count 269225 real_backward_count 59591  22.134%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  1.734863/  1.895997, val:  62.50%, val_best:  68.33%, tr:  94.99%, tr_best:  95.10%, epoch time: 40.77 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8278%\n",
      "layer   2  Sparsity: 84.4272%\n",
      "layer   3  Sparsity: 88.1432%\n",
      "total_backward_count 274120 real_backward_count 60576  22.098%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  1.760618/  1.904873, val:  50.42%, val_best:  68.33%, tr:  93.97%, tr_best:  95.10%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8651%\n",
      "layer   2  Sparsity: 84.6877%\n",
      "layer   3  Sparsity: 88.6728%\n",
      "total_backward_count 279015 real_backward_count 61568  22.066%\n",
      "lif layer 1 self.abs_max_v: 27803.0\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  1.758972/  1.936998, val:  58.75%, val_best:  68.33%, tr:  93.56%, tr_best:  95.10%, epoch time: 40.77 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8060%\n",
      "layer   2  Sparsity: 85.2572%\n",
      "layer   3  Sparsity: 88.6472%\n",
      "total_backward_count 283910 real_backward_count 62585  22.044%\n",
      "lif layer 1 self.abs_max_v: 27962.0\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  1.785529/  1.894815, val:  60.83%, val_best:  68.33%, tr:  93.36%, tr_best:  95.10%, epoch time: 40.01 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8818%\n",
      "layer   2  Sparsity: 85.3117%\n",
      "layer   3  Sparsity: 88.3947%\n",
      "total_backward_count 288805 real_backward_count 63552  22.005%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  1.755348/  1.893454, val:  48.75%, val_best:  68.33%, tr:  93.56%, tr_best:  95.10%, epoch time: 41.18 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.8368%\n",
      "layer   2  Sparsity: 85.1636%\n",
      "layer   3  Sparsity: 88.3421%\n",
      "total_backward_count 293700 real_backward_count 64504  21.963%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  1.719684/  1.856505, val:  62.08%, val_best:  68.33%, tr:  94.48%, tr_best:  95.10%, epoch time: 40.25 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8464%\n",
      "layer   2  Sparsity: 85.2822%\n",
      "layer   3  Sparsity: 87.8786%\n",
      "total_backward_count 298595 real_backward_count 65471  21.926%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  1.743828/  1.877308, val:  69.17%, val_best:  69.17%, tr:  94.38%, tr_best:  95.10%, epoch time: 40.70 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8696%\n",
      "layer   2  Sparsity: 85.4050%\n",
      "layer   3  Sparsity: 88.3266%\n",
      "total_backward_count 303490 real_backward_count 66454  21.897%\n",
      "lif layer 1 self.abs_max_v: 28104.0\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  1.733679/  1.969712, val:  48.33%, val_best:  69.17%, tr:  92.54%, tr_best:  95.10%, epoch time: 40.58 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8453%\n",
      "layer   2  Sparsity: 85.7490%\n",
      "layer   3  Sparsity: 88.2929%\n",
      "total_backward_count 308385 real_backward_count 67435  21.867%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  1.778306/  1.950181, val:  49.17%, val_best:  69.17%, tr:  93.56%, tr_best:  95.10%, epoch time: 40.64 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.7822%\n",
      "layer   2  Sparsity: 85.8993%\n",
      "layer   3  Sparsity: 88.8383%\n",
      "total_backward_count 313280 real_backward_count 68421  21.840%\n",
      "lif layer 1 self.abs_max_v: 28117.5\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  1.757044/  1.929769, val:  57.08%, val_best:  69.17%, tr:  93.36%, tr_best:  95.10%, epoch time: 40.00 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8521%\n",
      "layer   2  Sparsity: 84.8605%\n",
      "layer   3  Sparsity: 87.4158%\n",
      "total_backward_count 318175 real_backward_count 69389  21.808%\n",
      "lif layer 1 self.abs_max_v: 28175.0\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  1.762372/  1.933181, val:  58.75%, val_best:  69.17%, tr:  93.46%, tr_best:  95.10%, epoch time: 40.38 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8733%\n",
      "layer   2  Sparsity: 85.5194%\n",
      "layer   3  Sparsity: 87.6358%\n",
      "total_backward_count 323070 real_backward_count 70391  21.788%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  1.760943/  1.962155, val:  52.92%, val_best:  69.17%, tr:  94.08%, tr_best:  95.10%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9290%\n",
      "layer   2  Sparsity: 86.3132%\n",
      "layer   3  Sparsity: 87.8363%\n",
      "total_backward_count 327965 real_backward_count 71361  21.759%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  1.779314/  1.921212, val:  65.42%, val_best:  69.17%, tr:  93.36%, tr_best:  95.10%, epoch time: 40.12 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8769%\n",
      "layer   2  Sparsity: 86.1937%\n",
      "layer   3  Sparsity: 88.3859%\n",
      "total_backward_count 332860 real_backward_count 72348  21.735%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  1.756757/  1.884891, val:  60.83%, val_best:  69.17%, tr:  93.36%, tr_best:  95.10%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8589%\n",
      "layer   2  Sparsity: 86.9833%\n",
      "layer   3  Sparsity: 87.3570%\n",
      "total_backward_count 337755 real_backward_count 73318  21.707%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  1.741581/  1.894031, val:  54.58%, val_best:  69.17%, tr:  93.46%, tr_best:  95.10%, epoch time: 39.73 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8243%\n",
      "layer   2  Sparsity: 86.7587%\n",
      "layer   3  Sparsity: 87.2886%\n",
      "total_backward_count 342650 real_backward_count 74312  21.687%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  1.744499/  1.903231, val:  65.00%, val_best:  69.17%, tr:  93.87%, tr_best:  95.10%, epoch time: 40.85 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8597%\n",
      "layer   2  Sparsity: 86.9007%\n",
      "layer   3  Sparsity: 87.7576%\n",
      "total_backward_count 347545 real_backward_count 75310  21.669%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  1.766957/  1.928645, val:  57.08%, val_best:  69.17%, tr:  93.56%, tr_best:  95.10%, epoch time: 40.15 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8668%\n",
      "layer   2  Sparsity: 86.6716%\n",
      "layer   3  Sparsity: 88.4804%\n",
      "total_backward_count 352440 real_backward_count 76306  21.651%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  1.778307/  1.888098, val:  60.00%, val_best:  69.17%, tr:  92.75%, tr_best:  95.10%, epoch time: 40.64 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8844%\n",
      "layer   2  Sparsity: 87.2627%\n",
      "layer   3  Sparsity: 87.9661%\n",
      "total_backward_count 357335 real_backward_count 77300  21.632%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  1.734406/  1.886450, val:  63.75%, val_best:  69.17%, tr:  94.18%, tr_best:  95.10%, epoch time: 40.16 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8231%\n",
      "layer   2  Sparsity: 87.0729%\n",
      "layer   3  Sparsity: 87.0621%\n",
      "total_backward_count 362230 real_backward_count 78204  21.590%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  1.720235/  1.899378, val:  54.17%, val_best:  69.17%, tr:  94.18%, tr_best:  95.10%, epoch time: 41.01 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8892%\n",
      "layer   2  Sparsity: 87.4272%\n",
      "layer   3  Sparsity: 86.4906%\n",
      "total_backward_count 367125 real_backward_count 79184  21.569%\n",
      "fc layer 3 self.abs_max_out: 1821.0\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  1.712784/  1.838661, val:  71.67%, val_best:  71.67%, tr:  93.46%, tr_best:  95.10%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8237%\n",
      "layer   2  Sparsity: 87.4046%\n",
      "layer   3  Sparsity: 86.8418%\n",
      "total_backward_count 372020 real_backward_count 80123  21.537%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  1.725269/  1.862305, val:  60.00%, val_best:  71.67%, tr:  92.24%, tr_best:  95.10%, epoch time: 40.44 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8917%\n",
      "layer   2  Sparsity: 87.1955%\n",
      "layer   3  Sparsity: 87.5715%\n",
      "total_backward_count 376915 real_backward_count 81129  21.524%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  1.726350/  1.921400, val:  55.42%, val_best:  71.67%, tr:  93.67%, tr_best:  95.10%, epoch time: 39.60 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.9032%\n",
      "layer   2  Sparsity: 88.1108%\n",
      "layer   3  Sparsity: 88.1965%\n",
      "total_backward_count 381810 real_backward_count 82012  21.480%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  1.747107/  1.904974, val:  55.83%, val_best:  71.67%, tr:  94.38%, tr_best:  95.10%, epoch time: 40.27 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8504%\n",
      "layer   2  Sparsity: 88.1193%\n",
      "layer   3  Sparsity: 87.7646%\n",
      "total_backward_count 386705 real_backward_count 83007  21.465%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  1.756784/  1.911128, val:  61.67%, val_best:  71.67%, tr:  92.85%, tr_best:  95.10%, epoch time: 40.25 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8860%\n",
      "layer   2  Sparsity: 88.3977%\n",
      "layer   3  Sparsity: 87.2969%\n",
      "total_backward_count 391600 real_backward_count 83977  21.445%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  1.771848/  1.895942, val:  59.58%, val_best:  71.67%, tr:  93.36%, tr_best:  95.10%, epoch time: 40.66 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8665%\n",
      "layer   2  Sparsity: 88.6035%\n",
      "layer   3  Sparsity: 88.4221%\n",
      "total_backward_count 396495 real_backward_count 84972  21.431%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  1.788023/  1.949581, val:  49.17%, val_best:  71.67%, tr:  92.03%, tr_best:  95.10%, epoch time: 40.02 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8784%\n",
      "layer   2  Sparsity: 88.7830%\n",
      "layer   3  Sparsity: 88.3300%\n",
      "total_backward_count 401390 real_backward_count 85967  21.417%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  1.761974/  1.877559, val:  54.17%, val_best:  71.67%, tr:  93.36%, tr_best:  95.10%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8501%\n",
      "layer   2  Sparsity: 88.1623%\n",
      "layer   3  Sparsity: 87.2224%\n",
      "total_backward_count 406285 real_backward_count 86930  21.396%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  1.742662/  1.859525, val:  58.75%, val_best:  71.67%, tr:  94.28%, tr_best:  95.10%, epoch time: 40.47 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8629%\n",
      "layer   2  Sparsity: 88.3847%\n",
      "layer   3  Sparsity: 86.4933%\n",
      "total_backward_count 411180 real_backward_count 87917  21.382%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  1.725610/  1.928555, val:  60.00%, val_best:  71.67%, tr:  93.77%, tr_best:  95.10%, epoch time: 40.31 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8267%\n",
      "layer   2  Sparsity: 88.9440%\n",
      "layer   3  Sparsity: 87.0601%\n",
      "total_backward_count 416075 real_backward_count 88906  21.368%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  1.739334/  1.877308, val:  57.50%, val_best:  71.67%, tr:  91.93%, tr_best:  95.10%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8899%\n",
      "layer   2  Sparsity: 88.7958%\n",
      "layer   3  Sparsity: 86.5109%\n",
      "total_backward_count 420970 real_backward_count 89981  21.375%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  1.718953/  1.884274, val:  56.67%, val_best:  71.67%, tr:  94.79%, tr_best:  95.10%, epoch time: 40.70 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9045%\n",
      "layer   2  Sparsity: 88.7767%\n",
      "layer   3  Sparsity: 86.6938%\n",
      "total_backward_count 425865 real_backward_count 90936  21.353%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  1.719258/  1.897335, val:  65.00%, val_best:  71.67%, tr:  90.70%, tr_best:  95.10%, epoch time: 40.92 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9119%\n",
      "layer   2  Sparsity: 88.6824%\n",
      "layer   3  Sparsity: 86.8090%\n",
      "total_backward_count 430760 real_backward_count 91933  21.342%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  1.744985/  1.868417, val:  67.08%, val_best:  71.67%, tr:  91.22%, tr_best:  95.10%, epoch time: 40.62 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8433%\n",
      "layer   2  Sparsity: 89.0674%\n",
      "layer   3  Sparsity: 87.6462%\n",
      "total_backward_count 435655 real_backward_count 92966  21.339%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  1.735576/  1.886247, val:  65.83%, val_best:  71.67%, tr:  94.18%, tr_best:  95.10%, epoch time: 40.55 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8404%\n",
      "layer   2  Sparsity: 88.6419%\n",
      "layer   3  Sparsity: 86.3428%\n",
      "total_backward_count 440550 real_backward_count 93970  21.330%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  1.709222/  1.821118, val:  58.33%, val_best:  71.67%, tr:  94.18%, tr_best:  95.10%, epoch time: 40.06 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8924%\n",
      "layer   2  Sparsity: 87.8464%\n",
      "layer   3  Sparsity: 85.7335%\n",
      "total_backward_count 445445 real_backward_count 94961  21.318%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  1.737303/  1.863527, val:  65.42%, val_best:  71.67%, tr:  91.42%, tr_best:  95.10%, epoch time: 40.37 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8641%\n",
      "layer   2  Sparsity: 87.8595%\n",
      "layer   3  Sparsity: 86.9838%\n",
      "total_backward_count 450340 real_backward_count 95981  21.313%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  1.701592/  1.811879, val:  50.42%, val_best:  71.67%, tr:  93.26%, tr_best:  95.10%, epoch time: 40.13 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8146%\n",
      "layer   2  Sparsity: 87.7796%\n",
      "layer   3  Sparsity: 86.1356%\n",
      "total_backward_count 455235 real_backward_count 96973  21.302%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  1.650347/  1.835373, val:  71.25%, val_best:  71.67%, tr:  95.10%, tr_best:  95.10%, epoch time: 40.30 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8677%\n",
      "layer   2  Sparsity: 88.3080%\n",
      "layer   3  Sparsity: 85.6123%\n",
      "total_backward_count 460130 real_backward_count 97903  21.277%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  1.670217/  1.773060, val:  59.17%, val_best:  71.67%, tr:  95.71%, tr_best:  95.71%, epoch time: 39.93 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8234%\n",
      "layer   2  Sparsity: 88.3565%\n",
      "layer   3  Sparsity: 84.5705%\n",
      "total_backward_count 465025 real_backward_count 98852  21.257%\n",
      "lif layer 1 self.abs_max_v: 28632.5\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  1.663261/  1.825115, val:  72.08%, val_best:  72.08%, tr:  93.56%, tr_best:  95.71%, epoch time: 40.20 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8831%\n",
      "layer   2  Sparsity: 88.2687%\n",
      "layer   3  Sparsity: 85.3678%\n",
      "total_backward_count 469920 real_backward_count 99860  21.250%\n",
      "fc layer 1 self.abs_max_out: 17866.0\n",
      "lif layer 1 self.abs_max_v: 30423.5\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  1.678708/  1.889941, val:  53.33%, val_best:  72.08%, tr:  92.34%, tr_best:  95.71%, epoch time: 40.66 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8758%\n",
      "layer   2  Sparsity: 88.4363%\n",
      "layer   3  Sparsity: 86.2432%\n",
      "total_backward_count 474815 real_backward_count 100856  21.241%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  1.690483/  1.801649, val:  67.92%, val_best:  72.08%, tr:  92.95%, tr_best:  95.71%, epoch time: 40.94 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9146%\n",
      "layer   2  Sparsity: 88.5869%\n",
      "layer   3  Sparsity: 86.0835%\n",
      "total_backward_count 479710 real_backward_count 101822  21.226%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  1.669603/  1.882067, val:  56.67%, val_best:  72.08%, tr:  93.77%, tr_best:  95.71%, epoch time: 40.14 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8788%\n",
      "layer   2  Sparsity: 88.8144%\n",
      "layer   3  Sparsity: 86.6593%\n",
      "total_backward_count 484605 real_backward_count 102758  21.204%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  1.664906/  1.851716, val:  61.67%, val_best:  72.08%, tr:  93.46%, tr_best:  95.71%, epoch time: 40.76 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8845%\n",
      "layer   2  Sparsity: 88.0457%\n",
      "layer   3  Sparsity: 86.0422%\n",
      "total_backward_count 489500 real_backward_count 103725  21.190%\n",
      "fc layer 3 self.abs_max_out: 2084.0\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  1.638103/  1.837222, val:  52.92%, val_best:  72.08%, tr:  95.81%, tr_best:  95.81%, epoch time: 40.67 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8460%\n",
      "layer   2  Sparsity: 87.8735%\n",
      "layer   3  Sparsity: 85.8795%\n",
      "total_backward_count 494395 real_backward_count 104645  21.166%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  1.663435/  1.795555, val:  60.83%, val_best:  72.08%, tr:  93.56%, tr_best:  95.81%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8549%\n",
      "layer   2  Sparsity: 88.4699%\n",
      "layer   3  Sparsity: 85.7753%\n",
      "total_backward_count 499290 real_backward_count 105656  21.161%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  1.665752/  1.800017, val:  62.50%, val_best:  72.08%, tr:  93.46%, tr_best:  95.81%, epoch time: 41.11 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.8554%\n",
      "layer   2  Sparsity: 87.7788%\n",
      "layer   3  Sparsity: 85.4236%\n",
      "total_backward_count 504185 real_backward_count 106626  21.148%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  1.640578/  1.812427, val:  69.58%, val_best:  72.08%, tr:  94.99%, tr_best:  95.81%, epoch time: 40.32 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8394%\n",
      "layer   2  Sparsity: 87.6663%\n",
      "layer   3  Sparsity: 85.3843%\n",
      "total_backward_count 509080 real_backward_count 107547  21.126%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  1.634334/  1.821708, val:  63.33%, val_best:  72.08%, tr:  92.44%, tr_best:  95.81%, epoch time: 40.73 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8572%\n",
      "layer   2  Sparsity: 87.9618%\n",
      "layer   3  Sparsity: 84.9388%\n",
      "total_backward_count 513975 real_backward_count 108581  21.126%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  1.633730/  1.745575, val:  68.33%, val_best:  72.08%, tr:  94.59%, tr_best:  95.81%, epoch time: 40.73 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8398%\n",
      "layer   2  Sparsity: 88.0069%\n",
      "layer   3  Sparsity: 84.2774%\n",
      "total_backward_count 518870 real_backward_count 109535  21.110%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  1.652774/  1.849910, val:  62.08%, val_best:  72.08%, tr:  93.97%, tr_best:  95.81%, epoch time: 40.99 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8595%\n",
      "layer   2  Sparsity: 87.5911%\n",
      "layer   3  Sparsity: 85.3957%\n",
      "total_backward_count 523765 real_backward_count 110499  21.097%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  1.688028/  1.813419, val:  54.17%, val_best:  72.08%, tr:  92.34%, tr_best:  95.81%, epoch time: 40.43 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9072%\n",
      "layer   2  Sparsity: 87.0721%\n",
      "layer   3  Sparsity: 85.5330%\n",
      "total_backward_count 528660 real_backward_count 111521  21.095%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  1.647123/  1.870661, val:  57.50%, val_best:  72.08%, tr:  94.59%, tr_best:  95.81%, epoch time: 40.91 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8416%\n",
      "layer   2  Sparsity: 87.4125%\n",
      "layer   3  Sparsity: 85.5838%\n",
      "total_backward_count 533555 real_backward_count 112477  21.081%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  1.634946/  1.819061, val:  57.50%, val_best:  72.08%, tr:  94.38%, tr_best:  95.81%, epoch time: 40.79 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8736%\n",
      "layer   2  Sparsity: 87.2945%\n",
      "layer   3  Sparsity: 84.7952%\n",
      "total_backward_count 538450 real_backward_count 113413  21.063%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  1.615825/  1.813694, val:  63.33%, val_best:  72.08%, tr:  93.97%, tr_best:  95.81%, epoch time: 40.66 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8588%\n",
      "layer   2  Sparsity: 86.9605%\n",
      "layer   3  Sparsity: 84.4130%\n",
      "total_backward_count 543345 real_backward_count 114354  21.046%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  1.643245/  1.755554, val:  53.75%, val_best:  72.08%, tr:  94.99%, tr_best:  95.81%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8693%\n",
      "layer   2  Sparsity: 86.9633%\n",
      "layer   3  Sparsity: 84.8631%\n",
      "total_backward_count 548240 real_backward_count 115327  21.036%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  1.596274/  1.738555, val:  59.17%, val_best:  72.08%, tr:  94.18%, tr_best:  95.81%, epoch time: 40.71 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8683%\n",
      "layer   2  Sparsity: 86.8944%\n",
      "layer   3  Sparsity: 84.4171%\n",
      "total_backward_count 553135 real_backward_count 116265  21.019%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  1.625268/  1.824631, val:  57.92%, val_best:  72.08%, tr:  94.59%, tr_best:  95.81%, epoch time: 40.41 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8899%\n",
      "layer   2  Sparsity: 87.1578%\n",
      "layer   3  Sparsity: 84.8646%\n",
      "total_backward_count 558030 real_backward_count 117195  21.002%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  1.584490/  1.787971, val:  60.42%, val_best:  72.08%, tr:  95.10%, tr_best:  95.81%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8554%\n",
      "layer   2  Sparsity: 86.4833%\n",
      "layer   3  Sparsity: 83.5654%\n",
      "total_backward_count 562925 real_backward_count 118084  20.977%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  1.605879/  1.780173, val:  65.42%, val_best:  72.08%, tr:  94.38%, tr_best:  95.81%, epoch time: 40.91 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8525%\n",
      "layer   2  Sparsity: 86.4002%\n",
      "layer   3  Sparsity: 84.1847%\n",
      "total_backward_count 567820 real_backward_count 119031  20.963%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  1.532221/  1.747903, val:  64.58%, val_best:  72.08%, tr:  95.51%, tr_best:  95.81%, epoch time: 39.97 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9155%\n",
      "layer   2  Sparsity: 86.0956%\n",
      "layer   3  Sparsity: 82.4527%\n",
      "total_backward_count 572715 real_backward_count 119880  20.932%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  1.533453/  1.755301, val:  61.67%, val_best:  72.08%, tr:  94.38%, tr_best:  95.81%, epoch time: 40.45 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8725%\n",
      "layer   2  Sparsity: 86.1669%\n",
      "layer   3  Sparsity: 82.7445%\n",
      "total_backward_count 577610 real_backward_count 120750  20.905%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  1.515681/  1.709836, val:  54.17%, val_best:  72.08%, tr:  94.08%, tr_best:  95.81%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8551%\n",
      "layer   2  Sparsity: 86.3601%\n",
      "layer   3  Sparsity: 83.1026%\n",
      "total_backward_count 582505 real_backward_count 121651  20.884%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  1.525514/  1.777038, val:  54.17%, val_best:  72.08%, tr:  94.79%, tr_best:  95.81%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8921%\n",
      "layer   2  Sparsity: 86.5232%\n",
      "layer   3  Sparsity: 83.8119%\n",
      "total_backward_count 587400 real_backward_count 122560  20.865%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  1.549984/  1.745921, val:  60.83%, val_best:  72.08%, tr:  94.59%, tr_best:  95.81%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8458%\n",
      "layer   2  Sparsity: 86.1303%\n",
      "layer   3  Sparsity: 83.0302%\n",
      "total_backward_count 592295 real_backward_count 123494  20.850%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  1.535260/  1.721469, val:  60.42%, val_best:  72.08%, tr:  95.10%, tr_best:  95.81%, epoch time: 40.50 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8343%\n",
      "layer   2  Sparsity: 86.4186%\n",
      "layer   3  Sparsity: 82.0607%\n",
      "total_backward_count 597190 real_backward_count 124399  20.831%\n",
      "lif layer 1 self.abs_max_v: 30678.5\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  1.548746/  1.811817, val:  64.17%, val_best:  72.08%, tr:  94.28%, tr_best:  95.81%, epoch time: 40.84 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8755%\n",
      "layer   2  Sparsity: 86.6168%\n",
      "layer   3  Sparsity: 84.1182%\n",
      "total_backward_count 602085 real_backward_count 125335  20.817%\n",
      "lif layer 1 self.abs_max_v: 31253.5\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  1.642700/  1.779568, val:  54.17%, val_best:  72.08%, tr:  94.28%, tr_best:  95.81%, epoch time: 40.42 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8819%\n",
      "layer   2  Sparsity: 86.6737%\n",
      "layer   3  Sparsity: 85.8030%\n",
      "total_backward_count 606980 real_backward_count 126323  20.812%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  1.588127/  1.777869, val:  64.58%, val_best:  72.08%, tr:  94.38%, tr_best:  95.81%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.7995%\n",
      "layer   2  Sparsity: 86.6368%\n",
      "layer   3  Sparsity: 84.3861%\n",
      "total_backward_count 611875 real_backward_count 127235  20.794%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  1.597126/  1.827660, val:  58.75%, val_best:  72.08%, tr:  93.77%, tr_best:  95.81%, epoch time: 40.01 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8186%\n",
      "layer   2  Sparsity: 86.4228%\n",
      "layer   3  Sparsity: 85.2011%\n",
      "total_backward_count 616770 real_backward_count 128132  20.775%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  1.640237/  1.852557, val:  61.25%, val_best:  72.08%, tr:  94.38%, tr_best:  95.81%, epoch time: 39.89 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8489%\n",
      "layer   2  Sparsity: 86.8868%\n",
      "layer   3  Sparsity: 86.1100%\n",
      "total_backward_count 621665 real_backward_count 129053  20.759%\n",
      "lif layer 1 self.abs_max_v: 31412.5\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  1.610273/  1.785130, val:  57.50%, val_best:  72.08%, tr:  94.48%, tr_best:  95.81%, epoch time: 39.91 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8653%\n",
      "layer   2  Sparsity: 86.6040%\n",
      "layer   3  Sparsity: 85.1223%\n",
      "total_backward_count 626560 real_backward_count 129955  20.741%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  1.603202/  1.774172, val:  68.33%, val_best:  72.08%, tr:  94.08%, tr_best:  95.81%, epoch time: 40.46 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8650%\n",
      "layer   2  Sparsity: 87.1787%\n",
      "layer   3  Sparsity: 84.3346%\n",
      "total_backward_count 631455 real_backward_count 130915  20.732%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  1.609799/  1.761302, val:  63.75%, val_best:  72.08%, tr:  93.97%, tr_best:  95.81%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8589%\n",
      "layer   2  Sparsity: 86.9946%\n",
      "layer   3  Sparsity: 85.2207%\n",
      "total_backward_count 636350 real_backward_count 131880  20.724%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  1.602511/  1.811025, val:  52.92%, val_best:  72.08%, tr:  93.67%, tr_best:  95.81%, epoch time: 40.32 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8669%\n",
      "layer   2  Sparsity: 86.2876%\n",
      "layer   3  Sparsity: 84.1068%\n",
      "total_backward_count 641245 real_backward_count 132866  20.720%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  1.595388/  1.798049, val:  55.00%, val_best:  72.08%, tr:  92.95%, tr_best:  95.81%, epoch time: 40.44 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8929%\n",
      "layer   2  Sparsity: 86.3723%\n",
      "layer   3  Sparsity: 84.9794%\n",
      "total_backward_count 646140 real_backward_count 133874  20.719%\n",
      "fc layer 3 self.abs_max_out: 2112.0\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  1.608050/  1.779851, val:  60.00%, val_best:  72.08%, tr:  93.77%, tr_best:  95.81%, epoch time: 40.52 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8528%\n",
      "layer   2  Sparsity: 86.3463%\n",
      "layer   3  Sparsity: 84.5103%\n",
      "total_backward_count 651035 real_backward_count 134798  20.705%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  1.595618/  1.776950, val:  57.50%, val_best:  72.08%, tr:  94.69%, tr_best:  95.81%, epoch time: 39.96 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9000%\n",
      "layer   2  Sparsity: 85.9887%\n",
      "layer   3  Sparsity: 84.2929%\n",
      "total_backward_count 655930 real_backward_count 135742  20.695%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  1.592680/  1.754095, val:  53.75%, val_best:  72.08%, tr:  94.08%, tr_best:  95.81%, epoch time: 40.08 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8680%\n",
      "layer   2  Sparsity: 86.3422%\n",
      "layer   3  Sparsity: 83.5308%\n",
      "total_backward_count 660825 real_backward_count 136688  20.684%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  1.604386/  1.733466, val:  62.08%, val_best:  72.08%, tr:  94.48%, tr_best:  95.81%, epoch time: 39.53 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8526%\n",
      "layer   2  Sparsity: 86.0911%\n",
      "layer   3  Sparsity: 85.3098%\n",
      "total_backward_count 665720 real_backward_count 137675  20.681%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  1.611505/  1.785689, val:  58.33%, val_best:  72.08%, tr:  94.69%, tr_best:  95.81%, epoch time: 40.08 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8654%\n",
      "layer   2  Sparsity: 86.2171%\n",
      "layer   3  Sparsity: 85.6738%\n",
      "total_backward_count 670615 real_backward_count 138607  20.669%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  1.592149/  1.794971, val:  61.67%, val_best:  72.08%, tr:  93.97%, tr_best:  95.81%, epoch time: 40.43 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8402%\n",
      "layer   2  Sparsity: 86.7372%\n",
      "layer   3  Sparsity: 85.2241%\n",
      "total_backward_count 675510 real_backward_count 139565  20.661%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  1.653783/  1.776007, val:  61.67%, val_best:  72.08%, tr:  93.16%, tr_best:  95.81%, epoch time: 40.43 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8594%\n",
      "layer   2  Sparsity: 86.5253%\n",
      "layer   3  Sparsity: 85.9411%\n",
      "total_backward_count 680405 real_backward_count 140556  20.658%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  1.643399/  1.784541, val:  59.17%, val_best:  72.08%, tr:  94.89%, tr_best:  95.81%, epoch time: 39.31 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8733%\n",
      "layer   2  Sparsity: 86.0893%\n",
      "layer   3  Sparsity: 84.6389%\n",
      "total_backward_count 685300 real_backward_count 141512  20.650%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  1.601257/  1.776170, val:  57.50%, val_best:  72.08%, tr:  94.38%, tr_best:  95.81%, epoch time: 38.51 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 88.8719%\n",
      "layer   2  Sparsity: 85.8517%\n",
      "layer   3  Sparsity: 84.8951%\n",
      "total_backward_count 690195 real_backward_count 142478  20.643%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  1.620654/  1.794631, val:  47.92%, val_best:  72.08%, tr:  94.59%, tr_best:  95.81%, epoch time: 40.08 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8507%\n",
      "layer   2  Sparsity: 85.9272%\n",
      "layer   3  Sparsity: 84.0634%\n",
      "total_backward_count 695090 real_backward_count 143397  20.630%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  1.584916/  1.761935, val:  49.17%, val_best:  72.08%, tr:  94.69%, tr_best:  95.81%, epoch time: 40.63 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8505%\n",
      "layer   2  Sparsity: 86.0907%\n",
      "layer   3  Sparsity: 83.9736%\n",
      "total_backward_count 699985 real_backward_count 144374  20.625%\n",
      "fc layer 3 self.abs_max_out: 2214.0\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  1.602649/  1.768718, val:  53.75%, val_best:  72.08%, tr:  94.89%, tr_best:  95.81%, epoch time: 40.39 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8651%\n",
      "layer   2  Sparsity: 86.2669%\n",
      "layer   3  Sparsity: 84.9121%\n",
      "total_backward_count 704880 real_backward_count 145325  20.617%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  1.601649/  1.768322, val:  56.67%, val_best:  72.08%, tr:  93.26%, tr_best:  95.81%, epoch time: 40.26 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9177%\n",
      "layer   2  Sparsity: 86.4318%\n",
      "layer   3  Sparsity: 83.7540%\n",
      "total_backward_count 709775 real_backward_count 146303  20.613%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  1.541850/  1.755534, val:  59.58%, val_best:  72.08%, tr:  96.42%, tr_best:  96.42%, epoch time: 40.24 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8903%\n",
      "layer   2  Sparsity: 86.5139%\n",
      "layer   3  Sparsity: 83.0466%\n",
      "total_backward_count 714670 real_backward_count 147214  20.599%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  1.582408/  1.774552, val:  62.08%, val_best:  72.08%, tr:  94.69%, tr_best:  96.42%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8330%\n",
      "layer   2  Sparsity: 86.8403%\n",
      "layer   3  Sparsity: 84.3616%\n",
      "total_backward_count 719565 real_backward_count 148219  20.598%\n",
      "fc layer 3 self.abs_max_out: 2270.0\n",
      "fc layer 3 self.abs_max_out: 2449.0\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  1.600580/  1.777718, val:  54.58%, val_best:  72.08%, tr:  93.36%, tr_best:  96.42%, epoch time: 40.43 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8384%\n",
      "layer   2  Sparsity: 87.5411%\n",
      "layer   3  Sparsity: 84.9776%\n",
      "total_backward_count 724460 real_backward_count 149126  20.584%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  1.617095/  1.820378, val:  69.58%, val_best:  72.08%, tr:  94.99%, tr_best:  96.42%, epoch time: 40.05 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8154%\n",
      "layer   2  Sparsity: 87.4766%\n",
      "layer   3  Sparsity: 85.2578%\n",
      "total_backward_count 729355 real_backward_count 150055  20.574%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  1.595340/  1.826871, val:  60.83%, val_best:  72.08%, tr:  94.99%, tr_best:  96.42%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8546%\n",
      "layer   2  Sparsity: 87.3710%\n",
      "layer   3  Sparsity: 84.6956%\n",
      "total_backward_count 734250 real_backward_count 151015  20.567%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  1.556335/  1.757029, val:  61.25%, val_best:  72.08%, tr:  94.69%, tr_best:  96.42%, epoch time: 40.32 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8559%\n",
      "layer   2  Sparsity: 87.2789%\n",
      "layer   3  Sparsity: 82.8645%\n",
      "total_backward_count 739145 real_backward_count 151938  20.556%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  1.619384/  1.790205, val:  58.33%, val_best:  72.08%, tr:  92.95%, tr_best:  96.42%, epoch time: 40.12 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8694%\n",
      "layer   2  Sparsity: 87.5012%\n",
      "layer   3  Sparsity: 85.1660%\n",
      "total_backward_count 744040 real_backward_count 152944  20.556%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  1.626144/  1.857594, val:  55.00%, val_best:  72.08%, tr:  94.28%, tr_best:  96.42%, epoch time: 40.09 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8722%\n",
      "layer   2  Sparsity: 87.5125%\n",
      "layer   3  Sparsity: 85.9850%\n",
      "total_backward_count 748935 real_backward_count 153906  20.550%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  1.690004/  1.784153, val:  77.08%, val_best:  77.08%, tr:  91.32%, tr_best:  96.42%, epoch time: 40.47 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8937%\n",
      "layer   2  Sparsity: 87.9020%\n",
      "layer   3  Sparsity: 85.8236%\n",
      "total_backward_count 753830 real_backward_count 154951  20.555%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  1.654384/  1.873524, val:  62.08%, val_best:  77.08%, tr:  93.36%, tr_best:  96.42%, epoch time: 40.79 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8184%\n",
      "layer   2  Sparsity: 87.7460%\n",
      "layer   3  Sparsity: 85.8704%\n",
      "total_backward_count 758725 real_backward_count 155893  20.547%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  1.686864/  1.882516, val:  47.50%, val_best:  77.08%, tr:  92.85%, tr_best:  96.42%, epoch time: 39.86 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8856%\n",
      "layer   2  Sparsity: 87.5523%\n",
      "layer   3  Sparsity: 86.3838%\n",
      "total_backward_count 763620 real_backward_count 156862  20.542%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  1.663450/  1.847195, val:  57.50%, val_best:  77.08%, tr:  92.85%, tr_best:  96.42%, epoch time: 40.33 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8573%\n",
      "layer   2  Sparsity: 87.7033%\n",
      "layer   3  Sparsity: 86.2779%\n",
      "total_backward_count 768515 real_backward_count 157832  20.537%\n",
      "fc layer 3 self.abs_max_out: 2525.0\n",
      "fc layer 3 self.abs_max_out: 2706.0\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  1.628165/  1.782552, val:  63.33%, val_best:  77.08%, tr:  93.67%, tr_best:  96.42%, epoch time: 40.78 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8612%\n",
      "layer   2  Sparsity: 87.9185%\n",
      "layer   3  Sparsity: 83.4952%\n",
      "total_backward_count 773410 real_backward_count 158763  20.528%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  1.617874/  1.848738, val:  46.25%, val_best:  77.08%, tr:  91.93%, tr_best:  96.42%, epoch time: 40.97 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8592%\n",
      "layer   2  Sparsity: 88.6505%\n",
      "layer   3  Sparsity: 84.3779%\n",
      "total_backward_count 778305 real_backward_count 159815  20.534%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  1.642274/  1.861123, val:  60.00%, val_best:  77.08%, tr:  92.85%, tr_best:  96.42%, epoch time: 40.18 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8435%\n",
      "layer   2  Sparsity: 88.7005%\n",
      "layer   3  Sparsity: 84.9494%\n",
      "total_backward_count 783200 real_backward_count 160797  20.531%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  1.647092/  1.832646, val:  60.42%, val_best:  77.08%, tr:  93.97%, tr_best:  96.42%, epoch time: 40.66 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9394%\n",
      "layer   2  Sparsity: 88.6451%\n",
      "layer   3  Sparsity: 85.4500%\n",
      "total_backward_count 788095 real_backward_count 161773  20.527%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  1.677724/  1.770396, val:  70.42%, val_best:  77.08%, tr:  93.77%, tr_best:  96.42%, epoch time: 40.61 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8288%\n",
      "layer   2  Sparsity: 88.6863%\n",
      "layer   3  Sparsity: 84.8954%\n",
      "total_backward_count 792990 real_backward_count 162751  20.524%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  1.642163/  1.837201, val:  60.00%, val_best:  77.08%, tr:  92.54%, tr_best:  96.42%, epoch time: 40.50 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8600%\n",
      "layer   2  Sparsity: 88.9881%\n",
      "layer   3  Sparsity: 83.8705%\n",
      "total_backward_count 797885 real_backward_count 163760  20.524%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  1.638036/  1.892926, val:  51.25%, val_best:  77.08%, tr:  93.16%, tr_best:  96.42%, epoch time: 40.73 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9106%\n",
      "layer   2  Sparsity: 88.9743%\n",
      "layer   3  Sparsity: 85.2486%\n",
      "total_backward_count 802780 real_backward_count 164704  20.517%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  1.655396/  1.836855, val:  66.25%, val_best:  77.08%, tr:  91.93%, tr_best:  96.42%, epoch time: 40.46 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8253%\n",
      "layer   2  Sparsity: 88.7537%\n",
      "layer   3  Sparsity: 83.9705%\n",
      "total_backward_count 807675 real_backward_count 165708  20.517%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  1.577263/  1.762252, val:  64.17%, val_best:  77.08%, tr:  93.05%, tr_best:  96.42%, epoch time: 40.63 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8552%\n",
      "layer   2  Sparsity: 88.7888%\n",
      "layer   3  Sparsity: 82.7762%\n",
      "total_backward_count 812570 real_backward_count 166647  20.509%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  1.562692/  1.786986, val:  50.00%, val_best:  77.08%, tr:  94.08%, tr_best:  96.42%, epoch time: 40.49 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9076%\n",
      "layer   2  Sparsity: 88.4833%\n",
      "layer   3  Sparsity: 82.5706%\n",
      "total_backward_count 817465 real_backward_count 167556  20.497%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  1.588374/  1.804048, val:  51.67%, val_best:  77.08%, tr:  93.67%, tr_best:  96.42%, epoch time: 40.31 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8907%\n",
      "layer   2  Sparsity: 88.3659%\n",
      "layer   3  Sparsity: 83.8412%\n",
      "total_backward_count 822360 real_backward_count 168522  20.492%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  1.594625/  1.863720, val:  57.50%, val_best:  77.08%, tr:  93.56%, tr_best:  96.42%, epoch time: 40.57 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8238%\n",
      "layer   2  Sparsity: 88.3714%\n",
      "layer   3  Sparsity: 84.1805%\n",
      "total_backward_count 827255 real_backward_count 169513  20.491%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  1.610199/  1.775241, val:  60.00%, val_best:  77.08%, tr:  92.95%, tr_best:  96.42%, epoch time: 40.31 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8839%\n",
      "layer   2  Sparsity: 88.0118%\n",
      "layer   3  Sparsity: 84.0611%\n",
      "total_backward_count 832150 real_backward_count 170497  20.489%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  1.594568/  1.820356, val:  60.83%, val_best:  77.08%, tr:  94.79%, tr_best:  96.42%, epoch time: 40.73 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.7795%\n",
      "layer   2  Sparsity: 88.0728%\n",
      "layer   3  Sparsity: 84.1388%\n",
      "total_backward_count 837045 real_backward_count 171406  20.478%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  1.586371/  1.797872, val:  62.92%, val_best:  77.08%, tr:  93.87%, tr_best:  96.42%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8609%\n",
      "layer   2  Sparsity: 87.9445%\n",
      "layer   3  Sparsity: 83.9449%\n",
      "total_backward_count 841940 real_backward_count 172327  20.468%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  1.617028/  1.792314, val:  57.92%, val_best:  77.08%, tr:  94.59%, tr_best:  96.42%, epoch time: 40.45 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8533%\n",
      "layer   2  Sparsity: 88.6426%\n",
      "layer   3  Sparsity: 84.8496%\n",
      "total_backward_count 846835 real_backward_count 173254  20.459%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  1.599147/  1.800548, val:  51.67%, val_best:  77.08%, tr:  93.36%, tr_best:  96.42%, epoch time: 40.12 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8653%\n",
      "layer   2  Sparsity: 88.4730%\n",
      "layer   3  Sparsity: 84.2607%\n",
      "total_backward_count 851730 real_backward_count 174168  20.449%\n",
      "fc layer 3 self.abs_max_out: 2817.0\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  1.553341/  1.765592, val:  67.50%, val_best:  77.08%, tr:  93.97%, tr_best:  96.42%, epoch time: 41.08 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8844%\n",
      "layer   2  Sparsity: 87.8083%\n",
      "layer   3  Sparsity: 83.4583%\n",
      "total_backward_count 856625 real_backward_count 175066  20.437%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  1.611637/  1.728697, val:  59.17%, val_best:  77.08%, tr:  94.59%, tr_best:  96.42%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8893%\n",
      "layer   2  Sparsity: 88.4395%\n",
      "layer   3  Sparsity: 83.9615%\n",
      "total_backward_count 861520 real_backward_count 176009  20.430%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  1.557330/  1.669426, val:  68.75%, val_best:  77.08%, tr:  94.28%, tr_best:  96.42%, epoch time: 40.90 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9205%\n",
      "layer   2  Sparsity: 88.2537%\n",
      "layer   3  Sparsity: 83.5901%\n",
      "total_backward_count 866415 real_backward_count 176954  20.424%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  1.581834/  1.805211, val:  62.08%, val_best:  77.08%, tr:  93.67%, tr_best:  96.42%, epoch time: 40.26 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8411%\n",
      "layer   2  Sparsity: 88.2947%\n",
      "layer   3  Sparsity: 84.2428%\n",
      "total_backward_count 871310 real_backward_count 177873  20.414%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  1.583310/  1.790412, val:  55.83%, val_best:  77.08%, tr:  93.77%, tr_best:  96.42%, epoch time: 40.63 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8369%\n",
      "layer   2  Sparsity: 88.5982%\n",
      "layer   3  Sparsity: 83.6532%\n",
      "total_backward_count 876205 real_backward_count 178765  20.402%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  1.624480/  1.806235, val:  64.17%, val_best:  77.08%, tr:  92.65%, tr_best:  96.42%, epoch time: 39.89 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8910%\n",
      "layer   2  Sparsity: 88.5197%\n",
      "layer   3  Sparsity: 84.0045%\n",
      "total_backward_count 881100 real_backward_count 179701  20.395%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  1.613663/  1.824145, val:  59.17%, val_best:  77.08%, tr:  93.36%, tr_best:  96.42%, epoch time: 40.79 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8480%\n",
      "layer   2  Sparsity: 89.0425%\n",
      "layer   3  Sparsity: 83.7573%\n",
      "total_backward_count 885995 real_backward_count 180631  20.387%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  1.611544/  1.775094, val:  68.75%, val_best:  77.08%, tr:  93.67%, tr_best:  96.42%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8035%\n",
      "layer   2  Sparsity: 89.4461%\n",
      "layer   3  Sparsity: 84.7764%\n",
      "total_backward_count 890890 real_backward_count 181535  20.377%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  1.601164/  1.855998, val:  51.25%, val_best:  77.08%, tr:  93.67%, tr_best:  96.42%, epoch time: 40.27 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8373%\n",
      "layer   2  Sparsity: 89.0239%\n",
      "layer   3  Sparsity: 84.0812%\n",
      "total_backward_count 895785 real_backward_count 182444  20.367%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  1.572763/  1.788673, val:  55.42%, val_best:  77.08%, tr:  93.97%, tr_best:  96.42%, epoch time: 40.06 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8959%\n",
      "layer   2  Sparsity: 88.6940%\n",
      "layer   3  Sparsity: 82.7335%\n",
      "total_backward_count 900680 real_backward_count 183379  20.360%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  1.602179/  1.794743, val:  58.33%, val_best:  77.08%, tr:  93.67%, tr_best:  96.42%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8702%\n",
      "layer   2  Sparsity: 89.0640%\n",
      "layer   3  Sparsity: 83.9999%\n",
      "total_backward_count 905575 real_backward_count 184355  20.358%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  1.645232/  1.868789, val:  65.00%, val_best:  77.08%, tr:  92.13%, tr_best:  96.42%, epoch time: 40.11 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8088%\n",
      "layer   2  Sparsity: 89.3080%\n",
      "layer   3  Sparsity: 85.2741%\n",
      "total_backward_count 910470 real_backward_count 185383  20.361%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  1.640072/  1.853980, val:  64.17%, val_best:  77.08%, tr:  93.67%, tr_best:  96.42%, epoch time: 40.31 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8874%\n",
      "layer   2  Sparsity: 89.3102%\n",
      "layer   3  Sparsity: 85.4145%\n",
      "total_backward_count 915365 real_backward_count 186387  20.362%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  1.659942/  1.828267, val:  63.33%, val_best:  77.08%, tr:  93.46%, tr_best:  96.42%, epoch time: 40.78 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8775%\n",
      "layer   2  Sparsity: 89.5732%\n",
      "layer   3  Sparsity: 85.3249%\n",
      "total_backward_count 920260 real_backward_count 187370  20.361%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  1.614541/  1.830642, val:  71.67%, val_best:  77.08%, tr:  93.36%, tr_best:  96.42%, epoch time: 40.05 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8535%\n",
      "layer   2  Sparsity: 89.2794%\n",
      "layer   3  Sparsity: 84.1757%\n",
      "total_backward_count 925155 real_backward_count 188317  20.355%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  1.591274/  1.761112, val:  65.42%, val_best:  77.08%, tr:  93.67%, tr_best:  96.42%, epoch time: 40.66 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8437%\n",
      "layer   2  Sparsity: 89.5334%\n",
      "layer   3  Sparsity: 83.0244%\n",
      "total_backward_count 930050 real_backward_count 189247  20.348%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  1.568099/  1.734713, val:  70.83%, val_best:  77.08%, tr:  94.48%, tr_best:  96.42%, epoch time: 39.83 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8464%\n",
      "layer   2  Sparsity: 89.3797%\n",
      "layer   3  Sparsity: 83.0774%\n",
      "total_backward_count 934945 real_backward_count 190178  20.341%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  1.565462/  1.833033, val:  47.50%, val_best:  77.08%, tr:  93.97%, tr_best:  96.42%, epoch time: 40.29 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8463%\n",
      "layer   2  Sparsity: 89.2210%\n",
      "layer   3  Sparsity: 82.8073%\n",
      "total_backward_count 939840 real_backward_count 191132  20.337%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  1.587121/  1.778837, val:  57.50%, val_best:  77.08%, tr:  92.75%, tr_best:  96.42%, epoch time: 39.99 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8605%\n",
      "layer   2  Sparsity: 89.3053%\n",
      "layer   3  Sparsity: 83.1036%\n",
      "total_backward_count 944735 real_backward_count 192131  20.337%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  1.547471/  1.783005, val:  51.25%, val_best:  77.08%, tr:  94.08%, tr_best:  96.42%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8641%\n",
      "layer   2  Sparsity: 89.2462%\n",
      "layer   3  Sparsity: 82.5992%\n",
      "total_backward_count 949630 real_backward_count 193072  20.331%\n",
      "fc layer 1 self.abs_max_out: 18003.0\n",
      "fc layer 1 self.abs_max_out: 18414.0\n",
      "lif layer 1 self.abs_max_v: 31862.0\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  1.581170/  1.803714, val:  55.00%, val_best:  77.08%, tr:  93.56%, tr_best:  96.42%, epoch time: 40.25 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8705%\n",
      "layer   2  Sparsity: 89.0094%\n",
      "layer   3  Sparsity: 83.2715%\n",
      "total_backward_count 954525 real_backward_count 193985  20.323%\n",
      "fc layer 1 self.abs_max_out: 18468.0\n",
      "lif layer 1 self.abs_max_v: 31965.5\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  1.609749/  1.783450, val:  51.25%, val_best:  77.08%, tr:  93.16%, tr_best:  96.42%, epoch time: 41.41 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 88.8267%\n",
      "layer   2  Sparsity: 88.7105%\n",
      "layer   3  Sparsity: 84.2275%\n",
      "total_backward_count 959420 real_backward_count 195037  20.329%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  1.623937/  1.767499, val:  70.00%, val_best:  77.08%, tr:  93.46%, tr_best:  96.42%, epoch time: 40.00 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8760%\n",
      "layer   2  Sparsity: 89.1348%\n",
      "layer   3  Sparsity: 84.9162%\n",
      "total_backward_count 964315 real_backward_count 195992  20.324%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  1.606115/  1.746088, val:  62.50%, val_best:  77.08%, tr:  94.48%, tr_best:  96.42%, epoch time: 40.65 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8955%\n",
      "layer   2  Sparsity: 89.1079%\n",
      "layer   3  Sparsity: 84.1597%\n",
      "total_backward_count 969210 real_backward_count 196919  20.317%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  1.583150/  1.782361, val:  58.33%, val_best:  77.08%, tr:  92.95%, tr_best:  96.42%, epoch time: 40.49 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8483%\n",
      "layer   2  Sparsity: 89.1754%\n",
      "layer   3  Sparsity: 84.2874%\n",
      "total_backward_count 974105 real_backward_count 197858  20.312%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  1.580209/  1.709627, val:  63.33%, val_best:  77.08%, tr:  94.69%, tr_best:  96.42%, epoch time: 41.03 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8678%\n",
      "layer   2  Sparsity: 88.9906%\n",
      "layer   3  Sparsity: 84.1077%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4acdb7e18c9497db740bf74e55af961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÖ‚ñÑ‚ñÜ‚ñÉ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÉ‚ñÇ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñà‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÉ‚ñÅ‚ñÇ‚ñÖ‚ñÜ‚ñÑ‚ñà‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñà‚ñÖ‚ñÇ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá</td></tr><tr><td>tr_epoch_loss</td><td>‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÇ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÖ‚ñÑ‚ñÜ‚ñÉ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.94688</td></tr><tr><td>tr_epoch_loss</td><td>1.58021</td></tr><tr><td>val_acc_best</td><td>0.77083</td></tr><tr><td>val_acc_now</td><td>0.63333</td></tr><tr><td>val_loss</td><td>1.70963</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">young-sweep-11</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/orbkz6c8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/orbkz6c8</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_061822-orbkz6c8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mo03a3fr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 12000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_083355-mo03a3fr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mo03a3fr' target=\"_blank\">youthful-sweep-13</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mo03a3fr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mo03a3fr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': '20251118_083405_080', 'my_seed': 42, 'TIME': 5, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 12000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 67f09733060e9328908e01cda0ab3532\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 592\n",
      "fc layer 1 self.abs_max_out: 437.0\n",
      "lif layer 1 self.abs_max_v: 437.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 488.0\n",
      "lif layer 1 self.abs_max_v: 626.0\n",
      "fc layer 2 self.abs_max_out: 128.0\n",
      "lif layer 2 self.abs_max_v: 128.0\n",
      "fc layer 1 self.abs_max_out: 527.0\n",
      "lif layer 1 self.abs_max_v: 666.5\n",
      "fc layer 2 self.abs_max_out: 517.0\n",
      "lif layer 2 self.abs_max_v: 532.0\n",
      "fc layer 3 self.abs_max_out: 70.0\n",
      "smallest_now_T updated: 534\n",
      "fc layer 1 self.abs_max_out: 684.0\n",
      "lif layer 1 self.abs_max_v: 684.0\n",
      "fc layer 1 self.abs_max_out: 717.0\n",
      "lif layer 1 self.abs_max_v: 972.0\n",
      "fc layer 2 self.abs_max_out: 849.0\n",
      "lif layer 2 self.abs_max_v: 891.0\n",
      "fc layer 3 self.abs_max_out: 212.0\n",
      "fc layer 1 self.abs_max_out: 733.0\n",
      "lif layer 1 self.abs_max_v: 1074.5\n",
      "lif layer 2 self.abs_max_v: 967.0\n",
      "fc layer 3 self.abs_max_out: 331.0\n",
      "fc layer 1 self.abs_max_out: 831.0\n",
      "lif layer 1 self.abs_max_v: 1364.5\n",
      "fc layer 2 self.abs_max_out: 1209.0\n",
      "lif layer 2 self.abs_max_v: 1220.0\n",
      "fc layer 3 self.abs_max_out: 375.0\n",
      "smallest_now_T updated: 407\n",
      "fc layer 1 self.abs_max_out: 894.0\n",
      "lif layer 2 self.abs_max_v: 1225.0\n",
      "fc layer 2 self.abs_max_out: 1238.0\n",
      "lif layer 2 self.abs_max_v: 1235.0\n",
      "fc layer 1 self.abs_max_out: 1135.0\n",
      "fc layer 2 self.abs_max_out: 1556.0\n",
      "lif layer 2 self.abs_max_v: 1556.0\n",
      "fc layer 3 self.abs_max_out: 448.0\n",
      "lif layer 1 self.abs_max_v: 1451.5\n",
      "fc layer 3 self.abs_max_out: 667.0\n",
      "lif layer 1 self.abs_max_v: 1577.0\n",
      "lif layer 2 self.abs_max_v: 1652.5\n",
      "fc layer 1 self.abs_max_out: 1152.0\n",
      "lif layer 2 self.abs_max_v: 1680.0\n",
      "lif layer 1 self.abs_max_v: 1621.0\n",
      "smallest_now_T updated: 345\n",
      "fc layer 1 self.abs_max_out: 1295.0\n",
      "lif layer 1 self.abs_max_v: 1670.0\n",
      "lif layer 2 self.abs_max_v: 1728.5\n",
      "fc layer 1 self.abs_max_out: 1509.0\n",
      "lif layer 2 self.abs_max_v: 1885.5\n",
      "fc layer 1 self.abs_max_out: 1514.0\n",
      "lif layer 1 self.abs_max_v: 1736.5\n",
      "lif layer 2 self.abs_max_v: 2290.0\n",
      "lif layer 1 self.abs_max_v: 1748.0\n",
      "lif layer 1 self.abs_max_v: 1764.0\n",
      "smallest_now_T updated: 317\n",
      "lif layer 1 self.abs_max_v: 1856.0\n",
      "fc layer 2 self.abs_max_out: 1688.0\n",
      "lif layer 2 self.abs_max_v: 2410.5\n",
      "fc layer 1 self.abs_max_out: 1576.0\n",
      "fc layer 2 self.abs_max_out: 1694.0\n",
      "fc layer 1 self.abs_max_out: 1653.0\n",
      "fc layer 1 self.abs_max_out: 1757.0\n",
      "lif layer 1 self.abs_max_v: 1863.0\n",
      "smallest_now_T updated: 286\n",
      "fc layer 3 self.abs_max_out: 730.0\n",
      "lif layer 1 self.abs_max_v: 1974.5\n",
      "fc layer 2 self.abs_max_out: 1763.0\n",
      "lif layer 2 self.abs_max_v: 2638.0\n",
      "fc layer 1 self.abs_max_out: 1834.0\n",
      "fc layer 1 self.abs_max_out: 2021.0\n",
      "lif layer 1 self.abs_max_v: 2021.0\n",
      "fc layer 2 self.abs_max_out: 1928.0\n",
      "fc layer 1 self.abs_max_out: 2050.0\n",
      "lif layer 1 self.abs_max_v: 2050.0\n",
      "fc layer 1 self.abs_max_out: 2290.0\n",
      "lif layer 1 self.abs_max_v: 2290.0\n",
      "fc layer 2 self.abs_max_out: 1970.0\n",
      "fc layer 2 self.abs_max_out: 2129.0\n",
      "fc layer 3 self.abs_max_out: 873.0\n",
      "lif layer 2 self.abs_max_v: 2894.0\n",
      "fc layer 1 self.abs_max_out: 2890.0\n",
      "lif layer 1 self.abs_max_v: 2890.0\n",
      "fc layer 1 self.abs_max_out: 3011.0\n",
      "lif layer 1 self.abs_max_v: 3011.0\n",
      "fc layer 2 self.abs_max_out: 2164.0\n",
      "fc layer 2 self.abs_max_out: 2195.0\n",
      "fc layer 2 self.abs_max_out: 2219.0\n",
      "lif layer 2 self.abs_max_v: 2908.0\n",
      "fc layer 1 self.abs_max_out: 3224.0\n",
      "lif layer 1 self.abs_max_v: 3224.0\n",
      "fc layer 2 self.abs_max_out: 2332.0\n",
      "fc layer 2 self.abs_max_out: 2445.0\n",
      "smallest_now_T updated: 247\n",
      "smallest_now_T updated: 192\n",
      "lif layer 2 self.abs_max_v: 3368.5\n",
      "fc layer 2 self.abs_max_out: 2465.0\n",
      "fc layer 2 self.abs_max_out: 2484.0\n",
      "fc layer 2 self.abs_max_out: 2582.0\n",
      "lif layer 2 self.abs_max_v: 3540.0\n",
      "fc layer 2 self.abs_max_out: 2611.0\n",
      "fc layer 2 self.abs_max_out: 2658.0\n",
      "lif layer 2 self.abs_max_v: 3662.5\n",
      "lif layer 2 self.abs_max_v: 3727.5\n",
      "lif layer 2 self.abs_max_v: 4190.0\n",
      "fc layer 2 self.abs_max_out: 2728.0\n",
      "fc layer 2 self.abs_max_out: 2757.0\n",
      "fc layer 2 self.abs_max_out: 2929.0\n",
      "fc layer 2 self.abs_max_out: 3049.0\n",
      "fc layer 2 self.abs_max_out: 3138.0\n",
      "fc layer 2 self.abs_max_out: 3305.0\n",
      "fc layer 1 self.abs_max_out: 3361.0\n",
      "lif layer 1 self.abs_max_v: 3361.0\n",
      "fc layer 1 self.abs_max_out: 3374.0\n",
      "lif layer 1 self.abs_max_v: 3374.0\n",
      "fc layer 1 self.abs_max_out: 3787.0\n",
      "lif layer 1 self.abs_max_v: 3787.0\n",
      "fc layer 3 self.abs_max_out: 908.0\n",
      "fc layer 3 self.abs_max_out: 940.0\n",
      "fc layer 1 self.abs_max_out: 4017.0\n",
      "lif layer 1 self.abs_max_v: 4017.0\n",
      "lif layer 2 self.abs_max_v: 4252.0\n",
      "fc layer 2 self.abs_max_out: 3365.0\n",
      "smallest_now_T_val updated: 552\n",
      "smallest_now_T_val updated: 456\n",
      "smallest_now_T_val updated: 448\n",
      "smallest_now_T_val updated: 440\n",
      "fc layer 1 self.abs_max_out: 4143.0\n",
      "lif layer 1 self.abs_max_v: 4143.0\n",
      "smallest_now_T_val updated: 368\n",
      "fc layer 2 self.abs_max_out: 3507.0\n",
      "smallest_now_T_val updated: 137\n",
      "fc layer 3 self.abs_max_out: 958.0\n",
      "lif layer 2 self.abs_max_v: 4361.5\n",
      "lif layer 2 self.abs_max_v: 4567.5\n",
      "fc layer 1 self.abs_max_out: 4182.0\n",
      "lif layer 1 self.abs_max_v: 4182.0\n",
      "fc layer 1 self.abs_max_out: 4705.0\n",
      "lif layer 1 self.abs_max_v: 4705.0\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  2.139594/  2.125220, val:  32.08%, val_best:  32.08%, tr:  46.58%, tr_best:  46.58%, epoch time: 40.92 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0699%\n",
      "layer   2  Sparsity: 83.7026%\n",
      "layer   3  Sparsity: 83.6042%\n",
      "total_backward_count 4895 real_backward_count 3124  63.820%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 3593.0\n",
      "fc layer 3 self.abs_max_out: 974.0\n",
      "fc layer 3 self.abs_max_out: 1027.0\n",
      "fc layer 2 self.abs_max_out: 3599.0\n",
      "fc layer 2 self.abs_max_out: 3810.0\n",
      "fc layer 1 self.abs_max_out: 5378.0\n",
      "lif layer 1 self.abs_max_v: 5378.0\n",
      "fc layer 2 self.abs_max_out: 3836.0\n",
      "lif layer 2 self.abs_max_v: 4568.0\n",
      "lif layer 2 self.abs_max_v: 4914.0\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  2.081826/  2.126623, val:  42.50%, val_best:  42.50%, tr:  63.33%, tr_best:  63.33%, epoch time: 40.60 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0836%\n",
      "layer   2  Sparsity: 80.0945%\n",
      "layer   3  Sparsity: 80.4797%\n",
      "total_backward_count 9790 real_backward_count 5642  57.630%\n",
      "fc layer 1 self.abs_max_out: 5616.0\n",
      "lif layer 1 self.abs_max_v: 5616.0\n",
      "lif layer 2 self.abs_max_v: 5077.0\n",
      "fc layer 2 self.abs_max_out: 3840.0\n",
      "fc layer 1 self.abs_max_out: 5655.0\n",
      "lif layer 1 self.abs_max_v: 5655.0\n",
      "fc layer 2 self.abs_max_out: 4045.0\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  2.085284/  2.137821, val:  42.08%, val_best:  42.50%, tr:  69.97%, tr_best:  69.97%, epoch time: 40.81 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0968%\n",
      "layer   2  Sparsity: 79.7471%\n",
      "layer   3  Sparsity: 79.8802%\n",
      "total_backward_count 14685 real_backward_count 7881  53.667%\n",
      "fc layer 1 self.abs_max_out: 5700.0\n",
      "lif layer 1 self.abs_max_v: 5700.0\n",
      "lif layer 2 self.abs_max_v: 5348.0\n",
      "lif layer 2 self.abs_max_v: 5521.0\n",
      "fc layer 1 self.abs_max_out: 5984.0\n",
      "lif layer 1 self.abs_max_v: 5984.0\n",
      "fc layer 2 self.abs_max_out: 4225.0\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  2.092079/  2.130752, val:  47.08%, val_best:  47.08%, tr:  73.14%, tr_best:  73.14%, epoch time: 40.87 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0795%\n",
      "layer   2  Sparsity: 79.1298%\n",
      "layer   3  Sparsity: 78.3728%\n",
      "total_backward_count 19580 real_backward_count 10015  51.149%\n",
      "fc layer 1 self.abs_max_out: 6183.0\n",
      "lif layer 1 self.abs_max_v: 6183.0\n",
      "lif layer 2 self.abs_max_v: 5674.5\n",
      "fc layer 1 self.abs_max_out: 6202.0\n",
      "lif layer 1 self.abs_max_v: 6202.0\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  2.097911/  2.128279, val:  44.17%, val_best:  47.08%, tr:  75.28%, tr_best:  75.28%, epoch time: 40.24 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0828%\n",
      "layer   2  Sparsity: 78.7090%\n",
      "layer   3  Sparsity: 77.8204%\n",
      "total_backward_count 24475 real_backward_count 12019  49.107%\n",
      "lif layer 2 self.abs_max_v: 6200.0\n",
      "fc layer 1 self.abs_max_out: 6443.0\n",
      "lif layer 1 self.abs_max_v: 6443.0\n",
      "fc layer 2 self.abs_max_out: 4438.0\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  2.100290/  2.143527, val:  47.92%, val_best:  47.92%, tr:  77.63%, tr_best:  77.63%, epoch time: 40.47 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0688%\n",
      "layer   2  Sparsity: 77.9290%\n",
      "layer   3  Sparsity: 77.0530%\n",
      "total_backward_count 29370 real_backward_count 13860  47.191%\n",
      "lif layer 2 self.abs_max_v: 6694.0\n",
      "fc layer 1 self.abs_max_out: 6512.0\n",
      "lif layer 1 self.abs_max_v: 6512.0\n",
      "fc layer 1 self.abs_max_out: 6565.0\n",
      "lif layer 1 self.abs_max_v: 6565.0\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  2.112118/  2.159098, val:  46.67%, val_best:  47.92%, tr:  78.55%, tr_best:  78.55%, epoch time: 40.50 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0980%\n",
      "layer   2  Sparsity: 78.1039%\n",
      "layer   3  Sparsity: 77.0352%\n",
      "total_backward_count 34265 real_backward_count 15695  45.805%\n",
      "fc layer 1 self.abs_max_out: 6663.0\n",
      "lif layer 1 self.abs_max_v: 6663.0\n",
      "fc layer 1 self.abs_max_out: 6667.0\n",
      "lif layer 1 self.abs_max_v: 6667.0\n",
      "fc layer 1 self.abs_max_out: 7007.0\n",
      "lif layer 1 self.abs_max_v: 7007.0\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  2.118473/  2.159067, val:  43.75%, val_best:  47.92%, tr:  81.31%, tr_best:  81.31%, epoch time: 40.68 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0853%\n",
      "layer   2  Sparsity: 77.6376%\n",
      "layer   3  Sparsity: 76.9123%\n",
      "total_backward_count 39160 real_backward_count 17432  44.515%\n",
      "fc layer 1 self.abs_max_out: 7404.0\n",
      "lif layer 1 self.abs_max_v: 7404.0\n",
      "fc layer 2 self.abs_max_out: 4468.0\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  2.129045/  2.156359, val:  42.08%, val_best:  47.92%, tr:  79.26%, tr_best:  81.31%, epoch time: 40.58 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0807%\n",
      "layer   2  Sparsity: 77.8469%\n",
      "layer   3  Sparsity: 76.4508%\n",
      "total_backward_count 44055 real_backward_count 19237  43.666%\n",
      "fc layer 2 self.abs_max_out: 4469.0\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  2.126653/  2.178281, val:  42.50%, val_best:  47.92%, tr:  84.58%, tr_best:  84.58%, epoch time: 40.26 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0659%\n",
      "layer   2  Sparsity: 77.3965%\n",
      "layer   3  Sparsity: 75.8663%\n",
      "total_backward_count 48950 real_backward_count 20890  42.676%\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  2.131570/  2.175884, val:  40.42%, val_best:  47.92%, tr:  84.47%, tr_best:  84.58%, epoch time: 40.54 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0488%\n",
      "layer   2  Sparsity: 77.3701%\n",
      "layer   3  Sparsity: 75.5470%\n",
      "total_backward_count 53845 real_backward_count 22502  41.790%\n",
      "fc layer 1 self.abs_max_out: 7901.0\n",
      "lif layer 1 self.abs_max_v: 7901.0\n",
      "lif layer 2 self.abs_max_v: 6947.0\n",
      "lif layer 2 self.abs_max_v: 7213.0\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  2.136904/  2.184192, val:  45.00%, val_best:  47.92%, tr:  84.37%, tr_best:  84.58%, epoch time: 40.94 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0959%\n",
      "layer   2  Sparsity: 77.1025%\n",
      "layer   3  Sparsity: 75.6511%\n",
      "total_backward_count 58740 real_backward_count 24148  41.110%\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  2.138783/  2.176416, val:  40.42%, val_best:  47.92%, tr:  84.58%, tr_best:  84.58%, epoch time: 40.98 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0593%\n",
      "layer   2  Sparsity: 76.8906%\n",
      "layer   3  Sparsity: 75.5163%\n",
      "total_backward_count 63635 real_backward_count 25712  40.405%\n",
      "fc layer 1 self.abs_max_out: 8182.0\n",
      "lif layer 1 self.abs_max_v: 8182.0\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  2.132781/  2.174170, val:  40.83%, val_best:  47.92%, tr:  85.60%, tr_best:  85.60%, epoch time: 41.21 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0778%\n",
      "layer   2  Sparsity: 77.1067%\n",
      "layer   3  Sparsity: 75.0801%\n",
      "total_backward_count 68530 real_backward_count 27275  39.800%\n",
      "fc layer 1 self.abs_max_out: 8303.0\n",
      "lif layer 1 self.abs_max_v: 8303.0\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  2.137378/  2.177222, val:  38.33%, val_best:  47.92%, tr:  85.29%, tr_best:  85.60%, epoch time: 40.69 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0908%\n",
      "layer   2  Sparsity: 77.3772%\n",
      "layer   3  Sparsity: 74.9541%\n",
      "total_backward_count 73425 real_backward_count 28786  39.205%\n",
      "fc layer 2 self.abs_max_out: 4626.0\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  2.135235/  2.184298, val:  37.50%, val_best:  47.92%, tr:  84.78%, tr_best:  85.60%, epoch time: 41.25 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0742%\n",
      "layer   2  Sparsity: 76.9545%\n",
      "layer   3  Sparsity: 74.2466%\n",
      "total_backward_count 78320 real_backward_count 30362  38.767%\n",
      "fc layer 1 self.abs_max_out: 8438.0\n",
      "lif layer 1 self.abs_max_v: 8438.0\n",
      "lif layer 2 self.abs_max_v: 7511.5\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  2.138352/  2.182293, val:  45.83%, val_best:  47.92%, tr:  86.01%, tr_best:  86.01%, epoch time: 40.15 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0874%\n",
      "layer   2  Sparsity: 76.9710%\n",
      "layer   3  Sparsity: 74.9824%\n",
      "total_backward_count 83215 real_backward_count 31897  38.331%\n",
      "fc layer 2 self.abs_max_out: 4844.0\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  2.143412/  2.182524, val:  49.17%, val_best:  49.17%, tr:  87.64%, tr_best:  87.64%, epoch time: 41.10 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0834%\n",
      "layer   2  Sparsity: 76.9958%\n",
      "layer   3  Sparsity: 75.0799%\n",
      "total_backward_count 88110 real_backward_count 33398  37.905%\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  2.145024/  2.178507, val:  48.75%, val_best:  49.17%, tr:  86.93%, tr_best:  87.64%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0816%\n",
      "layer   2  Sparsity: 76.9631%\n",
      "layer   3  Sparsity: 74.4668%\n",
      "total_backward_count 93005 real_backward_count 34926  37.553%\n",
      "fc layer 2 self.abs_max_out: 4860.0\n",
      "fc layer 2 self.abs_max_out: 5167.0\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  2.136645/  2.176630, val:  37.50%, val_best:  49.17%, tr:  86.11%, tr_best:  87.64%, epoch time: 40.65 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0590%\n",
      "layer   2  Sparsity: 77.0022%\n",
      "layer   3  Sparsity: 73.9139%\n",
      "total_backward_count 97900 real_backward_count 36399  37.180%\n",
      "fc layer 1 self.abs_max_out: 8642.0\n",
      "lif layer 1 self.abs_max_v: 8642.0\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  2.139197/  2.187643, val:  47.50%, val_best:  49.17%, tr:  85.19%, tr_best:  87.64%, epoch time: 41.13 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0833%\n",
      "layer   2  Sparsity: 76.8717%\n",
      "layer   3  Sparsity: 74.3449%\n",
      "total_backward_count 102795 real_backward_count 37935  36.904%\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  2.146990/  2.188797, val:  41.67%, val_best:  49.17%, tr:  83.45%, tr_best:  87.64%, epoch time: 40.99 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0888%\n",
      "layer   2  Sparsity: 76.7675%\n",
      "layer   3  Sparsity: 74.7530%\n",
      "total_backward_count 107690 real_backward_count 39493  36.673%\n",
      "lif layer 2 self.abs_max_v: 7574.5\n",
      "lif layer 2 self.abs_max_v: 7654.0\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  2.147598/  2.184290, val:  52.50%, val_best:  52.50%, tr:  87.84%, tr_best:  87.84%, epoch time: 40.70 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0977%\n",
      "layer   2  Sparsity: 76.5790%\n",
      "layer   3  Sparsity: 74.8758%\n",
      "total_backward_count 112585 real_backward_count 40941  36.365%\n",
      "fc layer 1 self.abs_max_out: 8788.0\n",
      "lif layer 1 self.abs_max_v: 8788.0\n",
      "lif layer 2 self.abs_max_v: 7789.5\n",
      "lif layer 2 self.abs_max_v: 8071.0\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  2.149291/  2.188465, val:  49.17%, val_best:  52.50%, tr:  86.82%, tr_best:  87.84%, epoch time: 41.28 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0767%\n",
      "layer   2  Sparsity: 76.8008%\n",
      "layer   3  Sparsity: 74.8537%\n",
      "total_backward_count 117480 real_backward_count 42383  36.077%\n",
      "fc layer 1 self.abs_max_out: 9117.0\n",
      "lif layer 1 self.abs_max_v: 9117.0\n",
      "lif layer 2 self.abs_max_v: 8237.5\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  2.151403/  2.198293, val:  44.17%, val_best:  52.50%, tr:  87.84%, tr_best:  87.84%, epoch time: 40.81 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0984%\n",
      "layer   2  Sparsity: 76.5748%\n",
      "layer   3  Sparsity: 74.2495%\n",
      "total_backward_count 122375 real_backward_count 43833  35.819%\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  2.155407/  2.194589, val:  45.42%, val_best:  52.50%, tr:  87.23%, tr_best:  87.84%, epoch time: 40.78 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0944%\n",
      "layer   2  Sparsity: 76.3978%\n",
      "layer   3  Sparsity: 74.3613%\n",
      "total_backward_count 127270 real_backward_count 45312  35.603%\n",
      "lif layer 2 self.abs_max_v: 8329.5\n",
      "fc layer 1 self.abs_max_out: 9281.0\n",
      "lif layer 1 self.abs_max_v: 9281.0\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  2.149938/  2.175234, val:  51.25%, val_best:  52.50%, tr:  87.54%, tr_best:  87.84%, epoch time: 40.88 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0747%\n",
      "layer   2  Sparsity: 76.2221%\n",
      "layer   3  Sparsity: 74.4586%\n",
      "total_backward_count 132165 real_backward_count 46763  35.382%\n",
      "lif layer 2 self.abs_max_v: 8371.0\n",
      "lif layer 2 self.abs_max_v: 8510.0\n",
      "lif layer 1 self.abs_max_v: 9456.0\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  2.146319/  2.190612, val:  56.67%, val_best:  56.67%, tr:  88.25%, tr_best:  88.25%, epoch time: 40.56 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0820%\n",
      "layer   2  Sparsity: 76.2640%\n",
      "layer   3  Sparsity: 74.2002%\n",
      "total_backward_count 137060 real_backward_count 48243  35.198%\n",
      "lif layer 2 self.abs_max_v: 9016.0\n",
      "lif layer 1 self.abs_max_v: 9729.0\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  2.151484/  2.182894, val:  47.92%, val_best:  56.67%, tr:  88.05%, tr_best:  88.25%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0858%\n",
      "layer   2  Sparsity: 76.1326%\n",
      "layer   3  Sparsity: 74.3337%\n",
      "total_backward_count 141955 real_backward_count 49652  34.977%\n",
      "fc layer 1 self.abs_max_out: 9499.0\n",
      "fc layer 2 self.abs_max_out: 5187.0\n",
      "lif layer 1 self.abs_max_v: 9816.0\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  2.144789/  2.178993, val:  37.08%, val_best:  56.67%, tr:  88.15%, tr_best:  88.25%, epoch time: 40.49 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0911%\n",
      "layer   2  Sparsity: 76.1377%\n",
      "layer   3  Sparsity: 74.5410%\n",
      "total_backward_count 146850 real_backward_count 51056  34.767%\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  2.145769/  2.177839, val:  44.17%, val_best:  56.67%, tr:  87.84%, tr_best:  88.25%, epoch time: 40.61 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0907%\n",
      "layer   2  Sparsity: 76.1743%\n",
      "layer   3  Sparsity: 74.0378%\n",
      "total_backward_count 151745 real_backward_count 52493  34.593%\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  2.145182/  2.185890, val:  47.92%, val_best:  56.67%, tr:  88.36%, tr_best:  88.36%, epoch time: 40.81 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.1021%\n",
      "layer   2  Sparsity: 76.0776%\n",
      "layer   3  Sparsity: 73.4641%\n",
      "total_backward_count 156640 real_backward_count 53961  34.449%\n",
      "lif layer 1 self.abs_max_v: 10007.5\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  2.151134/  2.196419, val:  43.33%, val_best:  56.67%, tr:  88.15%, tr_best:  88.36%, epoch time: 40.55 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0489%\n",
      "layer   2  Sparsity: 76.0270%\n",
      "layer   3  Sparsity: 73.3682%\n",
      "total_backward_count 161535 real_backward_count 55334  34.255%\n",
      "lif layer 1 self.abs_max_v: 10208.5\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  2.153621/  2.191263, val:  45.83%, val_best:  56.67%, tr:  88.97%, tr_best:  88.97%, epoch time: 40.73 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0674%\n",
      "layer   2  Sparsity: 76.1160%\n",
      "layer   3  Sparsity: 74.2471%\n",
      "total_backward_count 166430 real_backward_count 56746  34.096%\n",
      "lif layer 1 self.abs_max_v: 10342.5\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  2.148889/  2.185568, val:  44.58%, val_best:  56.67%, tr:  87.44%, tr_best:  88.97%, epoch time: 40.89 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0414%\n",
      "layer   2  Sparsity: 75.9107%\n",
      "layer   3  Sparsity: 74.8159%\n",
      "total_backward_count 171325 real_backward_count 58111  33.919%\n",
      "fc layer 1 self.abs_max_out: 9618.0\n",
      "lif layer 1 self.abs_max_v: 10623.5\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  2.150318/  2.194224, val:  55.83%, val_best:  56.67%, tr:  88.46%, tr_best:  88.97%, epoch time: 40.68 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0912%\n",
      "layer   2  Sparsity: 76.4719%\n",
      "layer   3  Sparsity: 74.9543%\n",
      "total_backward_count 176220 real_backward_count 59543  33.789%\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  2.149350/  2.182241, val:  57.08%, val_best:  57.08%, tr:  87.33%, tr_best:  88.97%, epoch time: 40.87 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0981%\n",
      "layer   2  Sparsity: 76.4139%\n",
      "layer   3  Sparsity: 74.4674%\n",
      "total_backward_count 181115 real_backward_count 60962  33.659%\n",
      "fc layer 2 self.abs_max_out: 5304.0\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  2.147172/  2.195606, val:  41.67%, val_best:  57.08%, tr:  89.79%, tr_best:  89.79%, epoch time: 40.81 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0625%\n",
      "layer   2  Sparsity: 75.9523%\n",
      "layer   3  Sparsity: 74.4483%\n",
      "total_backward_count 186010 real_backward_count 62306  33.496%\n",
      "fc layer 1 self.abs_max_out: 9766.0\n",
      "lif layer 1 self.abs_max_v: 10625.0\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  2.147224/  2.183855, val:  56.25%, val_best:  57.08%, tr:  89.68%, tr_best:  89.79%, epoch time: 40.54 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0904%\n",
      "layer   2  Sparsity: 76.0668%\n",
      "layer   3  Sparsity: 74.1170%\n",
      "total_backward_count 190905 real_backward_count 63692  33.363%\n",
      "fc layer 2 self.abs_max_out: 5305.0\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  2.150752/  2.188199, val:  45.83%, val_best:  57.08%, tr:  89.27%, tr_best:  89.79%, epoch time: 40.08 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.1028%\n",
      "layer   2  Sparsity: 75.9628%\n",
      "layer   3  Sparsity: 74.1365%\n",
      "total_backward_count 195800 real_backward_count 65065  33.230%\n",
      "fc layer 1 self.abs_max_out: 9775.0\n",
      "fc layer 2 self.abs_max_out: 5311.0\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  2.146586/  2.183286, val:  49.17%, val_best:  57.08%, tr:  89.27%, tr_best:  89.79%, epoch time: 40.40 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0416%\n",
      "layer   2  Sparsity: 75.6388%\n",
      "layer   3  Sparsity: 73.4154%\n",
      "total_backward_count 200695 real_backward_count 66402  33.086%\n",
      "fc layer 2 self.abs_max_out: 5364.0\n",
      "fc layer 1 self.abs_max_out: 9811.0\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  2.148100/  2.185105, val:  47.92%, val_best:  57.08%, tr:  88.76%, tr_best:  89.79%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0826%\n",
      "layer   2  Sparsity: 75.6788%\n",
      "layer   3  Sparsity: 73.7194%\n",
      "total_backward_count 205590 real_backward_count 67772  32.965%\n",
      "fc layer 2 self.abs_max_out: 5545.0\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  2.141159/  2.187245, val:  53.75%, val_best:  57.08%, tr:  88.05%, tr_best:  89.79%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0423%\n",
      "layer   2  Sparsity: 76.0789%\n",
      "layer   3  Sparsity: 74.3481%\n",
      "total_backward_count 210485 real_backward_count 69117  32.837%\n",
      "lif layer 1 self.abs_max_v: 10677.5\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  2.147549/  2.182510, val:  60.83%, val_best:  60.83%, tr:  90.50%, tr_best:  90.50%, epoch time: 40.23 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0695%\n",
      "layer   2  Sparsity: 76.0281%\n",
      "layer   3  Sparsity: 74.5708%\n",
      "total_backward_count 215380 real_backward_count 70438  32.704%\n",
      "lif layer 1 self.abs_max_v: 10911.0\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  2.150599/  2.191623, val:  48.75%, val_best:  60.83%, tr:  89.07%, tr_best:  90.50%, epoch time: 40.65 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0868%\n",
      "layer   2  Sparsity: 75.6742%\n",
      "layer   3  Sparsity: 74.6193%\n",
      "total_backward_count 220275 real_backward_count 71825  32.607%\n",
      "fc layer 1 self.abs_max_out: 9872.0\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  2.155422/  2.198300, val:  40.42%, val_best:  60.83%, tr:  89.89%, tr_best:  90.50%, epoch time: 40.46 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0759%\n",
      "layer   2  Sparsity: 75.7775%\n",
      "layer   3  Sparsity: 74.4952%\n",
      "total_backward_count 225170 real_backward_count 73160  32.491%\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  2.156163/  2.186437, val:  53.33%, val_best:  60.83%, tr:  91.22%, tr_best:  91.22%, epoch time: 40.93 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0823%\n",
      "layer   2  Sparsity: 75.4624%\n",
      "layer   3  Sparsity: 74.6713%\n",
      "total_backward_count 230065 real_backward_count 74497  32.381%\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  2.153912/  2.189855, val:  45.42%, val_best:  60.83%, tr:  90.30%, tr_best:  91.22%, epoch time: 40.42 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.1056%\n",
      "layer   2  Sparsity: 75.4070%\n",
      "layer   3  Sparsity: 75.1063%\n",
      "total_backward_count 234960 real_backward_count 75816  32.268%\n",
      "fc layer 1 self.abs_max_out: 9936.0\n",
      "fc layer 1 self.abs_max_out: 10357.0\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  2.150746/  2.193888, val:  44.58%, val_best:  60.83%, tr:  89.58%, tr_best:  91.22%, epoch time: 40.55 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0906%\n",
      "layer   2  Sparsity: 75.5712%\n",
      "layer   3  Sparsity: 74.4703%\n",
      "total_backward_count 239855 real_backward_count 77150  32.165%\n",
      "fc layer 2 self.abs_max_out: 5577.0\n",
      "lif layer 1 self.abs_max_v: 10969.5\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  2.157348/  2.188907, val:  44.58%, val_best:  60.83%, tr:  90.91%, tr_best:  91.22%, epoch time: 40.16 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0623%\n",
      "layer   2  Sparsity: 75.2102%\n",
      "layer   3  Sparsity: 74.8372%\n",
      "total_backward_count 244750 real_backward_count 78557  32.097%\n",
      "lif layer 1 self.abs_max_v: 10994.0\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  2.156892/  2.196411, val:  38.75%, val_best:  60.83%, tr:  89.79%, tr_best:  91.22%, epoch time: 40.99 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0599%\n",
      "layer   2  Sparsity: 75.4263%\n",
      "layer   3  Sparsity: 74.6380%\n",
      "total_backward_count 249645 real_backward_count 79904  32.007%\n",
      "lif layer 1 self.abs_max_v: 11134.0\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  2.156816/  2.190055, val:  48.33%, val_best:  60.83%, tr:  89.79%, tr_best:  91.22%, epoch time: 40.91 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0512%\n",
      "layer   2  Sparsity: 75.3827%\n",
      "layer   3  Sparsity: 74.9257%\n",
      "total_backward_count 254540 real_backward_count 81234  31.914%\n",
      "fc layer 2 self.abs_max_out: 5585.0\n",
      "lif layer 1 self.abs_max_v: 11181.5\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  2.159876/  2.195708, val:  62.08%, val_best:  62.08%, tr:  88.87%, tr_best:  91.22%, epoch time: 40.82 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0880%\n",
      "layer   2  Sparsity: 75.4280%\n",
      "layer   3  Sparsity: 74.2144%\n",
      "total_backward_count 259435 real_backward_count 82635  31.852%\n",
      "lif layer 1 self.abs_max_v: 11196.5\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  2.157286/  2.190722, val:  48.33%, val_best:  62.08%, tr:  89.38%, tr_best:  91.22%, epoch time: 41.29 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0749%\n",
      "layer   2  Sparsity: 75.2759%\n",
      "layer   3  Sparsity: 74.3921%\n",
      "total_backward_count 264330 real_backward_count 84030  31.790%\n",
      "fc layer 2 self.abs_max_out: 5601.0\n",
      "fc layer 2 self.abs_max_out: 5641.0\n",
      "lif layer 1 self.abs_max_v: 11314.5\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  2.152659/  2.185756, val:  58.75%, val_best:  62.08%, tr:  89.58%, tr_best:  91.22%, epoch time: 40.95 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0784%\n",
      "layer   2  Sparsity: 75.1953%\n",
      "layer   3  Sparsity: 73.8075%\n",
      "total_backward_count 269225 real_backward_count 85393  31.718%\n",
      "fc layer 2 self.abs_max_out: 5705.0\n",
      "fc layer 2 self.abs_max_out: 5760.0\n",
      "lif layer 1 self.abs_max_v: 11478.0\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  2.150985/  2.198113, val:  55.00%, val_best:  62.08%, tr:  90.30%, tr_best:  91.22%, epoch time: 40.57 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0955%\n",
      "layer   2  Sparsity: 75.2386%\n",
      "layer   3  Sparsity: 74.0518%\n",
      "total_backward_count 274120 real_backward_count 86724  31.637%\n",
      "fc layer 2 self.abs_max_out: 5786.0\n",
      "lif layer 1 self.abs_max_v: 11545.5\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  2.153839/  2.185035, val:  45.00%, val_best:  62.08%, tr:  89.48%, tr_best:  91.22%, epoch time: 41.06 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0696%\n",
      "layer   2  Sparsity: 74.8742%\n",
      "layer   3  Sparsity: 74.2920%\n",
      "total_backward_count 279015 real_backward_count 88040  31.554%\n",
      "fc layer 2 self.abs_max_out: 5837.0\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  2.152837/  2.182734, val:  55.42%, val_best:  62.08%, tr:  91.73%, tr_best:  91.73%, epoch time: 40.30 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.1177%\n",
      "layer   2  Sparsity: 75.3320%\n",
      "layer   3  Sparsity: 74.3987%\n",
      "total_backward_count 283910 real_backward_count 89347  31.470%\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  2.147432/  2.184300, val:  54.58%, val_best:  62.08%, tr:  90.50%, tr_best:  91.73%, epoch time: 41.28 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0912%\n",
      "layer   2  Sparsity: 75.2143%\n",
      "layer   3  Sparsity: 74.0444%\n",
      "total_backward_count 288805 real_backward_count 90648  31.387%\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  2.147889/  2.193902, val:  45.83%, val_best:  62.08%, tr:  90.60%, tr_best:  91.73%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0979%\n",
      "layer   2  Sparsity: 75.2696%\n",
      "layer   3  Sparsity: 74.4228%\n",
      "total_backward_count 293700 real_backward_count 91950  31.307%\n",
      "fc layer 1 self.abs_max_out: 10509.0\n",
      "fc layer 1 self.abs_max_out: 10857.0\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  2.148235/  2.188895, val:  43.75%, val_best:  62.08%, tr:  90.70%, tr_best:  91.73%, epoch time: 41.14 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0701%\n",
      "layer   2  Sparsity: 75.0049%\n",
      "layer   3  Sparsity: 74.6860%\n",
      "total_backward_count 298595 real_backward_count 93303  31.247%\n",
      "lif layer 1 self.abs_max_v: 11604.5\n",
      "lif layer 1 self.abs_max_v: 11857.0\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  2.143064/  2.180669, val:  50.83%, val_best:  62.08%, tr:  92.13%, tr_best:  92.13%, epoch time: 40.39 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0457%\n",
      "layer   2  Sparsity: 74.8775%\n",
      "layer   3  Sparsity: 74.6486%\n",
      "total_backward_count 303490 real_backward_count 94584  31.165%\n",
      "lif layer 1 self.abs_max_v: 12103.0\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  2.143048/  2.185775, val:  55.00%, val_best:  62.08%, tr:  91.22%, tr_best:  92.13%, epoch time: 40.97 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0803%\n",
      "layer   2  Sparsity: 75.2533%\n",
      "layer   3  Sparsity: 74.7078%\n",
      "total_backward_count 308385 real_backward_count 95876  31.090%\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  2.150664/  2.189459, val:  47.50%, val_best:  62.08%, tr:  90.30%, tr_best:  92.13%, epoch time: 40.49 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.1007%\n",
      "layer   2  Sparsity: 75.4753%\n",
      "layer   3  Sparsity: 75.2959%\n",
      "total_backward_count 313280 real_backward_count 97152  31.011%\n",
      "lif layer 1 self.abs_max_v: 12327.5\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  2.154829/  2.187264, val:  51.25%, val_best:  62.08%, tr:  88.97%, tr_best:  92.13%, epoch time: 40.30 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0890%\n",
      "layer   2  Sparsity: 75.2074%\n",
      "layer   3  Sparsity: 75.0709%\n",
      "total_backward_count 318175 real_backward_count 98501  30.958%\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  2.147936/  2.183302, val:  56.25%, val_best:  62.08%, tr:  92.34%, tr_best:  92.34%, epoch time: 40.29 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.1084%\n",
      "layer   2  Sparsity: 75.3575%\n",
      "layer   3  Sparsity: 74.8896%\n",
      "total_backward_count 323070 real_backward_count 99726  30.868%\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  2.152984/  2.190108, val:  60.00%, val_best:  62.08%, tr:  91.62%, tr_best:  92.34%, epoch time: 40.56 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0939%\n",
      "layer   2  Sparsity: 75.3110%\n",
      "layer   3  Sparsity: 74.8909%\n",
      "total_backward_count 327965 real_backward_count 100978  30.789%\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  2.153722/  2.192738, val:  60.83%, val_best:  62.08%, tr:  90.09%, tr_best:  92.34%, epoch time: 40.22 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0716%\n",
      "layer   2  Sparsity: 75.0100%\n",
      "layer   3  Sparsity: 74.6270%\n",
      "total_backward_count 332860 real_backward_count 102261  30.722%\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  2.152904/  2.181172, val:  57.92%, val_best:  62.08%, tr:  90.19%, tr_best:  92.34%, epoch time: 40.52 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0811%\n",
      "layer   2  Sparsity: 75.3089%\n",
      "layer   3  Sparsity: 74.8618%\n",
      "total_backward_count 337755 real_backward_count 103521  30.650%\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  2.157597/  2.195617, val:  51.67%, val_best:  62.08%, tr:  89.89%, tr_best:  92.34%, epoch time: 40.52 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0668%\n",
      "layer   2  Sparsity: 75.2299%\n",
      "layer   3  Sparsity: 75.4304%\n",
      "total_backward_count 342650 real_backward_count 104810  30.588%\n",
      "fc layer 2 self.abs_max_out: 5853.0\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  2.156380/  2.190697, val:  53.33%, val_best:  62.08%, tr:  91.73%, tr_best:  92.34%, epoch time: 41.21 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0675%\n",
      "layer   2  Sparsity: 75.1030%\n",
      "layer   3  Sparsity: 74.9230%\n",
      "total_backward_count 347545 real_backward_count 106094  30.527%\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  2.157453/  2.192303, val:  38.33%, val_best:  62.08%, tr:  90.81%, tr_best:  92.34%, epoch time: 40.75 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0661%\n",
      "layer   2  Sparsity: 74.9887%\n",
      "layer   3  Sparsity: 74.9717%\n",
      "total_backward_count 352440 real_backward_count 107341  30.457%\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  2.153881/  2.187848, val:  61.25%, val_best:  62.08%, tr:  89.79%, tr_best:  92.34%, epoch time: 41.20 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0346%\n",
      "layer   2  Sparsity: 74.9537%\n",
      "layer   3  Sparsity: 74.8932%\n",
      "total_backward_count 357335 real_backward_count 108612  30.395%\n",
      "fc layer 1 self.abs_max_out: 11109.0\n",
      "fc layer 1 self.abs_max_out: 11429.0\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  2.154325/  2.186325, val:  61.25%, val_best:  62.08%, tr:  90.81%, tr_best:  92.34%, epoch time: 40.86 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.1057%\n",
      "layer   2  Sparsity: 74.8909%\n",
      "layer   3  Sparsity: 74.0865%\n",
      "total_backward_count 362230 real_backward_count 109891  30.337%\n",
      "lif layer 1 self.abs_max_v: 12552.5\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  2.154862/  2.189625, val:  51.67%, val_best:  62.08%, tr:  91.73%, tr_best:  92.34%, epoch time: 40.44 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0454%\n",
      "layer   2  Sparsity: 74.8722%\n",
      "layer   3  Sparsity: 74.0518%\n",
      "total_backward_count 367125 real_backward_count 111177  30.283%\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  2.155537/  2.187197, val:  61.25%, val_best:  62.08%, tr:  90.91%, tr_best:  92.34%, epoch time: 40.63 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.1008%\n",
      "layer   2  Sparsity: 74.9993%\n",
      "layer   3  Sparsity: 74.7405%\n",
      "total_backward_count 372020 real_backward_count 112391  30.211%\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  2.157032/  2.195027, val:  56.67%, val_best:  62.08%, tr:  92.03%, tr_best:  92.34%, epoch time: 40.78 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0735%\n",
      "layer   2  Sparsity: 74.5817%\n",
      "layer   3  Sparsity: 74.7472%\n",
      "total_backward_count 376915 real_backward_count 113680  30.161%\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  2.161817/  2.200253, val:  35.42%, val_best:  62.08%, tr:  89.58%, tr_best:  92.34%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0869%\n",
      "layer   2  Sparsity: 74.6679%\n",
      "layer   3  Sparsity: 74.8683%\n",
      "total_backward_count 381810 real_backward_count 114999  30.119%\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  2.153148/  2.187663, val:  49.17%, val_best:  62.08%, tr:  91.73%, tr_best:  92.34%, epoch time: 40.49 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0882%\n",
      "layer   2  Sparsity: 74.8477%\n",
      "layer   3  Sparsity: 74.7463%\n",
      "total_backward_count 386705 real_backward_count 116248  30.061%\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  2.151023/  2.185510, val:  48.33%, val_best:  62.08%, tr:  92.03%, tr_best:  92.34%, epoch time: 40.16 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0937%\n",
      "layer   2  Sparsity: 74.4646%\n",
      "layer   3  Sparsity: 74.3827%\n",
      "total_backward_count 391600 real_backward_count 117519  30.010%\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  2.149714/  2.184267, val:  62.92%, val_best:  62.92%, tr:  91.83%, tr_best:  92.34%, epoch time: 40.52 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0709%\n",
      "layer   2  Sparsity: 74.5021%\n",
      "layer   3  Sparsity: 74.6789%\n",
      "total_backward_count 396495 real_backward_count 118763  29.953%\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  2.154263/  2.193127, val:  55.83%, val_best:  62.92%, tr:  92.24%, tr_best:  92.34%, epoch time: 40.61 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.1009%\n",
      "layer   2  Sparsity: 74.8248%\n",
      "layer   3  Sparsity: 74.7910%\n",
      "total_backward_count 401390 real_backward_count 120018  29.901%\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  2.160798/  2.198278, val:  56.25%, val_best:  62.92%, tr:  90.70%, tr_best:  92.34%, epoch time: 40.94 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0852%\n",
      "layer   2  Sparsity: 74.5571%\n",
      "layer   3  Sparsity: 74.4655%\n",
      "total_backward_count 406285 real_backward_count 121333  29.864%\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  2.155496/  2.188137, val:  65.83%, val_best:  65.83%, tr:  91.32%, tr_best:  92.34%, epoch time: 40.97 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0490%\n",
      "layer   2  Sparsity: 74.5597%\n",
      "layer   3  Sparsity: 75.2606%\n",
      "total_backward_count 411180 real_backward_count 122618  29.821%\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  2.154113/  2.183939, val:  61.25%, val_best:  65.83%, tr:  91.22%, tr_best:  92.34%, epoch time: 40.60 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0741%\n",
      "layer   2  Sparsity: 74.5577%\n",
      "layer   3  Sparsity: 75.2418%\n",
      "total_backward_count 416075 real_backward_count 123856  29.768%\n",
      "lif layer 1 self.abs_max_v: 13114.5\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  2.155268/  2.193987, val:  45.00%, val_best:  65.83%, tr:  91.83%, tr_best:  92.34%, epoch time: 40.62 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0961%\n",
      "layer   2  Sparsity: 74.7937%\n",
      "layer   3  Sparsity: 75.1523%\n",
      "total_backward_count 420970 real_backward_count 125090  29.715%\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  2.152690/  2.193872, val:  54.58%, val_best:  65.83%, tr:  92.03%, tr_best:  92.34%, epoch time: 40.72 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0975%\n",
      "layer   2  Sparsity: 74.7742%\n",
      "layer   3  Sparsity: 75.4461%\n",
      "total_backward_count 425865 real_backward_count 126327  29.664%\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  2.156716/  2.195808, val:  56.25%, val_best:  65.83%, tr:  91.01%, tr_best:  92.34%, epoch time: 40.81 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0572%\n",
      "layer   2  Sparsity: 74.6751%\n",
      "layer   3  Sparsity: 75.3148%\n",
      "total_backward_count 430760 real_backward_count 127572  29.616%\n",
      "lif layer 1 self.abs_max_v: 13476.0\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  2.162006/  2.185855, val:  52.08%, val_best:  65.83%, tr:  91.62%, tr_best:  92.34%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0823%\n",
      "layer   2  Sparsity: 74.8281%\n",
      "layer   3  Sparsity: 75.3263%\n",
      "total_backward_count 435655 real_backward_count 128874  29.582%\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  2.160704/  2.188622, val:  64.58%, val_best:  65.83%, tr:  91.83%, tr_best:  92.34%, epoch time: 40.47 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.1044%\n",
      "layer   2  Sparsity: 74.5448%\n",
      "layer   3  Sparsity: 75.3016%\n",
      "total_backward_count 440550 real_backward_count 130099  29.531%\n",
      "fc layer 2 self.abs_max_out: 5875.0\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  2.151870/  2.187792, val:  64.17%, val_best:  65.83%, tr:  92.85%, tr_best:  92.85%, epoch time: 41.07 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0893%\n",
      "layer   2  Sparsity: 74.8066%\n",
      "layer   3  Sparsity: 75.3280%\n",
      "total_backward_count 445445 real_backward_count 131310  29.478%\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  2.150705/  2.178893, val:  59.58%, val_best:  65.83%, tr:  92.85%, tr_best:  92.85%, epoch time: 40.82 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0599%\n",
      "layer   2  Sparsity: 74.5084%\n",
      "layer   3  Sparsity: 75.2415%\n",
      "total_backward_count 450340 real_backward_count 132447  29.410%\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  2.150663/  2.178931, val:  65.42%, val_best:  65.83%, tr:  92.85%, tr_best:  92.85%, epoch time: 41.08 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0882%\n",
      "layer   2  Sparsity: 74.4090%\n",
      "layer   3  Sparsity: 74.4772%\n",
      "total_backward_count 455235 real_backward_count 133636  29.355%\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  2.149886/  2.187838, val:  53.33%, val_best:  65.83%, tr:  93.36%, tr_best:  93.36%, epoch time: 40.81 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0762%\n",
      "layer   2  Sparsity: 74.5153%\n",
      "layer   3  Sparsity: 74.6710%\n",
      "total_backward_count 460130 real_backward_count 134827  29.302%\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  2.155164/  2.186364, val:  57.50%, val_best:  65.83%, tr:  93.26%, tr_best:  93.36%, epoch time: 40.87 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.1191%\n",
      "layer   2  Sparsity: 74.7153%\n",
      "layer   3  Sparsity: 74.9448%\n",
      "total_backward_count 465025 real_backward_count 136038  29.254%\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  2.149972/  2.183811, val:  61.67%, val_best:  65.83%, tr:  92.65%, tr_best:  93.36%, epoch time: 40.44 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0747%\n",
      "layer   2  Sparsity: 74.5346%\n",
      "layer   3  Sparsity: 74.5217%\n",
      "total_backward_count 469920 real_backward_count 137252  29.208%\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  2.150331/  2.186486, val:  53.75%, val_best:  65.83%, tr:  93.26%, tr_best:  93.36%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0757%\n",
      "layer   2  Sparsity: 74.4797%\n",
      "layer   3  Sparsity: 74.4645%\n",
      "total_backward_count 474815 real_backward_count 138462  29.161%\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  2.149999/  2.182696, val:  61.25%, val_best:  65.83%, tr:  91.73%, tr_best:  93.36%, epoch time: 40.80 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0952%\n",
      "layer   2  Sparsity: 74.4110%\n",
      "layer   3  Sparsity: 75.1153%\n",
      "total_backward_count 479710 real_backward_count 139703  29.122%\n",
      "fc layer 1 self.abs_max_out: 11651.0\n",
      "fc layer 1 self.abs_max_out: 11856.0\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  2.154301/  2.196343, val:  50.00%, val_best:  65.83%, tr:  92.65%, tr_best:  93.36%, epoch time: 41.12 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.1165%\n",
      "layer   2  Sparsity: 74.7633%\n",
      "layer   3  Sparsity: 75.5967%\n",
      "total_backward_count 484605 real_backward_count 140916  29.079%\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  2.153589/  2.191606, val:  55.00%, val_best:  65.83%, tr:  92.75%, tr_best:  93.36%, epoch time: 40.45 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.1006%\n",
      "layer   2  Sparsity: 74.8405%\n",
      "layer   3  Sparsity: 75.3487%\n",
      "total_backward_count 489500 real_backward_count 142166  29.043%\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  2.153923/  2.186545, val:  57.92%, val_best:  65.83%, tr:  92.85%, tr_best:  93.36%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0889%\n",
      "layer   2  Sparsity: 74.6468%\n",
      "layer   3  Sparsity: 75.3193%\n",
      "total_backward_count 494395 real_backward_count 143362  28.997%\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  2.153673/  2.186191, val:  64.17%, val_best:  65.83%, tr:  92.24%, tr_best:  93.36%, epoch time: 40.14 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0626%\n",
      "layer   2  Sparsity: 74.1640%\n",
      "layer   3  Sparsity: 74.9561%\n",
      "total_backward_count 499290 real_backward_count 144554  28.952%\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  2.150739/  2.183639, val:  68.33%, val_best:  68.33%, tr:  92.85%, tr_best:  93.36%, epoch time: 40.64 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0698%\n",
      "layer   2  Sparsity: 74.1993%\n",
      "layer   3  Sparsity: 75.0565%\n",
      "total_backward_count 504185 real_backward_count 145784  28.915%\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  2.149829/  2.183733, val:  70.83%, val_best:  70.83%, tr:  92.34%, tr_best:  93.36%, epoch time: 40.38 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0561%\n",
      "layer   2  Sparsity: 73.8655%\n",
      "layer   3  Sparsity: 74.4973%\n",
      "total_backward_count 509080 real_backward_count 147015  28.879%\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  2.154276/  2.185473, val:  68.33%, val_best:  70.83%, tr:  91.83%, tr_best:  93.36%, epoch time: 40.44 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0523%\n",
      "layer   2  Sparsity: 73.8596%\n",
      "layer   3  Sparsity: 74.9070%\n",
      "total_backward_count 513975 real_backward_count 148235  28.841%\n",
      "fc layer 2 self.abs_max_out: 5927.0\n",
      "fc layer 2 self.abs_max_out: 5928.0\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  2.149700/  2.181968, val:  61.25%, val_best:  70.83%, tr:  92.95%, tr_best:  93.36%, epoch time: 40.47 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.1001%\n",
      "layer   2  Sparsity: 73.9165%\n",
      "layer   3  Sparsity: 74.6881%\n",
      "total_backward_count 518870 real_backward_count 149493  28.811%\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  2.143518/  2.172508, val:  49.58%, val_best:  70.83%, tr:  92.85%, tr_best:  93.36%, epoch time: 40.11 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0952%\n",
      "layer   2  Sparsity: 74.1676%\n",
      "layer   3  Sparsity: 74.8665%\n",
      "total_backward_count 523765 real_backward_count 150730  28.778%\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  2.144176/  2.181609, val:  54.58%, val_best:  70.83%, tr:  93.36%, tr_best:  93.36%, epoch time: 40.74 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.1034%\n",
      "layer   2  Sparsity: 74.1226%\n",
      "layer   3  Sparsity: 74.8799%\n",
      "total_backward_count 528660 real_backward_count 151925  28.738%\n",
      "lif layer 1 self.abs_max_v: 13536.0\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  2.150814/  2.179374, val:  58.75%, val_best:  70.83%, tr:  92.54%, tr_best:  93.36%, epoch time: 40.84 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.1066%\n",
      "layer   2  Sparsity: 73.9465%\n",
      "layer   3  Sparsity: 75.1420%\n",
      "total_backward_count 533555 real_backward_count 153169  28.707%\n",
      "lif layer 1 self.abs_max_v: 13697.0\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  2.146190/  2.181923, val:  56.67%, val_best:  70.83%, tr:  92.65%, tr_best:  93.36%, epoch time: 40.87 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0839%\n",
      "layer   2  Sparsity: 74.0696%\n",
      "layer   3  Sparsity: 75.4954%\n",
      "total_backward_count 538450 real_backward_count 154334  28.663%\n",
      "lif layer 1 self.abs_max_v: 14061.5\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  2.150851/  2.181226, val:  65.42%, val_best:  70.83%, tr:  91.73%, tr_best:  93.36%, epoch time: 40.88 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0905%\n",
      "layer   2  Sparsity: 73.9686%\n",
      "layer   3  Sparsity: 75.6040%\n",
      "total_backward_count 543345 real_backward_count 155526  28.624%\n",
      "fc layer 2 self.abs_max_out: 6009.0\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  2.149233/  2.175674, val:  58.33%, val_best:  70.83%, tr:  92.85%, tr_best:  93.36%, epoch time: 40.39 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0699%\n",
      "layer   2  Sparsity: 73.6619%\n",
      "layer   3  Sparsity: 75.3167%\n",
      "total_backward_count 548240 real_backward_count 156692  28.581%\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  2.142810/  2.187701, val:  59.58%, val_best:  70.83%, tr:  93.67%, tr_best:  93.67%, epoch time: 40.59 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0919%\n",
      "layer   2  Sparsity: 73.9524%\n",
      "layer   3  Sparsity: 74.7993%\n",
      "total_backward_count 553135 real_backward_count 157887  28.544%\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  2.146164/  2.183594, val:  41.25%, val_best:  70.83%, tr:  93.67%, tr_best:  93.67%, epoch time: 40.75 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0676%\n",
      "layer   2  Sparsity: 73.8378%\n",
      "layer   3  Sparsity: 74.6995%\n",
      "total_backward_count 558030 real_backward_count 159071  28.506%\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  2.147657/  2.177935, val:  52.50%, val_best:  70.83%, tr:  91.62%, tr_best:  93.67%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0713%\n",
      "layer   2  Sparsity: 74.0750%\n",
      "layer   3  Sparsity: 74.8349%\n",
      "total_backward_count 562925 real_backward_count 160256  28.468%\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  2.148401/  2.179986, val:  74.58%, val_best:  74.58%, tr:  92.54%, tr_best:  93.67%, epoch time: 41.03 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0918%\n",
      "layer   2  Sparsity: 73.8150%\n",
      "layer   3  Sparsity: 74.0280%\n",
      "total_backward_count 567820 real_backward_count 161425  28.429%\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  2.147640/  2.180344, val:  66.25%, val_best:  74.58%, tr:  93.05%, tr_best:  93.67%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0841%\n",
      "layer   2  Sparsity: 73.7836%\n",
      "layer   3  Sparsity: 75.0620%\n",
      "total_backward_count 572715 real_backward_count 162630  28.396%\n",
      "epoch-117 lr=['0.0009766'], tr/val_loss:  2.149752/  2.181727, val:  64.17%, val_best:  74.58%, tr:  92.24%, tr_best:  93.67%, epoch time: 40.11 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0854%\n",
      "layer   2  Sparsity: 73.7987%\n",
      "layer   3  Sparsity: 74.3250%\n",
      "total_backward_count 577610 real_backward_count 163856  28.368%\n",
      "epoch-118 lr=['0.0009766'], tr/val_loss:  2.150128/  2.180146, val:  54.58%, val_best:  74.58%, tr:  92.65%, tr_best:  93.67%, epoch time: 40.29 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.1080%\n",
      "layer   2  Sparsity: 74.1272%\n",
      "layer   3  Sparsity: 74.2766%\n",
      "total_backward_count 582505 real_backward_count 165052  28.335%\n",
      "epoch-119 lr=['0.0009766'], tr/val_loss:  2.142442/  2.181010, val:  67.92%, val_best:  74.58%, tr:  92.13%, tr_best:  93.67%, epoch time: 40.23 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0746%\n",
      "layer   2  Sparsity: 74.1993%\n",
      "layer   3  Sparsity: 74.6070%\n",
      "total_backward_count 587400 real_backward_count 166218  28.297%\n",
      "epoch-120 lr=['0.0009766'], tr/val_loss:  2.146607/  2.183721, val:  55.00%, val_best:  74.58%, tr:  92.24%, tr_best:  93.67%, epoch time: 40.54 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0854%\n",
      "layer   2  Sparsity: 74.1876%\n",
      "layer   3  Sparsity: 75.0876%\n",
      "total_backward_count 592295 real_backward_count 167383  28.260%\n",
      "epoch-121 lr=['0.0009766'], tr/val_loss:  2.153390/  2.177501, val:  66.67%, val_best:  74.58%, tr:  93.46%, tr_best:  93.67%, epoch time: 40.02 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0895%\n",
      "layer   2  Sparsity: 74.2483%\n",
      "layer   3  Sparsity: 74.8769%\n",
      "total_backward_count 597190 real_backward_count 168579  28.229%\n",
      "epoch-122 lr=['0.0009766'], tr/val_loss:  2.145459/  2.181360, val:  53.33%, val_best:  74.58%, tr:  92.85%, tr_best:  93.67%, epoch time: 40.30 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0506%\n",
      "layer   2  Sparsity: 73.9294%\n",
      "layer   3  Sparsity: 74.5966%\n",
      "total_backward_count 602085 real_backward_count 169787  28.200%\n",
      "epoch-123 lr=['0.0009766'], tr/val_loss:  2.150211/  2.179523, val:  63.33%, val_best:  74.58%, tr:  93.05%, tr_best:  93.67%, epoch time: 40.04 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0922%\n",
      "layer   2  Sparsity: 74.1340%\n",
      "layer   3  Sparsity: 74.5507%\n",
      "total_backward_count 606980 real_backward_count 170991  28.171%\n",
      "lif layer 1 self.abs_max_v: 14394.5\n",
      "epoch-124 lr=['0.0009766'], tr/val_loss:  2.141894/  2.176201, val:  56.67%, val_best:  74.58%, tr:  92.95%, tr_best:  93.67%, epoch time: 40.01 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0639%\n",
      "layer   2  Sparsity: 73.8505%\n",
      "layer   3  Sparsity: 74.3593%\n",
      "total_backward_count 611875 real_backward_count 172123  28.130%\n",
      "epoch-125 lr=['0.0009766'], tr/val_loss:  2.139550/  2.175762, val:  62.92%, val_best:  74.58%, tr:  92.75%, tr_best:  93.67%, epoch time: 40.20 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0678%\n",
      "layer   2  Sparsity: 73.8148%\n",
      "layer   3  Sparsity: 74.4421%\n",
      "total_backward_count 616770 real_backward_count 173292  28.097%\n",
      "lif layer 1 self.abs_max_v: 14501.0\n",
      "epoch-126 lr=['0.0009766'], tr/val_loss:  2.144843/  2.178756, val:  62.08%, val_best:  74.58%, tr:  93.16%, tr_best:  93.67%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0557%\n",
      "layer   2  Sparsity: 73.7326%\n",
      "layer   3  Sparsity: 74.6526%\n",
      "total_backward_count 621665 real_backward_count 174445  28.061%\n",
      "epoch-127 lr=['0.0009766'], tr/val_loss:  2.143845/  2.182323, val:  69.17%, val_best:  74.58%, tr:  94.28%, tr_best:  94.28%, epoch time: 40.98 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0629%\n",
      "layer   2  Sparsity: 73.7726%\n",
      "layer   3  Sparsity: 75.4972%\n",
      "total_backward_count 626560 real_backward_count 175608  28.027%\n",
      "epoch-128 lr=['0.0009766'], tr/val_loss:  2.149329/  2.181325, val:  64.58%, val_best:  74.58%, tr:  94.28%, tr_best:  94.28%, epoch time: 40.01 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0712%\n",
      "layer   2  Sparsity: 73.6534%\n",
      "layer   3  Sparsity: 74.6731%\n",
      "total_backward_count 631455 real_backward_count 176838  28.005%\n",
      "epoch-129 lr=['0.0009766'], tr/val_loss:  2.141589/  2.183296, val:  60.83%, val_best:  74.58%, tr:  93.46%, tr_best:  94.28%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0971%\n",
      "layer   2  Sparsity: 73.8790%\n",
      "layer   3  Sparsity: 74.7132%\n",
      "total_backward_count 636350 real_backward_count 178007  27.973%\n",
      "epoch-130 lr=['0.0009766'], tr/val_loss:  2.144687/  2.171610, val:  46.25%, val_best:  74.58%, tr:  93.97%, tr_best:  94.28%, epoch time: 40.06 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0930%\n",
      "layer   2  Sparsity: 73.8097%\n",
      "layer   3  Sparsity: 74.5711%\n",
      "total_backward_count 641245 real_backward_count 179156  27.939%\n",
      "epoch-131 lr=['0.0009766'], tr/val_loss:  2.133607/  2.172102, val:  65.42%, val_best:  74.58%, tr:  93.97%, tr_best:  94.28%, epoch time: 40.43 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0960%\n",
      "layer   2  Sparsity: 73.9086%\n",
      "layer   3  Sparsity: 75.0731%\n",
      "total_backward_count 646140 real_backward_count 180304  27.905%\n",
      "fc layer 2 self.abs_max_out: 6080.0\n",
      "epoch-132 lr=['0.0009766'], tr/val_loss:  2.138889/  2.171449, val:  56.67%, val_best:  74.58%, tr:  94.18%, tr_best:  94.28%, epoch time: 40.33 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0748%\n",
      "layer   2  Sparsity: 74.0269%\n",
      "layer   3  Sparsity: 75.1387%\n",
      "total_backward_count 651035 real_backward_count 181447  27.871%\n",
      "epoch-133 lr=['0.0009766'], tr/val_loss:  2.140653/  2.176718, val:  56.25%, val_best:  74.58%, tr:  93.77%, tr_best:  94.28%, epoch time: 40.60 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0448%\n",
      "layer   2  Sparsity: 73.8724%\n",
      "layer   3  Sparsity: 74.6831%\n",
      "total_backward_count 655930 real_backward_count 182603  27.839%\n",
      "epoch-134 lr=['0.0009766'], tr/val_loss:  2.137234/  2.180407, val:  55.83%, val_best:  74.58%, tr:  93.77%, tr_best:  94.28%, epoch time: 40.09 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0890%\n",
      "layer   2  Sparsity: 74.1511%\n",
      "layer   3  Sparsity: 74.1683%\n",
      "total_backward_count 660825 real_backward_count 183817  27.816%\n",
      "epoch-135 lr=['0.0009766'], tr/val_loss:  2.141605/  2.168864, val:  57.50%, val_best:  74.58%, tr:  93.56%, tr_best:  94.28%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0874%\n",
      "layer   2  Sparsity: 74.1041%\n",
      "layer   3  Sparsity: 74.7801%\n",
      "total_backward_count 665720 real_backward_count 184960  27.783%\n",
      "epoch-136 lr=['0.0009766'], tr/val_loss:  2.137799/  2.170043, val:  63.33%, val_best:  74.58%, tr:  93.16%, tr_best:  94.28%, epoch time: 40.18 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0833%\n",
      "layer   2  Sparsity: 74.1580%\n",
      "layer   3  Sparsity: 74.9767%\n",
      "total_backward_count 670615 real_backward_count 186120  27.754%\n",
      "lif layer 1 self.abs_max_v: 14821.0\n",
      "epoch-137 lr=['0.0009766'], tr/val_loss:  2.133180/  2.170507, val:  60.42%, val_best:  74.58%, tr:  94.38%, tr_best:  94.38%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0546%\n",
      "layer   2  Sparsity: 73.7713%\n",
      "layer   3  Sparsity: 74.8300%\n",
      "total_backward_count 675510 real_backward_count 187204  27.713%\n",
      "epoch-138 lr=['0.0009766'], tr/val_loss:  2.140556/  2.176663, val:  48.75%, val_best:  74.58%, tr:  93.36%, tr_best:  94.38%, epoch time: 40.15 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0836%\n",
      "layer   2  Sparsity: 74.0922%\n",
      "layer   3  Sparsity: 75.2283%\n",
      "total_backward_count 680405 real_backward_count 188418  27.692%\n",
      "lif layer 1 self.abs_max_v: 14840.5\n",
      "epoch-139 lr=['0.0009766'], tr/val_loss:  2.140504/  2.176978, val:  67.08%, val_best:  74.58%, tr:  93.67%, tr_best:  94.38%, epoch time: 40.23 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0693%\n",
      "layer   2  Sparsity: 73.8938%\n",
      "layer   3  Sparsity: 74.5102%\n",
      "total_backward_count 685300 real_backward_count 189618  27.669%\n",
      "epoch-140 lr=['0.0009766'], tr/val_loss:  2.139210/  2.174664, val:  66.25%, val_best:  74.58%, tr:  94.59%, tr_best:  94.59%, epoch time: 39.89 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 96.0761%\n",
      "layer   2  Sparsity: 73.6678%\n",
      "layer   3  Sparsity: 75.0392%\n",
      "total_backward_count 690195 real_backward_count 190756  27.638%\n",
      "epoch-141 lr=['0.0009766'], tr/val_loss:  2.144654/  2.181750, val:  60.42%, val_best:  74.58%, tr:  93.36%, tr_best:  94.59%, epoch time: 41.17 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0790%\n",
      "layer   2  Sparsity: 73.8067%\n",
      "layer   3  Sparsity: 75.0390%\n",
      "total_backward_count 695090 real_backward_count 191935  27.613%\n",
      "epoch-142 lr=['0.0009766'], tr/val_loss:  2.144073/  2.165945, val:  62.92%, val_best:  74.58%, tr:  92.34%, tr_best:  94.59%, epoch time: 40.08 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0826%\n",
      "layer   2  Sparsity: 73.8001%\n",
      "layer   3  Sparsity: 75.1134%\n",
      "total_backward_count 699985 real_backward_count 193112  27.588%\n",
      "lif layer 1 self.abs_max_v: 15043.0\n",
      "epoch-143 lr=['0.0009766'], tr/val_loss:  2.140650/  2.179852, val:  65.83%, val_best:  74.58%, tr:  93.46%, tr_best:  94.59%, epoch time: 40.97 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0689%\n",
      "layer   2  Sparsity: 73.6899%\n",
      "layer   3  Sparsity: 74.7626%\n",
      "total_backward_count 704880 real_backward_count 194282  27.562%\n",
      "lif layer 1 self.abs_max_v: 15070.5\n",
      "epoch-144 lr=['0.0009766'], tr/val_loss:  2.144377/  2.180624, val:  64.17%, val_best:  74.58%, tr:  92.85%, tr_best:  94.59%, epoch time: 40.58 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0843%\n",
      "layer   2  Sparsity: 73.7220%\n",
      "layer   3  Sparsity: 74.1906%\n",
      "total_backward_count 709775 real_backward_count 195495  27.543%\n",
      "epoch-145 lr=['0.0009766'], tr/val_loss:  2.141905/  2.174118, val:  54.58%, val_best:  74.58%, tr:  94.69%, tr_best:  94.69%, epoch time: 40.72 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0887%\n",
      "layer   2  Sparsity: 73.4897%\n",
      "layer   3  Sparsity: 74.4880%\n",
      "total_backward_count 714670 real_backward_count 196641  27.515%\n",
      "lif layer 1 self.abs_max_v: 15072.0\n",
      "epoch-146 lr=['0.0009766'], tr/val_loss:  2.143497/  2.177753, val:  59.17%, val_best:  74.58%, tr:  93.05%, tr_best:  94.69%, epoch time: 39.71 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 96.1241%\n",
      "layer   2  Sparsity: 73.6820%\n",
      "layer   3  Sparsity: 75.0643%\n",
      "total_backward_count 719565 real_backward_count 197852  27.496%\n",
      "epoch-147 lr=['0.0009766'], tr/val_loss:  2.136496/  2.167578, val:  54.17%, val_best:  74.58%, tr:  94.48%, tr_best:  94.69%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0959%\n",
      "layer   2  Sparsity: 73.8326%\n",
      "layer   3  Sparsity: 75.1489%\n",
      "total_backward_count 724460 real_backward_count 198941  27.461%\n",
      "lif layer 1 self.abs_max_v: 15591.5\n",
      "epoch-148 lr=['0.0009766'], tr/val_loss:  2.135320/  2.170508, val:  67.50%, val_best:  74.58%, tr:  93.36%, tr_best:  94.69%, epoch time: 40.20 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0506%\n",
      "layer   2  Sparsity: 73.6562%\n",
      "layer   3  Sparsity: 74.9935%\n",
      "total_backward_count 729355 real_backward_count 200084  27.433%\n",
      "fc layer 2 self.abs_max_out: 6128.0\n",
      "epoch-149 lr=['0.0009766'], tr/val_loss:  2.144439/  2.173539, val:  63.33%, val_best:  74.58%, tr:  93.16%, tr_best:  94.69%, epoch time: 40.47 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.1114%\n",
      "layer   2  Sparsity: 73.9801%\n",
      "layer   3  Sparsity: 75.1358%\n",
      "total_backward_count 734250 real_backward_count 201283  27.413%\n",
      "epoch-150 lr=['0.0009766'], tr/val_loss:  2.143894/  2.177744, val:  55.83%, val_best:  74.58%, tr:  94.08%, tr_best:  94.69%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0660%\n",
      "layer   2  Sparsity: 73.8796%\n",
      "layer   3  Sparsity: 75.0409%\n",
      "total_backward_count 739145 real_backward_count 202425  27.386%\n",
      "epoch-151 lr=['0.0009766'], tr/val_loss:  2.146538/  2.173616, val:  68.33%, val_best:  74.58%, tr:  93.16%, tr_best:  94.69%, epoch time: 40.80 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0912%\n",
      "layer   2  Sparsity: 73.9662%\n",
      "layer   3  Sparsity: 75.1876%\n",
      "total_backward_count 744040 real_backward_count 203603  27.365%\n",
      "epoch-152 lr=['0.0009766'], tr/val_loss:  2.147178/  2.180626, val:  64.17%, val_best:  74.58%, tr:  94.08%, tr_best:  94.69%, epoch time: 40.30 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0951%\n",
      "layer   2  Sparsity: 73.8162%\n",
      "layer   3  Sparsity: 75.1863%\n",
      "total_backward_count 748935 real_backward_count 204737  27.337%\n",
      "epoch-153 lr=['0.0009766'], tr/val_loss:  2.140271/  2.164488, val:  68.75%, val_best:  74.58%, tr:  94.18%, tr_best:  94.69%, epoch time: 40.43 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.1049%\n",
      "layer   2  Sparsity: 73.5778%\n",
      "layer   3  Sparsity: 74.5101%\n",
      "total_backward_count 753830 real_backward_count 205926  27.317%\n",
      "epoch-154 lr=['0.0009766'], tr/val_loss:  2.138824/  2.161971, val:  70.42%, val_best:  74.58%, tr:  93.46%, tr_best:  94.69%, epoch time: 40.14 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0807%\n",
      "layer   2  Sparsity: 73.4892%\n",
      "layer   3  Sparsity: 74.4416%\n",
      "total_backward_count 758725 real_backward_count 207093  27.295%\n",
      "epoch-155 lr=['0.0009766'], tr/val_loss:  2.134372/  2.167077, val:  59.17%, val_best:  74.58%, tr:  93.77%, tr_best:  94.69%, epoch time: 40.12 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0633%\n",
      "layer   2  Sparsity: 73.4993%\n",
      "layer   3  Sparsity: 74.3334%\n",
      "total_backward_count 763620 real_backward_count 208221  27.268%\n",
      "epoch-156 lr=['0.0009766'], tr/val_loss:  2.134760/  2.174836, val:  55.83%, val_best:  74.58%, tr:  94.18%, tr_best:  94.69%, epoch time: 39.91 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0767%\n",
      "layer   2  Sparsity: 73.3339%\n",
      "layer   3  Sparsity: 74.6251%\n",
      "total_backward_count 768515 real_backward_count 209342  27.240%\n",
      "epoch-157 lr=['0.0009766'], tr/val_loss:  2.145369/  2.169811, val:  72.92%, val_best:  74.58%, tr:  92.54%, tr_best:  94.69%, epoch time: 39.82 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 96.1027%\n",
      "layer   2  Sparsity: 73.6221%\n",
      "layer   3  Sparsity: 75.0653%\n",
      "total_backward_count 773410 real_backward_count 210501  27.217%\n",
      "epoch-158 lr=['0.0009766'], tr/val_loss:  2.141854/  2.180286, val:  56.25%, val_best:  74.58%, tr:  93.26%, tr_best:  94.69%, epoch time: 39.59 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 96.0820%\n",
      "layer   2  Sparsity: 73.6055%\n",
      "layer   3  Sparsity: 74.6260%\n",
      "total_backward_count 778305 real_backward_count 211668  27.196%\n",
      "epoch-159 lr=['0.0009766'], tr/val_loss:  2.143053/  2.171003, val:  68.75%, val_best:  74.58%, tr:  93.36%, tr_best:  94.69%, epoch time: 41.13 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0684%\n",
      "layer   2  Sparsity: 73.7416%\n",
      "layer   3  Sparsity: 74.7693%\n",
      "total_backward_count 783200 real_backward_count 212866  27.179%\n",
      "epoch-160 lr=['0.0009766'], tr/val_loss:  2.138552/  2.171871, val:  65.00%, val_best:  74.58%, tr:  93.77%, tr_best:  94.69%, epoch time: 39.90 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 96.0825%\n",
      "layer   2  Sparsity: 73.6820%\n",
      "layer   3  Sparsity: 74.2183%\n",
      "total_backward_count 788095 real_backward_count 214003  27.154%\n",
      "epoch-161 lr=['0.0009766'], tr/val_loss:  2.139375/  2.163963, val:  58.75%, val_best:  74.58%, tr:  93.77%, tr_best:  94.69%, epoch time: 40.39 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0821%\n",
      "layer   2  Sparsity: 73.5460%\n",
      "layer   3  Sparsity: 74.0031%\n",
      "total_backward_count 792990 real_backward_count 215138  27.130%\n",
      "epoch-162 lr=['0.0009766'], tr/val_loss:  2.137769/  2.173053, val:  55.83%, val_best:  74.58%, tr:  92.34%, tr_best:  94.69%, epoch time: 39.92 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0815%\n",
      "layer   2  Sparsity: 73.8106%\n",
      "layer   3  Sparsity: 74.4084%\n",
      "total_backward_count 797885 real_backward_count 216288  27.108%\n",
      "lif layer 1 self.abs_max_v: 16029.0\n",
      "epoch-163 lr=['0.0009766'], tr/val_loss:  2.135376/  2.179463, val:  61.67%, val_best:  74.58%, tr:  94.28%, tr_best:  94.69%, epoch time: 40.57 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0590%\n",
      "layer   2  Sparsity: 73.6171%\n",
      "layer   3  Sparsity: 74.3978%\n",
      "total_backward_count 802780 real_backward_count 217380  27.078%\n",
      "epoch-164 lr=['0.0009766'], tr/val_loss:  2.143510/  2.172440, val:  63.75%, val_best:  74.58%, tr:  93.36%, tr_best:  94.69%, epoch time: 40.12 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0802%\n",
      "layer   2  Sparsity: 73.6651%\n",
      "layer   3  Sparsity: 74.9144%\n",
      "total_backward_count 807675 real_backward_count 218523  27.056%\n",
      "fc layer 2 self.abs_max_out: 6317.0\n",
      "epoch-165 lr=['0.0009766'], tr/val_loss:  2.144149/  2.177877, val:  58.75%, val_best:  74.58%, tr:  92.85%, tr_best:  94.69%, epoch time: 40.62 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0681%\n",
      "layer   2  Sparsity: 73.9146%\n",
      "layer   3  Sparsity: 75.5482%\n",
      "total_backward_count 812570 real_backward_count 219700  27.038%\n",
      "epoch-166 lr=['0.0009766'], tr/val_loss:  2.139504/  2.168972, val:  64.58%, val_best:  74.58%, tr:  94.28%, tr_best:  94.69%, epoch time: 40.18 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.1115%\n",
      "layer   2  Sparsity: 73.8093%\n",
      "layer   3  Sparsity: 75.4519%\n",
      "total_backward_count 817465 real_backward_count 220800  27.010%\n",
      "epoch-167 lr=['0.0009766'], tr/val_loss:  2.141406/  2.170675, val:  64.17%, val_best:  74.58%, tr:  93.16%, tr_best:  94.69%, epoch time: 40.46 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0452%\n",
      "layer   2  Sparsity: 73.4963%\n",
      "layer   3  Sparsity: 74.7086%\n",
      "total_backward_count 822360 real_backward_count 221935  26.988%\n",
      "epoch-168 lr=['0.0009766'], tr/val_loss:  2.144431/  2.171417, val:  65.42%, val_best:  74.58%, tr:  93.16%, tr_best:  94.69%, epoch time: 40.24 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0967%\n",
      "layer   2  Sparsity: 73.7954%\n",
      "layer   3  Sparsity: 75.0724%\n",
      "total_backward_count 827255 real_backward_count 223088  26.967%\n",
      "fc layer 2 self.abs_max_out: 6400.0\n",
      "epoch-169 lr=['0.0009766'], tr/val_loss:  2.143529/  2.172762, val:  65.83%, val_best:  74.58%, tr:  93.46%, tr_best:  94.69%, epoch time: 40.52 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0538%\n",
      "layer   2  Sparsity: 73.8263%\n",
      "layer   3  Sparsity: 74.8476%\n",
      "total_backward_count 832150 real_backward_count 224264  26.950%\n",
      "epoch-170 lr=['0.0009766'], tr/val_loss:  2.138672/  2.180791, val:  54.58%, val_best:  74.58%, tr:  94.79%, tr_best:  94.79%, epoch time: 40.43 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0976%\n",
      "layer   2  Sparsity: 73.9301%\n",
      "layer   3  Sparsity: 75.1943%\n",
      "total_backward_count 837045 real_backward_count 225359  26.923%\n",
      "epoch-171 lr=['0.0009766'], tr/val_loss:  2.136624/  2.173335, val:  65.83%, val_best:  74.58%, tr:  94.48%, tr_best:  94.79%, epoch time: 40.46 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0818%\n",
      "layer   2  Sparsity: 73.6534%\n",
      "layer   3  Sparsity: 74.7069%\n",
      "total_backward_count 841940 real_backward_count 226457  26.897%\n",
      "fc layer 1 self.abs_max_out: 12045.0\n",
      "fc layer 1 self.abs_max_out: 12271.0\n",
      "epoch-172 lr=['0.0009766'], tr/val_loss:  2.144420/  2.175494, val:  68.33%, val_best:  74.58%, tr:  93.67%, tr_best:  94.79%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0803%\n",
      "layer   2  Sparsity: 73.7157%\n",
      "layer   3  Sparsity: 74.5779%\n",
      "total_backward_count 846835 real_backward_count 227622  26.879%\n",
      "epoch-173 lr=['0.0009766'], tr/val_loss:  2.139758/  2.166800, val:  67.08%, val_best:  74.58%, tr:  92.34%, tr_best:  94.79%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0991%\n",
      "layer   2  Sparsity: 73.8106%\n",
      "layer   3  Sparsity: 74.6189%\n",
      "total_backward_count 851730 real_backward_count 228747  26.857%\n",
      "epoch-174 lr=['0.0009766'], tr/val_loss:  2.139059/  2.165940, val:  64.58%, val_best:  74.58%, tr:  93.46%, tr_best:  94.79%, epoch time: 40.16 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0641%\n",
      "layer   2  Sparsity: 73.8158%\n",
      "layer   3  Sparsity: 74.7395%\n",
      "total_backward_count 856625 real_backward_count 229861  26.833%\n",
      "lif layer 1 self.abs_max_v: 16208.5\n",
      "epoch-175 lr=['0.0009766'], tr/val_loss:  2.145347/  2.182049, val:  62.50%, val_best:  74.58%, tr:  93.16%, tr_best:  94.79%, epoch time: 40.37 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0601%\n",
      "layer   2  Sparsity: 73.6817%\n",
      "layer   3  Sparsity: 74.6088%\n",
      "total_backward_count 861520 real_backward_count 230983  26.811%\n",
      "epoch-176 lr=['0.0009766'], tr/val_loss:  2.138263/  2.160579, val:  65.83%, val_best:  74.58%, tr:  93.46%, tr_best:  94.79%, epoch time: 40.96 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0994%\n",
      "layer   2  Sparsity: 73.5751%\n",
      "layer   3  Sparsity: 75.1825%\n",
      "total_backward_count 866415 real_backward_count 232087  26.787%\n",
      "epoch-177 lr=['0.0009766'], tr/val_loss:  2.144882/  2.184595, val:  63.75%, val_best:  74.58%, tr:  94.08%, tr_best:  94.79%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0569%\n",
      "layer   2  Sparsity: 73.5482%\n",
      "layer   3  Sparsity: 75.3719%\n",
      "total_backward_count 871310 real_backward_count 233188  26.763%\n",
      "epoch-178 lr=['0.0009766'], tr/val_loss:  2.143146/  2.171163, val:  49.58%, val_best:  74.58%, tr:  94.28%, tr_best:  94.79%, epoch time: 39.84 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 96.0772%\n",
      "layer   2  Sparsity: 73.6381%\n",
      "layer   3  Sparsity: 74.6798%\n",
      "total_backward_count 876205 real_backward_count 234315  26.742%\n",
      "epoch-179 lr=['0.0009766'], tr/val_loss:  2.134313/  2.168111, val:  73.75%, val_best:  74.58%, tr:  94.08%, tr_best:  94.79%, epoch time: 40.02 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.1000%\n",
      "layer   2  Sparsity: 73.6053%\n",
      "layer   3  Sparsity: 74.8256%\n",
      "total_backward_count 881100 real_backward_count 235414  26.718%\n",
      "epoch-180 lr=['0.0009766'], tr/val_loss:  2.141099/  2.174091, val:  62.50%, val_best:  74.58%, tr:  94.18%, tr_best:  94.79%, epoch time: 39.88 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 96.0653%\n",
      "layer   2  Sparsity: 73.4963%\n",
      "layer   3  Sparsity: 74.3940%\n",
      "total_backward_count 885995 real_backward_count 236516  26.695%\n",
      "epoch-181 lr=['0.0009766'], tr/val_loss:  2.134156/  2.171407, val:  57.50%, val_best:  74.58%, tr:  93.46%, tr_best:  94.79%, epoch time: 39.57 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 96.0768%\n",
      "layer   2  Sparsity: 73.5835%\n",
      "layer   3  Sparsity: 74.4379%\n",
      "total_backward_count 890890 real_backward_count 237661  26.677%\n",
      "epoch-182 lr=['0.0009766'], tr/val_loss:  2.137194/  2.168082, val:  51.67%, val_best:  74.58%, tr:  94.59%, tr_best:  94.79%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0624%\n",
      "layer   2  Sparsity: 73.5604%\n",
      "layer   3  Sparsity: 74.3448%\n",
      "total_backward_count 895785 real_backward_count 238745  26.652%\n",
      "lif layer 1 self.abs_max_v: 16245.0\n",
      "epoch-183 lr=['0.0009766'], tr/val_loss:  2.133127/  2.167473, val:  51.67%, val_best:  74.58%, tr:  93.97%, tr_best:  94.79%, epoch time: 40.81 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0736%\n",
      "layer   2  Sparsity: 73.5013%\n",
      "layer   3  Sparsity: 73.8860%\n",
      "total_backward_count 900680 real_backward_count 239877  26.633%\n",
      "fc layer 2 self.abs_max_out: 6594.0\n",
      "lif layer 1 self.abs_max_v: 16359.5\n",
      "epoch-184 lr=['0.0009766'], tr/val_loss:  2.134146/  2.167192, val:  55.00%, val_best:  74.58%, tr:  93.87%, tr_best:  94.79%, epoch time: 40.57 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0627%\n",
      "layer   2  Sparsity: 73.3696%\n",
      "layer   3  Sparsity: 73.9275%\n",
      "total_backward_count 905575 real_backward_count 241000  26.613%\n",
      "epoch-185 lr=['0.0009766'], tr/val_loss:  2.136181/  2.162494, val:  63.75%, val_best:  74.58%, tr:  95.40%, tr_best:  95.40%, epoch time: 40.32 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.1035%\n",
      "layer   2  Sparsity: 73.3489%\n",
      "layer   3  Sparsity: 74.0948%\n",
      "total_backward_count 910470 real_backward_count 242110  26.592%\n",
      "epoch-186 lr=['0.0009766'], tr/val_loss:  2.138911/  2.175376, val:  58.33%, val_best:  74.58%, tr:  94.59%, tr_best:  95.40%, epoch time: 40.65 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0663%\n",
      "layer   2  Sparsity: 73.3667%\n",
      "layer   3  Sparsity: 74.6231%\n",
      "total_backward_count 915365 real_backward_count 243171  26.565%\n",
      "epoch-187 lr=['0.0009766'], tr/val_loss:  2.138027/  2.170990, val:  60.00%, val_best:  74.58%, tr:  93.56%, tr_best:  95.40%, epoch time: 40.31 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0794%\n",
      "layer   2  Sparsity: 73.6149%\n",
      "layer   3  Sparsity: 74.7395%\n",
      "total_backward_count 920260 real_backward_count 244309  26.548%\n",
      "epoch-188 lr=['0.0009766'], tr/val_loss:  2.135710/  2.173497, val:  71.67%, val_best:  74.58%, tr:  93.46%, tr_best:  95.40%, epoch time: 41.11 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0674%\n",
      "layer   2  Sparsity: 73.3779%\n",
      "layer   3  Sparsity: 74.9656%\n",
      "total_backward_count 925155 real_backward_count 245429  26.528%\n",
      "epoch-189 lr=['0.0009766'], tr/val_loss:  2.132389/  2.164644, val:  60.00%, val_best:  74.58%, tr:  95.30%, tr_best:  95.40%, epoch time: 40.08 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0637%\n",
      "layer   2  Sparsity: 73.3474%\n",
      "layer   3  Sparsity: 73.8554%\n",
      "total_backward_count 930050 real_backward_count 246496  26.504%\n",
      "epoch-190 lr=['0.0009766'], tr/val_loss:  2.133269/  2.169425, val:  70.83%, val_best:  74.58%, tr:  93.16%, tr_best:  95.40%, epoch time: 40.27 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0963%\n",
      "layer   2  Sparsity: 73.4211%\n",
      "layer   3  Sparsity: 74.5519%\n",
      "total_backward_count 934945 real_backward_count 247551  26.478%\n",
      "epoch-191 lr=['0.0009766'], tr/val_loss:  2.140024/  2.167870, val:  57.92%, val_best:  74.58%, tr:  93.67%, tr_best:  95.40%, epoch time: 39.88 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 96.1051%\n",
      "layer   2  Sparsity: 73.4135%\n",
      "layer   3  Sparsity: 74.2543%\n",
      "total_backward_count 939840 real_backward_count 248661  26.458%\n",
      "epoch-192 lr=['0.0009766'], tr/val_loss:  2.136697/  2.171493, val:  67.92%, val_best:  74.58%, tr:  93.87%, tr_best:  95.40%, epoch time: 40.32 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0606%\n",
      "layer   2  Sparsity: 73.4769%\n",
      "layer   3  Sparsity: 74.0048%\n",
      "total_backward_count 944735 real_backward_count 249734  26.434%\n",
      "epoch-193 lr=['0.0009766'], tr/val_loss:  2.129873/  2.164285, val:  52.50%, val_best:  74.58%, tr:  95.20%, tr_best:  95.40%, epoch time: 39.84 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 96.0419%\n",
      "layer   2  Sparsity: 73.4051%\n",
      "layer   3  Sparsity: 73.7411%\n",
      "total_backward_count 949630 real_backward_count 250793  26.410%\n",
      "epoch-194 lr=['0.0009766'], tr/val_loss:  2.129858/  2.161290, val:  66.67%, val_best:  74.58%, tr:  94.59%, tr_best:  95.40%, epoch time: 41.11 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 96.0667%\n",
      "layer   2  Sparsity: 73.5419%\n",
      "layer   3  Sparsity: 74.0951%\n",
      "total_backward_count 954525 real_backward_count 251908  26.391%\n",
      "epoch-195 lr=['0.0009766'], tr/val_loss:  2.134579/  2.167817, val:  65.83%, val_best:  74.58%, tr:  94.89%, tr_best:  95.40%, epoch time: 40.31 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.1052%\n",
      "layer   2  Sparsity: 73.7386%\n",
      "layer   3  Sparsity: 74.6306%\n",
      "total_backward_count 959420 real_backward_count 253006  26.371%\n",
      "epoch-196 lr=['0.0009766'], tr/val_loss:  2.135884/  2.158897, val:  73.33%, val_best:  74.58%, tr:  95.81%, tr_best:  95.81%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0844%\n",
      "layer   2  Sparsity: 73.5078%\n",
      "layer   3  Sparsity: 74.2919%\n",
      "total_backward_count 964315 real_backward_count 254083  26.349%\n",
      "epoch-197 lr=['0.0009766'], tr/val_loss:  2.134857/  2.162761, val:  60.42%, val_best:  74.58%, tr:  94.38%, tr_best:  95.81%, epoch time: 40.54 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 96.0740%\n",
      "layer   2  Sparsity: 73.3484%\n",
      "layer   3  Sparsity: 74.4080%\n",
      "total_backward_count 969210 real_backward_count 255194  26.330%\n",
      "epoch-198 lr=['0.0009766'], tr/val_loss:  2.126215/  2.163353, val:  59.58%, val_best:  74.58%, tr:  93.87%, tr_best:  95.81%, epoch time: 40.32 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0493%\n",
      "layer   2  Sparsity: 73.3715%\n",
      "layer   3  Sparsity: 74.4888%\n",
      "total_backward_count 974105 real_backward_count 256264  26.308%\n",
      "epoch-199 lr=['0.0009766'], tr/val_loss:  2.123659/  2.151421, val:  64.17%, val_best:  74.58%, tr:  94.89%, tr_best:  95.81%, epoch time: 40.43 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 96.0819%\n",
      "layer   2  Sparsity: 73.3370%\n",
      "layer   3  Sparsity: 74.0790%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d467bcc569418fa10d1b4048363c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÇ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñá‚ñà‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÖ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÇ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñá‚ñà‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.94893</td></tr><tr><td>tr_epoch_loss</td><td>2.12366</td></tr><tr><td>val_acc_best</td><td>0.74583</td></tr><tr><td>val_acc_now</td><td>0.64167</td></tr><tr><td>val_loss</td><td>2.15142</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">youthful-sweep-13</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mo03a3fr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mo03a3fr</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_083355-mo03a3fr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ipdugg4b with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 50000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_104947-ipdugg4b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ipdugg4b' target=\"_blank\">robust-sweep-15</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ipdugg4b' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ipdugg4b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': '20251118_104955_781', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 50000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 575149142d3019108310063e0e922290\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 139\n",
      "fc layer 1 self.abs_max_out: 232.0\n",
      "lif layer 1 self.abs_max_v: 232.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 202.0\n",
      "lif layer 2 self.abs_max_v: 202.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 62.0\n",
      "fc layer 1 self.abs_max_out: 261.0\n",
      "lif layer 1 self.abs_max_v: 274.0\n",
      "fc layer 2 self.abs_max_out: 292.0\n",
      "lif layer 2 self.abs_max_v: 328.0\n",
      "fc layer 1 self.abs_max_out: 275.0\n",
      "lif layer 1 self.abs_max_v: 345.0\n",
      "fc layer 2 self.abs_max_out: 363.0\n",
      "lif layer 2 self.abs_max_v: 408.5\n",
      "fc layer 3 self.abs_max_out: 129.0\n",
      "fc layer 1 self.abs_max_out: 309.0\n",
      "fc layer 1 self.abs_max_out: 479.0\n",
      "lif layer 1 self.abs_max_v: 479.0\n",
      "fc layer 2 self.abs_max_out: 436.0\n",
      "lif layer 2 self.abs_max_v: 514.5\n",
      "fc layer 3 self.abs_max_out: 167.0\n",
      "fc layer 1 self.abs_max_out: 718.0\n",
      "lif layer 1 self.abs_max_v: 718.0\n",
      "fc layer 1 self.abs_max_out: 774.0\n",
      "lif layer 1 self.abs_max_v: 774.0\n",
      "lif layer 2 self.abs_max_v: 525.0\n",
      "smallest_now_T updated: 125\n",
      "fc layer 1 self.abs_max_out: 797.0\n",
      "lif layer 1 self.abs_max_v: 797.0\n",
      "lif layer 2 self.abs_max_v: 556.5\n",
      "lif layer 1 self.abs_max_v: 822.5\n",
      "lif layer 2 self.abs_max_v: 599.0\n",
      "fc layer 3 self.abs_max_out: 230.0\n",
      "fc layer 1 self.abs_max_out: 923.0\n",
      "lif layer 1 self.abs_max_v: 923.0\n",
      "smallest_now_T updated: 94\n",
      "lif layer 2 self.abs_max_v: 644.5\n",
      "lif layer 2 self.abs_max_v: 660.5\n",
      "lif layer 1 self.abs_max_v: 1071.0\n",
      "lif layer 2 self.abs_max_v: 678.5\n",
      "lif layer 2 self.abs_max_v: 729.5\n",
      "fc layer 2 self.abs_max_out: 472.0\n",
      "lif layer 2 self.abs_max_v: 817.5\n",
      "fc layer 1 self.abs_max_out: 973.0\n",
      "fc layer 1 self.abs_max_out: 1050.0\n",
      "fc layer 2 self.abs_max_out: 523.0\n",
      "lif layer 1 self.abs_max_v: 1076.0\n",
      "lif layer 1 self.abs_max_v: 1082.5\n",
      "fc layer 1 self.abs_max_out: 1262.0\n",
      "lif layer 1 self.abs_max_v: 1262.0\n",
      "fc layer 2 self.abs_max_out: 661.0\n",
      "fc layer 2 self.abs_max_out: 664.0\n",
      "fc layer 3 self.abs_max_out: 262.0\n",
      "lif layer 1 self.abs_max_v: 1268.5\n",
      "fc layer 1 self.abs_max_out: 1279.0\n",
      "lif layer 1 self.abs_max_v: 1279.0\n",
      "fc layer 3 self.abs_max_out: 266.0\n",
      "lif layer 1 self.abs_max_v: 1338.5\n",
      "fc layer 2 self.abs_max_out: 695.0\n",
      "lif layer 1 self.abs_max_v: 1941.5\n",
      "fc layer 1 self.abs_max_out: 1629.0\n",
      "lif layer 1 self.abs_max_v: 2600.0\n",
      "lif layer 1 self.abs_max_v: 2729.0\n",
      "fc layer 2 self.abs_max_out: 807.0\n",
      "lif layer 1 self.abs_max_v: 2924.5\n",
      "lif layer 1 self.abs_max_v: 2984.5\n",
      "smallest_now_T updated: 79\n",
      "fc layer 3 self.abs_max_out: 270.0\n",
      "fc layer 3 self.abs_max_out: 276.0\n",
      "lif layer 2 self.abs_max_v: 858.0\n",
      "fc layer 3 self.abs_max_out: 296.0\n",
      "lif layer 1 self.abs_max_v: 2995.5\n",
      "lif layer 2 self.abs_max_v: 879.5\n",
      "lif layer 2 self.abs_max_v: 963.0\n",
      "lif layer 2 self.abs_max_v: 968.5\n",
      "lif layer 2 self.abs_max_v: 988.0\n",
      "smallest_now_T updated: 73\n",
      "fc layer 2 self.abs_max_out: 809.0\n",
      "fc layer 3 self.abs_max_out: 305.0\n",
      "fc layer 3 self.abs_max_out: 323.0\n",
      "fc layer 2 self.abs_max_out: 872.0\n",
      "lif layer 2 self.abs_max_v: 1030.0\n",
      "lif layer 2 self.abs_max_v: 1203.5\n",
      "lif layer 2 self.abs_max_v: 1268.0\n",
      "lif layer 2 self.abs_max_v: 1359.0\n",
      "fc layer 1 self.abs_max_out: 1646.0\n",
      "fc layer 2 self.abs_max_out: 892.0\n",
      "smallest_now_T updated: 65\n",
      "fc layer 2 self.abs_max_out: 902.0\n",
      "fc layer 1 self.abs_max_out: 1647.0\n",
      "fc layer 1 self.abs_max_out: 1800.0\n",
      "fc layer 3 self.abs_max_out: 370.0\n",
      "fc layer 3 self.abs_max_out: 376.0\n",
      "fc layer 2 self.abs_max_out: 904.0\n",
      "fc layer 2 self.abs_max_out: 954.0\n",
      "lif layer 2 self.abs_max_v: 1362.5\n",
      "lif layer 2 self.abs_max_v: 1414.5\n",
      "fc layer 1 self.abs_max_out: 1843.0\n",
      "fc layer 1 self.abs_max_out: 2018.0\n",
      "lif layer 1 self.abs_max_v: 3429.5\n",
      "lif layer 2 self.abs_max_v: 1472.5\n",
      "lif layer 2 self.abs_max_v: 1479.0\n",
      "lif layer 2 self.abs_max_v: 1480.5\n",
      "fc layer 3 self.abs_max_out: 377.0\n",
      "smallest_now_T updated: 56\n",
      "smallest_now_T updated: 50\n",
      "fc layer 3 self.abs_max_out: 381.0\n",
      "fc layer 3 self.abs_max_out: 393.0\n",
      "fc layer 3 self.abs_max_out: 411.0\n",
      "fc layer 3 self.abs_max_out: 415.0\n",
      "fc layer 2 self.abs_max_out: 961.0\n",
      "fc layer 2 self.abs_max_out: 962.0\n",
      "fc layer 3 self.abs_max_out: 425.0\n",
      "fc layer 2 self.abs_max_out: 970.0\n",
      "fc layer 2 self.abs_max_out: 973.0\n",
      "lif layer 2 self.abs_max_v: 1533.5\n",
      "fc layer 2 self.abs_max_out: 1017.0\n",
      "fc layer 3 self.abs_max_out: 440.0\n",
      "fc layer 2 self.abs_max_out: 1028.0\n",
      "fc layer 2 self.abs_max_out: 1036.0\n",
      "fc layer 3 self.abs_max_out: 441.0\n",
      "fc layer 3 self.abs_max_out: 453.0\n",
      "fc layer 3 self.abs_max_out: 468.0\n",
      "lif layer 1 self.abs_max_v: 3434.0\n",
      "fc layer 1 self.abs_max_out: 2154.0\n",
      "lif layer 1 self.abs_max_v: 3871.0\n",
      "fc layer 2 self.abs_max_out: 1053.0\n",
      "lif layer 2 self.abs_max_v: 1560.5\n",
      "lif layer 2 self.abs_max_v: 1582.5\n",
      "lif layer 2 self.abs_max_v: 1606.5\n",
      "lif layer 2 self.abs_max_v: 1611.5\n",
      "lif layer 2 self.abs_max_v: 1621.5\n",
      "lif layer 2 self.abs_max_v: 1731.5\n",
      "lif layer 2 self.abs_max_v: 1808.0\n",
      "fc layer 2 self.abs_max_out: 1066.0\n",
      "fc layer 3 self.abs_max_out: 470.0\n",
      "fc layer 3 self.abs_max_out: 487.0\n",
      "fc layer 1 self.abs_max_out: 2173.0\n",
      "fc layer 1 self.abs_max_out: 2212.0\n",
      "fc layer 2 self.abs_max_out: 1081.0\n",
      "fc layer 3 self.abs_max_out: 515.0\n",
      "fc layer 2 self.abs_max_out: 1090.0\n",
      "smallest_now_T_val updated: 129\n",
      "smallest_now_T_val updated: 106\n",
      "smallest_now_T_val updated: 104\n",
      "smallest_now_T_val updated: 102\n",
      "smallest_now_T_val updated: 85\n",
      "smallest_now_T_val updated: 50\n",
      "fc layer 1 self.abs_max_out: 2268.0\n",
      "lif layer 1 self.abs_max_v: 3991.0\n",
      "fc layer 1 self.abs_max_out: 2322.0\n",
      "fc layer 1 self.abs_max_out: 2362.0\n",
      "fc layer 1 self.abs_max_out: 2371.0\n",
      "fc layer 1 self.abs_max_out: 2519.0\n",
      "lif layer 1 self.abs_max_v: 4036.5\n",
      "lif layer 1 self.abs_max_v: 4094.5\n",
      "lif layer 1 self.abs_max_v: 4347.5\n",
      "lif layer 1 self.abs_max_v: 4608.0\n",
      "lif layer 1 self.abs_max_v: 4701.0\n",
      "fc layer 2 self.abs_max_out: 1106.0\n",
      "fc layer 2 self.abs_max_out: 1107.0\n",
      "fc layer 2 self.abs_max_out: 1140.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.569317/  1.839823, val:  40.42%, val_best:  40.42%, tr:  99.49%, tr_best:  99.49%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8928%\n",
      "layer   2  Sparsity: 77.4971%\n",
      "layer   3  Sparsity: 74.0058%\n",
      "total_backward_count 9790 real_backward_count 1431  14.617%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1143.0\n",
      "fc layer 3 self.abs_max_out: 520.0\n",
      "fc layer 3 self.abs_max_out: 526.0\n",
      "fc layer 1 self.abs_max_out: 2661.0\n",
      "fc layer 3 self.abs_max_out: 540.0\n",
      "fc layer 2 self.abs_max_out: 1198.0\n",
      "lif layer 2 self.abs_max_v: 1855.5\n",
      "lif layer 2 self.abs_max_v: 1885.0\n",
      "lif layer 2 self.abs_max_v: 1929.5\n",
      "lif layer 2 self.abs_max_v: 2023.0\n",
      "lif layer 2 self.abs_max_v: 2023.5\n",
      "lif layer 2 self.abs_max_v: 2042.0\n",
      "fc layer 1 self.abs_max_out: 2744.0\n",
      "fc layer 1 self.abs_max_out: 3005.0\n",
      "lif layer 1 self.abs_max_v: 4957.0\n",
      "lif layer 1 self.abs_max_v: 5001.5\n",
      "lif layer 1 self.abs_max_v: 5207.0\n",
      "fc layer 2 self.abs_max_out: 1219.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.468432/  1.817886, val:  38.33%, val_best:  40.42%, tr:  99.39%, tr_best:  99.49%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.9037%\n",
      "layer   2  Sparsity: 79.8514%\n",
      "layer   3  Sparsity: 77.7969%\n",
      "total_backward_count 19580 real_backward_count 2650  13.534%\n",
      "fc layer 2 self.abs_max_out: 1239.0\n",
      "fc layer 3 self.abs_max_out: 544.0\n",
      "fc layer 3 self.abs_max_out: 553.0\n",
      "fc layer 3 self.abs_max_out: 559.0\n",
      "fc layer 3 self.abs_max_out: 593.0\n",
      "fc layer 3 self.abs_max_out: 598.0\n",
      "lif layer 1 self.abs_max_v: 5351.5\n",
      "fc layer 3 self.abs_max_out: 629.0\n",
      "fc layer 3 self.abs_max_out: 651.0\n",
      "lif layer 2 self.abs_max_v: 2066.0\n",
      "lif layer 1 self.abs_max_v: 5357.0\n",
      "fc layer 2 self.abs_max_out: 1264.0\n",
      "lif layer 2 self.abs_max_v: 2120.5\n",
      "lif layer 2 self.abs_max_v: 2147.5\n",
      "fc layer 2 self.abs_max_out: 1273.0\n",
      "fc layer 1 self.abs_max_out: 3130.0\n",
      "lif layer 1 self.abs_max_v: 5385.0\n",
      "lif layer 1 self.abs_max_v: 5574.5\n",
      "lif layer 1 self.abs_max_v: 5843.5\n",
      "lif layer 2 self.abs_max_v: 2204.0\n",
      "lif layer 2 self.abs_max_v: 2223.0\n",
      "lif layer 2 self.abs_max_v: 2286.0\n",
      "fc layer 2 self.abs_max_out: 1307.0\n",
      "fc layer 2 self.abs_max_out: 1324.0\n",
      "fc layer 2 self.abs_max_out: 1360.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.429202/  1.735282, val:  45.00%, val_best:  45.00%, tr:  99.59%, tr_best:  99.59%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.8818%\n",
      "layer   2  Sparsity: 80.1194%\n",
      "layer   3  Sparsity: 77.3180%\n",
      "total_backward_count 29370 real_backward_count 3871  13.180%\n",
      "fc layer 2 self.abs_max_out: 1388.0\n",
      "fc layer 2 self.abs_max_out: 1396.0\n",
      "fc layer 2 self.abs_max_out: 1434.0\n",
      "fc layer 2 self.abs_max_out: 1441.0\n",
      "fc layer 2 self.abs_max_out: 1442.0\n",
      "fc layer 2 self.abs_max_out: 1462.0\n",
      "fc layer 2 self.abs_max_out: 1503.0\n",
      "fc layer 1 self.abs_max_out: 3167.0\n",
      "lif layer 2 self.abs_max_v: 2290.5\n",
      "lif layer 2 self.abs_max_v: 2295.5\n",
      "lif layer 2 self.abs_max_v: 2303.5\n",
      "fc layer 3 self.abs_max_out: 653.0\n",
      "fc layer 3 self.abs_max_out: 660.0\n",
      "fc layer 2 self.abs_max_out: 1562.0\n",
      "fc layer 1 self.abs_max_out: 3197.0\n",
      "lif layer 1 self.abs_max_v: 5898.5\n",
      "lif layer 1 self.abs_max_v: 6075.5\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.383341/  1.778767, val:  40.42%, val_best:  45.00%, tr:  99.49%, tr_best:  99.59%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8905%\n",
      "layer   2  Sparsity: 79.9422%\n",
      "layer   3  Sparsity: 77.4077%\n",
      "total_backward_count 39160 real_backward_count 5063  12.929%\n",
      "fc layer 1 self.abs_max_out: 3330.0\n",
      "lif layer 1 self.abs_max_v: 6096.5\n",
      "lif layer 1 self.abs_max_v: 6175.0\n",
      "fc layer 1 self.abs_max_out: 3401.0\n",
      "lif layer 1 self.abs_max_v: 6296.5\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.389611/  1.714751, val:  50.42%, val_best:  50.42%, tr:  99.49%, tr_best:  99.59%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8855%\n",
      "layer   2  Sparsity: 81.2572%\n",
      "layer   3  Sparsity: 78.5647%\n",
      "total_backward_count 48950 real_backward_count 6190  12.646%\n",
      "lif layer 2 self.abs_max_v: 2322.0\n",
      "fc layer 2 self.abs_max_out: 1582.0\n",
      "lif layer 2 self.abs_max_v: 2418.5\n",
      "lif layer 2 self.abs_max_v: 2444.0\n",
      "lif layer 2 self.abs_max_v: 2649.0\n",
      "fc layer 1 self.abs_max_out: 3541.0\n",
      "fc layer 1 self.abs_max_out: 4124.0\n",
      "lif layer 1 self.abs_max_v: 6610.0\n",
      "lif layer 1 self.abs_max_v: 6677.0\n",
      "lif layer 1 self.abs_max_v: 7040.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.386134/  1.680601, val:  54.17%, val_best:  54.17%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8964%\n",
      "layer   2  Sparsity: 81.0919%\n",
      "layer   3  Sparsity: 79.5306%\n",
      "total_backward_count 58740 real_backward_count 7310  12.445%\n",
      "fc layer 3 self.abs_max_out: 667.0\n",
      "fc layer 3 self.abs_max_out: 677.0\n",
      "fc layer 3 self.abs_max_out: 693.0\n",
      "lif layer 1 self.abs_max_v: 7106.5\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.382078/  1.718138, val:  46.25%, val_best:  54.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.8964%\n",
      "layer   2  Sparsity: 81.7002%\n",
      "layer   3  Sparsity: 79.2574%\n",
      "total_backward_count 68530 real_backward_count 8414  12.278%\n",
      "fc layer 3 self.abs_max_out: 721.0\n",
      "lif layer 2 self.abs_max_v: 2667.0\n",
      "lif layer 1 self.abs_max_v: 7333.0\n",
      "lif layer 1 self.abs_max_v: 7532.5\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.314650/  1.573764, val:  51.67%, val_best:  54.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.8990%\n",
      "layer   2  Sparsity: 81.3179%\n",
      "layer   3  Sparsity: 78.4594%\n",
      "total_backward_count 78320 real_backward_count 9450  12.066%\n",
      "fc layer 2 self.abs_max_out: 1593.0\n",
      "fc layer 3 self.abs_max_out: 736.0\n",
      "fc layer 3 self.abs_max_out: 843.0\n",
      "fc layer 2 self.abs_max_out: 1623.0\n",
      "fc layer 2 self.abs_max_out: 1669.0\n",
      "lif layer 1 self.abs_max_v: 7590.0\n",
      "lif layer 1 self.abs_max_v: 7595.0\n",
      "lif layer 1 self.abs_max_v: 7889.5\n",
      "fc layer 1 self.abs_max_out: 4141.0\n",
      "lif layer 1 self.abs_max_v: 8086.0\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.281337/  1.594239, val:  55.00%, val_best:  55.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.8861%\n",
      "layer   2  Sparsity: 81.6760%\n",
      "layer   3  Sparsity: 78.3598%\n",
      "total_backward_count 88110 real_backward_count 10502  11.919%\n",
      "fc layer 2 self.abs_max_out: 1773.0\n",
      "lif layer 2 self.abs_max_v: 2717.5\n",
      "lif layer 2 self.abs_max_v: 2763.0\n",
      "lif layer 2 self.abs_max_v: 2914.5\n",
      "fc layer 1 self.abs_max_out: 4304.0\n",
      "lif layer 1 self.abs_max_v: 8135.5\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.253732/  1.587601, val:  53.33%, val_best:  55.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8901%\n",
      "layer   2  Sparsity: 81.2132%\n",
      "layer   3  Sparsity: 78.1718%\n",
      "total_backward_count 97900 real_backward_count 11497  11.744%\n",
      "fc layer 2 self.abs_max_out: 1797.0\n",
      "fc layer 1 self.abs_max_out: 4398.0\n",
      "lif layer 1 self.abs_max_v: 8242.5\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.220043/  1.558747, val:  50.42%, val_best:  55.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8497%\n",
      "layer   2  Sparsity: 81.7701%\n",
      "layer   3  Sparsity: 77.7249%\n",
      "total_backward_count 107690 real_backward_count 12508  11.615%\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.202841/  1.524781, val:  57.08%, val_best:  57.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9281%\n",
      "layer   2  Sparsity: 81.2630%\n",
      "layer   3  Sparsity: 77.6602%\n",
      "total_backward_count 117480 real_backward_count 13488  11.481%\n",
      "lif layer 2 self.abs_max_v: 2970.0\n",
      "lif layer 2 self.abs_max_v: 3073.0\n",
      "lif layer 1 self.abs_max_v: 8327.5\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.157403/  1.519377, val:  48.75%, val_best:  57.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9155%\n",
      "layer   2  Sparsity: 80.6856%\n",
      "layer   3  Sparsity: 77.1709%\n",
      "total_backward_count 127270 real_backward_count 14445  11.350%\n",
      "fc layer 1 self.abs_max_out: 4593.0\n",
      "lif layer 1 self.abs_max_v: 8347.5\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.142883/  1.501209, val:  60.42%, val_best:  60.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8941%\n",
      "layer   2  Sparsity: 81.1796%\n",
      "layer   3  Sparsity: 76.8981%\n",
      "total_backward_count 137060 real_backward_count 15420  11.251%\n",
      "fc layer 3 self.abs_max_out: 846.0\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.133298/  1.524983, val:  49.58%, val_best:  60.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8706%\n",
      "layer   2  Sparsity: 81.2242%\n",
      "layer   3  Sparsity: 78.3822%\n",
      "total_backward_count 146850 real_backward_count 16331  11.121%\n",
      "fc layer 1 self.abs_max_out: 4596.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.114225/  1.407040, val:  56.25%, val_best:  60.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.8769%\n",
      "layer   2  Sparsity: 81.4348%\n",
      "layer   3  Sparsity: 77.7495%\n",
      "total_backward_count 156640 real_backward_count 17325  11.060%\n",
      "fc layer 1 self.abs_max_out: 4602.0\n",
      "fc layer 1 self.abs_max_out: 4636.0\n",
      "fc layer 1 self.abs_max_out: 4962.0\n",
      "fc layer 1 self.abs_max_out: 4997.0\n",
      "fc layer 1 self.abs_max_out: 5121.0\n",
      "fc layer 3 self.abs_max_out: 861.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.118739/  1.423035, val:  56.67%, val_best:  60.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8742%\n",
      "layer   2  Sparsity: 80.6739%\n",
      "layer   3  Sparsity: 77.4528%\n",
      "total_backward_count 166430 real_backward_count 18242  10.961%\n",
      "fc layer 3 self.abs_max_out: 870.0\n",
      "fc layer 1 self.abs_max_out: 5152.0\n",
      "fc layer 2 self.abs_max_out: 1814.0\n",
      "fc layer 1 self.abs_max_out: 5406.0\n",
      "fc layer 1 self.abs_max_out: 5721.0\n",
      "fc layer 1 self.abs_max_out: 6005.0\n",
      "fc layer 1 self.abs_max_out: 6120.0\n",
      "fc layer 1 self.abs_max_out: 6233.0\n",
      "fc layer 1 self.abs_max_out: 6627.0\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.087068/  1.399678, val:  67.92%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.8953%\n",
      "layer   2  Sparsity: 80.2297%\n",
      "layer   3  Sparsity: 76.8605%\n",
      "total_backward_count 176220 real_backward_count 19136  10.859%\n",
      "fc layer 2 self.abs_max_out: 1835.0\n",
      "fc layer 2 self.abs_max_out: 1977.0\n",
      "lif layer 1 self.abs_max_v: 8401.0\n",
      "lif layer 1 self.abs_max_v: 8489.5\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.080874/  1.462747, val:  58.33%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8772%\n",
      "layer   2  Sparsity: 80.4645%\n",
      "layer   3  Sparsity: 75.5927%\n",
      "total_backward_count 186010 real_backward_count 20049  10.778%\n",
      "fc layer 2 self.abs_max_out: 2011.0\n",
      "lif layer 1 self.abs_max_v: 8591.5\n",
      "lif layer 1 self.abs_max_v: 8645.0\n",
      "fc layer 1 self.abs_max_out: 6715.0\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.035413/  1.409947, val:  58.33%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.8852%\n",
      "layer   2  Sparsity: 81.3248%\n",
      "layer   3  Sparsity: 75.0680%\n",
      "total_backward_count 195800 real_backward_count 20896  10.672%\n",
      "lif layer 2 self.abs_max_v: 3198.5\n",
      "lif layer 2 self.abs_max_v: 3232.0\n",
      "fc layer 3 self.abs_max_out: 891.0\n",
      "fc layer 3 self.abs_max_out: 930.0\n",
      "fc layer 3 self.abs_max_out: 937.0\n",
      "lif layer 1 self.abs_max_v: 8719.0\n",
      "lif layer 1 self.abs_max_v: 8780.5\n",
      "fc layer 1 self.abs_max_out: 6867.0\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.010659/  1.465976, val:  49.17%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.8635%\n",
      "layer   2  Sparsity: 81.6052%\n",
      "layer   3  Sparsity: 74.4012%\n",
      "total_backward_count 205590 real_backward_count 21718  10.564%\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.004948/  1.384337, val:  62.08%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8776%\n",
      "layer   2  Sparsity: 81.7010%\n",
      "layer   3  Sparsity: 74.3097%\n",
      "total_backward_count 215380 real_backward_count 22624  10.504%\n",
      "fc layer 3 self.abs_max_out: 955.0\n",
      "fc layer 3 self.abs_max_out: 973.0\n",
      "fc layer 3 self.abs_max_out: 1001.0\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  0.987226/  1.333398, val:  66.25%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8904%\n",
      "layer   2  Sparsity: 81.9251%\n",
      "layer   3  Sparsity: 74.7947%\n",
      "total_backward_count 225170 real_backward_count 23496  10.435%\n",
      "lif layer 1 self.abs_max_v: 8817.5\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  0.965247/  1.355056, val:  59.17%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8707%\n",
      "layer   2  Sparsity: 81.4537%\n",
      "layer   3  Sparsity: 75.2307%\n",
      "total_backward_count 234960 real_backward_count 24343  10.360%\n",
      "lif layer 1 self.abs_max_v: 9024.5\n",
      "lif layer 1 self.abs_max_v: 9029.0\n",
      "lif layer 1 self.abs_max_v: 9114.5\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  0.961910/  1.281585, val:  71.67%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8853%\n",
      "layer   2  Sparsity: 81.7030%\n",
      "layer   3  Sparsity: 75.4644%\n",
      "total_backward_count 244750 real_backward_count 25189  10.292%\n",
      "lif layer 1 self.abs_max_v: 9173.5\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  0.937842/  1.315773, val:  76.67%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.9076%\n",
      "layer   2  Sparsity: 82.4392%\n",
      "layer   3  Sparsity: 76.1722%\n",
      "total_backward_count 254540 real_backward_count 26057  10.237%\n",
      "lif layer 1 self.abs_max_v: 9414.0\n",
      "lif layer 1 self.abs_max_v: 9500.0\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  0.997042/  1.335130, val:  66.25%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8985%\n",
      "layer   2  Sparsity: 82.1520%\n",
      "layer   3  Sparsity: 76.0844%\n",
      "total_backward_count 264330 real_backward_count 26879  10.169%\n",
      "lif layer 2 self.abs_max_v: 3345.0\n",
      "fc layer 2 self.abs_max_out: 2038.0\n",
      "lif layer 2 self.abs_max_v: 3374.0\n",
      "lif layer 2 self.abs_max_v: 3482.0\n",
      "lif layer 2 self.abs_max_v: 3629.0\n",
      "fc layer 2 self.abs_max_out: 2041.0\n",
      "fc layer 3 self.abs_max_out: 1003.0\n",
      "fc layer 3 self.abs_max_out: 1011.0\n",
      "fc layer 3 self.abs_max_out: 1066.0\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  0.952906/  1.260580, val:  62.50%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8880%\n",
      "layer   2  Sparsity: 82.1089%\n",
      "layer   3  Sparsity: 75.1807%\n",
      "total_backward_count 274120 real_backward_count 27709  10.108%\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  0.892026/  1.304954, val:  60.42%, val_best:  76.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8743%\n",
      "layer   2  Sparsity: 82.7704%\n",
      "layer   3  Sparsity: 74.9799%\n",
      "total_backward_count 283910 real_backward_count 28480  10.031%\n",
      "fc layer 2 self.abs_max_out: 2075.0\n",
      "lif layer 2 self.abs_max_v: 3671.5\n",
      "fc layer 2 self.abs_max_out: 2087.0\n",
      "lif layer 2 self.abs_max_v: 3923.0\n",
      "fc layer 2 self.abs_max_out: 2163.0\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  0.906544/  1.270449, val:  69.17%, val_best:  76.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8944%\n",
      "layer   2  Sparsity: 83.1443%\n",
      "layer   3  Sparsity: 74.3365%\n",
      "total_backward_count 293700 real_backward_count 29272   9.967%\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  0.892587/  1.253957, val:  75.42%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8985%\n",
      "layer   2  Sparsity: 82.6080%\n",
      "layer   3  Sparsity: 74.6667%\n",
      "total_backward_count 303490 real_backward_count 30091   9.915%\n",
      "fc layer 3 self.abs_max_out: 1075.0\n",
      "fc layer 3 self.abs_max_out: 1079.0\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  0.906532/  1.260915, val:  60.00%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.9350%\n",
      "layer   2  Sparsity: 82.6817%\n",
      "layer   3  Sparsity: 75.2130%\n",
      "total_backward_count 313280 real_backward_count 30871   9.854%\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  0.861090/  1.230730, val:  71.25%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8993%\n",
      "layer   2  Sparsity: 82.8927%\n",
      "layer   3  Sparsity: 74.2922%\n",
      "total_backward_count 323070 real_backward_count 31548   9.765%\n",
      "fc layer 3 self.abs_max_out: 1082.0\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  0.855220/  1.189493, val:  74.17%, val_best:  76.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8893%\n",
      "layer   2  Sparsity: 82.5521%\n",
      "layer   3  Sparsity: 73.7681%\n",
      "total_backward_count 332860 real_backward_count 32309   9.706%\n",
      "fc layer 3 self.abs_max_out: 1083.0\n",
      "fc layer 3 self.abs_max_out: 1121.0\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  0.839946/  1.221480, val:  64.17%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8843%\n",
      "layer   2  Sparsity: 82.1573%\n",
      "layer   3  Sparsity: 73.8759%\n",
      "total_backward_count 342650 real_backward_count 33035   9.641%\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  0.841060/  1.200980, val:  75.00%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8864%\n",
      "layer   2  Sparsity: 81.8900%\n",
      "layer   3  Sparsity: 74.0792%\n",
      "total_backward_count 352440 real_backward_count 33778   9.584%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  0.829990/  1.200749, val:  70.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9193%\n",
      "layer   2  Sparsity: 81.7610%\n",
      "layer   3  Sparsity: 74.1717%\n",
      "total_backward_count 362230 real_backward_count 34463   9.514%\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  0.819871/  1.246305, val:  64.17%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8804%\n",
      "layer   2  Sparsity: 82.0012%\n",
      "layer   3  Sparsity: 74.2176%\n",
      "total_backward_count 372020 real_backward_count 35108   9.437%\n",
      "fc layer 2 self.abs_max_out: 2243.0\n",
      "lif layer 2 self.abs_max_v: 4110.0\n",
      "lif layer 2 self.abs_max_v: 4271.0\n",
      "fc layer 2 self.abs_max_out: 2245.0\n",
      "lif layer 2 self.abs_max_v: 4321.0\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  0.813807/  1.166592, val:  69.17%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9289%\n",
      "layer   2  Sparsity: 82.1374%\n",
      "layer   3  Sparsity: 74.4078%\n",
      "total_backward_count 381810 real_backward_count 35823   9.382%\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  0.789137/  1.163205, val:  70.00%, val_best:  76.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8823%\n",
      "layer   2  Sparsity: 82.1881%\n",
      "layer   3  Sparsity: 74.4801%\n",
      "total_backward_count 391600 real_backward_count 36494   9.319%\n",
      "fc layer 1 self.abs_max_out: 6981.0\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  0.778868/  1.149443, val:  75.83%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8957%\n",
      "layer   2  Sparsity: 82.2627%\n",
      "layer   3  Sparsity: 73.6610%\n",
      "total_backward_count 401390 real_backward_count 37144   9.254%\n",
      "fc layer 1 self.abs_max_out: 7289.0\n",
      "lif layer 1 self.abs_max_v: 9809.5\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  0.802440/  1.136837, val:  72.92%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9395%\n",
      "layer   2  Sparsity: 82.1600%\n",
      "layer   3  Sparsity: 74.5760%\n",
      "total_backward_count 411180 real_backward_count 37781   9.188%\n",
      "fc layer 3 self.abs_max_out: 1129.0\n",
      "fc layer 3 self.abs_max_out: 1155.0\n",
      "fc layer 3 self.abs_max_out: 1179.0\n",
      "lif layer 1 self.abs_max_v: 10324.0\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  0.771941/  1.115327, val:  72.50%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9042%\n",
      "layer   2  Sparsity: 82.2101%\n",
      "layer   3  Sparsity: 74.3510%\n",
      "total_backward_count 420970 real_backward_count 38412   9.125%\n",
      "fc layer 1 self.abs_max_out: 7303.0\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  0.776730/  1.123072, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8639%\n",
      "layer   2  Sparsity: 82.1715%\n",
      "layer   3  Sparsity: 73.6904%\n",
      "total_backward_count 430760 real_backward_count 39079   9.072%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  0.769477/  1.100216, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.9409%\n",
      "layer   2  Sparsity: 82.0548%\n",
      "layer   3  Sparsity: 73.4498%\n",
      "total_backward_count 440550 real_backward_count 39790   9.032%\n",
      "fc layer 3 self.abs_max_out: 1210.0\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  0.739084/  1.157967, val:  72.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8856%\n",
      "layer   2  Sparsity: 82.4530%\n",
      "layer   3  Sparsity: 73.4545%\n",
      "total_backward_count 450340 real_backward_count 40497   8.993%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  0.738974/  1.213474, val:  66.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8861%\n",
      "layer   2  Sparsity: 82.5733%\n",
      "layer   3  Sparsity: 74.4821%\n",
      "total_backward_count 460130 real_backward_count 41156   8.944%\n",
      "fc layer 1 self.abs_max_out: 7393.0\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  0.767710/  1.184919, val:  70.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8870%\n",
      "layer   2  Sparsity: 82.6510%\n",
      "layer   3  Sparsity: 74.6068%\n",
      "total_backward_count 469920 real_backward_count 41793   8.894%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  0.766011/  1.127093, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9162%\n",
      "layer   2  Sparsity: 82.7107%\n",
      "layer   3  Sparsity: 74.6183%\n",
      "total_backward_count 479710 real_backward_count 42403   8.839%\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  0.750297/  1.116628, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.9020%\n",
      "layer   2  Sparsity: 82.9560%\n",
      "layer   3  Sparsity: 74.9992%\n",
      "total_backward_count 489500 real_backward_count 43012   8.787%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  0.743005/  1.149439, val:  73.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.8861%\n",
      "layer   2  Sparsity: 83.1374%\n",
      "layer   3  Sparsity: 74.6930%\n",
      "total_backward_count 499290 real_backward_count 43627   8.738%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  0.709379/  1.058856, val:  82.50%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.9304%\n",
      "layer   2  Sparsity: 82.8387%\n",
      "layer   3  Sparsity: 74.2960%\n",
      "total_backward_count 509080 real_backward_count 44219   8.686%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  0.708625/  1.049441, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8783%\n",
      "layer   2  Sparsity: 83.1663%\n",
      "layer   3  Sparsity: 74.2773%\n",
      "total_backward_count 518870 real_backward_count 44800   8.634%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  0.703903/  1.098267, val:  75.83%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8923%\n",
      "layer   2  Sparsity: 83.2165%\n",
      "layer   3  Sparsity: 74.5476%\n",
      "total_backward_count 528660 real_backward_count 45374   8.583%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  0.682475/  1.101788, val:  77.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8509%\n",
      "layer   2  Sparsity: 83.3390%\n",
      "layer   3  Sparsity: 73.9095%\n",
      "total_backward_count 538450 real_backward_count 45973   8.538%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  0.687476/  1.093194, val:  72.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9014%\n",
      "layer   2  Sparsity: 83.2868%\n",
      "layer   3  Sparsity: 74.4346%\n",
      "total_backward_count 548240 real_backward_count 46578   8.496%\n",
      "fc layer 3 self.abs_max_out: 1212.0\n",
      "fc layer 3 self.abs_max_out: 1215.0\n",
      "fc layer 3 self.abs_max_out: 1218.0\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  0.663864/  1.093400, val:  71.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.9075%\n",
      "layer   2  Sparsity: 83.1517%\n",
      "layer   3  Sparsity: 74.3825%\n",
      "total_backward_count 558030 real_backward_count 47136   8.447%\n",
      "fc layer 1 self.abs_max_out: 7434.0\n",
      "lif layer 1 self.abs_max_v: 10351.5\n",
      "lif layer 1 self.abs_max_v: 10551.5\n",
      "lif layer 1 self.abs_max_v: 10882.0\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  0.653787/  1.017759, val:  77.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8905%\n",
      "layer   2  Sparsity: 83.0438%\n",
      "layer   3  Sparsity: 74.2813%\n",
      "total_backward_count 567820 real_backward_count 47639   8.390%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  0.666361/  1.056564, val:  78.75%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8879%\n",
      "layer   2  Sparsity: 83.3166%\n",
      "layer   3  Sparsity: 74.5884%\n",
      "total_backward_count 577610 real_backward_count 48174   8.340%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  0.673148/  1.148466, val:  67.92%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8563%\n",
      "layer   2  Sparsity: 83.2350%\n",
      "layer   3  Sparsity: 74.5177%\n",
      "total_backward_count 587400 real_backward_count 48708   8.292%\n",
      "fc layer 1 self.abs_max_out: 7525.0\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  0.675150/  1.040324, val:  80.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8613%\n",
      "layer   2  Sparsity: 83.3370%\n",
      "layer   3  Sparsity: 73.9875%\n",
      "total_backward_count 597190 real_backward_count 49292   8.254%\n",
      "fc layer 3 self.abs_max_out: 1219.0\n",
      "fc layer 3 self.abs_max_out: 1267.0\n",
      "lif layer 1 self.abs_max_v: 10941.0\n",
      "lif layer 1 self.abs_max_v: 11123.5\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  0.669801/  1.036556, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8971%\n",
      "layer   2  Sparsity: 83.4438%\n",
      "layer   3  Sparsity: 74.6418%\n",
      "total_backward_count 606980 real_backward_count 49846   8.212%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  0.657708/  1.018161, val:  81.67%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8861%\n",
      "layer   2  Sparsity: 83.4626%\n",
      "layer   3  Sparsity: 74.4352%\n",
      "total_backward_count 616770 real_backward_count 50349   8.163%\n",
      "lif layer 1 self.abs_max_v: 11229.5\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  0.664193/  1.037575, val:  75.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8968%\n",
      "layer   2  Sparsity: 83.2358%\n",
      "layer   3  Sparsity: 74.1884%\n",
      "total_backward_count 626560 real_backward_count 50883   8.121%\n",
      "fc layer 3 self.abs_max_out: 1271.0\n",
      "lif layer 1 self.abs_max_v: 11263.5\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  0.643360/  1.052231, val:  76.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.9020%\n",
      "layer   2  Sparsity: 83.2187%\n",
      "layer   3  Sparsity: 74.5032%\n",
      "total_backward_count 636350 real_backward_count 51385   8.075%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  0.628195/  1.067388, val:  74.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8956%\n",
      "layer   2  Sparsity: 82.9862%\n",
      "layer   3  Sparsity: 74.2262%\n",
      "total_backward_count 646140 real_backward_count 51883   8.030%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  0.629412/  1.027460, val:  78.33%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.8834%\n",
      "layer   2  Sparsity: 83.1632%\n",
      "layer   3  Sparsity: 74.3397%\n",
      "total_backward_count 655930 real_backward_count 52366   7.983%\n",
      "lif layer 1 self.abs_max_v: 11272.5\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  0.626028/  1.000910, val:  79.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.64 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 88.8889%\n",
      "layer   2  Sparsity: 83.3498%\n",
      "layer   3  Sparsity: 74.1842%\n",
      "total_backward_count 665720 real_backward_count 52831   7.936%\n",
      "lif layer 1 self.abs_max_v: 11331.0\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  0.622203/  0.964595, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8717%\n",
      "layer   2  Sparsity: 83.4413%\n",
      "layer   3  Sparsity: 74.8795%\n",
      "total_backward_count 675510 real_backward_count 53312   7.892%\n",
      "lif layer 1 self.abs_max_v: 11360.5\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  0.605803/  0.999815, val:  78.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.8641%\n",
      "layer   2  Sparsity: 83.2663%\n",
      "layer   3  Sparsity: 75.0433%\n",
      "total_backward_count 685300 real_backward_count 53780   7.848%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  0.617601/  0.958998, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9133%\n",
      "layer   2  Sparsity: 83.3032%\n",
      "layer   3  Sparsity: 75.5368%\n",
      "total_backward_count 695090 real_backward_count 54287   7.810%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  0.607852/  0.988227, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.8942%\n",
      "layer   2  Sparsity: 83.6476%\n",
      "layer   3  Sparsity: 75.7565%\n",
      "total_backward_count 704880 real_backward_count 54743   7.766%\n",
      "fc layer 1 self.abs_max_out: 7550.0\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  0.599288/  0.939132, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8824%\n",
      "layer   2  Sparsity: 83.5575%\n",
      "layer   3  Sparsity: 75.4279%\n",
      "total_backward_count 714670 real_backward_count 55216   7.726%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  0.597037/  1.052136, val:  71.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.9112%\n",
      "layer   2  Sparsity: 83.5461%\n",
      "layer   3  Sparsity: 75.5402%\n",
      "total_backward_count 724460 real_backward_count 55664   7.684%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  0.627345/  1.005191, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8763%\n",
      "layer   2  Sparsity: 83.4852%\n",
      "layer   3  Sparsity: 75.1840%\n",
      "total_backward_count 734250 real_backward_count 56151   7.647%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  0.603018/  0.975476, val:  78.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8734%\n",
      "layer   2  Sparsity: 83.2210%\n",
      "layer   3  Sparsity: 74.6884%\n",
      "total_backward_count 744040 real_backward_count 56617   7.609%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  0.604842/  0.978224, val:  79.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.8700%\n",
      "layer   2  Sparsity: 83.2275%\n",
      "layer   3  Sparsity: 74.6761%\n",
      "total_backward_count 753830 real_backward_count 57053   7.568%\n",
      "fc layer 3 self.abs_max_out: 1277.0\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  0.613652/  1.062280, val:  73.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9181%\n",
      "layer   2  Sparsity: 83.1485%\n",
      "layer   3  Sparsity: 74.8249%\n",
      "total_backward_count 763620 real_backward_count 57497   7.530%\n",
      "fc layer 3 self.abs_max_out: 1292.0\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  0.593915/  0.986663, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8702%\n",
      "layer   2  Sparsity: 83.3017%\n",
      "layer   3  Sparsity: 74.8987%\n",
      "total_backward_count 773410 real_backward_count 57910   7.488%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  0.580734/  0.930885, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8883%\n",
      "layer   2  Sparsity: 83.4943%\n",
      "layer   3  Sparsity: 74.7116%\n",
      "total_backward_count 783200 real_backward_count 58313   7.445%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  0.586408/  0.975218, val:  80.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8710%\n",
      "layer   2  Sparsity: 83.4573%\n",
      "layer   3  Sparsity: 75.0081%\n",
      "total_backward_count 792990 real_backward_count 58762   7.410%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  0.578283/  0.963242, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9076%\n",
      "layer   2  Sparsity: 83.6093%\n",
      "layer   3  Sparsity: 74.4035%\n",
      "total_backward_count 802780 real_backward_count 59201   7.374%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  0.579542/  0.988103, val:  80.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9123%\n",
      "layer   2  Sparsity: 83.8443%\n",
      "layer   3  Sparsity: 74.9103%\n",
      "total_backward_count 812570 real_backward_count 59655   7.342%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  0.589136/  1.026023, val:  81.25%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.8907%\n",
      "layer   2  Sparsity: 83.7685%\n",
      "layer   3  Sparsity: 74.6587%\n",
      "total_backward_count 822360 real_backward_count 60104   7.309%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  0.584253/  0.994972, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9329%\n",
      "layer   2  Sparsity: 83.7992%\n",
      "layer   3  Sparsity: 74.8133%\n",
      "total_backward_count 832150 real_backward_count 60507   7.271%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  0.565431/  0.976502, val:  82.92%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8805%\n",
      "layer   2  Sparsity: 83.9006%\n",
      "layer   3  Sparsity: 75.3007%\n",
      "total_backward_count 841940 real_backward_count 60885   7.232%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  0.576128/  0.973683, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8905%\n",
      "layer   2  Sparsity: 83.7110%\n",
      "layer   3  Sparsity: 74.9062%\n",
      "total_backward_count 851730 real_backward_count 61253   7.192%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  0.582632/  0.999872, val:  78.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8788%\n",
      "layer   2  Sparsity: 83.9100%\n",
      "layer   3  Sparsity: 74.6725%\n",
      "total_backward_count 861520 real_backward_count 61667   7.158%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  0.579880/  0.971343, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8937%\n",
      "layer   2  Sparsity: 83.8521%\n",
      "layer   3  Sparsity: 74.7599%\n",
      "total_backward_count 871310 real_backward_count 62042   7.121%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  0.574232/  0.975379, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8999%\n",
      "layer   2  Sparsity: 83.8111%\n",
      "layer   3  Sparsity: 74.6180%\n",
      "total_backward_count 881100 real_backward_count 62471   7.090%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  0.576797/  1.006651, val:  80.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.8889%\n",
      "layer   2  Sparsity: 83.7112%\n",
      "layer   3  Sparsity: 74.7059%\n",
      "total_backward_count 890890 real_backward_count 62879   7.058%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  0.563082/  0.968413, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8971%\n",
      "layer   2  Sparsity: 83.8121%\n",
      "layer   3  Sparsity: 74.9859%\n",
      "total_backward_count 900680 real_backward_count 63263   7.024%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  0.540623/  0.936097, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9037%\n",
      "layer   2  Sparsity: 83.9706%\n",
      "layer   3  Sparsity: 75.0339%\n",
      "total_backward_count 910470 real_backward_count 63620   6.988%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  0.565272/  0.978695, val:  81.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9046%\n",
      "layer   2  Sparsity: 84.0409%\n",
      "layer   3  Sparsity: 75.1462%\n",
      "total_backward_count 920260 real_backward_count 64001   6.955%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  0.572431/  0.950796, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9100%\n",
      "layer   2  Sparsity: 83.6203%\n",
      "layer   3  Sparsity: 75.1669%\n",
      "total_backward_count 930050 real_backward_count 64428   6.927%\n",
      "lif layer 1 self.abs_max_v: 11371.5\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  0.555767/  0.990560, val:  73.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8642%\n",
      "layer   2  Sparsity: 83.5155%\n",
      "layer   3  Sparsity: 75.1450%\n",
      "total_backward_count 939840 real_backward_count 64783   6.893%\n",
      "fc layer 3 self.abs_max_out: 1293.0\n",
      "fc layer 3 self.abs_max_out: 1297.0\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  0.523397/  0.979229, val:  75.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8833%\n",
      "layer   2  Sparsity: 83.8176%\n",
      "layer   3  Sparsity: 75.0269%\n",
      "total_backward_count 949630 real_backward_count 65122   6.858%\n",
      "fc layer 3 self.abs_max_out: 1302.0\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  0.513820/  0.947167, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.8767%\n",
      "layer   2  Sparsity: 83.4360%\n",
      "layer   3  Sparsity: 75.8145%\n",
      "total_backward_count 959420 real_backward_count 65473   6.824%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  0.519062/  0.967516, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.8890%\n",
      "layer   2  Sparsity: 83.4682%\n",
      "layer   3  Sparsity: 75.5393%\n",
      "total_backward_count 969210 real_backward_count 65830   6.792%\n",
      "fc layer 3 self.abs_max_out: 1320.0\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  0.538671/  0.931763, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8924%\n",
      "layer   2  Sparsity: 83.3858%\n",
      "layer   3  Sparsity: 75.1026%\n",
      "total_backward_count 979000 real_backward_count 66193   6.761%\n",
      "lif layer 1 self.abs_max_v: 11496.5\n",
      "fc layer 3 self.abs_max_out: 1343.0\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  0.532261/  0.922349, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8479%\n",
      "layer   2  Sparsity: 83.2383%\n",
      "layer   3  Sparsity: 74.9162%\n",
      "total_backward_count 988790 real_backward_count 66572   6.733%\n",
      "lif layer 1 self.abs_max_v: 11709.0\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  0.535989/  0.943307, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8793%\n",
      "layer   2  Sparsity: 83.4699%\n",
      "layer   3  Sparsity: 75.1815%\n",
      "total_backward_count 998580 real_backward_count 66971   6.707%\n",
      "lif layer 1 self.abs_max_v: 11948.5\n",
      "fc layer 3 self.abs_max_out: 1348.0\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  0.534638/  0.942775, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8891%\n",
      "layer   2  Sparsity: 83.4938%\n",
      "layer   3  Sparsity: 75.1111%\n",
      "total_backward_count 1008370 real_backward_count 67318   6.676%\n",
      "lif layer 1 self.abs_max_v: 12102.0\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  0.520880/  0.947111, val:  78.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8605%\n",
      "layer   2  Sparsity: 83.5046%\n",
      "layer   3  Sparsity: 75.0128%\n",
      "total_backward_count 1018160 real_backward_count 67671   6.646%\n",
      "fc layer 3 self.abs_max_out: 1366.0\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  0.508665/  0.946429, val:  77.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8744%\n",
      "layer   2  Sparsity: 83.5189%\n",
      "layer   3  Sparsity: 75.1824%\n",
      "total_backward_count 1027950 real_backward_count 68008   6.616%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  0.512701/  0.928602, val:  80.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.8827%\n",
      "layer   2  Sparsity: 83.8547%\n",
      "layer   3  Sparsity: 75.4687%\n",
      "total_backward_count 1037740 real_backward_count 68335   6.585%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  0.522900/  0.940588, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8937%\n",
      "layer   2  Sparsity: 83.7062%\n",
      "layer   3  Sparsity: 75.4882%\n",
      "total_backward_count 1047530 real_backward_count 68689   6.557%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  0.522840/  0.939495, val:  81.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8814%\n",
      "layer   2  Sparsity: 83.3066%\n",
      "layer   3  Sparsity: 74.9415%\n",
      "total_backward_count 1057320 real_backward_count 69044   6.530%\n",
      "lif layer 1 self.abs_max_v: 12249.5\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  0.507504/  0.937657, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8855%\n",
      "layer   2  Sparsity: 83.2569%\n",
      "layer   3  Sparsity: 75.2900%\n",
      "total_backward_count 1067110 real_backward_count 69362   6.500%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  0.506209/  0.901254, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.8808%\n",
      "layer   2  Sparsity: 83.2045%\n",
      "layer   3  Sparsity: 75.2848%\n",
      "total_backward_count 1076900 real_backward_count 69686   6.471%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  0.495387/  0.892021, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.8860%\n",
      "layer   2  Sparsity: 83.2335%\n",
      "layer   3  Sparsity: 75.6654%\n",
      "total_backward_count 1086690 real_backward_count 69970   6.439%\n",
      "fc layer 1 self.abs_max_out: 7551.0\n",
      "lif layer 1 self.abs_max_v: 12383.5\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  0.501172/  0.880540, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.8762%\n",
      "layer   2  Sparsity: 83.3441%\n",
      "layer   3  Sparsity: 75.8422%\n",
      "total_backward_count 1096480 real_backward_count 70266   6.408%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  0.494602/  0.910985, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9048%\n",
      "layer   2  Sparsity: 83.4612%\n",
      "layer   3  Sparsity: 75.4400%\n",
      "total_backward_count 1106270 real_backward_count 70571   6.379%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  0.494451/  0.908897, val:  82.08%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8991%\n",
      "layer   2  Sparsity: 83.7897%\n",
      "layer   3  Sparsity: 75.4144%\n",
      "total_backward_count 1116060 real_backward_count 70889   6.352%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  0.491104/  0.918991, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9084%\n",
      "layer   2  Sparsity: 83.8488%\n",
      "layer   3  Sparsity: 75.3381%\n",
      "total_backward_count 1125850 real_backward_count 71209   6.325%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  0.477084/  0.875065, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8673%\n",
      "layer   2  Sparsity: 83.8199%\n",
      "layer   3  Sparsity: 75.1906%\n",
      "total_backward_count 1135640 real_backward_count 71539   6.299%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  0.481385/  0.945333, val:  77.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9039%\n",
      "layer   2  Sparsity: 83.8653%\n",
      "layer   3  Sparsity: 74.8556%\n",
      "total_backward_count 1145430 real_backward_count 71820   6.270%\n",
      "fc layer 3 self.abs_max_out: 1386.0\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  0.474325/  0.883759, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9069%\n",
      "layer   2  Sparsity: 83.9120%\n",
      "layer   3  Sparsity: 74.9934%\n",
      "total_backward_count 1155220 real_backward_count 72124   6.243%\n",
      "fc layer 3 self.abs_max_out: 1392.0\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  0.490645/  0.900396, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8597%\n",
      "layer   2  Sparsity: 83.9893%\n",
      "layer   3  Sparsity: 75.2745%\n",
      "total_backward_count 1165010 real_backward_count 72422   6.216%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  0.482714/  0.944462, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.8867%\n",
      "layer   2  Sparsity: 83.8257%\n",
      "layer   3  Sparsity: 74.7857%\n",
      "total_backward_count 1174800 real_backward_count 72688   6.187%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  0.488930/  0.949983, val:  80.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.9038%\n",
      "layer   2  Sparsity: 83.7865%\n",
      "layer   3  Sparsity: 75.2067%\n",
      "total_backward_count 1184590 real_backward_count 72980   6.161%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  0.476563/  0.931507, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8869%\n",
      "layer   2  Sparsity: 83.7658%\n",
      "layer   3  Sparsity: 75.6381%\n",
      "total_backward_count 1194380 real_backward_count 73271   6.135%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  0.462768/  0.927743, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.9193%\n",
      "layer   2  Sparsity: 83.5103%\n",
      "layer   3  Sparsity: 75.8482%\n",
      "total_backward_count 1204170 real_backward_count 73534   6.107%\n",
      "fc layer 3 self.abs_max_out: 1449.0\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  0.463145/  0.952343, val:  82.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.8695%\n",
      "layer   2  Sparsity: 83.4914%\n",
      "layer   3  Sparsity: 75.2023%\n",
      "total_backward_count 1213960 real_backward_count 73794   6.079%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  0.488024/  0.927807, val:  86.67%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8906%\n",
      "layer   2  Sparsity: 83.4748%\n",
      "layer   3  Sparsity: 75.1832%\n",
      "total_backward_count 1223750 real_backward_count 74109   6.056%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  0.479172/  0.903844, val:  80.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8585%\n",
      "layer   2  Sparsity: 83.8644%\n",
      "layer   3  Sparsity: 75.3292%\n",
      "total_backward_count 1233540 real_backward_count 74373   6.029%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  0.463057/  0.922166, val:  78.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.8583%\n",
      "layer   2  Sparsity: 83.9252%\n",
      "layer   3  Sparsity: 74.7546%\n",
      "total_backward_count 1243330 real_backward_count 74653   6.004%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  0.466621/  0.897696, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8933%\n",
      "layer   2  Sparsity: 84.0985%\n",
      "layer   3  Sparsity: 75.2659%\n",
      "total_backward_count 1253120 real_backward_count 74927   5.979%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  0.465563/  0.903910, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9037%\n",
      "layer   2  Sparsity: 83.8052%\n",
      "layer   3  Sparsity: 75.3897%\n",
      "total_backward_count 1262910 real_backward_count 75218   5.956%\n",
      "fc layer 1 self.abs_max_out: 7598.0\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  0.477372/  0.890866, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.8803%\n",
      "layer   2  Sparsity: 83.8571%\n",
      "layer   3  Sparsity: 75.3253%\n",
      "total_backward_count 1272700 real_backward_count 75488   5.931%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  0.469085/  0.927937, val:  80.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.8944%\n",
      "layer   2  Sparsity: 83.8719%\n",
      "layer   3  Sparsity: 75.1018%\n",
      "total_backward_count 1282490 real_backward_count 75775   5.908%\n",
      "lif layer 1 self.abs_max_v: 12469.5\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  0.461621/  0.920854, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8944%\n",
      "layer   2  Sparsity: 83.8109%\n",
      "layer   3  Sparsity: 75.1588%\n",
      "total_backward_count 1292280 real_backward_count 76036   5.884%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  0.443312/  0.920985, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8795%\n",
      "layer   2  Sparsity: 83.7401%\n",
      "layer   3  Sparsity: 75.5276%\n",
      "total_backward_count 1302070 real_backward_count 76278   5.858%\n",
      "fc layer 1 self.abs_max_out: 7621.0\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  0.444979/  0.897228, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8812%\n",
      "layer   2  Sparsity: 83.7163%\n",
      "layer   3  Sparsity: 75.8274%\n",
      "total_backward_count 1311860 real_backward_count 76519   5.833%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  0.444747/  0.901175, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9010%\n",
      "layer   2  Sparsity: 83.8474%\n",
      "layer   3  Sparsity: 75.7060%\n",
      "total_backward_count 1321650 real_backward_count 76772   5.809%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  0.456753/  0.893113, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8871%\n",
      "layer   2  Sparsity: 83.7524%\n",
      "layer   3  Sparsity: 75.4039%\n",
      "total_backward_count 1331440 real_backward_count 77033   5.786%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  0.441957/  0.904657, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8872%\n",
      "layer   2  Sparsity: 83.6635%\n",
      "layer   3  Sparsity: 75.2504%\n",
      "total_backward_count 1341230 real_backward_count 77273   5.761%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  0.438045/  0.889642, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9039%\n",
      "layer   2  Sparsity: 83.7849%\n",
      "layer   3  Sparsity: 75.2831%\n",
      "total_backward_count 1351020 real_backward_count 77514   5.737%\n",
      "fc layer 1 self.abs_max_out: 7625.0\n",
      "lif layer 1 self.abs_max_v: 12532.0\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  0.430942/  0.922026, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8977%\n",
      "layer   2  Sparsity: 84.0804%\n",
      "layer   3  Sparsity: 75.1267%\n",
      "total_backward_count 1360810 real_backward_count 77777   5.715%\n",
      "lif layer 1 self.abs_max_v: 12535.5\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  0.435213/  0.883048, val:  77.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9167%\n",
      "layer   2  Sparsity: 84.0258%\n",
      "layer   3  Sparsity: 74.6239%\n",
      "total_backward_count 1370600 real_backward_count 78053   5.695%\n",
      "fc layer 1 self.abs_max_out: 7677.0\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  0.421005/  0.902140, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9043%\n",
      "layer   2  Sparsity: 84.0252%\n",
      "layer   3  Sparsity: 75.0508%\n",
      "total_backward_count 1380390 real_backward_count 78298   5.672%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  0.411417/  0.875923, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.8957%\n",
      "layer   2  Sparsity: 84.0144%\n",
      "layer   3  Sparsity: 74.6740%\n",
      "total_backward_count 1390180 real_backward_count 78535   5.649%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  0.407656/  0.929346, val:  80.83%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8869%\n",
      "layer   2  Sparsity: 83.9159%\n",
      "layer   3  Sparsity: 74.5724%\n",
      "total_backward_count 1399970 real_backward_count 78772   5.627%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  0.420630/  0.893321, val:  81.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.9273%\n",
      "layer   2  Sparsity: 83.8096%\n",
      "layer   3  Sparsity: 74.2033%\n",
      "total_backward_count 1409760 real_backward_count 79004   5.604%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  0.418096/  0.880079, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8615%\n",
      "layer   2  Sparsity: 83.5716%\n",
      "layer   3  Sparsity: 74.3338%\n",
      "total_backward_count 1419550 real_backward_count 79203   5.579%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  0.427096/  0.878811, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8932%\n",
      "layer   2  Sparsity: 83.5527%\n",
      "layer   3  Sparsity: 74.4110%\n",
      "total_backward_count 1429340 real_backward_count 79430   5.557%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  0.419633/  0.922660, val:  80.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.9122%\n",
      "layer   2  Sparsity: 83.4059%\n",
      "layer   3  Sparsity: 74.5584%\n",
      "total_backward_count 1439130 real_backward_count 79676   5.536%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  0.418961/  0.917765, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9160%\n",
      "layer   2  Sparsity: 83.5888%\n",
      "layer   3  Sparsity: 74.5732%\n",
      "total_backward_count 1448920 real_backward_count 79898   5.514%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  0.417632/  0.929322, val:  81.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.9046%\n",
      "layer   2  Sparsity: 83.6510%\n",
      "layer   3  Sparsity: 74.6107%\n",
      "total_backward_count 1458710 real_backward_count 80136   5.494%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  0.417001/  0.915274, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8595%\n",
      "layer   2  Sparsity: 83.6727%\n",
      "layer   3  Sparsity: 74.0623%\n",
      "total_backward_count 1468500 real_backward_count 80367   5.473%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  0.422619/  0.905056, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8852%\n",
      "layer   2  Sparsity: 83.8572%\n",
      "layer   3  Sparsity: 73.8968%\n",
      "total_backward_count 1478290 real_backward_count 80585   5.451%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  0.412602/  0.912204, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9123%\n",
      "layer   2  Sparsity: 83.8893%\n",
      "layer   3  Sparsity: 74.0581%\n",
      "total_backward_count 1488080 real_backward_count 80844   5.433%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  0.398242/  0.908028, val:  80.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9009%\n",
      "layer   2  Sparsity: 83.6962%\n",
      "layer   3  Sparsity: 74.5235%\n",
      "total_backward_count 1497870 real_backward_count 81073   5.413%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  0.388586/  0.829982, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8576%\n",
      "layer   2  Sparsity: 83.6299%\n",
      "layer   3  Sparsity: 74.3204%\n",
      "total_backward_count 1507660 real_backward_count 81283   5.391%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  0.381313/  0.867255, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.8839%\n",
      "layer   2  Sparsity: 83.6927%\n",
      "layer   3  Sparsity: 74.1711%\n",
      "total_backward_count 1517450 real_backward_count 81482   5.370%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  0.379881/  0.890068, val:  80.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8991%\n",
      "layer   2  Sparsity: 83.7165%\n",
      "layer   3  Sparsity: 74.5434%\n",
      "total_backward_count 1527240 real_backward_count 81691   5.349%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  0.385419/  0.887574, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.9181%\n",
      "layer   2  Sparsity: 83.5614%\n",
      "layer   3  Sparsity: 74.6823%\n",
      "total_backward_count 1537030 real_backward_count 81921   5.330%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  0.374623/  0.812721, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8874%\n",
      "layer   2  Sparsity: 83.4852%\n",
      "layer   3  Sparsity: 75.1292%\n",
      "total_backward_count 1546820 real_backward_count 82134   5.310%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  0.380097/  0.854917, val:  82.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8939%\n",
      "layer   2  Sparsity: 83.8095%\n",
      "layer   3  Sparsity: 74.6006%\n",
      "total_backward_count 1556610 real_backward_count 82352   5.290%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  0.367352/  0.831544, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8721%\n",
      "layer   2  Sparsity: 83.8246%\n",
      "layer   3  Sparsity: 74.8080%\n",
      "total_backward_count 1566400 real_backward_count 82552   5.270%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  0.361955/  0.878777, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9109%\n",
      "layer   2  Sparsity: 83.8401%\n",
      "layer   3  Sparsity: 75.1202%\n",
      "total_backward_count 1576190 real_backward_count 82748   5.250%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  0.376358/  0.822354, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9069%\n",
      "layer   2  Sparsity: 83.9125%\n",
      "layer   3  Sparsity: 75.1657%\n",
      "total_backward_count 1585980 real_backward_count 82934   5.229%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  0.370809/  0.836654, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8706%\n",
      "layer   2  Sparsity: 83.9149%\n",
      "layer   3  Sparsity: 74.9674%\n",
      "total_backward_count 1595770 real_backward_count 83132   5.210%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  0.371897/  0.868441, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9215%\n",
      "layer   2  Sparsity: 84.0184%\n",
      "layer   3  Sparsity: 75.0017%\n",
      "total_backward_count 1605560 real_backward_count 83334   5.190%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  0.368966/  0.859957, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8789%\n",
      "layer   2  Sparsity: 83.9591%\n",
      "layer   3  Sparsity: 75.2647%\n",
      "total_backward_count 1615350 real_backward_count 83526   5.171%\n",
      "fc layer 3 self.abs_max_out: 1453.0\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  0.381514/  0.864708, val:  80.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9099%\n",
      "layer   2  Sparsity: 83.8265%\n",
      "layer   3  Sparsity: 74.7036%\n",
      "total_backward_count 1625140 real_backward_count 83743   5.153%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  0.377217/  0.884200, val:  81.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8795%\n",
      "layer   2  Sparsity: 83.9201%\n",
      "layer   3  Sparsity: 74.2851%\n",
      "total_backward_count 1634930 real_backward_count 83949   5.135%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  0.368636/  0.828204, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8787%\n",
      "layer   2  Sparsity: 84.0178%\n",
      "layer   3  Sparsity: 74.3870%\n",
      "total_backward_count 1644720 real_backward_count 84168   5.117%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  0.364007/  0.849988, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9029%\n",
      "layer   2  Sparsity: 83.9368%\n",
      "layer   3  Sparsity: 74.7248%\n",
      "total_backward_count 1654510 real_backward_count 84363   5.099%\n",
      "fc layer 3 self.abs_max_out: 1487.0\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.372392/  0.833968, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8792%\n",
      "layer   2  Sparsity: 84.0095%\n",
      "layer   3  Sparsity: 74.5126%\n",
      "total_backward_count 1664300 real_backward_count 84560   5.081%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.360966/  0.826645, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9216%\n",
      "layer   2  Sparsity: 84.1055%\n",
      "layer   3  Sparsity: 74.5838%\n",
      "total_backward_count 1674090 real_backward_count 84721   5.061%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  0.359148/  0.839753, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9079%\n",
      "layer   2  Sparsity: 83.9824%\n",
      "layer   3  Sparsity: 74.4368%\n",
      "total_backward_count 1683880 real_backward_count 84916   5.043%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  0.367855/  0.850398, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9006%\n",
      "layer   2  Sparsity: 83.9569%\n",
      "layer   3  Sparsity: 75.0832%\n",
      "total_backward_count 1693670 real_backward_count 85074   5.023%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.362634/  0.867412, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8851%\n",
      "layer   2  Sparsity: 83.9554%\n",
      "layer   3  Sparsity: 75.2281%\n",
      "total_backward_count 1703460 real_backward_count 85269   5.006%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  0.360259/  0.847354, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9155%\n",
      "layer   2  Sparsity: 84.1090%\n",
      "layer   3  Sparsity: 75.0387%\n",
      "total_backward_count 1713250 real_backward_count 85451   4.988%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  0.353929/  0.843278, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8980%\n",
      "layer   2  Sparsity: 84.2417%\n",
      "layer   3  Sparsity: 74.7677%\n",
      "total_backward_count 1723040 real_backward_count 85637   4.970%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  0.355394/  0.816524, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8753%\n",
      "layer   2  Sparsity: 84.0237%\n",
      "layer   3  Sparsity: 74.6691%\n",
      "total_backward_count 1732830 real_backward_count 85800   4.951%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  0.357348/  0.830341, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8859%\n",
      "layer   2  Sparsity: 84.0792%\n",
      "layer   3  Sparsity: 74.5720%\n",
      "total_backward_count 1742620 real_backward_count 85990   4.935%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  0.363512/  0.861672, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8907%\n",
      "layer   2  Sparsity: 84.3099%\n",
      "layer   3  Sparsity: 74.1703%\n",
      "total_backward_count 1752410 real_backward_count 86198   4.919%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  0.370826/  0.837749, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.8663%\n",
      "layer   2  Sparsity: 84.3636%\n",
      "layer   3  Sparsity: 74.6591%\n",
      "total_backward_count 1762200 real_backward_count 86388   4.902%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  0.357033/  0.832268, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9052%\n",
      "layer   2  Sparsity: 84.3971%\n",
      "layer   3  Sparsity: 74.6703%\n",
      "total_backward_count 1771990 real_backward_count 86531   4.883%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  0.360689/  0.826090, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.8665%\n",
      "layer   2  Sparsity: 84.4117%\n",
      "layer   3  Sparsity: 74.7479%\n",
      "total_backward_count 1781780 real_backward_count 86716   4.867%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  0.363971/  0.817422, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.8737%\n",
      "layer   2  Sparsity: 84.2912%\n",
      "layer   3  Sparsity: 74.7020%\n",
      "total_backward_count 1791570 real_backward_count 86917   4.851%\n",
      "fc layer 3 self.abs_max_out: 1511.0\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  0.371101/  0.852822, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8865%\n",
      "layer   2  Sparsity: 84.5242%\n",
      "layer   3  Sparsity: 74.5998%\n",
      "total_backward_count 1801360 real_backward_count 87124   4.837%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  0.365215/  0.856659, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8879%\n",
      "layer   2  Sparsity: 84.3792%\n",
      "layer   3  Sparsity: 74.6990%\n",
      "total_backward_count 1811150 real_backward_count 87323   4.821%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  0.366029/  0.866303, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.9297%\n",
      "layer   2  Sparsity: 84.4582%\n",
      "layer   3  Sparsity: 74.4820%\n",
      "total_backward_count 1820940 real_backward_count 87522   4.806%\n",
      "fc layer 3 self.abs_max_out: 1517.0\n",
      "fc layer 3 self.abs_max_out: 1537.0\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  0.364201/  0.854340, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9149%\n",
      "layer   2  Sparsity: 84.5497%\n",
      "layer   3  Sparsity: 74.5403%\n",
      "total_backward_count 1830730 real_backward_count 87709   4.791%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  0.358253/  0.870526, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.8983%\n",
      "layer   2  Sparsity: 84.3687%\n",
      "layer   3  Sparsity: 74.3942%\n",
      "total_backward_count 1840520 real_backward_count 87884   4.775%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  0.362742/  0.854063, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9094%\n",
      "layer   2  Sparsity: 84.3226%\n",
      "layer   3  Sparsity: 74.1509%\n",
      "total_backward_count 1850310 real_backward_count 88074   4.760%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  0.359040/  0.838433, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9075%\n",
      "layer   2  Sparsity: 84.3772%\n",
      "layer   3  Sparsity: 74.2010%\n",
      "total_backward_count 1860100 real_backward_count 88250   4.744%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  0.366977/  0.851201, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.8845%\n",
      "layer   2  Sparsity: 84.5025%\n",
      "layer   3  Sparsity: 74.0509%\n",
      "total_backward_count 1869890 real_backward_count 88449   4.730%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  0.342289/  0.850720, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.9140%\n",
      "layer   2  Sparsity: 84.4794%\n",
      "layer   3  Sparsity: 74.3953%\n",
      "total_backward_count 1879680 real_backward_count 88589   4.713%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  0.341543/  0.851769, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9023%\n",
      "layer   2  Sparsity: 84.4837%\n",
      "layer   3  Sparsity: 74.9299%\n",
      "total_backward_count 1889470 real_backward_count 88709   4.695%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  0.342025/  0.814019, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9018%\n",
      "layer   2  Sparsity: 84.5270%\n",
      "layer   3  Sparsity: 75.0278%\n",
      "total_backward_count 1899260 real_backward_count 88850   4.678%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  0.340353/  0.830491, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.8406%\n",
      "layer   2  Sparsity: 84.5487%\n",
      "layer   3  Sparsity: 74.8661%\n",
      "total_backward_count 1909050 real_backward_count 89033   4.664%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  0.348226/  0.857918, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.9008%\n",
      "layer   2  Sparsity: 84.5804%\n",
      "layer   3  Sparsity: 75.1325%\n",
      "total_backward_count 1918840 real_backward_count 89202   4.649%\n",
      "fc layer 2 self.abs_max_out: 2263.0\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  0.353284/  0.860757, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8955%\n",
      "layer   2  Sparsity: 84.6550%\n",
      "layer   3  Sparsity: 75.1696%\n",
      "total_backward_count 1928630 real_backward_count 89395   4.635%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  0.367388/  0.875841, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8744%\n",
      "layer   2  Sparsity: 84.5304%\n",
      "layer   3  Sparsity: 74.9276%\n",
      "total_backward_count 1938420 real_backward_count 89607   4.623%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.362813/  0.884229, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.8984%\n",
      "layer   2  Sparsity: 84.4895%\n",
      "layer   3  Sparsity: 74.8361%\n",
      "total_backward_count 1948210 real_backward_count 89771   4.608%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.358589/  0.855988, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.8956%\n",
      "layer   2  Sparsity: 84.5030%\n",
      "layer   3  Sparsity: 75.0626%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5b4e0816ce4061a2d316f772436d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñÉ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.35859</td></tr><tr><td>val_acc_best</td><td>0.87917</td></tr><tr><td>val_acc_now</td><td>0.87083</td></tr><tr><td>val_loss</td><td>0.85599</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">robust-sweep-15</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ipdugg4b' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ipdugg4b</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_104947-ipdugg4b/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2v82vmb3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_150643-2v82vmb3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2v82vmb3' target=\"_blank\">golden-sweep-19</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2v82vmb3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2v82vmb3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': '20251118_150652_854', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 15, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=15, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=15, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 282\n",
      "fc layer 1 self.abs_max_out: 219.0\n",
      "lif layer 1 self.abs_max_v: 219.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 750.0\n",
      "lif layer 2 self.abs_max_v: 750.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 346.0\n",
      "fc layer 1 self.abs_max_out: 295.0\n",
      "lif layer 1 self.abs_max_v: 364.0\n",
      "fc layer 2 self.abs_max_out: 1100.0\n",
      "lif layer 2 self.abs_max_v: 1398.5\n",
      "fc layer 1 self.abs_max_out: 323.0\n",
      "lif layer 1 self.abs_max_v: 503.0\n",
      "lif layer 2 self.abs_max_v: 1464.5\n",
      "fc layer 1 self.abs_max_out: 352.0\n",
      "lif layer 2 self.abs_max_v: 1496.5\n",
      "fc layer 3 self.abs_max_out: 442.0\n",
      "fc layer 3 self.abs_max_out: 500.0\n",
      "fc layer 1 self.abs_max_out: 434.0\n",
      "lif layer 1 self.abs_max_v: 556.5\n",
      "fc layer 3 self.abs_max_out: 647.0\n",
      "lif layer 1 self.abs_max_v: 621.5\n",
      "lif layer 2 self.abs_max_v: 1542.5\n",
      "lif layer 2 self.abs_max_v: 1646.5\n",
      "smallest_now_T updated: 254\n",
      "fc layer 1 self.abs_max_out: 577.0\n",
      "fc layer 1 self.abs_max_out: 590.0\n",
      "lif layer 1 self.abs_max_v: 673.0\n",
      "fc layer 3 self.abs_max_out: 827.0\n",
      "lif layer 1 self.abs_max_v: 704.5\n",
      "fc layer 2 self.abs_max_out: 1265.0\n",
      "lif layer 2 self.abs_max_v: 1739.0\n",
      "smallest_now_T updated: 193\n",
      "fc layer 3 self.abs_max_out: 845.0\n",
      "fc layer 3 self.abs_max_out: 876.0\n",
      "fc layer 1 self.abs_max_out: 696.0\n",
      "lif layer 1 self.abs_max_v: 756.0\n",
      "fc layer 3 self.abs_max_out: 915.0\n",
      "fc layer 1 self.abs_max_out: 724.0\n",
      "lif layer 1 self.abs_max_v: 769.0\n",
      "fc layer 1 self.abs_max_out: 960.0\n",
      "lif layer 1 self.abs_max_v: 984.0\n",
      "lif layer 2 self.abs_max_v: 1876.0\n",
      "fc layer 2 self.abs_max_out: 1304.0\n",
      "lif layer 2 self.abs_max_v: 2064.5\n",
      "lif layer 2 self.abs_max_v: 2137.5\n",
      "lif layer 2 self.abs_max_v: 2259.0\n",
      "lif layer 1 self.abs_max_v: 1003.0\n",
      "fc layer 2 self.abs_max_out: 1446.0\n",
      "smallest_now_T updated: 163\n",
      "lif layer 2 self.abs_max_v: 2261.5\n",
      "fc layer 1 self.abs_max_out: 1084.0\n",
      "lif layer 1 self.abs_max_v: 1084.0\n",
      "lif layer 1 self.abs_max_v: 1238.5\n",
      "fc layer 1 self.abs_max_out: 1188.0\n",
      "lif layer 1 self.abs_max_v: 1479.5\n",
      "lif layer 1 self.abs_max_v: 1540.5\n",
      "lif layer 2 self.abs_max_v: 2316.0\n",
      "fc layer 2 self.abs_max_out: 1481.0\n",
      "smallest_now_T updated: 150\n",
      "fc layer 2 self.abs_max_out: 1553.0\n",
      "lif layer 2 self.abs_max_v: 2332.0\n",
      "lif layer 2 self.abs_max_v: 2417.0\n",
      "fc layer 2 self.abs_max_out: 1555.0\n",
      "lif layer 2 self.abs_max_v: 2494.5\n",
      "lif layer 2 self.abs_max_v: 2694.5\n",
      "fc layer 2 self.abs_max_out: 1571.0\n",
      "fc layer 2 self.abs_max_out: 1684.0\n",
      "lif layer 2 self.abs_max_v: 2890.5\n",
      "lif layer 2 self.abs_max_v: 2907.5\n",
      "lif layer 2 self.abs_max_v: 3048.5\n",
      "fc layer 2 self.abs_max_out: 1754.0\n",
      "fc layer 1 self.abs_max_out: 1209.0\n",
      "smallest_now_T updated: 135\n",
      "lif layer 2 self.abs_max_v: 3055.5\n",
      "lif layer 2 self.abs_max_v: 3170.5\n",
      "fc layer 2 self.abs_max_out: 1819.0\n",
      "fc layer 2 self.abs_max_out: 1826.0\n",
      "lif layer 2 self.abs_max_v: 3385.0\n",
      "fc layer 2 self.abs_max_out: 1921.0\n",
      "lif layer 2 self.abs_max_v: 3387.5\n",
      "lif layer 2 self.abs_max_v: 3554.0\n",
      "fc layer 1 self.abs_max_out: 1318.0\n",
      "lif layer 1 self.abs_max_v: 1835.5\n",
      "fc layer 3 self.abs_max_out: 938.0\n",
      "fc layer 3 self.abs_max_out: 997.0\n",
      "fc layer 1 self.abs_max_out: 1473.0\n",
      "lif layer 1 self.abs_max_v: 1842.5\n",
      "lif layer 1 self.abs_max_v: 2239.5\n",
      "fc layer 1 self.abs_max_out: 1627.0\n",
      "smallest_now_T updated: 116\n",
      "smallest_now_T updated: 90\n",
      "fc layer 1 self.abs_max_out: 1648.0\n",
      "fc layer 3 self.abs_max_out: 1044.0\n",
      "fc layer 3 self.abs_max_out: 1095.0\n",
      "fc layer 3 self.abs_max_out: 1110.0\n",
      "fc layer 3 self.abs_max_out: 1162.0\n",
      "fc layer 3 self.abs_max_out: 1168.0\n",
      "fc layer 3 self.abs_max_out: 1227.0\n",
      "fc layer 1 self.abs_max_out: 1735.0\n",
      "lif layer 1 self.abs_max_v: 2326.0\n",
      "fc layer 2 self.abs_max_out: 1925.0\n",
      "lif layer 1 self.abs_max_v: 2461.5\n",
      "lif layer 1 self.abs_max_v: 2640.0\n",
      "fc layer 1 self.abs_max_out: 1903.0\n",
      "fc layer 2 self.abs_max_out: 1985.0\n",
      "fc layer 2 self.abs_max_out: 2065.0\n",
      "fc layer 2 self.abs_max_out: 2132.0\n",
      "fc layer 3 self.abs_max_out: 1249.0\n",
      "fc layer 2 self.abs_max_out: 2175.0\n",
      "lif layer 2 self.abs_max_v: 3590.0\n",
      "lif layer 2 self.abs_max_v: 3617.5\n",
      "fc layer 2 self.abs_max_out: 2201.0\n",
      "lif layer 2 self.abs_max_v: 3857.0\n",
      "lif layer 2 self.abs_max_v: 3929.5\n",
      "fc layer 1 self.abs_max_out: 1926.0\n",
      "fc layer 1 self.abs_max_out: 2164.0\n",
      "lif layer 1 self.abs_max_v: 2896.5\n",
      "lif layer 1 self.abs_max_v: 3046.5\n",
      "lif layer 1 self.abs_max_v: 3058.5\n",
      "fc layer 2 self.abs_max_out: 2298.0\n",
      "fc layer 3 self.abs_max_out: 1267.0\n",
      "fc layer 3 self.abs_max_out: 1335.0\n",
      "lif layer 1 self.abs_max_v: 3152.0\n",
      "lif layer 1 self.abs_max_v: 3214.0\n",
      "lif layer 1 self.abs_max_v: 3762.0\n",
      "smallest_now_T_val updated: 262\n",
      "smallest_now_T_val updated: 217\n",
      "smallest_now_T_val updated: 213\n",
      "smallest_now_T_val updated: 209\n",
      "smallest_now_T_val updated: 174\n",
      "smallest_now_T_val updated: 63\n",
      "fc layer 1 self.abs_max_out: 2368.0\n",
      "fc layer 1 self.abs_max_out: 2530.0\n",
      "fc layer 1 self.abs_max_out: 2561.0\n",
      "lif layer 1 self.abs_max_v: 4220.5\n",
      "fc layer 1 self.abs_max_out: 2911.0\n",
      "lif layer 1 self.abs_max_v: 5021.5\n",
      "lif layer 1 self.abs_max_v: 5225.0\n",
      "lif layer 1 self.abs_max_v: 5363.5\n",
      "lif layer 2 self.abs_max_v: 3931.0\n",
      "lif layer 2 self.abs_max_v: 4043.5\n",
      "fc layer 2 self.abs_max_out: 2478.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.374402/  1.876147, val:  30.42%, val_best:  30.42%, tr:  99.49%, tr_best:  99.49%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0698%\n",
      "layer   2  Sparsity: 70.1538%\n",
      "layer   3  Sparsity: 63.1051%\n",
      "total_backward_count 9790 real_backward_count 1429  14.597%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 1354.0\n",
      "fc layer 3 self.abs_max_out: 1364.0\n",
      "fc layer 3 self.abs_max_out: 1412.0\n",
      "fc layer 3 self.abs_max_out: 1419.0\n",
      "fc layer 3 self.abs_max_out: 1452.0\n",
      "fc layer 3 self.abs_max_out: 1551.0\n",
      "fc layer 3 self.abs_max_out: 1569.0\n",
      "fc layer 3 self.abs_max_out: 1682.0\n",
      "fc layer 3 self.abs_max_out: 1704.0\n",
      "fc layer 3 self.abs_max_out: 1723.0\n",
      "lif layer 2 self.abs_max_v: 4136.0\n",
      "lif layer 2 self.abs_max_v: 4437.0\n",
      "fc layer 3 self.abs_max_out: 1729.0\n",
      "fc layer 3 self.abs_max_out: 1822.0\n",
      "fc layer 1 self.abs_max_out: 3057.0\n",
      "fc layer 1 self.abs_max_out: 3169.0\n",
      "lif layer 1 self.abs_max_v: 5670.5\n",
      "fc layer 1 self.abs_max_out: 3588.0\n",
      "lif layer 1 self.abs_max_v: 6052.5\n",
      "lif layer 1 self.abs_max_v: 6434.5\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.236061/  1.813023, val:  35.83%, val_best:  35.83%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0856%\n",
      "layer   2  Sparsity: 71.5180%\n",
      "layer   3  Sparsity: 64.3370%\n",
      "total_backward_count 19580 real_backward_count 2711  13.846%\n",
      "fc layer 2 self.abs_max_out: 2483.0\n",
      "lif layer 2 self.abs_max_v: 4450.0\n",
      "fc layer 2 self.abs_max_out: 2535.0\n",
      "lif layer 2 self.abs_max_v: 4593.0\n",
      "lif layer 2 self.abs_max_v: 4685.5\n",
      "lif layer 2 self.abs_max_v: 4688.5\n",
      "fc layer 2 self.abs_max_out: 2580.0\n",
      "lif layer 2 self.abs_max_v: 4704.5\n",
      "fc layer 2 self.abs_max_out: 2607.0\n",
      "fc layer 2 self.abs_max_out: 2702.0\n",
      "lif layer 2 self.abs_max_v: 4852.0\n",
      "lif layer 2 self.abs_max_v: 5002.0\n",
      "lif layer 2 self.abs_max_v: 5015.0\n",
      "fc layer 2 self.abs_max_out: 2775.0\n",
      "lif layer 2 self.abs_max_v: 5282.5\n",
      "lif layer 2 self.abs_max_v: 5317.5\n",
      "fc layer 2 self.abs_max_out: 2856.0\n",
      "fc layer 2 self.abs_max_out: 2935.0\n",
      "lif layer 2 self.abs_max_v: 5327.0\n",
      "lif layer 2 self.abs_max_v: 5509.5\n",
      "lif layer 2 self.abs_max_v: 5676.0\n",
      "fc layer 2 self.abs_max_out: 3065.0\n",
      "fc layer 2 self.abs_max_out: 3148.0\n",
      "lif layer 2 self.abs_max_v: 5686.0\n",
      "lif layer 2 self.abs_max_v: 5788.0\n",
      "lif layer 2 self.abs_max_v: 5808.0\n",
      "lif layer 2 self.abs_max_v: 5828.0\n",
      "lif layer 2 self.abs_max_v: 5930.5\n",
      "fc layer 1 self.abs_max_out: 3945.0\n",
      "lif layer 1 self.abs_max_v: 6741.5\n",
      "fc layer 1 self.abs_max_out: 4141.0\n",
      "lif layer 1 self.abs_max_v: 7512.0\n",
      "fc layer 1 self.abs_max_out: 4222.0\n",
      "lif layer 1 self.abs_max_v: 7975.0\n",
      "fc layer 2 self.abs_max_out: 3205.0\n",
      "fc layer 2 self.abs_max_out: 3271.0\n",
      "lif layer 2 self.abs_max_v: 6026.5\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.185950/  1.778767, val:  30.00%, val_best:  35.83%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0819%\n",
      "layer   2  Sparsity: 71.9301%\n",
      "layer   3  Sparsity: 65.1636%\n",
      "total_backward_count 29370 real_backward_count 3975  13.534%\n",
      "fc layer 3 self.abs_max_out: 1835.0\n",
      "fc layer 1 self.abs_max_out: 4259.0\n",
      "fc layer 1 self.abs_max_out: 4648.0\n",
      "lif layer 1 self.abs_max_v: 8337.0\n",
      "lif layer 1 self.abs_max_v: 8529.0\n",
      "fc layer 1 self.abs_max_out: 4700.0\n",
      "lif layer 1 self.abs_max_v: 8964.5\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.202277/  1.728232, val:  36.67%, val_best:  36.67%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0551%\n",
      "layer   2  Sparsity: 72.8247%\n",
      "layer   3  Sparsity: 66.8593%\n",
      "total_backward_count 39160 real_backward_count 5261  13.435%\n",
      "fc layer 3 self.abs_max_out: 1908.0\n",
      "fc layer 3 self.abs_max_out: 2059.0\n",
      "fc layer 3 self.abs_max_out: 2137.0\n",
      "fc layer 2 self.abs_max_out: 3276.0\n",
      "fc layer 2 self.abs_max_out: 3279.0\n",
      "fc layer 2 self.abs_max_out: 3461.0\n",
      "lif layer 2 self.abs_max_v: 6028.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.174436/  1.660193, val:  43.75%, val_best:  43.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0631%\n",
      "layer   2  Sparsity: 73.3917%\n",
      "layer   3  Sparsity: 67.8882%\n",
      "total_backward_count 48950 real_backward_count 6529  13.338%\n",
      "fc layer 3 self.abs_max_out: 2185.0\n",
      "fc layer 3 self.abs_max_out: 2194.0\n",
      "fc layer 1 self.abs_max_out: 4705.0\n",
      "fc layer 1 self.abs_max_out: 4996.0\n",
      "lif layer 1 self.abs_max_v: 9395.5\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.176706/  1.683951, val:  41.25%, val_best:  43.75%, tr:  99.59%, tr_best:  99.80%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0577%\n",
      "layer   2  Sparsity: 73.2568%\n",
      "layer   3  Sparsity: 69.0765%\n",
      "total_backward_count 58740 real_backward_count 7803  13.284%\n",
      "fc layer 3 self.abs_max_out: 2238.0\n",
      "fc layer 3 self.abs_max_out: 2260.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.168283/  1.693300, val:  42.08%, val_best:  43.75%, tr:  99.59%, tr_best:  99.80%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0692%\n",
      "layer   2  Sparsity: 73.2465%\n",
      "layer   3  Sparsity: 69.8556%\n",
      "total_backward_count 68530 real_backward_count 9079  13.248%\n",
      "fc layer 3 self.abs_max_out: 2298.0\n",
      "fc layer 2 self.abs_max_out: 3500.0\n",
      "lif layer 2 self.abs_max_v: 6050.5\n",
      "lif layer 2 self.abs_max_v: 6376.0\n",
      "lif layer 2 self.abs_max_v: 6502.0\n",
      "fc layer 2 self.abs_max_out: 3849.0\n",
      "lif layer 2 self.abs_max_v: 6567.5\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.158992/  1.646311, val:  44.17%, val_best:  44.17%, tr:  99.59%, tr_best:  99.80%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0786%\n",
      "layer   2  Sparsity: 72.7386%\n",
      "layer   3  Sparsity: 69.7726%\n",
      "total_backward_count 78320 real_backward_count 10294  13.144%\n",
      "lif layer 2 self.abs_max_v: 6668.0\n",
      "fc layer 3 self.abs_max_out: 2321.0\n",
      "fc layer 3 self.abs_max_out: 2341.0\n",
      "lif layer 2 self.abs_max_v: 6722.0\n",
      "lif layer 2 self.abs_max_v: 6905.5\n",
      "fc layer 1 self.abs_max_out: 5106.0\n",
      "fc layer 1 self.abs_max_out: 5403.0\n",
      "lif layer 1 self.abs_max_v: 9819.0\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.177284/  1.700500, val:  36.67%, val_best:  44.17%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0581%\n",
      "layer   2  Sparsity: 72.5275%\n",
      "layer   3  Sparsity: 70.6952%\n",
      "total_backward_count 88110 real_backward_count 11534  13.090%\n",
      "fc layer 2 self.abs_max_out: 3866.0\n",
      "lif layer 2 self.abs_max_v: 6931.0\n",
      "lif layer 2 self.abs_max_v: 7037.0\n",
      "fc layer 1 self.abs_max_out: 5818.0\n",
      "lif layer 1 self.abs_max_v: 10486.5\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.159076/  1.669316, val:  46.25%, val_best:  46.25%, tr:  99.69%, tr_best:  99.80%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0972%\n",
      "layer   2  Sparsity: 72.3053%\n",
      "layer   3  Sparsity: 70.5411%\n",
      "total_backward_count 97900 real_backward_count 12715  12.988%\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.146668/  1.656645, val:  41.67%, val_best:  46.25%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0721%\n",
      "layer   2  Sparsity: 72.9233%\n",
      "layer   3  Sparsity: 70.8041%\n",
      "total_backward_count 107690 real_backward_count 13910  12.917%\n",
      "fc layer 2 self.abs_max_out: 4107.0\n",
      "fc layer 1 self.abs_max_out: 6048.0\n",
      "lif layer 1 self.abs_max_v: 10902.0\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.145418/  1.630978, val:  46.25%, val_best:  46.25%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0706%\n",
      "layer   2  Sparsity: 73.7444%\n",
      "layer   3  Sparsity: 72.0832%\n",
      "total_backward_count 117480 real_backward_count 15096  12.850%\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.133275/  1.626254, val:  42.92%, val_best:  46.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0765%\n",
      "layer   2  Sparsity: 73.9218%\n",
      "layer   3  Sparsity: 72.3201%\n",
      "total_backward_count 127270 real_backward_count 16256  12.773%\n",
      "fc layer 2 self.abs_max_out: 4111.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.178860/  1.647446, val:  46.67%, val_best:  46.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0948%\n",
      "layer   2  Sparsity: 73.7152%\n",
      "layer   3  Sparsity: 73.5123%\n",
      "total_backward_count 137060 real_backward_count 17441  12.725%\n",
      "fc layer 1 self.abs_max_out: 6303.0\n",
      "lif layer 1 self.abs_max_v: 11246.5\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.163762/  1.595100, val:  51.67%, val_best:  51.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0816%\n",
      "layer   2  Sparsity: 74.0575%\n",
      "layer   3  Sparsity: 73.2591%\n",
      "total_backward_count 146850 real_backward_count 18586  12.656%\n",
      "fc layer 1 self.abs_max_out: 6759.0\n",
      "lif layer 1 self.abs_max_v: 12134.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.151903/  1.658446, val:  51.25%, val_best:  51.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0753%\n",
      "layer   2  Sparsity: 73.8042%\n",
      "layer   3  Sparsity: 73.2121%\n",
      "total_backward_count 156640 real_backward_count 19739  12.602%\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.121847/  1.608084, val:  44.58%, val_best:  51.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0733%\n",
      "layer   2  Sparsity: 73.9837%\n",
      "layer   3  Sparsity: 72.9751%\n",
      "total_backward_count 166430 real_backward_count 20828  12.515%\n",
      "fc layer 3 self.abs_max_out: 2351.0\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.137529/  1.561175, val:  55.42%, val_best:  55.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0634%\n",
      "layer   2  Sparsity: 74.0711%\n",
      "layer   3  Sparsity: 73.4355%\n",
      "total_backward_count 176220 real_backward_count 21936  12.448%\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.151036/  1.534525, val:  59.58%, val_best:  59.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0710%\n",
      "layer   2  Sparsity: 73.6926%\n",
      "layer   3  Sparsity: 73.6205%\n",
      "total_backward_count 186010 real_backward_count 23085  12.411%\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.101362/  1.496100, val:  57.50%, val_best:  59.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0701%\n",
      "layer   2  Sparsity: 74.1493%\n",
      "layer   3  Sparsity: 72.8727%\n",
      "total_backward_count 195800 real_backward_count 24157  12.338%\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.094202/  1.582826, val:  54.17%, val_best:  59.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0751%\n",
      "layer   2  Sparsity: 74.0598%\n",
      "layer   3  Sparsity: 72.6152%\n",
      "total_backward_count 205590 real_backward_count 25191  12.253%\n",
      "fc layer 1 self.abs_max_out: 6978.0\n",
      "lif layer 1 self.abs_max_v: 12968.0\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.100007/  1.496445, val:  66.25%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0747%\n",
      "layer   2  Sparsity: 73.9506%\n",
      "layer   3  Sparsity: 72.8198%\n",
      "total_backward_count 215380 real_backward_count 26290  12.206%\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.114822/  1.493163, val:  67.92%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0931%\n",
      "layer   2  Sparsity: 73.9099%\n",
      "layer   3  Sparsity: 73.0476%\n",
      "total_backward_count 225170 real_backward_count 27337  12.141%\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.076082/  1.506343, val:  57.08%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0627%\n",
      "layer   2  Sparsity: 74.3003%\n",
      "layer   3  Sparsity: 72.6473%\n",
      "total_backward_count 234960 real_backward_count 28381  12.079%\n",
      "fc layer 3 self.abs_max_out: 2434.0\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.068981/  1.516049, val:  45.83%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0713%\n",
      "layer   2  Sparsity: 74.3562%\n",
      "layer   3  Sparsity: 72.3607%\n",
      "total_backward_count 244750 real_backward_count 29400  12.012%\n",
      "fc layer 1 self.abs_max_out: 8234.0\n",
      "lif layer 1 self.abs_max_v: 14348.0\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.068775/  1.401663, val:  77.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0807%\n",
      "layer   2  Sparsity: 73.9595%\n",
      "layer   3  Sparsity: 72.1045%\n",
      "total_backward_count 254540 real_backward_count 30449  11.962%\n",
      "lif layer 2 self.abs_max_v: 7079.0\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.035822/  1.392983, val:  65.00%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0730%\n",
      "layer   2  Sparsity: 73.7550%\n",
      "layer   3  Sparsity: 71.0228%\n",
      "total_backward_count 264330 real_backward_count 31462  11.903%\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.018000/  1.338751, val:  75.83%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0761%\n",
      "layer   2  Sparsity: 73.6723%\n",
      "layer   3  Sparsity: 71.3281%\n",
      "total_backward_count 274120 real_backward_count 32480  11.849%\n",
      "fc layer 3 self.abs_max_out: 2453.0\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  0.986501/  1.438292, val:  54.58%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0780%\n",
      "layer   2  Sparsity: 73.3584%\n",
      "layer   3  Sparsity: 71.0582%\n",
      "total_backward_count 283910 real_backward_count 33454  11.783%\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.008779/  1.541232, val:  40.83%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0798%\n",
      "layer   2  Sparsity: 73.7720%\n",
      "layer   3  Sparsity: 71.1283%\n",
      "total_backward_count 293700 real_backward_count 34466  11.735%\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.024092/  1.375831, val:  69.58%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0976%\n",
      "layer   2  Sparsity: 73.4900%\n",
      "layer   3  Sparsity: 71.7185%\n",
      "total_backward_count 303490 real_backward_count 35439  11.677%\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.013278/  1.455598, val:  58.33%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0871%\n",
      "layer   2  Sparsity: 73.2207%\n",
      "layer   3  Sparsity: 71.5819%\n",
      "total_backward_count 313280 real_backward_count 36370  11.609%\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  0.985464/  1.347226, val:  63.75%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0866%\n",
      "layer   2  Sparsity: 73.5029%\n",
      "layer   3  Sparsity: 72.0259%\n",
      "total_backward_count 323070 real_backward_count 37298  11.545%\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  0.994367/  1.337375, val:  75.83%, val_best:  77.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0732%\n",
      "layer   2  Sparsity: 73.5126%\n",
      "layer   3  Sparsity: 71.5562%\n",
      "total_backward_count 332860 real_backward_count 38258  11.494%\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  0.952251/  1.390277, val:  54.58%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0829%\n",
      "layer   2  Sparsity: 73.3903%\n",
      "layer   3  Sparsity: 71.1173%\n",
      "total_backward_count 342650 real_backward_count 39176  11.433%\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  0.932569/  1.271598, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0815%\n",
      "layer   2  Sparsity: 73.5395%\n",
      "layer   3  Sparsity: 70.5559%\n",
      "total_backward_count 352440 real_backward_count 40084  11.373%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  0.902768/  1.281095, val:  70.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0626%\n",
      "layer   2  Sparsity: 73.8805%\n",
      "layer   3  Sparsity: 70.4212%\n",
      "total_backward_count 362230 real_backward_count 40983  11.314%\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  0.923514/  1.502123, val:  47.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0791%\n",
      "layer   2  Sparsity: 74.0553%\n",
      "layer   3  Sparsity: 70.7212%\n",
      "total_backward_count 372020 real_backward_count 41845  11.248%\n",
      "fc layer 1 self.abs_max_out: 8335.0\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  0.917315/  1.350597, val:  64.17%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0786%\n",
      "layer   2  Sparsity: 74.0558%\n",
      "layer   3  Sparsity: 70.7925%\n",
      "total_backward_count 381810 real_backward_count 42749  11.196%\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  0.916734/  1.344230, val:  67.50%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0722%\n",
      "layer   2  Sparsity: 74.2208%\n",
      "layer   3  Sparsity: 71.9339%\n",
      "total_backward_count 391600 real_backward_count 43653  11.147%\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  0.943182/  1.323361, val:  66.67%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0957%\n",
      "layer   2  Sparsity: 73.8837%\n",
      "layer   3  Sparsity: 72.5082%\n",
      "total_backward_count 401390 real_backward_count 44536  11.095%\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  0.912743/  1.313450, val:  76.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0763%\n",
      "layer   2  Sparsity: 73.9204%\n",
      "layer   3  Sparsity: 72.0985%\n",
      "total_backward_count 411180 real_backward_count 45405  11.043%\n",
      "fc layer 1 self.abs_max_out: 8518.0\n",
      "lif layer 1 self.abs_max_v: 14663.5\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  0.912533/  1.359075, val:  66.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0669%\n",
      "layer   2  Sparsity: 74.1463%\n",
      "layer   3  Sparsity: 72.8532%\n",
      "total_backward_count 420970 real_backward_count 46222  10.980%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  0.950597/  1.315421, val:  71.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0550%\n",
      "layer   2  Sparsity: 74.1594%\n",
      "layer   3  Sparsity: 73.0818%\n",
      "total_backward_count 430760 real_backward_count 47145  10.945%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  0.960418/  1.357013, val:  69.58%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0848%\n",
      "layer   2  Sparsity: 74.1555%\n",
      "layer   3  Sparsity: 73.4658%\n",
      "total_backward_count 440550 real_backward_count 48058  10.909%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  0.955062/  1.276133, val:  72.92%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0752%\n",
      "layer   2  Sparsity: 74.0018%\n",
      "layer   3  Sparsity: 72.9429%\n",
      "total_backward_count 450340 real_backward_count 49014  10.884%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  0.926081/  1.347827, val:  68.75%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0927%\n",
      "layer   2  Sparsity: 74.1073%\n",
      "layer   3  Sparsity: 72.4420%\n",
      "total_backward_count 460130 real_backward_count 49865  10.837%\n",
      "lif layer 2 self.abs_max_v: 7167.5\n",
      "fc layer 1 self.abs_max_out: 8676.0\n",
      "lif layer 1 self.abs_max_v: 14862.5\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  0.903937/  1.365872, val:  56.67%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0787%\n",
      "layer   2  Sparsity: 74.0410%\n",
      "layer   3  Sparsity: 72.2054%\n",
      "total_backward_count 469920 real_backward_count 50738  10.797%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  0.905846/  1.269747, val:  76.25%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0788%\n",
      "layer   2  Sparsity: 74.3439%\n",
      "layer   3  Sparsity: 71.9477%\n",
      "total_backward_count 479710 real_backward_count 51550  10.746%\n",
      "fc layer 1 self.abs_max_out: 8993.0\n",
      "lif layer 1 self.abs_max_v: 15354.0\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  0.879175/  1.307037, val:  70.00%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0402%\n",
      "layer   2  Sparsity: 74.3468%\n",
      "layer   3  Sparsity: 71.6287%\n",
      "total_backward_count 489500 real_backward_count 52383  10.701%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  0.895773/  1.302971, val:  71.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0785%\n",
      "layer   2  Sparsity: 74.0314%\n",
      "layer   3  Sparsity: 71.5947%\n",
      "total_backward_count 499290 real_backward_count 53212  10.658%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  0.912607/  1.281241, val:  78.33%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0728%\n",
      "layer   2  Sparsity: 74.1123%\n",
      "layer   3  Sparsity: 73.2402%\n",
      "total_backward_count 509080 real_backward_count 54076  10.622%\n",
      "fc layer 3 self.abs_max_out: 2534.0\n",
      "fc layer 3 self.abs_max_out: 2620.0\n",
      "fc layer 3 self.abs_max_out: 2630.0\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  0.908960/  1.224496, val:  77.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0642%\n",
      "layer   2  Sparsity: 73.7613%\n",
      "layer   3  Sparsity: 73.3850%\n",
      "total_backward_count 518870 real_backward_count 54941  10.589%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  0.882194/  1.241436, val:  75.83%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0640%\n",
      "layer   2  Sparsity: 74.0158%\n",
      "layer   3  Sparsity: 72.5803%\n",
      "total_backward_count 528660 real_backward_count 55798  10.555%\n",
      "lif layer 1 self.abs_max_v: 15358.5\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  0.884806/  1.225380, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0631%\n",
      "layer   2  Sparsity: 74.2160%\n",
      "layer   3  Sparsity: 72.6279%\n",
      "total_backward_count 538450 real_backward_count 56570  10.506%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  0.892256/  1.264523, val:  77.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0945%\n",
      "layer   2  Sparsity: 73.8866%\n",
      "layer   3  Sparsity: 72.4266%\n",
      "total_backward_count 548240 real_backward_count 57397  10.469%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  0.893824/  1.308277, val:  69.17%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0491%\n",
      "layer   2  Sparsity: 73.8565%\n",
      "layer   3  Sparsity: 72.1605%\n",
      "total_backward_count 558030 real_backward_count 58261  10.440%\n",
      "lif layer 2 self.abs_max_v: 7391.0\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  0.878399/  1.279309, val:  72.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0568%\n",
      "layer   2  Sparsity: 73.9132%\n",
      "layer   3  Sparsity: 71.9512%\n",
      "total_backward_count 567820 real_backward_count 59104  10.409%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  0.877731/  1.232713, val:  76.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0639%\n",
      "layer   2  Sparsity: 73.6294%\n",
      "layer   3  Sparsity: 71.8913%\n",
      "total_backward_count 577610 real_backward_count 59921  10.374%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  0.891906/  1.320141, val:  61.25%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0830%\n",
      "layer   2  Sparsity: 74.1344%\n",
      "layer   3  Sparsity: 71.5945%\n",
      "total_backward_count 587400 real_backward_count 60715  10.336%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  0.849841/  1.270013, val:  63.75%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0747%\n",
      "layer   2  Sparsity: 73.8468%\n",
      "layer   3  Sparsity: 71.3011%\n",
      "total_backward_count 597190 real_backward_count 61520  10.302%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  0.848956/  1.190015, val:  75.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0521%\n",
      "layer   2  Sparsity: 74.0384%\n",
      "layer   3  Sparsity: 72.5777%\n",
      "total_backward_count 606980 real_backward_count 62358  10.273%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  0.852001/  1.213388, val:  76.67%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0806%\n",
      "layer   2  Sparsity: 73.8457%\n",
      "layer   3  Sparsity: 73.0661%\n",
      "total_backward_count 616770 real_backward_count 63185  10.244%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  0.883269/  1.210058, val:  82.08%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0702%\n",
      "layer   2  Sparsity: 73.8958%\n",
      "layer   3  Sparsity: 73.2589%\n",
      "total_backward_count 626560 real_backward_count 63989  10.213%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  0.880329/  1.296380, val:  63.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0797%\n",
      "layer   2  Sparsity: 73.7908%\n",
      "layer   3  Sparsity: 72.7278%\n",
      "total_backward_count 636350 real_backward_count 64836  10.189%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  0.864714/  1.278339, val:  70.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0580%\n",
      "layer   2  Sparsity: 73.8239%\n",
      "layer   3  Sparsity: 72.6998%\n",
      "total_backward_count 646140 real_backward_count 65669  10.163%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  0.891966/  1.227846, val:  77.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0610%\n",
      "layer   2  Sparsity: 73.5489%\n",
      "layer   3  Sparsity: 72.9382%\n",
      "total_backward_count 655930 real_backward_count 66478  10.135%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  0.885741/  1.205222, val:  83.33%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0588%\n",
      "layer   2  Sparsity: 73.4085%\n",
      "layer   3  Sparsity: 73.2034%\n",
      "total_backward_count 665720 real_backward_count 67257  10.103%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  0.876447/  1.232899, val:  72.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0777%\n",
      "layer   2  Sparsity: 73.1664%\n",
      "layer   3  Sparsity: 73.1911%\n",
      "total_backward_count 675510 real_backward_count 68053  10.074%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  0.857492/  1.237648, val:  74.58%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0776%\n",
      "layer   2  Sparsity: 73.0790%\n",
      "layer   3  Sparsity: 72.8533%\n",
      "total_backward_count 685300 real_backward_count 68809  10.041%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  0.860371/  1.193989, val:  76.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0764%\n",
      "layer   2  Sparsity: 73.1452%\n",
      "layer   3  Sparsity: 72.3573%\n",
      "total_backward_count 695090 real_backward_count 69599  10.013%\n",
      "fc layer 1 self.abs_max_out: 9086.0\n",
      "lif layer 1 self.abs_max_v: 15409.0\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  0.843270/  1.363077, val:  47.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0810%\n",
      "layer   2  Sparsity: 73.2664%\n",
      "layer   3  Sparsity: 72.6753%\n",
      "total_backward_count 704880 real_backward_count 70352   9.981%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  0.850299/  1.227932, val:  79.58%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0859%\n",
      "layer   2  Sparsity: 72.8257%\n",
      "layer   3  Sparsity: 72.9955%\n",
      "total_backward_count 714670 real_backward_count 71134   9.953%\n",
      "fc layer 2 self.abs_max_out: 4254.0\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  0.889110/  1.356709, val:  60.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.29 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 95.0737%\n",
      "layer   2  Sparsity: 72.8395%\n",
      "layer   3  Sparsity: 73.3923%\n",
      "total_backward_count 724460 real_backward_count 71877   9.921%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  0.886257/  1.241314, val:  77.50%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0862%\n",
      "layer   2  Sparsity: 73.0473%\n",
      "layer   3  Sparsity: 73.7798%\n",
      "total_backward_count 734250 real_backward_count 72702   9.902%\n",
      "fc layer 2 self.abs_max_out: 4255.0\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  0.898310/  1.186122, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0718%\n",
      "layer   2  Sparsity: 73.0452%\n",
      "layer   3  Sparsity: 73.7417%\n",
      "total_backward_count 744040 real_backward_count 73482   9.876%\n",
      "lif layer 1 self.abs_max_v: 15558.0\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  0.891641/  1.189846, val:  77.08%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0671%\n",
      "layer   2  Sparsity: 72.6936%\n",
      "layer   3  Sparsity: 73.6849%\n",
      "total_backward_count 753830 real_backward_count 74268   9.852%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  0.884139/  1.388000, val:  50.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0808%\n",
      "layer   2  Sparsity: 72.4601%\n",
      "layer   3  Sparsity: 73.9140%\n",
      "total_backward_count 763620 real_backward_count 75001   9.822%\n",
      "fc layer 1 self.abs_max_out: 9406.0\n",
      "lif layer 1 self.abs_max_v: 16196.0\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  0.899939/  1.231601, val:  79.17%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0649%\n",
      "layer   2  Sparsity: 72.6563%\n",
      "layer   3  Sparsity: 74.3978%\n",
      "total_backward_count 773410 real_backward_count 75772   9.797%\n",
      "lif layer 2 self.abs_max_v: 7569.0\n",
      "fc layer 1 self.abs_max_out: 9567.0\n",
      "lif layer 1 self.abs_max_v: 16571.0\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  0.879261/  1.238420, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0704%\n",
      "layer   2  Sparsity: 73.1356%\n",
      "layer   3  Sparsity: 74.2755%\n",
      "total_backward_count 783200 real_backward_count 76534   9.772%\n",
      "lif layer 2 self.abs_max_v: 7643.5\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  0.879914/  1.188504, val:  81.25%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0886%\n",
      "layer   2  Sparsity: 72.9141%\n",
      "layer   3  Sparsity: 73.4749%\n",
      "total_backward_count 792990 real_backward_count 77314   9.750%\n",
      "fc layer 2 self.abs_max_out: 4333.0\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  0.856630/  1.308257, val:  59.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0671%\n",
      "layer   2  Sparsity: 72.9861%\n",
      "layer   3  Sparsity: 72.7537%\n",
      "total_backward_count 802780 real_backward_count 78062   9.724%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  0.866371/  1.188579, val:  81.25%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0696%\n",
      "layer   2  Sparsity: 73.3260%\n",
      "layer   3  Sparsity: 73.1998%\n",
      "total_backward_count 812570 real_backward_count 78837   9.702%\n",
      "lif layer 2 self.abs_max_v: 7686.5\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  0.847348/  1.220321, val:  76.25%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0571%\n",
      "layer   2  Sparsity: 73.3220%\n",
      "layer   3  Sparsity: 73.3432%\n",
      "total_backward_count 822360 real_backward_count 79563   9.675%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  0.824868/  1.211204, val:  79.17%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0811%\n",
      "layer   2  Sparsity: 73.4397%\n",
      "layer   3  Sparsity: 72.1479%\n",
      "total_backward_count 832150 real_backward_count 80347   9.655%\n",
      "lif layer 2 self.abs_max_v: 7770.0\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  0.813334/  1.227319, val:  75.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0702%\n",
      "layer   2  Sparsity: 73.4982%\n",
      "layer   3  Sparsity: 72.4791%\n",
      "total_backward_count 841940 real_backward_count 81096   9.632%\n",
      "lif layer 2 self.abs_max_v: 7887.0\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  0.853144/  1.257725, val:  73.33%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0911%\n",
      "layer   2  Sparsity: 73.2868%\n",
      "layer   3  Sparsity: 73.3000%\n",
      "total_backward_count 851730 real_backward_count 81849   9.610%\n",
      "lif layer 2 self.abs_max_v: 8104.0\n",
      "lif layer 2 self.abs_max_v: 8132.0\n",
      "lif layer 2 self.abs_max_v: 8135.0\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  0.876564/  1.274372, val:  71.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0972%\n",
      "layer   2  Sparsity: 73.3823%\n",
      "layer   3  Sparsity: 73.2192%\n",
      "total_backward_count 861520 real_backward_count 82587   9.586%\n",
      "fc layer 2 self.abs_max_out: 4358.0\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  0.839090/  1.195114, val:  77.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0648%\n",
      "layer   2  Sparsity: 73.1825%\n",
      "layer   3  Sparsity: 73.1308%\n",
      "total_backward_count 871310 real_backward_count 83296   9.560%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  0.881892/  1.213423, val:  79.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.17 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 95.0734%\n",
      "layer   2  Sparsity: 73.3769%\n",
      "layer   3  Sparsity: 73.6483%\n",
      "total_backward_count 881100 real_backward_count 84079   9.543%\n",
      "fc layer 2 self.abs_max_out: 4386.0\n",
      "fc layer 2 self.abs_max_out: 4430.0\n",
      "fc layer 2 self.abs_max_out: 4491.0\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  0.889383/  1.213670, val:  83.33%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.10 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 95.0806%\n",
      "layer   2  Sparsity: 73.6494%\n",
      "layer   3  Sparsity: 73.8537%\n",
      "total_backward_count 890890 real_backward_count 84834   9.522%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  0.860607/  1.150741, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.59 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 95.0839%\n",
      "layer   2  Sparsity: 73.4703%\n",
      "layer   3  Sparsity: 73.9196%\n",
      "total_backward_count 900680 real_backward_count 85538   9.497%\n",
      "fc layer 2 self.abs_max_out: 4526.0\n",
      "lif layer 2 self.abs_max_v: 8208.5\n",
      "lif layer 2 self.abs_max_v: 8219.5\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  0.850700/  1.203448, val:  74.17%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.06 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 95.0650%\n",
      "layer   2  Sparsity: 73.4388%\n",
      "layer   3  Sparsity: 73.8520%\n",
      "total_backward_count 910470 real_backward_count 86259   9.474%\n",
      "lif layer 2 self.abs_max_v: 8305.5\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  0.860751/  1.228824, val:  79.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0869%\n",
      "layer   2  Sparsity: 73.5397%\n",
      "layer   3  Sparsity: 74.0655%\n",
      "total_backward_count 920260 real_backward_count 86994   9.453%\n",
      "fc layer 2 self.abs_max_out: 4538.0\n",
      "lif layer 2 self.abs_max_v: 8444.0\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  0.847515/  1.196179, val:  78.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 95.0847%\n",
      "layer   2  Sparsity: 73.8064%\n",
      "layer   3  Sparsity: 73.7445%\n",
      "total_backward_count 930050 real_backward_count 87674   9.427%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  0.840363/  1.219516, val:  76.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0914%\n",
      "layer   2  Sparsity: 74.0226%\n",
      "layer   3  Sparsity: 73.7536%\n",
      "total_backward_count 939840 real_backward_count 88384   9.404%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  0.843863/  1.191677, val:  77.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0854%\n",
      "layer   2  Sparsity: 73.9587%\n",
      "layer   3  Sparsity: 73.5265%\n",
      "total_backward_count 949630 real_backward_count 89094   9.382%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  0.836242/  1.164669, val:  79.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0777%\n",
      "layer   2  Sparsity: 73.7221%\n",
      "layer   3  Sparsity: 74.2354%\n",
      "total_backward_count 959420 real_backward_count 89784   9.358%\n",
      "fc layer 2 self.abs_max_out: 4569.0\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  0.810371/  1.237171, val:  77.92%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0646%\n",
      "layer   2  Sparsity: 73.3398%\n",
      "layer   3  Sparsity: 73.6405%\n",
      "total_backward_count 969210 real_backward_count 90450   9.332%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  0.834660/  1.210626, val:  79.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0891%\n",
      "layer   2  Sparsity: 73.1747%\n",
      "layer   3  Sparsity: 73.6743%\n",
      "total_backward_count 979000 real_backward_count 91143   9.310%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  0.822862/  1.158536, val:  83.75%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0658%\n",
      "layer   2  Sparsity: 73.0830%\n",
      "layer   3  Sparsity: 74.4107%\n",
      "total_backward_count 988790 real_backward_count 91820   9.286%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  0.837058/  1.174326, val:  80.42%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0689%\n",
      "layer   2  Sparsity: 73.1669%\n",
      "layer   3  Sparsity: 74.5302%\n",
      "total_backward_count 998580 real_backward_count 92585   9.272%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  0.866865/  1.260100, val:  76.25%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 71.58 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 95.0804%\n",
      "layer   2  Sparsity: 72.9577%\n",
      "layer   3  Sparsity: 75.0595%\n",
      "total_backward_count 1008370 real_backward_count 93290   9.252%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  0.885297/  1.268815, val:  69.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0700%\n",
      "layer   2  Sparsity: 73.3564%\n",
      "layer   3  Sparsity: 74.5176%\n",
      "total_backward_count 1018160 real_backward_count 93984   9.231%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  0.856651/  1.149456, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0571%\n",
      "layer   2  Sparsity: 73.1733%\n",
      "layer   3  Sparsity: 74.4705%\n",
      "total_backward_count 1027950 real_backward_count 94704   9.213%\n",
      "fc layer 2 self.abs_max_out: 4594.0\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  0.838850/  1.213859, val:  81.67%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0704%\n",
      "layer   2  Sparsity: 72.9893%\n",
      "layer   3  Sparsity: 74.9262%\n",
      "total_backward_count 1037740 real_backward_count 95387   9.192%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  0.842150/  1.295907, val:  62.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0736%\n",
      "layer   2  Sparsity: 73.0992%\n",
      "layer   3  Sparsity: 74.4601%\n",
      "total_backward_count 1047530 real_backward_count 96109   9.175%\n",
      "fc layer 2 self.abs_max_out: 4598.0\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  0.825285/  1.180373, val:  75.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0745%\n",
      "layer   2  Sparsity: 73.0274%\n",
      "layer   3  Sparsity: 73.7458%\n",
      "total_backward_count 1057320 real_backward_count 96830   9.158%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  0.843618/  1.210284, val:  77.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0697%\n",
      "layer   2  Sparsity: 73.1599%\n",
      "layer   3  Sparsity: 74.0559%\n",
      "total_backward_count 1067110 real_backward_count 97544   9.141%\n",
      "lif layer 2 self.abs_max_v: 8655.0\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  0.849604/  1.237551, val:  73.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0630%\n",
      "layer   2  Sparsity: 72.7468%\n",
      "layer   3  Sparsity: 74.6557%\n",
      "total_backward_count 1076900 real_backward_count 98226   9.121%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  0.845252/  1.168261, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0764%\n",
      "layer   2  Sparsity: 72.7005%\n",
      "layer   3  Sparsity: 73.7876%\n",
      "total_backward_count 1086690 real_backward_count 98882   9.099%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  0.841893/  1.212930, val:  74.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0687%\n",
      "layer   2  Sparsity: 72.8043%\n",
      "layer   3  Sparsity: 74.0468%\n",
      "total_backward_count 1096480 real_backward_count 99576   9.081%\n",
      "fc layer 2 self.abs_max_out: 4615.0\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  0.866415/  1.159176, val:  86.67%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0646%\n",
      "layer   2  Sparsity: 72.5754%\n",
      "layer   3  Sparsity: 74.0808%\n",
      "total_backward_count 1106270 real_backward_count 100256   9.063%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  0.854656/  1.227945, val:  75.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.1060%\n",
      "layer   2  Sparsity: 72.7020%\n",
      "layer   3  Sparsity: 73.7132%\n",
      "total_backward_count 1116060 real_backward_count 100943   9.045%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  0.869176/  1.208052, val:  77.92%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0685%\n",
      "layer   2  Sparsity: 72.8869%\n",
      "layer   3  Sparsity: 74.1404%\n",
      "total_backward_count 1125850 real_backward_count 101655   9.029%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  0.838035/  1.217460, val:  76.67%, val_best:  86.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0858%\n",
      "layer   2  Sparsity: 73.2265%\n",
      "layer   3  Sparsity: 73.5823%\n",
      "total_backward_count 1135640 real_backward_count 102325   9.010%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  0.825187/  1.322992, val:  58.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0893%\n",
      "layer   2  Sparsity: 72.8699%\n",
      "layer   3  Sparsity: 73.1134%\n",
      "total_backward_count 1145430 real_backward_count 103023   8.994%\n",
      "fc layer 2 self.abs_max_out: 4637.0\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  0.843206/  1.192352, val:  81.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0654%\n",
      "layer   2  Sparsity: 72.7945%\n",
      "layer   3  Sparsity: 73.5447%\n",
      "total_backward_count 1155220 real_backward_count 103732   8.979%\n",
      "fc layer 2 self.abs_max_out: 4645.0\n",
      "lif layer 2 self.abs_max_v: 8733.0\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  0.855473/  1.181532, val:  77.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0775%\n",
      "layer   2  Sparsity: 73.0781%\n",
      "layer   3  Sparsity: 74.4622%\n",
      "total_backward_count 1165010 real_backward_count 104391   8.961%\n",
      "lif layer 2 self.abs_max_v: 8806.5\n",
      "fc layer 2 self.abs_max_out: 4664.0\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  0.828601/  1.151612, val:  82.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0775%\n",
      "layer   2  Sparsity: 73.2911%\n",
      "layer   3  Sparsity: 73.6364%\n",
      "total_backward_count 1174800 real_backward_count 105078   8.944%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  0.814934/  1.168555, val:  77.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0768%\n",
      "layer   2  Sparsity: 73.5265%\n",
      "layer   3  Sparsity: 73.8243%\n",
      "total_backward_count 1184590 real_backward_count 105725   8.925%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  0.852189/  1.256940, val:  75.83%, val_best:  86.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0794%\n",
      "layer   2  Sparsity: 73.6071%\n",
      "layer   3  Sparsity: 73.9303%\n",
      "total_backward_count 1194380 real_backward_count 106417   8.910%\n",
      "fc layer 2 self.abs_max_out: 4773.0\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  0.862681/  1.182012, val:  75.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0738%\n",
      "layer   2  Sparsity: 73.5970%\n",
      "layer   3  Sparsity: 74.0234%\n",
      "total_backward_count 1204170 real_backward_count 107075   8.892%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  0.838529/  1.139346, val:  82.50%, val_best:  86.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0729%\n",
      "layer   2  Sparsity: 73.6249%\n",
      "layer   3  Sparsity: 74.1935%\n",
      "total_backward_count 1213960 real_backward_count 107763   8.877%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  0.816356/  1.131089, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0872%\n",
      "layer   2  Sparsity: 73.4882%\n",
      "layer   3  Sparsity: 73.0272%\n",
      "total_backward_count 1223750 real_backward_count 108396   8.858%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  0.802280/  1.215768, val:  75.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0508%\n",
      "layer   2  Sparsity: 73.1557%\n",
      "layer   3  Sparsity: 73.4715%\n",
      "total_backward_count 1233540 real_backward_count 109080   8.843%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  0.790603/  1.116783, val:  81.25%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0842%\n",
      "layer   2  Sparsity: 73.2618%\n",
      "layer   3  Sparsity: 73.3468%\n",
      "total_backward_count 1243330 real_backward_count 109754   8.827%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  0.771454/  1.180694, val:  77.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0752%\n",
      "layer   2  Sparsity: 73.2562%\n",
      "layer   3  Sparsity: 73.0395%\n",
      "total_backward_count 1253120 real_backward_count 110359   8.807%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  0.800319/  1.136775, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0583%\n",
      "layer   2  Sparsity: 73.4973%\n",
      "layer   3  Sparsity: 73.3781%\n",
      "total_backward_count 1262910 real_backward_count 110982   8.788%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  0.799951/  1.212220, val:  76.67%, val_best:  86.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0742%\n",
      "layer   2  Sparsity: 73.3742%\n",
      "layer   3  Sparsity: 72.7342%\n",
      "total_backward_count 1272700 real_backward_count 111664   8.774%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  0.790402/  1.181097, val:  71.67%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0879%\n",
      "layer   2  Sparsity: 73.4658%\n",
      "layer   3  Sparsity: 73.2573%\n",
      "total_backward_count 1282490 real_backward_count 112293   8.756%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  0.767879/  1.127139, val:  84.17%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0785%\n",
      "layer   2  Sparsity: 73.6304%\n",
      "layer   3  Sparsity: 73.0086%\n",
      "total_backward_count 1292280 real_backward_count 112975   8.742%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  0.757205/  1.212958, val:  68.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0783%\n",
      "layer   2  Sparsity: 73.4846%\n",
      "layer   3  Sparsity: 72.5259%\n",
      "total_backward_count 1302070 real_backward_count 113582   8.723%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  0.774875/  1.171457, val:  78.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0858%\n",
      "layer   2  Sparsity: 73.3876%\n",
      "layer   3  Sparsity: 73.4267%\n",
      "total_backward_count 1311860 real_backward_count 114240   8.708%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  0.775529/  1.229816, val:  74.17%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0849%\n",
      "layer   2  Sparsity: 73.4336%\n",
      "layer   3  Sparsity: 73.7090%\n",
      "total_backward_count 1321650 real_backward_count 114909   8.694%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  0.791640/  1.094718, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0723%\n",
      "layer   2  Sparsity: 73.6016%\n",
      "layer   3  Sparsity: 72.5904%\n",
      "total_backward_count 1331440 real_backward_count 115575   8.680%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  0.751799/  1.089959, val:  82.08%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0714%\n",
      "layer   2  Sparsity: 73.7432%\n",
      "layer   3  Sparsity: 73.4037%\n",
      "total_backward_count 1341230 real_backward_count 116198   8.664%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  0.768308/  1.174954, val:  72.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0659%\n",
      "layer   2  Sparsity: 73.4267%\n",
      "layer   3  Sparsity: 73.9727%\n",
      "total_backward_count 1351020 real_backward_count 116831   8.648%\n",
      "fc layer 2 self.abs_max_out: 4784.0\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  0.791482/  1.132180, val:  79.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0874%\n",
      "layer   2  Sparsity: 73.4235%\n",
      "layer   3  Sparsity: 74.7820%\n",
      "total_backward_count 1360810 real_backward_count 117479   8.633%\n",
      "lif layer 2 self.abs_max_v: 8841.5\n",
      "fc layer 2 self.abs_max_out: 4882.0\n",
      "lif layer 2 self.abs_max_v: 9303.0\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  0.805922/  1.201248, val:  80.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0754%\n",
      "layer   2  Sparsity: 73.1206%\n",
      "layer   3  Sparsity: 74.6942%\n",
      "total_backward_count 1370600 real_backward_count 118141   8.620%\n",
      "fc layer 2 self.abs_max_out: 4886.0\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  0.801631/  1.153648, val:  78.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0765%\n",
      "layer   2  Sparsity: 73.1983%\n",
      "layer   3  Sparsity: 74.6684%\n",
      "total_backward_count 1380390 real_backward_count 118790   8.606%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  0.786575/  1.179814, val:  77.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0894%\n",
      "layer   2  Sparsity: 73.4121%\n",
      "layer   3  Sparsity: 74.2011%\n",
      "total_backward_count 1390180 real_backward_count 119436   8.591%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  0.855547/  1.214856, val:  75.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0979%\n",
      "layer   2  Sparsity: 73.3310%\n",
      "layer   3  Sparsity: 75.2119%\n",
      "total_backward_count 1399970 real_backward_count 120096   8.578%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  0.824894/  1.165627, val:  79.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0656%\n",
      "layer   2  Sparsity: 73.8848%\n",
      "layer   3  Sparsity: 75.2256%\n",
      "total_backward_count 1409760 real_backward_count 120700   8.562%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  0.812895/  1.142059, val:  80.83%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0556%\n",
      "layer   2  Sparsity: 73.8570%\n",
      "layer   3  Sparsity: 74.9147%\n",
      "total_backward_count 1419550 real_backward_count 121336   8.547%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  0.805539/  1.191208, val:  82.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0673%\n",
      "layer   2  Sparsity: 73.6719%\n",
      "layer   3  Sparsity: 75.1437%\n",
      "total_backward_count 1429340 real_backward_count 121935   8.531%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  0.821200/  1.211768, val:  76.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0772%\n",
      "layer   2  Sparsity: 73.4310%\n",
      "layer   3  Sparsity: 75.2932%\n",
      "total_backward_count 1439130 real_backward_count 122563   8.516%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  0.820346/  1.201139, val:  69.58%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0637%\n",
      "layer   2  Sparsity: 73.3263%\n",
      "layer   3  Sparsity: 75.2445%\n",
      "total_backward_count 1448920 real_backward_count 123153   8.500%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  0.813907/  1.117794, val:  80.42%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0701%\n",
      "layer   2  Sparsity: 73.4108%\n",
      "layer   3  Sparsity: 75.4922%\n",
      "total_backward_count 1458710 real_backward_count 123788   8.486%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  0.798459/  1.195828, val:  75.00%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0882%\n",
      "layer   2  Sparsity: 73.5882%\n",
      "layer   3  Sparsity: 75.2189%\n",
      "total_backward_count 1468500 real_backward_count 124400   8.471%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  0.806460/  1.284437, val:  70.00%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0746%\n",
      "layer   2  Sparsity: 73.3096%\n",
      "layer   3  Sparsity: 75.1024%\n",
      "total_backward_count 1478290 real_backward_count 125020   8.457%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  0.822196/  1.202633, val:  77.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0738%\n",
      "layer   2  Sparsity: 73.5461%\n",
      "layer   3  Sparsity: 75.7594%\n",
      "total_backward_count 1488080 real_backward_count 125645   8.443%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  0.818310/  1.174226, val:  77.08%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0705%\n",
      "layer   2  Sparsity: 73.6018%\n",
      "layer   3  Sparsity: 75.8580%\n",
      "total_backward_count 1497870 real_backward_count 126252   8.429%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  0.839425/  1.181035, val:  80.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0762%\n",
      "layer   2  Sparsity: 74.0884%\n",
      "layer   3  Sparsity: 75.7727%\n",
      "total_backward_count 1507660 real_backward_count 126860   8.414%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  0.828406/  1.192692, val:  77.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0806%\n",
      "layer   2  Sparsity: 73.9286%\n",
      "layer   3  Sparsity: 75.9265%\n",
      "total_backward_count 1517450 real_backward_count 127442   8.398%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  0.840306/  1.309171, val:  69.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0834%\n",
      "layer   2  Sparsity: 73.8026%\n",
      "layer   3  Sparsity: 75.8899%\n",
      "total_backward_count 1527240 real_backward_count 128059   8.385%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  0.838107/  1.205238, val:  78.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0867%\n",
      "layer   2  Sparsity: 73.7654%\n",
      "layer   3  Sparsity: 75.8126%\n",
      "total_backward_count 1537030 real_backward_count 128699   8.373%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  0.820093/  1.185099, val:  81.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0630%\n",
      "layer   2  Sparsity: 73.7490%\n",
      "layer   3  Sparsity: 75.5268%\n",
      "total_backward_count 1546820 real_backward_count 129276   8.358%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  0.826982/  1.231296, val:  74.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0643%\n",
      "layer   2  Sparsity: 73.6062%\n",
      "layer   3  Sparsity: 75.1618%\n",
      "total_backward_count 1556610 real_backward_count 129879   8.344%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  0.820852/  1.197444, val:  78.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.1017%\n",
      "layer   2  Sparsity: 73.3877%\n",
      "layer   3  Sparsity: 75.6098%\n",
      "total_backward_count 1566400 real_backward_count 130457   8.328%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  0.821773/  1.146674, val:  80.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0789%\n",
      "layer   2  Sparsity: 73.2611%\n",
      "layer   3  Sparsity: 75.5018%\n",
      "total_backward_count 1576190 real_backward_count 131086   8.317%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  0.815728/  1.178043, val:  79.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0666%\n",
      "layer   2  Sparsity: 73.1549%\n",
      "layer   3  Sparsity: 74.8200%\n",
      "total_backward_count 1585980 real_backward_count 131723   8.305%\n",
      "fc layer 2 self.abs_max_out: 4887.0\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  0.817128/  1.176172, val:  74.17%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0708%\n",
      "layer   2  Sparsity: 73.4352%\n",
      "layer   3  Sparsity: 74.6910%\n",
      "total_backward_count 1595770 real_backward_count 132282   8.290%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  0.786177/  1.163520, val:  79.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0775%\n",
      "layer   2  Sparsity: 73.4341%\n",
      "layer   3  Sparsity: 74.7180%\n",
      "total_backward_count 1605560 real_backward_count 132870   8.276%\n",
      "fc layer 2 self.abs_max_out: 4895.0\n",
      "fc layer 2 self.abs_max_out: 5029.0\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  0.770316/  1.149351, val:  77.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0826%\n",
      "layer   2  Sparsity: 73.4935%\n",
      "layer   3  Sparsity: 74.3107%\n",
      "total_backward_count 1615350 real_backward_count 133435   8.260%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  0.773858/  1.160274, val:  75.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0684%\n",
      "layer   2  Sparsity: 73.6227%\n",
      "layer   3  Sparsity: 74.2085%\n",
      "total_backward_count 1625140 real_backward_count 134027   8.247%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  0.774836/  1.272320, val:  67.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0723%\n",
      "layer   2  Sparsity: 73.5276%\n",
      "layer   3  Sparsity: 74.0744%\n",
      "total_backward_count 1634930 real_backward_count 134603   8.233%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  0.803477/  1.136661, val:  78.75%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0826%\n",
      "layer   2  Sparsity: 73.6995%\n",
      "layer   3  Sparsity: 73.9788%\n",
      "total_backward_count 1644720 real_backward_count 135197   8.220%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  0.758284/  1.093837, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0808%\n",
      "layer   2  Sparsity: 73.6498%\n",
      "layer   3  Sparsity: 73.7184%\n",
      "total_backward_count 1654510 real_backward_count 135777   8.206%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.765408/  1.113804, val:  78.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0696%\n",
      "layer   2  Sparsity: 73.6135%\n",
      "layer   3  Sparsity: 74.3649%\n",
      "total_backward_count 1664300 real_backward_count 136310   8.190%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.768668/  1.173963, val:  73.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0895%\n",
      "layer   2  Sparsity: 73.8800%\n",
      "layer   3  Sparsity: 74.5510%\n",
      "total_backward_count 1674090 real_backward_count 136879   8.176%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  0.767492/  1.200436, val:  77.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0554%\n",
      "layer   2  Sparsity: 73.8181%\n",
      "layer   3  Sparsity: 74.4399%\n",
      "total_backward_count 1683880 real_backward_count 137430   8.162%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  0.781839/  1.159530, val:  77.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.1008%\n",
      "layer   2  Sparsity: 74.0316%\n",
      "layer   3  Sparsity: 75.0243%\n",
      "total_backward_count 1693670 real_backward_count 138018   8.149%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.798990/  1.156958, val:  79.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0696%\n",
      "layer   2  Sparsity: 73.8529%\n",
      "layer   3  Sparsity: 74.9492%\n",
      "total_backward_count 1703460 real_backward_count 138547   8.133%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  0.798487/  1.132314, val:  82.50%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0952%\n",
      "layer   2  Sparsity: 73.7941%\n",
      "layer   3  Sparsity: 74.3274%\n",
      "total_backward_count 1713250 real_backward_count 139145   8.122%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  0.808697/  1.161797, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0714%\n",
      "layer   2  Sparsity: 73.9676%\n",
      "layer   3  Sparsity: 74.9679%\n",
      "total_backward_count 1723040 real_backward_count 139701   8.108%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  0.799148/  1.179042, val:  79.58%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0833%\n",
      "layer   2  Sparsity: 73.8794%\n",
      "layer   3  Sparsity: 74.6189%\n",
      "total_backward_count 1732830 real_backward_count 140255   8.094%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  0.784237/  1.127268, val:  79.58%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0522%\n",
      "layer   2  Sparsity: 73.5756%\n",
      "layer   3  Sparsity: 73.8689%\n",
      "total_backward_count 1742620 real_backward_count 140839   8.082%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  0.765051/  1.246784, val:  70.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0608%\n",
      "layer   2  Sparsity: 73.4717%\n",
      "layer   3  Sparsity: 73.9299%\n",
      "total_backward_count 1752410 real_backward_count 141420   8.070%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  0.777080/  1.131397, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0751%\n",
      "layer   2  Sparsity: 73.5686%\n",
      "layer   3  Sparsity: 74.5555%\n",
      "total_backward_count 1762200 real_backward_count 142008   8.059%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  0.797592/  1.162080, val:  81.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0855%\n",
      "layer   2  Sparsity: 73.5595%\n",
      "layer   3  Sparsity: 74.9244%\n",
      "total_backward_count 1771990 real_backward_count 142598   8.047%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  0.762530/  1.123924, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0704%\n",
      "layer   2  Sparsity: 73.5323%\n",
      "layer   3  Sparsity: 73.7935%\n",
      "total_backward_count 1781780 real_backward_count 143165   8.035%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  0.762343/  1.133396, val:  81.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0693%\n",
      "layer   2  Sparsity: 73.5497%\n",
      "layer   3  Sparsity: 73.7970%\n",
      "total_backward_count 1791570 real_backward_count 143689   8.020%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  0.763217/  1.350875, val:  52.50%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0586%\n",
      "layer   2  Sparsity: 73.5328%\n",
      "layer   3  Sparsity: 74.1650%\n",
      "total_backward_count 1801360 real_backward_count 144190   8.005%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  0.752710/  1.154986, val:  75.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0720%\n",
      "layer   2  Sparsity: 73.7353%\n",
      "layer   3  Sparsity: 74.8011%\n",
      "total_backward_count 1811150 real_backward_count 144732   7.991%\n",
      "fc layer 3 self.abs_max_out: 2710.0\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  0.736286/  1.141284, val:  80.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0713%\n",
      "layer   2  Sparsity: 73.7563%\n",
      "layer   3  Sparsity: 75.2052%\n",
      "total_backward_count 1820940 real_backward_count 145300   7.979%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  0.739003/  1.148300, val:  77.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0837%\n",
      "layer   2  Sparsity: 73.8246%\n",
      "layer   3  Sparsity: 74.3874%\n",
      "total_backward_count 1830730 real_backward_count 145899   7.969%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  0.750276/  1.143266, val:  79.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0837%\n",
      "layer   2  Sparsity: 73.6012%\n",
      "layer   3  Sparsity: 73.7046%\n",
      "total_backward_count 1840520 real_backward_count 146477   7.958%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  0.721874/  1.123477, val:  80.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0710%\n",
      "layer   2  Sparsity: 73.4391%\n",
      "layer   3  Sparsity: 73.4003%\n",
      "total_backward_count 1850310 real_backward_count 147028   7.946%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  0.738529/  1.060657, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0740%\n",
      "layer   2  Sparsity: 73.7010%\n",
      "layer   3  Sparsity: 72.9385%\n",
      "total_backward_count 1860100 real_backward_count 147587   7.934%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  0.718610/  1.071735, val:  80.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0808%\n",
      "layer   2  Sparsity: 73.6970%\n",
      "layer   3  Sparsity: 72.9391%\n",
      "total_backward_count 1869890 real_backward_count 148084   7.919%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  0.735079/  1.152873, val:  75.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0649%\n",
      "layer   2  Sparsity: 73.6861%\n",
      "layer   3  Sparsity: 74.3228%\n",
      "total_backward_count 1879680 real_backward_count 148625   7.907%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  0.743285/  1.088246, val:  82.08%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0589%\n",
      "layer   2  Sparsity: 73.7883%\n",
      "layer   3  Sparsity: 74.3849%\n",
      "total_backward_count 1889470 real_backward_count 149217   7.897%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  0.739080/  1.128403, val:  80.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0690%\n",
      "layer   2  Sparsity: 73.6126%\n",
      "layer   3  Sparsity: 74.8326%\n",
      "total_backward_count 1899260 real_backward_count 149749   7.885%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  0.733391/  1.139364, val:  71.25%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0544%\n",
      "layer   2  Sparsity: 73.8585%\n",
      "layer   3  Sparsity: 74.5041%\n",
      "total_backward_count 1909050 real_backward_count 150281   7.872%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  0.716925/  1.100485, val:  79.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0796%\n",
      "layer   2  Sparsity: 74.0224%\n",
      "layer   3  Sparsity: 74.3265%\n",
      "total_backward_count 1918840 real_backward_count 150829   7.860%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  0.736337/  1.089477, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0654%\n",
      "layer   2  Sparsity: 74.1242%\n",
      "layer   3  Sparsity: 75.1041%\n",
      "total_backward_count 1928630 real_backward_count 151394   7.850%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  0.751837/  1.148807, val:  77.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0820%\n",
      "layer   2  Sparsity: 74.0917%\n",
      "layer   3  Sparsity: 74.8801%\n",
      "total_backward_count 1938420 real_backward_count 151999   7.841%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.758079/  1.160404, val:  76.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0833%\n",
      "layer   2  Sparsity: 74.1705%\n",
      "layer   3  Sparsity: 75.2910%\n",
      "total_backward_count 1948210 real_backward_count 152519   7.829%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.770151/  1.170988, val:  73.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0666%\n",
      "layer   2  Sparsity: 73.9383%\n",
      "layer   3  Sparsity: 74.8798%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f594195b3ca44b4b8d47041f88ef68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñá‚ñÑ‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÉ‚ñá‚ñá‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÖ‚ñÅ‚ñÖ‚ñà‚ñÖ‚ñà‚ñÖ‚ñà‚ñà‚ñÜ‚ñÜ‚ñà‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñÉ‚ñà‚ñà‚ñà‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñá‚ñÑ‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÉ‚ñá‚ñá‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.77015</td></tr><tr><td>val_acc_best</td><td>0.86667</td></tr><tr><td>val_acc_now</td><td>0.73333</td></tr><tr><td>val_loss</td><td>1.17099</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">golden-sweep-19</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2v82vmb3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2v82vmb3</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_150643-2v82vmb3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: goshdfah with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_192549-goshdfah</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/goshdfah' target=\"_blank\">trim-sweep-22</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/goshdfah' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/goshdfah</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': '20251118_192558_827', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 282\n",
      "fc layer 1 self.abs_max_out: 125.0\n",
      "lif layer 1 self.abs_max_v: 125.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 287.0\n",
      "lif layer 2 self.abs_max_v: 287.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 246.0\n",
      "fc layer 1 self.abs_max_out: 165.0\n",
      "lif layer 1 self.abs_max_v: 183.5\n",
      "fc layer 2 self.abs_max_out: 417.0\n",
      "lif layer 2 self.abs_max_v: 506.0\n",
      "fc layer 1 self.abs_max_out: 214.0\n",
      "lif layer 1 self.abs_max_v: 284.0\n",
      "lif layer 1 self.abs_max_v: 288.5\n",
      "lif layer 2 self.abs_max_v: 530.5\n",
      "fc layer 1 self.abs_max_out: 307.0\n",
      "lif layer 1 self.abs_max_v: 307.0\n",
      "lif layer 2 self.abs_max_v: 573.5\n",
      "fc layer 1 self.abs_max_out: 361.0\n",
      "lif layer 1 self.abs_max_v: 361.0\n",
      "lif layer 2 self.abs_max_v: 645.0\n",
      "fc layer 1 self.abs_max_out: 434.0\n",
      "lif layer 1 self.abs_max_v: 434.0\n",
      "fc layer 2 self.abs_max_out: 448.0\n",
      "lif layer 2 self.abs_max_v: 699.0\n",
      "fc layer 1 self.abs_max_out: 465.0\n",
      "lif layer 1 self.abs_max_v: 465.0\n",
      "fc layer 2 self.abs_max_out: 462.0\n",
      "lif layer 2 self.abs_max_v: 760.0\n",
      "fc layer 1 self.abs_max_out: 482.0\n",
      "lif layer 1 self.abs_max_v: 482.0\n",
      "smallest_now_T updated: 254\n",
      "fc layer 2 self.abs_max_out: 471.0\n",
      "fc layer 2 self.abs_max_out: 489.0\n",
      "fc layer 3 self.abs_max_out: 267.0\n",
      "fc layer 1 self.abs_max_out: 543.0\n",
      "lif layer 1 self.abs_max_v: 543.0\n",
      "fc layer 2 self.abs_max_out: 491.0\n",
      "lif layer 2 self.abs_max_v: 773.5\n",
      "lif layer 1 self.abs_max_v: 576.5\n",
      "fc layer 2 self.abs_max_out: 660.0\n",
      "lif layer 2 self.abs_max_v: 832.0\n",
      "smallest_now_T updated: 193\n",
      "fc layer 3 self.abs_max_out: 353.0\n",
      "lif layer 1 self.abs_max_v: 596.5\n",
      "fc layer 1 self.abs_max_out: 630.0\n",
      "lif layer 1 self.abs_max_v: 630.0\n",
      "fc layer 1 self.abs_max_out: 870.0\n",
      "lif layer 1 self.abs_max_v: 870.0\n",
      "lif layer 2 self.abs_max_v: 844.5\n",
      "lif layer 2 self.abs_max_v: 870.5\n",
      "lif layer 2 self.abs_max_v: 1014.5\n",
      "lif layer 2 self.abs_max_v: 1096.5\n",
      "fc layer 1 self.abs_max_out: 902.0\n",
      "lif layer 1 self.abs_max_v: 902.0\n",
      "fc layer 1 self.abs_max_out: 916.0\n",
      "lif layer 1 self.abs_max_v: 916.0\n",
      "fc layer 2 self.abs_max_out: 699.0\n",
      "fc layer 2 self.abs_max_out: 703.0\n",
      "lif layer 2 self.abs_max_v: 1121.0\n",
      "fc layer 2 self.abs_max_out: 732.0\n",
      "lif layer 2 self.abs_max_v: 1167.5\n",
      "lif layer 2 self.abs_max_v: 1195.0\n",
      "fc layer 2 self.abs_max_out: 741.0\n",
      "smallest_now_T updated: 163\n",
      "fc layer 3 self.abs_max_out: 368.0\n",
      "fc layer 3 self.abs_max_out: 383.0\n",
      "fc layer 2 self.abs_max_out: 814.0\n",
      "lif layer 1 self.abs_max_v: 956.5\n",
      "fc layer 1 self.abs_max_out: 1014.0\n",
      "lif layer 1 self.abs_max_v: 1014.0\n",
      "fc layer 1 self.abs_max_out: 1152.0\n",
      "lif layer 1 self.abs_max_v: 1283.5\n",
      "fc layer 1 self.abs_max_out: 1431.0\n",
      "lif layer 1 self.abs_max_v: 1449.0\n",
      "lif layer 1 self.abs_max_v: 1600.5\n",
      "lif layer 1 self.abs_max_v: 1729.5\n",
      "fc layer 2 self.abs_max_out: 898.0\n",
      "lif layer 2 self.abs_max_v: 1216.5\n",
      "lif layer 2 self.abs_max_v: 1224.5\n",
      "lif layer 2 self.abs_max_v: 1341.5\n",
      "lif layer 2 self.abs_max_v: 1408.0\n",
      "fc layer 2 self.abs_max_out: 942.0\n",
      "lif layer 2 self.abs_max_v: 1486.0\n",
      "smallest_now_T updated: 150\n",
      "lif layer 2 self.abs_max_v: 1510.5\n",
      "lif layer 2 self.abs_max_v: 1638.0\n",
      "lif layer 2 self.abs_max_v: 1702.0\n",
      "fc layer 2 self.abs_max_out: 1003.0\n",
      "lif layer 2 self.abs_max_v: 1756.5\n",
      "fc layer 2 self.abs_max_out: 1017.0\n",
      "fc layer 3 self.abs_max_out: 415.0\n",
      "fc layer 2 self.abs_max_out: 1045.0\n",
      "fc layer 2 self.abs_max_out: 1081.0\n",
      "fc layer 2 self.abs_max_out: 1240.0\n",
      "fc layer 2 self.abs_max_out: 1274.0\n",
      "fc layer 3 self.abs_max_out: 439.0\n",
      "fc layer 3 self.abs_max_out: 447.0\n",
      "fc layer 3 self.abs_max_out: 478.0\n",
      "fc layer 3 self.abs_max_out: 518.0\n",
      "smallest_now_T updated: 135\n",
      "lif layer 2 self.abs_max_v: 1772.0\n",
      "lif layer 2 self.abs_max_v: 1819.5\n",
      "lif layer 2 self.abs_max_v: 1840.0\n",
      "lif layer 2 self.abs_max_v: 1965.0\n",
      "lif layer 1 self.abs_max_v: 1773.5\n",
      "lif layer 1 self.abs_max_v: 1844.5\n",
      "fc layer 3 self.abs_max_out: 571.0\n",
      "fc layer 3 self.abs_max_out: 614.0\n",
      "fc layer 3 self.abs_max_out: 628.0\n",
      "lif layer 1 self.abs_max_v: 1902.5\n",
      "fc layer 1 self.abs_max_out: 1786.0\n",
      "lif layer 1 self.abs_max_v: 2114.0\n",
      "lif layer 1 self.abs_max_v: 2158.5\n",
      "lif layer 1 self.abs_max_v: 2263.5\n",
      "lif layer 2 self.abs_max_v: 1994.0\n",
      "lif layer 1 self.abs_max_v: 2358.0\n",
      "lif layer 2 self.abs_max_v: 2021.0\n",
      "lif layer 1 self.abs_max_v: 2370.0\n",
      "lif layer 2 self.abs_max_v: 2068.5\n",
      "fc layer 2 self.abs_max_out: 1289.0\n",
      "fc layer 2 self.abs_max_out: 1389.0\n",
      "smallest_now_T updated: 116\n",
      "smallest_now_T updated: 90\n",
      "lif layer 2 self.abs_max_v: 2098.5\n",
      "lif layer 2 self.abs_max_v: 2099.0\n",
      "lif layer 2 self.abs_max_v: 2251.0\n",
      "lif layer 2 self.abs_max_v: 2344.5\n",
      "fc layer 2 self.abs_max_out: 1400.0\n",
      "fc layer 2 self.abs_max_out: 1403.0\n",
      "fc layer 2 self.abs_max_out: 1418.0\n",
      "fc layer 2 self.abs_max_out: 1450.0\n",
      "fc layer 2 self.abs_max_out: 1466.0\n",
      "fc layer 2 self.abs_max_out: 1495.0\n",
      "fc layer 2 self.abs_max_out: 1577.0\n",
      "fc layer 2 self.abs_max_out: 1580.0\n",
      "lif layer 2 self.abs_max_v: 2346.5\n",
      "lif layer 2 self.abs_max_v: 2378.5\n",
      "lif layer 2 self.abs_max_v: 2642.5\n",
      "lif layer 2 self.abs_max_v: 2649.0\n",
      "fc layer 1 self.abs_max_out: 2058.0\n",
      "fc layer 2 self.abs_max_out: 1652.0\n",
      "fc layer 2 self.abs_max_out: 1674.0\n",
      "fc layer 3 self.abs_max_out: 630.0\n",
      "fc layer 3 self.abs_max_out: 683.0\n",
      "lif layer 1 self.abs_max_v: 2370.5\n",
      "lif layer 1 self.abs_max_v: 2458.5\n",
      "lif layer 1 self.abs_max_v: 2561.5\n",
      "lif layer 2 self.abs_max_v: 2669.0\n",
      "fc layer 3 self.abs_max_out: 722.0\n",
      "lif layer 2 self.abs_max_v: 2693.5\n",
      "lif layer 2 self.abs_max_v: 2704.5\n",
      "lif layer 2 self.abs_max_v: 2873.5\n",
      "lif layer 2 self.abs_max_v: 2893.0\n",
      "lif layer 2 self.abs_max_v: 2981.5\n",
      "lif layer 2 self.abs_max_v: 3045.0\n",
      "lif layer 2 self.abs_max_v: 3136.5\n",
      "lif layer 1 self.abs_max_v: 2712.0\n",
      "lif layer 1 self.abs_max_v: 3081.0\n",
      "fc layer 1 self.abs_max_out: 2389.0\n",
      "lif layer 1 self.abs_max_v: 3113.5\n",
      "fc layer 2 self.abs_max_out: 1691.0\n",
      "fc layer 2 self.abs_max_out: 1692.0\n",
      "fc layer 2 self.abs_max_out: 1745.0\n",
      "lif layer 1 self.abs_max_v: 3386.0\n",
      "fc layer 2 self.abs_max_out: 1967.0\n",
      "lif layer 2 self.abs_max_v: 3383.5\n",
      "lif layer 2 self.abs_max_v: 3474.0\n",
      "lif layer 2 self.abs_max_v: 3521.5\n",
      "fc layer 2 self.abs_max_out: 1999.0\n",
      "lif layer 1 self.abs_max_v: 3390.5\n",
      "lif layer 1 self.abs_max_v: 3611.5\n",
      "fc layer 3 self.abs_max_out: 802.0\n",
      "fc layer 3 self.abs_max_out: 823.0\n",
      "lif layer 1 self.abs_max_v: 3824.0\n",
      "fc layer 1 self.abs_max_out: 2701.0\n",
      "lif layer 1 self.abs_max_v: 4613.0\n",
      "lif layer 2 self.abs_max_v: 3590.5\n",
      "smallest_now_T_val updated: 262\n",
      "smallest_now_T_val updated: 217\n",
      "smallest_now_T_val updated: 213\n",
      "smallest_now_T_val updated: 209\n",
      "smallest_now_T_val updated: 174\n",
      "smallest_now_T_val updated: 63\n",
      "lif layer 1 self.abs_max_v: 4870.5\n",
      "lif layer 1 self.abs_max_v: 5058.5\n",
      "fc layer 2 self.abs_max_out: 2009.0\n",
      "lif layer 2 self.abs_max_v: 3700.0\n",
      "lif layer 2 self.abs_max_v: 3811.0\n",
      "lif layer 2 self.abs_max_v: 3836.5\n",
      "lif layer 2 self.abs_max_v: 3926.5\n",
      "fc layer 2 self.abs_max_out: 2068.0\n",
      "lif layer 2 self.abs_max_v: 3957.0\n",
      "lif layer 2 self.abs_max_v: 4022.5\n",
      "fc layer 2 self.abs_max_out: 2086.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.324251/  1.935393, val:  25.83%, val_best:  25.83%, tr:  99.49%, tr_best:  99.49%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2256%\n",
      "layer   2  Sparsity: 70.1976%\n",
      "layer   3  Sparsity: 60.7174%\n",
      "total_backward_count 9790 real_backward_count 1483  15.148%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 2186.0\n",
      "fc layer 3 self.abs_max_out: 832.0\n",
      "fc layer 2 self.abs_max_out: 2289.0\n",
      "lif layer 2 self.abs_max_v: 4141.0\n",
      "lif layer 2 self.abs_max_v: 4218.5\n",
      "lif layer 2 self.abs_max_v: 4246.5\n",
      "fc layer 2 self.abs_max_out: 2392.0\n",
      "lif layer 2 self.abs_max_v: 4389.0\n",
      "fc layer 2 self.abs_max_out: 2411.0\n",
      "lif layer 2 self.abs_max_v: 4524.0\n",
      "fc layer 2 self.abs_max_out: 2433.0\n",
      "lif layer 2 self.abs_max_v: 4527.5\n",
      "fc layer 2 self.abs_max_out: 2551.0\n",
      "lif layer 2 self.abs_max_v: 4777.5\n",
      "lif layer 2 self.abs_max_v: 4821.5\n",
      "lif layer 2 self.abs_max_v: 4843.0\n",
      "lif layer 2 self.abs_max_v: 4916.5\n",
      "lif layer 2 self.abs_max_v: 5009.5\n",
      "fc layer 2 self.abs_max_out: 2567.0\n",
      "fc layer 3 self.abs_max_out: 833.0\n",
      "fc layer 3 self.abs_max_out: 835.0\n",
      "fc layer 3 self.abs_max_out: 837.0\n",
      "fc layer 3 self.abs_max_out: 851.0\n",
      "fc layer 3 self.abs_max_out: 857.0\n",
      "fc layer 1 self.abs_max_out: 2798.0\n",
      "fc layer 2 self.abs_max_out: 2590.0\n",
      "fc layer 2 self.abs_max_out: 2652.0\n",
      "fc layer 2 self.abs_max_out: 2719.0\n",
      "lif layer 2 self.abs_max_v: 5018.5\n",
      "fc layer 2 self.abs_max_out: 2803.0\n",
      "fc layer 2 self.abs_max_out: 2889.0\n",
      "fc layer 2 self.abs_max_out: 2897.0\n",
      "fc layer 1 self.abs_max_out: 2838.0\n",
      "lif layer 1 self.abs_max_v: 5148.5\n",
      "lif layer 2 self.abs_max_v: 5035.5\n",
      "lif layer 2 self.abs_max_v: 5067.0\n",
      "lif layer 2 self.abs_max_v: 5121.0\n",
      "lif layer 2 self.abs_max_v: 5224.0\n",
      "lif layer 2 self.abs_max_v: 5378.5\n",
      "lif layer 2 self.abs_max_v: 5501.5\n",
      "fc layer 2 self.abs_max_out: 3016.0\n",
      "lif layer 2 self.abs_max_v: 5767.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.184505/  1.749298, val:  39.17%, val_best:  39.17%, tr:  99.39%, tr_best:  99.49%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2480%\n",
      "layer   2  Sparsity: 70.5129%\n",
      "layer   3  Sparsity: 62.8647%\n",
      "total_backward_count 19580 real_backward_count 2819  14.397%\n",
      "fc layer 1 self.abs_max_out: 2904.0\n",
      "fc layer 1 self.abs_max_out: 3048.0\n",
      "fc layer 3 self.abs_max_out: 862.0\n",
      "fc layer 3 self.abs_max_out: 925.0\n",
      "fc layer 3 self.abs_max_out: 934.0\n",
      "fc layer 3 self.abs_max_out: 1001.0\n",
      "fc layer 1 self.abs_max_out: 3138.0\n",
      "lif layer 1 self.abs_max_v: 5263.0\n",
      "fc layer 1 self.abs_max_out: 3357.0\n",
      "lif layer 1 self.abs_max_v: 5407.5\n",
      "lif layer 1 self.abs_max_v: 5422.0\n",
      "fc layer 2 self.abs_max_out: 3121.0\n",
      "fc layer 2 self.abs_max_out: 3130.0\n",
      "lif layer 1 self.abs_max_v: 5602.5\n",
      "lif layer 1 self.abs_max_v: 5646.5\n",
      "lif layer 1 self.abs_max_v: 5826.5\n",
      "fc layer 1 self.abs_max_out: 3486.0\n",
      "lif layer 1 self.abs_max_v: 6399.5\n",
      "lif layer 1 self.abs_max_v: 6642.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.158418/  1.835317, val:  25.83%, val_best:  39.17%, tr:  99.69%, tr_best:  99.69%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2466%\n",
      "layer   2  Sparsity: 69.7864%\n",
      "layer   3  Sparsity: 63.2773%\n",
      "total_backward_count 29370 real_backward_count 4110  13.994%\n",
      "fc layer 2 self.abs_max_out: 3273.0\n",
      "fc layer 1 self.abs_max_out: 3580.0\n",
      "fc layer 1 self.abs_max_out: 3609.0\n",
      "fc layer 1 self.abs_max_out: 3682.0\n",
      "lif layer 1 self.abs_max_v: 6987.5\n",
      "fc layer 1 self.abs_max_out: 4069.0\n",
      "lif layer 1 self.abs_max_v: 7563.0\n",
      "lif layer 1 self.abs_max_v: 7692.5\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.128413/  1.682320, val:  41.25%, val_best:  41.25%, tr:  99.80%, tr_best:  99.80%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2032%\n",
      "layer   2  Sparsity: 69.9777%\n",
      "layer   3  Sparsity: 62.8454%\n",
      "total_backward_count 39160 real_backward_count 5391  13.767%\n",
      "fc layer 3 self.abs_max_out: 1006.0\n",
      "fc layer 3 self.abs_max_out: 1026.0\n",
      "fc layer 3 self.abs_max_out: 1035.0\n",
      "fc layer 3 self.abs_max_out: 1057.0\n",
      "fc layer 1 self.abs_max_out: 4352.0\n",
      "lif layer 1 self.abs_max_v: 7831.0\n",
      "fc layer 1 self.abs_max_out: 4620.0\n",
      "lif layer 1 self.abs_max_v: 8535.5\n",
      "lif layer 1 self.abs_max_v: 8659.0\n",
      "fc layer 3 self.abs_max_out: 1064.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.043150/  1.676740, val:  42.08%, val_best:  42.08%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2195%\n",
      "layer   2  Sparsity: 70.6292%\n",
      "layer   3  Sparsity: 61.5549%\n",
      "total_backward_count 48950 real_backward_count 6655  13.596%\n",
      "fc layer 1 self.abs_max_out: 4699.0\n",
      "fc layer 1 self.abs_max_out: 4950.0\n",
      "fc layer 2 self.abs_max_out: 3326.0\n",
      "fc layer 2 self.abs_max_out: 3376.0\n",
      "lif layer 2 self.abs_max_v: 5815.5\n",
      "fc layer 1 self.abs_max_out: 5018.0\n",
      "lif layer 1 self.abs_max_v: 9249.5\n",
      "lif layer 1 self.abs_max_v: 9427.0\n",
      "fc layer 2 self.abs_max_out: 3423.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.069097/  1.625212, val:  49.17%, val_best:  49.17%, tr:  99.39%, tr_best:  99.80%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2025%\n",
      "layer   2  Sparsity: 71.5216%\n",
      "layer   3  Sparsity: 62.1621%\n",
      "total_backward_count 58740 real_backward_count 7868  13.395%\n",
      "fc layer 2 self.abs_max_out: 3445.0\n",
      "fc layer 2 self.abs_max_out: 3507.0\n",
      "lif layer 2 self.abs_max_v: 5871.0\n",
      "lif layer 2 self.abs_max_v: 6010.0\n",
      "lif layer 2 self.abs_max_v: 6133.5\n",
      "lif layer 2 self.abs_max_v: 6391.0\n",
      "lif layer 2 self.abs_max_v: 6414.5\n",
      "fc layer 2 self.abs_max_out: 3540.0\n",
      "lif layer 2 self.abs_max_v: 6459.0\n",
      "lif layer 2 self.abs_max_v: 6499.5\n",
      "fc layer 2 self.abs_max_out: 3663.0\n",
      "lif layer 2 self.abs_max_v: 6913.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.058442/  1.590559, val:  51.67%, val_best:  51.67%, tr:  99.80%, tr_best:  99.80%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2238%\n",
      "layer   2  Sparsity: 71.2034%\n",
      "layer   3  Sparsity: 62.1165%\n",
      "total_backward_count 68530 real_backward_count 9019  13.161%\n",
      "fc layer 2 self.abs_max_out: 3820.0\n",
      "lif layer 2 self.abs_max_v: 6937.5\n",
      "fc layer 3 self.abs_max_out: 1068.0\n",
      "fc layer 3 self.abs_max_out: 1081.0\n",
      "fc layer 3 self.abs_max_out: 1148.0\n",
      "fc layer 3 self.abs_max_out: 1162.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.026076/  1.488410, val:  53.33%, val_best:  53.33%, tr:  99.80%, tr_best:  99.80%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2330%\n",
      "layer   2  Sparsity: 70.3253%\n",
      "layer   3  Sparsity: 62.7543%\n",
      "total_backward_count 78320 real_backward_count 10133  12.938%\n",
      "fc layer 3 self.abs_max_out: 1173.0\n",
      "fc layer 1 self.abs_max_out: 5466.0\n",
      "lif layer 1 self.abs_max_v: 10034.5\n",
      "lif layer 1 self.abs_max_v: 10293.5\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.025494/  1.487980, val:  55.42%, val_best:  55.42%, tr:  99.80%, tr_best:  99.80%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2033%\n",
      "layer   2  Sparsity: 70.5813%\n",
      "layer   3  Sparsity: 63.5197%\n",
      "total_backward_count 88110 real_backward_count 11255  12.774%\n",
      "fc layer 3 self.abs_max_out: 1184.0\n",
      "fc layer 3 self.abs_max_out: 1247.0\n",
      "fc layer 3 self.abs_max_out: 1252.0\n",
      "fc layer 3 self.abs_max_out: 1265.0\n",
      "fc layer 1 self.abs_max_out: 5904.0\n",
      "lif layer 1 self.abs_max_v: 10771.5\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  0.960851/  1.638682, val:  42.08%, val_best:  55.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2574%\n",
      "layer   2  Sparsity: 69.4015%\n",
      "layer   3  Sparsity: 63.0040%\n",
      "total_backward_count 97900 real_backward_count 12364  12.629%\n",
      "lif layer 1 self.abs_max_v: 10971.0\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  0.925644/  1.547831, val:  44.17%, val_best:  55.42%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2250%\n",
      "layer   2  Sparsity: 70.2754%\n",
      "layer   3  Sparsity: 62.9416%\n",
      "total_backward_count 107690 real_backward_count 13423  12.464%\n",
      "fc layer 2 self.abs_max_out: 3855.0\n",
      "fc layer 2 self.abs_max_out: 3943.0\n",
      "fc layer 3 self.abs_max_out: 1280.0\n",
      "fc layer 2 self.abs_max_out: 4311.0\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  0.908641/  1.389886, val:  64.58%, val_best:  64.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2148%\n",
      "layer   2  Sparsity: 69.9915%\n",
      "layer   3  Sparsity: 62.5289%\n",
      "total_backward_count 117480 real_backward_count 14519  12.359%\n",
      "fc layer 1 self.abs_max_out: 6064.0\n",
      "lif layer 1 self.abs_max_v: 11208.5\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  0.872885/  1.513383, val:  39.17%, val_best:  64.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2416%\n",
      "layer   2  Sparsity: 70.1820%\n",
      "layer   3  Sparsity: 63.5309%\n",
      "total_backward_count 127270 real_backward_count 15543  12.213%\n",
      "lif layer 2 self.abs_max_v: 7039.5\n",
      "lif layer 2 self.abs_max_v: 7041.0\n",
      "lif layer 2 self.abs_max_v: 7268.0\n",
      "fc layer 1 self.abs_max_out: 6318.0\n",
      "lif layer 1 self.abs_max_v: 11641.0\n",
      "lif layer 1 self.abs_max_v: 11833.5\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  0.870440/  1.624816, val:  35.83%, val_best:  64.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2542%\n",
      "layer   2  Sparsity: 70.2451%\n",
      "layer   3  Sparsity: 63.7063%\n",
      "total_backward_count 137060 real_backward_count 16579  12.096%\n",
      "fc layer 3 self.abs_max_out: 1335.0\n",
      "fc layer 3 self.abs_max_out: 1413.0\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  0.817371/  1.342674, val:  60.00%, val_best:  64.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2445%\n",
      "layer   2  Sparsity: 69.4554%\n",
      "layer   3  Sparsity: 62.5063%\n",
      "total_backward_count 146850 real_backward_count 17623  12.001%\n",
      "fc layer 3 self.abs_max_out: 1420.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  0.815212/  1.445951, val:  48.33%, val_best:  64.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2217%\n",
      "layer   2  Sparsity: 69.9247%\n",
      "layer   3  Sparsity: 62.7686%\n",
      "total_backward_count 156640 real_backward_count 18645  11.903%\n",
      "fc layer 1 self.abs_max_out: 6436.0\n",
      "lif layer 1 self.abs_max_v: 11922.0\n",
      "lif layer 1 self.abs_max_v: 12064.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  0.807498/  1.310413, val:  62.08%, val_best:  64.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2285%\n",
      "layer   2  Sparsity: 69.7229%\n",
      "layer   3  Sparsity: 63.7049%\n",
      "total_backward_count 166430 real_backward_count 19607  11.781%\n",
      "lif layer 2 self.abs_max_v: 7299.0\n",
      "lif layer 2 self.abs_max_v: 7455.5\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  0.810515/  1.280882, val:  62.08%, val_best:  64.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2163%\n",
      "layer   2  Sparsity: 69.3685%\n",
      "layer   3  Sparsity: 62.9374%\n",
      "total_backward_count 176220 real_backward_count 20612  11.697%\n",
      "fc layer 1 self.abs_max_out: 6778.0\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  0.821487/  1.299999, val:  59.58%, val_best:  64.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2286%\n",
      "layer   2  Sparsity: 69.3795%\n",
      "layer   3  Sparsity: 62.7962%\n",
      "total_backward_count 186010 real_backward_count 21630  11.628%\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  0.779063/  1.404127, val:  50.42%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2256%\n",
      "layer   2  Sparsity: 69.1696%\n",
      "layer   3  Sparsity: 64.0055%\n",
      "total_backward_count 195800 real_backward_count 22556  11.520%\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  0.773699/  1.346074, val:  57.08%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2325%\n",
      "layer   2  Sparsity: 68.2452%\n",
      "layer   3  Sparsity: 64.2912%\n",
      "total_backward_count 205590 real_backward_count 23455  11.409%\n",
      "lif layer 2 self.abs_max_v: 7559.0\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  0.793051/  1.232648, val:  70.83%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2252%\n",
      "layer   2  Sparsity: 67.7461%\n",
      "layer   3  Sparsity: 64.8095%\n",
      "total_backward_count 215380 real_backward_count 24453  11.353%\n",
      "fc layer 3 self.abs_max_out: 1423.0\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  0.775912/  1.269291, val:  57.50%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.31 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 93.2465%\n",
      "layer   2  Sparsity: 68.1637%\n",
      "layer   3  Sparsity: 63.7754%\n",
      "total_backward_count 225170 real_backward_count 25406  11.283%\n",
      "lif layer 2 self.abs_max_v: 7800.0\n",
      "fc layer 2 self.abs_max_out: 4321.0\n",
      "lif layer 2 self.abs_max_v: 7897.5\n",
      "fc layer 2 self.abs_max_out: 4347.0\n",
      "lif layer 2 self.abs_max_v: 8273.0\n",
      "fc layer 2 self.abs_max_out: 4500.0\n",
      "fc layer 2 self.abs_max_out: 4505.0\n",
      "lif layer 2 self.abs_max_v: 8316.0\n",
      "lif layer 2 self.abs_max_v: 8345.5\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  0.751273/  1.259971, val:  54.17%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.74 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 93.2068%\n",
      "layer   2  Sparsity: 68.1262%\n",
      "layer   3  Sparsity: 64.1948%\n",
      "total_backward_count 234960 real_backward_count 26315  11.200%\n",
      "lif layer 2 self.abs_max_v: 8395.5\n",
      "lif layer 2 self.abs_max_v: 8518.0\n",
      "lif layer 2 self.abs_max_v: 8707.0\n",
      "lif layer 2 self.abs_max_v: 8799.5\n",
      "fc layer 2 self.abs_max_out: 4512.0\n",
      "fc layer 2 self.abs_max_out: 4570.0\n",
      "fc layer 2 self.abs_max_out: 4592.0\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        # \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [5, 10,]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.5, 0.25, 0.125, 0.0625]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [4.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "        \"learning_rate\": {\"values\": [1/128, 1/256, 1/512, 1/1024]}, \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [5, 10, 15, 20, 25, 30]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [12_000, 25_000, 50_000, 75_000, 100_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [-1]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [-11,-10,-9]},\n",
    "        # \"scale_exp_1w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        # \"scale_exp_2w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        # \"scale_exp_3w\": {\"values\": [-9]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"1\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w + 1,wandb.config.scale_exp_1w + 1]],\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'pyz704uj'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
