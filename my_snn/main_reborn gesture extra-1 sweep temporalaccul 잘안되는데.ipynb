{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11207/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "                now_T = inputs.shape[1]\n",
    "                now_time_steps = temporal_filter*TIME\n",
    "                # start_idx = random.randint(0, now_T - now_time_steps)\n",
    "                start_idx = random.choice(range(0, now_T - now_time_steps + 1, now_time_steps))\n",
    "                # start_idx = random.choice([i for i in range(0, now_T - now_time_steps + 1, now_time_steps)])\n",
    "                inputs = inputs[:, start_idx : start_idx + now_time_steps]\n",
    "                if dvs_clipping != 0:\n",
    "                    inputs[inputs<dvs_clipping] = 0.0\n",
    "                    inputs[inputs>=dvs_clipping] = 1.0\n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        for ttt in range(temporal_filter):\n",
    "                            if ttt == 0:\n",
    "                                pass\n",
    "                            else:\n",
    "                                slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] = slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] + slice_concat[..., shape_temp[-1] * (ttt-1) : shape_temp[-1] * (ttt)]\n",
    "                        slice_bucket.append(slice_concat)\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            now_T = inputs_val.shape[1]\n",
    "                            now_time_steps = temporal_filter*TIME\n",
    "                            start_idx = 0\n",
    "                            inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if dvs_clipping != 0:\n",
    "                                inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    for ttt in range(temporal_filter):\n",
    "                                        if ttt == 0:\n",
    "                                            pass\n",
    "                                        else:\n",
    "                                            slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] = slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] + slice_concat[..., shape_temp[-1] * (ttt-1) : shape_temp[-1] * (ttt)]\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs_val = (inputs_val != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "                        # ##############################################################################################\n",
    "                        # dvs_visualization(inputs_val, labels_val, TIME, BATCH, my_seed)\n",
    "                        # ##############################################################################################\n",
    "                        \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"5\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 20664,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.5,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 6.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 1/512, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 200,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = -1, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[-10,-10],[-10,-10],[-9,-9]], \n",
    "# # 1w -11~-9\n",
    "# # 1b -11~ -7\n",
    "# # 2w -10~-8\n",
    "# # 2b -10~-8\n",
    "# # 3w -10\n",
    "# # 3b -10\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hbfdnxsg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251110_154611-hbfdnxsg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hbfdnxsg' target=\"_blank\">breezy-sweep-95</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hpjdvxst' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hpjdvxst</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hpjdvxst' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hpjdvxst</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hbfdnxsg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hbfdnxsg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251110_154620_119', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 213.0\n",
      "lif layer 1 self.abs_max_v: 213.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 469.0\n",
      "lif layer 2 self.abs_max_v: 469.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 265.0\n",
      "fc layer 1 self.abs_max_out: 281.0\n",
      "lif layer 1 self.abs_max_v: 295.0\n",
      "fc layer 2 self.abs_max_out: 481.0\n",
      "lif layer 2 self.abs_max_v: 712.5\n",
      "fc layer 1 self.abs_max_out: 305.0\n",
      "lif layer 1 self.abs_max_v: 389.5\n",
      "fc layer 2 self.abs_max_out: 560.0\n",
      "lif layer 2 self.abs_max_v: 916.5\n",
      "lif layer 1 self.abs_max_v: 399.0\n",
      "fc layer 1 self.abs_max_out: 483.0\n",
      "lif layer 1 self.abs_max_v: 558.5\n",
      "fc layer 1 self.abs_max_out: 675.0\n",
      "lif layer 1 self.abs_max_v: 691.5\n",
      "lif layer 2 self.abs_max_v: 928.5\n",
      "fc layer 1 self.abs_max_out: 734.0\n",
      "lif layer 1 self.abs_max_v: 793.0\n",
      "fc layer 2 self.abs_max_out: 571.0\n",
      "lif layer 2 self.abs_max_v: 961.5\n",
      "fc layer 3 self.abs_max_out: 273.0\n",
      "lif layer 1 self.abs_max_v: 815.5\n",
      "fc layer 2 self.abs_max_out: 580.0\n",
      "lif layer 2 self.abs_max_v: 965.0\n",
      "fc layer 3 self.abs_max_out: 292.0\n",
      "fc layer 3 self.abs_max_out: 346.0\n",
      "fc layer 1 self.abs_max_out: 739.0\n",
      "fc layer 2 self.abs_max_out: 683.0\n",
      "lif layer 1 self.abs_max_v: 890.0\n",
      "fc layer 1 self.abs_max_out: 824.0\n",
      "fc layer 2 self.abs_max_out: 706.0\n",
      "fc layer 1 self.abs_max_out: 863.0\n",
      "lif layer 1 self.abs_max_v: 890.5\n",
      "fc layer 2 self.abs_max_out: 758.0\n",
      "lif layer 1 self.abs_max_v: 895.0\n",
      "fc layer 1 self.abs_max_out: 883.0\n",
      "lif layer 1 self.abs_max_v: 899.0\n",
      "lif layer 2 self.abs_max_v: 969.0\n",
      "lif layer 2 self.abs_max_v: 994.5\n",
      "fc layer 1 self.abs_max_out: 906.0\n",
      "lif layer 1 self.abs_max_v: 906.0\n",
      "fc layer 1 self.abs_max_out: 1076.0\n",
      "lif layer 1 self.abs_max_v: 1197.5\n",
      "lif layer 1 self.abs_max_v: 1395.0\n",
      "lif layer 1 self.abs_max_v: 1408.5\n",
      "fc layer 1 self.abs_max_out: 1362.0\n",
      "lif layer 1 self.abs_max_v: 1594.5\n",
      "fc layer 2 self.abs_max_out: 761.0\n",
      "fc layer 2 self.abs_max_out: 869.0\n",
      "lif layer 2 self.abs_max_v: 1017.5\n",
      "lif layer 2 self.abs_max_v: 1024.5\n",
      "lif layer 2 self.abs_max_v: 1068.5\n",
      "lif layer 2 self.abs_max_v: 1086.5\n",
      "fc layer 2 self.abs_max_out: 911.0\n",
      "lif layer 2 self.abs_max_v: 1163.0\n",
      "lif layer 2 self.abs_max_v: 1197.0\n",
      "lif layer 2 self.abs_max_v: 1373.5\n",
      "fc layer 1 self.abs_max_out: 1463.0\n",
      "fc layer 1 self.abs_max_out: 1483.0\n",
      "fc layer 2 self.abs_max_out: 915.0\n",
      "lif layer 2 self.abs_max_v: 1477.5\n",
      "fc layer 1 self.abs_max_out: 1497.0\n",
      "lif layer 2 self.abs_max_v: 1550.0\n",
      "fc layer 1 self.abs_max_out: 1527.0\n",
      "lif layer 1 self.abs_max_v: 1659.5\n",
      "fc layer 2 self.abs_max_out: 940.0\n",
      "fc layer 3 self.abs_max_out: 355.0\n",
      "fc layer 2 self.abs_max_out: 960.0\n",
      "fc layer 3 self.abs_max_out: 381.0\n",
      "fc layer 2 self.abs_max_out: 1040.0\n",
      "fc layer 2 self.abs_max_out: 1053.0\n",
      "lif layer 2 self.abs_max_v: 1622.0\n",
      "lif layer 1 self.abs_max_v: 1683.5\n",
      "lif layer 1 self.abs_max_v: 1712.5\n",
      "fc layer 3 self.abs_max_out: 441.0\n",
      "fc layer 3 self.abs_max_out: 453.0\n",
      "lif layer 1 self.abs_max_v: 1855.0\n",
      "lif layer 1 self.abs_max_v: 1953.5\n",
      "fc layer 1 self.abs_max_out: 1801.0\n",
      "fc layer 1 self.abs_max_out: 2097.0\n",
      "lif layer 1 self.abs_max_v: 2099.0\n",
      "lif layer 1 self.abs_max_v: 2593.0\n",
      "lif layer 1 self.abs_max_v: 3042.5\n",
      "lif layer 1 self.abs_max_v: 3091.0\n",
      "fc layer 2 self.abs_max_out: 1134.0\n",
      "lif layer 2 self.abs_max_v: 1624.5\n",
      "lif layer 2 self.abs_max_v: 1756.5\n",
      "fc layer 2 self.abs_max_out: 1136.0\n",
      "fc layer 3 self.abs_max_out: 479.0\n",
      "lif layer 2 self.abs_max_v: 1829.5\n",
      "lif layer 2 self.abs_max_v: 1872.0\n",
      "lif layer 2 self.abs_max_v: 1982.5\n",
      "lif layer 2 self.abs_max_v: 1996.5\n",
      "fc layer 2 self.abs_max_out: 1189.0\n",
      "lif layer 2 self.abs_max_v: 2015.5\n",
      "lif layer 2 self.abs_max_v: 2136.0\n",
      "fc layer 2 self.abs_max_out: 1233.0\n",
      "fc layer 2 self.abs_max_out: 1251.0\n",
      "fc layer 2 self.abs_max_out: 1267.0\n",
      "lif layer 2 self.abs_max_v: 2213.5\n",
      "lif layer 2 self.abs_max_v: 2268.0\n",
      "fc layer 2 self.abs_max_out: 1283.0\n",
      "lif layer 2 self.abs_max_v: 2287.5\n",
      "fc layer 2 self.abs_max_out: 1305.0\n",
      "lif layer 2 self.abs_max_v: 2289.5\n",
      "lif layer 2 self.abs_max_v: 2403.0\n",
      "fc layer 2 self.abs_max_out: 1334.0\n",
      "fc layer 2 self.abs_max_out: 1345.0\n",
      "fc layer 2 self.abs_max_out: 1355.0\n",
      "lif layer 2 self.abs_max_v: 2422.5\n",
      "lif layer 2 self.abs_max_v: 2450.5\n",
      "fc layer 2 self.abs_max_out: 1445.0\n",
      "lif layer 2 self.abs_max_v: 2461.0\n",
      "lif layer 2 self.abs_max_v: 2624.5\n",
      "fc layer 2 self.abs_max_out: 1449.0\n",
      "lif layer 2 self.abs_max_v: 2652.5\n",
      "lif layer 2 self.abs_max_v: 2684.5\n",
      "lif layer 2 self.abs_max_v: 2694.5\n",
      "fc layer 3 self.abs_max_out: 501.0\n",
      "fc layer 3 self.abs_max_out: 515.0\n",
      "fc layer 3 self.abs_max_out: 519.0\n",
      "fc layer 3 self.abs_max_out: 521.0\n",
      "fc layer 3 self.abs_max_out: 553.0\n",
      "lif layer 2 self.abs_max_v: 2695.0\n",
      "lif layer 2 self.abs_max_v: 2714.5\n",
      "fc layer 2 self.abs_max_out: 1453.0\n",
      "lif layer 2 self.abs_max_v: 2749.0\n",
      "lif layer 2 self.abs_max_v: 2760.5\n",
      "fc layer 2 self.abs_max_out: 1583.0\n",
      "lif layer 2 self.abs_max_v: 2832.0\n",
      "lif layer 2 self.abs_max_v: 2846.0\n",
      "lif layer 2 self.abs_max_v: 2895.5\n",
      "lif layer 1 self.abs_max_v: 3244.5\n",
      "lif layer 1 self.abs_max_v: 3691.5\n",
      "lif layer 1 self.abs_max_v: 3824.0\n",
      "lif layer 1 self.abs_max_v: 3941.0\n",
      "fc layer 1 self.abs_max_out: 2190.0\n",
      "fc layer 1 self.abs_max_out: 2332.0\n",
      "fc layer 1 self.abs_max_out: 2396.0\n",
      "fc layer 3 self.abs_max_out: 557.0\n",
      "fc layer 3 self.abs_max_out: 613.0\n",
      "fc layer 3 self.abs_max_out: 683.0\n",
      "fc layer 2 self.abs_max_out: 1629.0\n",
      "fc layer 2 self.abs_max_out: 1692.0\n",
      "lif layer 2 self.abs_max_v: 3018.5\n",
      "lif layer 2 self.abs_max_v: 3069.5\n",
      "lif layer 2 self.abs_max_v: 3148.0\n",
      "lif layer 2 self.abs_max_v: 3259.0\n",
      "lif layer 2 self.abs_max_v: 3310.5\n",
      "fc layer 2 self.abs_max_out: 1736.0\n",
      "lif layer 2 self.abs_max_v: 3325.5\n",
      "fc layer 2 self.abs_max_out: 1863.0\n",
      "lif layer 1 self.abs_max_v: 3992.5\n",
      "fc layer 1 self.abs_max_out: 2714.0\n",
      "lif layer 1 self.abs_max_v: 4475.5\n",
      "fc layer 3 self.abs_max_out: 816.0\n",
      "lif layer 1 self.abs_max_v: 4500.5\n",
      "lif layer 1 self.abs_max_v: 4837.5\n",
      "fc layer 2 self.abs_max_out: 1943.0\n",
      "fc layer 1 self.abs_max_out: 2937.0\n",
      "lif layer 1 self.abs_max_v: 4974.5\n",
      "lif layer 1 self.abs_max_v: 4982.0\n",
      "lif layer 1 self.abs_max_v: 5364.0\n",
      "fc layer 1 self.abs_max_out: 3014.0\n",
      "lif layer 1 self.abs_max_v: 5602.0\n",
      "lif layer 1 self.abs_max_v: 5813.0\n",
      "fc layer 1 self.abs_max_out: 3240.0\n",
      "lif layer 1 self.abs_max_v: 5846.5\n",
      "fc layer 1 self.abs_max_out: 3267.0\n",
      "fc layer 1 self.abs_max_out: 3408.0\n",
      "lif layer 1 self.abs_max_v: 5903.5\n",
      "fc layer 1 self.abs_max_out: 3547.0\n",
      "lif layer 1 self.abs_max_v: 6369.0\n",
      "fc layer 2 self.abs_max_out: 1947.0\n",
      "fc layer 2 self.abs_max_out: 1965.0\n",
      "lif layer 2 self.abs_max_v: 3373.5\n",
      "fc layer 2 self.abs_max_out: 2061.0\n",
      "lif layer 2 self.abs_max_v: 3379.5\n",
      "lif layer 2 self.abs_max_v: 3743.0\n",
      "lif layer 2 self.abs_max_v: 3824.5\n",
      "lif layer 2 self.abs_max_v: 3919.5\n",
      "fc layer 2 self.abs_max_out: 2099.0\n",
      "fc layer 2 self.abs_max_out: 2134.0\n",
      "lif layer 2 self.abs_max_v: 3979.5\n",
      "fc layer 2 self.abs_max_out: 2144.0\n",
      "lif layer 2 self.abs_max_v: 4134.0\n",
      "fc layer 1 self.abs_max_out: 4695.0\n",
      "lif layer 1 self.abs_max_v: 6425.5\n",
      "lif layer 1 self.abs_max_v: 6481.5\n",
      "lif layer 1 self.abs_max_v: 6596.0\n",
      "lif layer 1 self.abs_max_v: 7248.0\n",
      "fc layer 2 self.abs_max_out: 2272.0\n",
      "lif layer 2 self.abs_max_v: 4153.5\n",
      "lif layer 1 self.abs_max_v: 7316.5\n",
      "lif layer 1 self.abs_max_v: 7534.5\n",
      "lif layer 1 self.abs_max_v: 7640.0\n",
      "lif layer 1 self.abs_max_v: 7743.5\n",
      "fc layer 1 self.abs_max_out: 5049.0\n",
      "lif layer 1 self.abs_max_v: 8774.0\n",
      "lif layer 1 self.abs_max_v: 8825.0\n",
      "lif layer 1 self.abs_max_v: 8999.5\n",
      "lif layer 1 self.abs_max_v: 9027.0\n",
      "fc layer 2 self.abs_max_out: 2276.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.334606/  1.869895, val:  29.17%, val_best:  29.17%, tr:  99.18%, tr_best:  99.18%, epoch time: 81.17 seconds, 1.35 minutes\n",
      "total_backward_count 9790 real_backward_count 1302  13.299%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 842.0\n",
      "fc layer 3 self.abs_max_out: 878.0\n",
      "fc layer 2 self.abs_max_out: 2306.0\n",
      "lif layer 2 self.abs_max_v: 4225.5\n",
      "fc layer 3 self.abs_max_out: 895.0\n",
      "fc layer 2 self.abs_max_out: 2627.0\n",
      "lif layer 2 self.abs_max_v: 4379.0\n",
      "lif layer 2 self.abs_max_v: 4403.5\n",
      "lif layer 2 self.abs_max_v: 4512.0\n",
      "lif layer 2 self.abs_max_v: 4665.5\n",
      "lif layer 2 self.abs_max_v: 4727.0\n",
      "fc layer 3 self.abs_max_out: 926.0\n",
      "fc layer 2 self.abs_max_out: 2629.0\n",
      "fc layer 1 self.abs_max_out: 5108.0\n",
      "fc layer 1 self.abs_max_out: 5404.0\n",
      "lif layer 1 self.abs_max_v: 9390.0\n",
      "lif layer 1 self.abs_max_v: 9670.0\n",
      "lif layer 1 self.abs_max_v: 9782.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.122154/  1.636249, val:  42.50%, val_best:  42.50%, tr:  99.59%, tr_best:  99.59%, epoch time: 81.49 seconds, 1.36 minutes\n",
      "total_backward_count 19580 real_backward_count 2587  13.212%\n",
      "fc layer 2 self.abs_max_out: 2698.0\n",
      "fc layer 2 self.abs_max_out: 2783.0\n",
      "fc layer 3 self.abs_max_out: 983.0\n",
      "fc layer 1 self.abs_max_out: 5552.0\n",
      "lif layer 2 self.abs_max_v: 4746.5\n",
      "lif layer 2 self.abs_max_v: 4773.0\n",
      "lif layer 2 self.abs_max_v: 4887.5\n",
      "lif layer 2 self.abs_max_v: 4912.5\n",
      "lif layer 2 self.abs_max_v: 4966.5\n",
      "lif layer 2 self.abs_max_v: 5146.0\n",
      "fc layer 2 self.abs_max_out: 2786.0\n",
      "lif layer 2 self.abs_max_v: 5187.5\n",
      "fc layer 2 self.abs_max_out: 2801.0\n",
      "fc layer 2 self.abs_max_out: 2809.0\n",
      "fc layer 3 self.abs_max_out: 987.0\n",
      "fc layer 2 self.abs_max_out: 2908.0\n",
      "fc layer 3 self.abs_max_out: 993.0\n",
      "fc layer 3 self.abs_max_out: 1089.0\n",
      "fc layer 1 self.abs_max_out: 5601.0\n",
      "lif layer 2 self.abs_max_v: 5228.5\n",
      "lif layer 2 self.abs_max_v: 5454.5\n",
      "fc layer 2 self.abs_max_out: 2954.0\n",
      "lif layer 2 self.abs_max_v: 5613.5\n",
      "lif layer 2 self.abs_max_v: 5659.0\n",
      "fc layer 2 self.abs_max_out: 3002.0\n",
      "lif layer 2 self.abs_max_v: 5831.5\n",
      "fc layer 2 self.abs_max_out: 3010.0\n",
      "lif layer 2 self.abs_max_v: 5871.5\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.071373/  1.651973, val:  41.67%, val_best:  42.50%, tr:  99.28%, tr_best:  99.59%, epoch time: 80.32 seconds, 1.34 minutes\n",
      "total_backward_count 29370 real_backward_count 3870  13.177%\n",
      "fc layer 2 self.abs_max_out: 3167.0\n",
      "fc layer 1 self.abs_max_out: 5824.0\n",
      "fc layer 1 self.abs_max_out: 6037.0\n",
      "lif layer 1 self.abs_max_v: 9989.5\n",
      "lif layer 1 self.abs_max_v: 10330.5\n",
      "lif layer 1 self.abs_max_v: 10362.5\n",
      "fc layer 1 self.abs_max_out: 6040.0\n",
      "lif layer 1 self.abs_max_v: 10399.0\n",
      "lif layer 1 self.abs_max_v: 10476.5\n",
      "lif layer 1 self.abs_max_v: 10488.5\n",
      "lif layer 1 self.abs_max_v: 10612.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.047795/  1.740482, val:  36.67%, val_best:  42.50%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "total_backward_count 39160 real_backward_count 5108  13.044%\n",
      "fc layer 1 self.abs_max_out: 6242.0\n",
      "lif layer 1 self.abs_max_v: 10681.0\n",
      "fc layer 2 self.abs_max_out: 3176.0\n",
      "fc layer 3 self.abs_max_out: 1104.0\n",
      "lif layer 1 self.abs_max_v: 11044.5\n",
      "fc layer 1 self.abs_max_out: 6789.0\n",
      "lif layer 1 self.abs_max_v: 11216.5\n",
      "fc layer 1 self.abs_max_out: 6792.0\n",
      "lif layer 1 self.abs_max_v: 12251.0\n",
      "lif layer 1 self.abs_max_v: 12360.5\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.005919/  1.611508, val:  45.42%, val_best:  45.42%, tr:  99.28%, tr_best:  99.69%, epoch time: 79.96 seconds, 1.33 minutes\n",
      "total_backward_count 48950 real_backward_count 6334  12.940%\n",
      "fc layer 2 self.abs_max_out: 3297.0\n",
      "fc layer 2 self.abs_max_out: 3422.0\n",
      "fc layer 1 self.abs_max_out: 7230.0\n",
      "lif layer 2 self.abs_max_v: 5972.5\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.008986/  1.598697, val:  46.25%, val_best:  46.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "total_backward_count 58740 real_backward_count 7571  12.889%\n",
      "lif layer 2 self.abs_max_v: 5990.0\n",
      "fc layer 2 self.abs_max_out: 3571.0\n",
      "lif layer 2 self.abs_max_v: 6041.5\n",
      "lif layer 2 self.abs_max_v: 6117.5\n",
      "lif layer 2 self.abs_max_v: 6373.0\n",
      "lif layer 2 self.abs_max_v: 6561.5\n",
      "lif layer 1 self.abs_max_v: 12599.0\n",
      "fc layer 3 self.abs_max_out: 1121.0\n",
      "fc layer 3 self.abs_max_out: 1142.0\n",
      "lif layer 1 self.abs_max_v: 12959.0\n",
      "lif layer 1 self.abs_max_v: 13512.5\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.033059/  1.623488, val:  44.17%, val_best:  46.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "total_backward_count 68530 real_backward_count 8756  12.777%\n",
      "fc layer 2 self.abs_max_out: 3933.0\n",
      "lif layer 2 self.abs_max_v: 6601.5\n",
      "lif layer 2 self.abs_max_v: 6865.5\n",
      "lif layer 2 self.abs_max_v: 6975.0\n",
      "fc layer 1 self.abs_max_out: 7503.0\n",
      "fc layer 1 self.abs_max_out: 7514.0\n",
      "lif layer 1 self.abs_max_v: 14064.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.001991/  1.436236, val:  54.17%, val_best:  54.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "total_backward_count 78320 real_backward_count 9885  12.621%\n",
      "fc layer 1 self.abs_max_out: 7699.0\n",
      "fc layer 1 self.abs_max_out: 7941.0\n",
      "lif layer 1 self.abs_max_v: 14381.5\n",
      "lif layer 1 self.abs_max_v: 14716.0\n",
      "fc layer 3 self.abs_max_out: 1171.0\n",
      "lif layer 1 self.abs_max_v: 15015.5\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  0.938242/  1.503529, val:  51.67%, val_best:  54.17%, tr:  99.28%, tr_best:  99.90%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "total_backward_count 88110 real_backward_count 11111  12.610%\n",
      "fc layer 3 self.abs_max_out: 1213.0\n",
      "fc layer 3 self.abs_max_out: 1228.0\n",
      "fc layer 1 self.abs_max_out: 8406.0\n",
      "lif layer 1 self.abs_max_v: 15540.5\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  0.923497/  1.571165, val:  46.67%, val_best:  54.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "total_backward_count 97900 real_backward_count 12261  12.524%\n",
      "fc layer 3 self.abs_max_out: 1259.0\n",
      "fc layer 1 self.abs_max_out: 8746.0\n",
      "lif layer 1 self.abs_max_v: 15656.5\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  0.885570/  1.519750, val:  49.58%, val_best:  54.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 75.47 seconds, 1.26 minutes\n",
      "total_backward_count 107690 real_backward_count 13394  12.438%\n",
      "fc layer 2 self.abs_max_out: 3971.0\n",
      "lif layer 2 self.abs_max_v: 7041.0\n",
      "fc layer 2 self.abs_max_out: 4135.0\n",
      "lif layer 2 self.abs_max_v: 7143.0\n",
      "lif layer 2 self.abs_max_v: 7155.0\n",
      "lif layer 2 self.abs_max_v: 7382.5\n",
      "fc layer 2 self.abs_max_out: 4300.0\n",
      "lif layer 2 self.abs_max_v: 7405.5\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  0.885668/  1.407064, val:  57.50%, val_best:  57.50%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "total_backward_count 117480 real_backward_count 14553  12.388%\n",
      "fc layer 3 self.abs_max_out: 1279.0\n",
      "fc layer 3 self.abs_max_out: 1313.0\n",
      "lif layer 2 self.abs_max_v: 7450.0\n",
      "lif layer 2 self.abs_max_v: 7485.0\n",
      "lif layer 2 self.abs_max_v: 7753.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  0.845564/  1.529932, val:  49.17%, val_best:  57.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "total_backward_count 127270 real_backward_count 15680  12.320%\n",
      "fc layer 2 self.abs_max_out: 4437.0\n",
      "fc layer 1 self.abs_max_out: 8853.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  0.875041/  1.662353, val:  41.25%, val_best:  57.50%, tr:  99.39%, tr_best:  99.90%, epoch time: 75.07 seconds, 1.25 minutes\n",
      "total_backward_count 137060 real_backward_count 16813  12.267%\n",
      "fc layer 1 self.abs_max_out: 9256.0\n",
      "lif layer 1 self.abs_max_v: 16035.5\n",
      "lif layer 1 self.abs_max_v: 16202.0\n",
      "fc layer 1 self.abs_max_out: 9326.0\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  0.827070/  1.488378, val:  47.92%, val_best:  57.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "total_backward_count 146850 real_backward_count 17919  12.202%\n",
      "fc layer 1 self.abs_max_out: 9514.0\n",
      "lif layer 1 self.abs_max_v: 17067.5\n",
      "fc layer 1 self.abs_max_out: 9560.0\n",
      "fc layer 1 self.abs_max_out: 10029.0\n",
      "lif layer 1 self.abs_max_v: 18142.0\n",
      "fc layer 3 self.abs_max_out: 1349.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  0.845326/  1.577339, val:  40.83%, val_best:  57.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 74.83 seconds, 1.25 minutes\n",
      "total_backward_count 156640 real_backward_count 19016  12.140%\n",
      "lif layer 2 self.abs_max_v: 7852.5\n",
      "fc layer 3 self.abs_max_out: 1374.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  0.800060/  1.414612, val:  54.17%, val_best:  57.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "total_backward_count 166430 real_backward_count 20088  12.070%\n",
      "fc layer 3 self.abs_max_out: 1375.0\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  0.774535/  1.364121, val:  56.67%, val_best:  57.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "total_backward_count 176220 real_backward_count 21231  12.048%\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  0.783838/  1.508905, val:  41.67%, val_best:  57.50%, tr:  99.39%, tr_best:  99.90%, epoch time: 75.76 seconds, 1.26 minutes\n",
      "total_backward_count 186010 real_backward_count 22332  12.006%\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  0.799089/  1.470772, val:  44.17%, val_best:  57.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 73.78 seconds, 1.23 minutes\n",
      "total_backward_count 195800 real_backward_count 23358  11.930%\n",
      "fc layer 2 self.abs_max_out: 4701.0\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  0.764970/  1.417283, val:  48.75%, val_best:  57.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "total_backward_count 205590 real_backward_count 24426  11.881%\n",
      "lif layer 2 self.abs_max_v: 7856.5\n",
      "fc layer 2 self.abs_max_out: 4789.0\n",
      "lif layer 2 self.abs_max_v: 7888.0\n",
      "lif layer 2 self.abs_max_v: 8075.0\n",
      "fc layer 3 self.abs_max_out: 1377.0\n",
      "fc layer 2 self.abs_max_out: 4796.0\n",
      "lif layer 2 self.abs_max_v: 8442.5\n",
      "lif layer 2 self.abs_max_v: 8494.0\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  0.792277/  1.381369, val:  55.83%, val_best:  57.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "total_backward_count 215380 real_backward_count 25580  11.877%\n",
      "fc layer 1 self.abs_max_out: 11123.0\n",
      "lif layer 1 self.abs_max_v: 18769.0\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  0.769915/  1.279043, val:  61.25%, val_best:  61.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 74.51 seconds, 1.24 minutes\n",
      "total_backward_count 225170 real_backward_count 26642  11.832%\n",
      "fc layer 3 self.abs_max_out: 1399.0\n",
      "fc layer 3 self.abs_max_out: 1474.0\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  0.762145/  1.456699, val:  50.83%, val_best:  61.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "total_backward_count 234960 real_backward_count 27695  11.787%\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  0.768867/  1.334912, val:  56.25%, val_best:  61.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 73.89 seconds, 1.23 minutes\n",
      "total_backward_count 244750 real_backward_count 28717  11.733%\n",
      "lif layer 2 self.abs_max_v: 8499.5\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  0.786332/  1.314884, val:  57.92%, val_best:  61.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 75.17 seconds, 1.25 minutes\n",
      "total_backward_count 254540 real_backward_count 29807  11.710%\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  0.759342/  1.335839, val:  57.50%, val_best:  61.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "total_backward_count 264330 real_backward_count 30836  11.666%\n",
      "lif layer 1 self.abs_max_v: 18852.0\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  0.704281/  1.252241, val:  62.08%, val_best:  62.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "total_backward_count 274120 real_backward_count 31865  11.624%\n",
      "lif layer 1 self.abs_max_v: 19459.5\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  0.699014/  1.410179, val:  53.33%, val_best:  62.08%, tr:  99.39%, tr_best:  99.90%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "total_backward_count 283910 real_backward_count 32901  11.589%\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  0.703166/  1.771770, val:  39.58%, val_best:  62.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "total_backward_count 293700 real_backward_count 33912  11.546%\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  0.685948/  1.371715, val:  51.67%, val_best:  62.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 73.71 seconds, 1.23 minutes\n",
      "total_backward_count 303490 real_backward_count 34939  11.512%\n",
      "fc layer 1 self.abs_max_out: 11681.0\n",
      "lif layer 1 self.abs_max_v: 20130.5\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  0.698436/  1.461597, val:  42.50%, val_best:  62.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 75.30 seconds, 1.25 minutes\n",
      "total_backward_count 313280 real_backward_count 35962  11.479%\n",
      "fc layer 1 self.abs_max_out: 11836.0\n",
      "lif layer 1 self.abs_max_v: 20510.5\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  0.677139/  1.313176, val:  55.00%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "total_backward_count 323070 real_backward_count 36952  11.438%\n",
      "fc layer 1 self.abs_max_out: 12116.0\n",
      "lif layer 1 self.abs_max_v: 20953.5\n",
      "fc layer 3 self.abs_max_out: 1543.0\n",
      "fc layer 3 self.abs_max_out: 1558.0\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  0.668234/  1.346385, val:  52.08%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.54 seconds, 1.24 minutes\n",
      "total_backward_count 332860 real_backward_count 37997  11.415%\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  0.658886/  1.373493, val:  56.67%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.58 seconds, 1.24 minutes\n",
      "total_backward_count 342650 real_backward_count 39035  11.392%\n",
      "fc layer 3 self.abs_max_out: 1561.0\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  0.656585/  1.290501, val:  59.58%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.75 seconds, 1.26 minutes\n",
      "total_backward_count 352440 real_backward_count 40007  11.351%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  0.659375/  1.244421, val:  56.25%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.03 seconds, 1.25 minutes\n",
      "total_backward_count 362230 real_backward_count 41027  11.326%\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  0.639002/  1.299509, val:  56.67%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.23 seconds, 1.24 minutes\n",
      "total_backward_count 372020 real_backward_count 41979  11.284%\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  0.642072/  1.253489, val:  59.17%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "total_backward_count 381810 real_backward_count 43010  11.265%\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  0.650980/  1.528065, val:  47.50%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.85 seconds, 1.26 minutes\n",
      "total_backward_count 391600 real_backward_count 43962  11.226%\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  0.656350/  1.221530, val:  59.58%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.45 seconds, 1.24 minutes\n",
      "total_backward_count 401390 real_backward_count 44944  11.197%\n",
      "fc layer 3 self.abs_max_out: 1564.0\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  0.626248/  1.282724, val:  60.00%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "total_backward_count 411180 real_backward_count 45902  11.163%\n",
      "fc layer 3 self.abs_max_out: 1568.0\n",
      "fc layer 3 self.abs_max_out: 1579.0\n",
      "fc layer 3 self.abs_max_out: 1611.0\n",
      "fc layer 3 self.abs_max_out: 1687.0\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  0.608550/  1.310500, val:  52.50%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.17 seconds, 1.25 minutes\n",
      "total_backward_count 420970 real_backward_count 46868  11.133%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  0.615342/  1.214971, val:  57.92%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.91 seconds, 1.25 minutes\n",
      "total_backward_count 430760 real_backward_count 47844  11.107%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  0.636204/  1.231965, val:  57.50%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "total_backward_count 440550 real_backward_count 48829  11.084%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  0.622651/  1.249029, val:  53.75%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.46 seconds, 1.24 minutes\n",
      "total_backward_count 450340 real_backward_count 49825  11.064%\n",
      "fc layer 1 self.abs_max_out: 12477.0\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  0.631520/  1.220534, val:  55.83%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.51 seconds, 1.24 minutes\n",
      "total_backward_count 460130 real_backward_count 50783  11.037%\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  0.625975/  1.318977, val:  55.83%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.70 seconds, 1.26 minutes\n",
      "total_backward_count 469920 real_backward_count 51749  11.012%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  0.604602/  1.207287, val:  60.42%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.10 seconds, 1.25 minutes\n",
      "total_backward_count 479710 real_backward_count 52687  10.983%\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  0.590014/  1.189585, val:  58.33%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.28 seconds, 1.22 minutes\n",
      "total_backward_count 489500 real_backward_count 53651  10.960%\n",
      "fc layer 1 self.abs_max_out: 12909.0\n",
      "lif layer 2 self.abs_max_v: 8544.5\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  0.624769/  1.341682, val:  53.75%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "total_backward_count 499290 real_backward_count 54584  10.932%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  0.615193/  1.256259, val:  55.83%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.44 seconds, 1.26 minutes\n",
      "total_backward_count 509080 real_backward_count 55523  10.907%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  0.611879/  1.298667, val:  57.92%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.50 seconds, 1.21 minutes\n",
      "total_backward_count 518870 real_backward_count 56484  10.886%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  0.599711/  1.286862, val:  59.17%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "total_backward_count 528660 real_backward_count 57467  10.870%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  0.616316/  1.227628, val:  61.67%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "total_backward_count 538450 real_backward_count 58433  10.852%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  0.607166/  1.320343, val:  53.33%, val_best:  62.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 74.89 seconds, 1.25 minutes\n",
      "total_backward_count 548240 real_backward_count 59364  10.828%\n",
      "lif layer 1 self.abs_max_v: 21554.0\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  0.574911/  1.258006, val:  56.25%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.82 seconds, 1.25 minutes\n",
      "total_backward_count 558030 real_backward_count 60343  10.814%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  0.576921/  1.201768, val:  60.00%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "total_backward_count 567820 real_backward_count 61282  10.793%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  0.591951/  1.177056, val:  64.17%, val_best:  64.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.77 seconds, 1.23 minutes\n",
      "total_backward_count 577610 real_backward_count 62208  10.770%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  0.601916/  1.266888, val:  55.83%, val_best:  64.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "total_backward_count 587400 real_backward_count 63119  10.745%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  0.585887/  1.334931, val:  54.58%, val_best:  64.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "total_backward_count 597190 real_backward_count 64015  10.719%\n",
      "lif layer 1 self.abs_max_v: 21651.5\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  0.582961/  1.152728, val:  63.33%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.47 seconds, 1.22 minutes\n",
      "total_backward_count 606980 real_backward_count 64953  10.701%\n",
      "lif layer 1 self.abs_max_v: 22198.0\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  0.580370/  1.314532, val:  54.58%, val_best:  64.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.35 seconds, 1.24 minutes\n",
      "total_backward_count 616770 real_backward_count 65907  10.686%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  0.579808/  1.204448, val:  57.50%, val_best:  64.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "total_backward_count 626560 real_backward_count 66802  10.662%\n",
      "fc layer 3 self.abs_max_out: 1717.0\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  0.567047/  1.148553, val:  64.17%, val_best:  64.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.89 seconds, 1.23 minutes\n",
      "total_backward_count 636350 real_backward_count 67673  10.635%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  0.563185/  1.271416, val:  60.42%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.91 seconds, 1.25 minutes\n",
      "total_backward_count 646140 real_backward_count 68592  10.616%\n",
      "fc layer 2 self.abs_max_out: 4875.0\n",
      "lif layer 2 self.abs_max_v: 8596.5\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  0.546282/  1.182244, val:  62.08%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "total_backward_count 655930 real_backward_count 69512  10.597%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  0.582706/  1.180460, val:  61.67%, val_best:  64.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.14 seconds, 1.24 minutes\n",
      "total_backward_count 665720 real_backward_count 70443  10.581%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  0.566082/  1.163861, val:  66.25%, val_best:  66.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 74.28 seconds, 1.24 minutes\n",
      "total_backward_count 675510 real_backward_count 71367  10.565%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  0.566831/  1.326584, val:  51.67%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "total_backward_count 685300 real_backward_count 72285  10.548%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  0.578237/  1.174824, val:  63.33%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.51 seconds, 1.24 minutes\n",
      "total_backward_count 695090 real_backward_count 73260  10.540%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  0.553314/  1.300612, val:  51.67%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.88 seconds, 1.23 minutes\n",
      "total_backward_count 704880 real_backward_count 74196  10.526%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  0.554395/  1.115722, val:  64.17%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.20 seconds, 1.25 minutes\n",
      "total_backward_count 714670 real_backward_count 75086  10.506%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  0.548631/  1.553271, val:  41.25%, val_best:  66.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 74.16 seconds, 1.24 minutes\n",
      "total_backward_count 724460 real_backward_count 75989  10.489%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  0.540632/  1.446224, val:  53.33%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "total_backward_count 734250 real_backward_count 76933  10.478%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  0.551135/  1.111088, val:  65.42%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "total_backward_count 744040 real_backward_count 77820  10.459%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  0.570103/  1.257922, val:  53.75%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.98 seconds, 1.22 minutes\n",
      "total_backward_count 753830 real_backward_count 78755  10.447%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  0.571033/  1.320666, val:  50.42%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.39 seconds, 1.26 minutes\n",
      "total_backward_count 763620 real_backward_count 79642  10.430%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  0.559798/  1.314389, val:  51.67%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "total_backward_count 773410 real_backward_count 80553  10.415%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  0.544108/  1.067141, val:  68.75%, val_best:  68.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.89 seconds, 1.25 minutes\n",
      "total_backward_count 783200 real_backward_count 81424  10.396%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  0.560099/  1.291943, val:  54.17%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "total_backward_count 792990 real_backward_count 82347  10.384%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  0.527572/  1.195244, val:  56.25%, val_best:  68.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "total_backward_count 802780 real_backward_count 83198  10.364%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  0.549451/  1.134075, val:  59.58%, val_best:  68.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.23 seconds, 1.22 minutes\n",
      "total_backward_count 812570 real_backward_count 84174  10.359%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  0.519230/  1.195808, val:  57.08%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.47 seconds, 1.26 minutes\n",
      "total_backward_count 822360 real_backward_count 85066  10.344%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  0.527200/  1.087906, val:  64.17%, val_best:  68.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "total_backward_count 832150 real_backward_count 86005  10.335%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  0.515983/  1.128607, val:  64.58%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.96 seconds, 1.25 minutes\n",
      "total_backward_count 841940 real_backward_count 86925  10.324%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  0.544243/  1.201778, val:  62.50%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.82 seconds, 1.25 minutes\n",
      "total_backward_count 851730 real_backward_count 87843  10.313%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  0.546051/  1.192335, val:  60.00%, val_best:  68.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.70 seconds, 1.23 minutes\n",
      "total_backward_count 861520 real_backward_count 88711  10.297%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  0.522053/  1.338022, val:  57.50%, val_best:  68.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.22 seconds, 1.24 minutes\n",
      "total_backward_count 871310 real_backward_count 89610  10.285%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  0.537879/  1.107493, val:  69.58%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.08 seconds, 1.23 minutes\n",
      "total_backward_count 881100 real_backward_count 90567  10.279%\n",
      "fc layer 3 self.abs_max_out: 1766.0\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  0.547304/  1.259300, val:  54.58%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "total_backward_count 890890 real_backward_count 91470  10.267%\n",
      "lif layer 2 self.abs_max_v: 8739.0\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  0.544933/  1.081809, val:  67.08%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.42 seconds, 1.24 minutes\n",
      "total_backward_count 900680 real_backward_count 92360  10.254%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  0.550386/  1.201129, val:  57.08%, val_best:  69.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.51 seconds, 1.24 minutes\n",
      "total_backward_count 910470 real_backward_count 93285  10.246%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  0.561815/  1.318208, val:  55.83%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.72 seconds, 1.26 minutes\n",
      "total_backward_count 920260 real_backward_count 94173  10.233%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  0.549854/  1.163373, val:  63.75%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.64 seconds, 1.24 minutes\n",
      "total_backward_count 930050 real_backward_count 95048  10.220%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  0.544058/  1.154519, val:  64.17%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.30 seconds, 1.24 minutes\n",
      "total_backward_count 939840 real_backward_count 95941  10.208%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  0.532103/  1.327271, val:  50.83%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "total_backward_count 949630 real_backward_count 96785  10.192%\n",
      "lif layer 1 self.abs_max_v: 22317.5\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  0.513211/  1.096629, val:  69.17%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.66 seconds, 1.24 minutes\n",
      "total_backward_count 959420 real_backward_count 97599  10.173%\n",
      "fc layer 1 self.abs_max_out: 13030.0\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  0.505051/  1.345910, val:  53.75%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.06 seconds, 1.25 minutes\n",
      "total_backward_count 969210 real_backward_count 98461  10.159%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  0.530960/  1.182434, val:  66.25%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "total_backward_count 979000 real_backward_count 99332  10.146%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  0.509961/  1.146039, val:  69.58%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.75 seconds, 1.23 minutes\n",
      "total_backward_count 988790 real_backward_count 100166  10.130%\n",
      "fc layer 3 self.abs_max_out: 1778.0\n",
      "fc layer 3 self.abs_max_out: 1796.0\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  0.523694/  1.215379, val:  60.83%, val_best:  69.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.76 seconds, 1.23 minutes\n",
      "total_backward_count 998580 real_backward_count 101040  10.118%\n",
      "lif layer 1 self.abs_max_v: 22404.5\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  0.481432/  1.323016, val:  51.67%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "total_backward_count 1008370 real_backward_count 101873  10.103%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  0.479496/  1.193102, val:  59.58%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.87 seconds, 1.25 minutes\n",
      "total_backward_count 1018160 real_backward_count 102757  10.092%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  0.477229/  1.112635, val:  67.08%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.45 seconds, 1.24 minutes\n",
      "total_backward_count 1027950 real_backward_count 103620  10.080%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  0.483962/  1.225478, val:  63.33%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "total_backward_count 1037740 real_backward_count 104489  10.069%\n",
      "lif layer 1 self.abs_max_v: 22441.0\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  0.481358/  1.449314, val:  44.58%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.04 seconds, 1.22 minutes\n",
      "total_backward_count 1047530 real_backward_count 105319  10.054%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  0.484817/  1.168902, val:  62.08%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "total_backward_count 1057320 real_backward_count 106243  10.048%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  0.485928/  1.195482, val:  61.25%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "total_backward_count 1067110 real_backward_count 107097  10.036%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  0.489185/  1.181005, val:  62.08%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.23 seconds, 1.24 minutes\n",
      "total_backward_count 1076900 real_backward_count 107940  10.023%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  0.486696/  1.238425, val:  58.75%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.03 seconds, 1.23 minutes\n",
      "total_backward_count 1086690 real_backward_count 108808  10.013%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  0.478249/  1.156826, val:  67.50%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.81 seconds, 1.26 minutes\n",
      "total_backward_count 1096480 real_backward_count 109675  10.002%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  0.475224/  1.180607, val:  58.75%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.77 seconds, 1.26 minutes\n",
      "total_backward_count 1106270 real_backward_count 110558   9.994%\n",
      "fc layer 3 self.abs_max_out: 1801.0\n",
      "lif layer 1 self.abs_max_v: 22545.0\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  0.493670/  1.156819, val:  65.42%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "total_backward_count 1116060 real_backward_count 111406   9.982%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  0.497326/  1.306281, val:  52.08%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "total_backward_count 1125850 real_backward_count 112250   9.970%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  0.489077/  1.242314, val:  59.17%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.76 seconds, 1.25 minutes\n",
      "total_backward_count 1135640 real_backward_count 113122   9.961%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  0.490433/  1.150921, val:  64.17%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.12 seconds, 1.24 minutes\n",
      "total_backward_count 1145430 real_backward_count 113969   9.950%\n",
      "lif layer 1 self.abs_max_v: 22684.5\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  0.489777/  1.025229, val:  64.58%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "total_backward_count 1155220 real_backward_count 114798   9.937%\n",
      "lif layer 1 self.abs_max_v: 22721.5\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  0.480784/  1.102933, val:  64.58%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "total_backward_count 1165010 real_backward_count 115621   9.924%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  0.469308/  1.142918, val:  67.50%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.15 seconds, 1.24 minutes\n",
      "total_backward_count 1174800 real_backward_count 116473   9.914%\n",
      "fc layer 3 self.abs_max_out: 1883.0\n",
      "fc layer 3 self.abs_max_out: 1891.0\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  0.468285/  1.200808, val:  59.17%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "total_backward_count 1184590 real_backward_count 117286   9.901%\n",
      "fc layer 3 self.abs_max_out: 1918.0\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  0.464831/  1.277757, val:  50.00%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.69 seconds, 1.24 minutes\n",
      "total_backward_count 1194380 real_backward_count 118121   9.890%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  0.472514/  1.089656, val:  70.00%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.32 seconds, 1.24 minutes\n",
      "total_backward_count 1204170 real_backward_count 118975   9.880%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  0.468206/  1.107044, val:  61.25%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "total_backward_count 1213960 real_backward_count 119876   9.875%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  0.467455/  1.086325, val:  69.58%, val_best:  70.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.72 seconds, 1.25 minutes\n",
      "total_backward_count 1223750 real_backward_count 120728   9.865%\n",
      "lif layer 1 self.abs_max_v: 22765.5\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  0.467271/  1.298795, val:  57.08%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.31 seconds, 1.22 minutes\n",
      "total_backward_count 1233540 real_backward_count 121609   9.859%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  0.460306/  1.178266, val:  58.33%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "total_backward_count 1243330 real_backward_count 122457   9.849%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  0.459069/  1.090275, val:  67.50%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.48 seconds, 1.26 minutes\n",
      "total_backward_count 1253120 real_backward_count 123315   9.841%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  0.447813/  1.041063, val:  74.17%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.30 seconds, 1.25 minutes\n",
      "total_backward_count 1262910 real_backward_count 124148   9.830%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  0.448188/  1.021862, val:  79.17%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "total_backward_count 1272700 real_backward_count 124961   9.819%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  0.448376/  1.216048, val:  60.00%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.28 seconds, 1.24 minutes\n",
      "total_backward_count 1282490 real_backward_count 125811   9.810%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  0.472207/  1.245651, val:  62.92%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.02 seconds, 1.22 minutes\n",
      "total_backward_count 1292280 real_backward_count 126763   9.809%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  0.448856/  1.176481, val:  63.33%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "total_backward_count 1302070 real_backward_count 127591   9.799%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  0.446674/  1.176638, val:  58.75%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.04 seconds, 1.23 minutes\n",
      "total_backward_count 1311860 real_backward_count 128410   9.788%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  0.436942/  1.141017, val:  61.25%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.61 seconds, 1.23 minutes\n",
      "total_backward_count 1321650 real_backward_count 129231   9.778%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  0.429345/  1.172036, val:  61.25%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "total_backward_count 1331440 real_backward_count 130051   9.768%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  0.421963/  1.046812, val:  70.00%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "total_backward_count 1341230 real_backward_count 130833   9.755%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  0.425436/  1.193615, val:  58.33%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "total_backward_count 1351020 real_backward_count 131618   9.742%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  0.442428/  1.044182, val:  65.83%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.05 seconds, 1.25 minutes\n",
      "total_backward_count 1360810 real_backward_count 132431   9.732%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  0.428104/  1.130053, val:  70.00%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.84 seconds, 1.25 minutes\n",
      "total_backward_count 1370600 real_backward_count 133227   9.720%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  0.434420/  1.228917, val:  57.08%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.90 seconds, 1.23 minutes\n",
      "total_backward_count 1380390 real_backward_count 134028   9.709%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  0.412082/  1.169547, val:  62.08%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "total_backward_count 1390180 real_backward_count 134822   9.698%\n",
      "lif layer 1 self.abs_max_v: 22882.5\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  0.429554/  1.174873, val:  67.92%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "total_backward_count 1399970 real_backward_count 135649   9.689%\n",
      "lif layer 2 self.abs_max_v: 8841.0\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  0.429988/  1.181923, val:  66.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.54 seconds, 1.24 minutes\n",
      "total_backward_count 1409760 real_backward_count 136537   9.685%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  0.428747/  1.005599, val:  70.83%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.91 seconds, 1.23 minutes\n",
      "total_backward_count 1419550 real_backward_count 137344   9.675%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  0.420388/  1.273494, val:  60.42%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "total_backward_count 1429340 real_backward_count 138135   9.664%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  0.421461/  1.092073, val:  60.00%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.24 seconds, 1.24 minutes\n",
      "total_backward_count 1439130 real_backward_count 138965   9.656%\n",
      "lif layer 1 self.abs_max_v: 22923.0\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  0.397552/  1.261630, val:  60.00%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.95 seconds, 1.25 minutes\n",
      "total_backward_count 1448920 real_backward_count 139739   9.644%\n",
      "lif layer 1 self.abs_max_v: 22933.5\n",
      "fc layer 3 self.abs_max_out: 1947.0\n",
      "fc layer 3 self.abs_max_out: 1961.0\n",
      "fc layer 3 self.abs_max_out: 1982.0\n",
      "fc layer 3 self.abs_max_out: 1990.0\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  0.406491/  1.112394, val:  68.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.39 seconds, 1.26 minutes\n",
      "total_backward_count 1458710 real_backward_count 140619   9.640%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  0.418030/  1.159988, val:  66.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.59 seconds, 1.24 minutes\n",
      "total_backward_count 1468500 real_backward_count 141494   9.635%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  0.404492/  1.071715, val:  64.58%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.66 seconds, 1.26 minutes\n",
      "total_backward_count 1478290 real_backward_count 142302   9.626%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  0.434513/  1.160745, val:  57.92%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "total_backward_count 1488080 real_backward_count 143193   9.623%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  0.423100/  1.140682, val:  60.83%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.86 seconds, 1.25 minutes\n",
      "total_backward_count 1497870 real_backward_count 144015   9.615%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  0.429270/  1.110779, val:  67.08%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.05 seconds, 1.22 minutes\n",
      "total_backward_count 1507660 real_backward_count 144830   9.606%\n",
      "lif layer 1 self.abs_max_v: 22964.0\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  0.420872/  1.054606, val:  66.25%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.08 seconds, 1.23 minutes\n",
      "total_backward_count 1517450 real_backward_count 145620   9.596%\n",
      "lif layer 1 self.abs_max_v: 23029.5\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  0.419316/  1.193427, val:  63.75%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.82 seconds, 1.25 minutes\n",
      "total_backward_count 1527240 real_backward_count 146405   9.586%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  0.416813/  1.104084, val:  67.08%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "total_backward_count 1537030 real_backward_count 147201   9.577%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  0.411489/  1.091298, val:  65.00%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.75 seconds, 1.26 minutes\n",
      "total_backward_count 1546820 real_backward_count 148001   9.568%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  0.416914/  1.145133, val:  67.08%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.50 seconds, 1.21 minutes\n",
      "total_backward_count 1556610 real_backward_count 148806   9.560%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  0.406562/  1.010122, val:  75.00%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.06 seconds, 1.25 minutes\n",
      "total_backward_count 1566400 real_backward_count 149573   9.549%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  0.398318/  1.067295, val:  66.67%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "total_backward_count 1576190 real_backward_count 150366   9.540%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  0.399975/  1.076204, val:  64.58%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.23 seconds, 1.25 minutes\n",
      "total_backward_count 1585980 real_backward_count 151158   9.531%\n",
      "lif layer 1 self.abs_max_v: 23068.5\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  0.399302/  1.283215, val:  57.92%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.87 seconds, 1.26 minutes\n",
      "total_backward_count 1595770 real_backward_count 151927   9.521%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  0.401851/  1.133418, val:  60.83%, val_best:  79.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.74 seconds, 1.23 minutes\n",
      "total_backward_count 1605560 real_backward_count 152733   9.513%\n",
      "fc layer 3 self.abs_max_out: 2094.0\n",
      "fc layer 3 self.abs_max_out: 2124.0\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  0.386649/  1.027335, val:  72.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.07 seconds, 1.23 minutes\n",
      "total_backward_count 1615350 real_backward_count 153449   9.499%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  0.407547/  1.099897, val:  66.67%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.71 seconds, 1.23 minutes\n",
      "total_backward_count 1625140 real_backward_count 154251   9.492%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  0.387756/  1.111855, val:  62.08%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "total_backward_count 1634930 real_backward_count 154999   9.480%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  0.402476/  1.219737, val:  60.00%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.55 seconds, 1.26 minutes\n",
      "total_backward_count 1644720 real_backward_count 155791   9.472%\n",
      "fc layer 2 self.abs_max_out: 4885.0\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  0.400682/  1.288712, val:  51.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.60 seconds, 1.26 minutes\n",
      "total_backward_count 1654510 real_backward_count 156587   9.464%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.380243/  1.153731, val:  59.17%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "total_backward_count 1664300 real_backward_count 157328   9.453%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.377922/  1.274580, val:  58.75%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.36 seconds, 1.22 minutes\n",
      "total_backward_count 1674090 real_backward_count 158059   9.441%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  0.396679/  1.082338, val:  63.33%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.89 seconds, 1.25 minutes\n",
      "total_backward_count 1683880 real_backward_count 158846   9.433%\n",
      "lif layer 2 self.abs_max_v: 8882.5\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  0.418336/  1.129248, val:  62.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "total_backward_count 1693670 real_backward_count 159643   9.426%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.401002/  1.170634, val:  64.58%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.45 seconds, 1.24 minutes\n",
      "total_backward_count 1703460 real_backward_count 160390   9.416%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  0.392441/  1.174418, val:  61.25%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.59 seconds, 1.21 minutes\n",
      "total_backward_count 1713250 real_backward_count 161158   9.407%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  0.390514/  1.123544, val:  61.67%, val_best:  79.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.32 seconds, 1.26 minutes\n",
      "total_backward_count 1723040 real_backward_count 161960   9.400%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  0.391736/  1.134983, val:  61.25%, val_best:  79.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.84 seconds, 1.25 minutes\n",
      "total_backward_count 1732830 real_backward_count 162753   9.392%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  0.378570/  1.061706, val:  67.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.97 seconds, 1.23 minutes\n",
      "total_backward_count 1742620 real_backward_count 163523   9.384%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  0.373604/  1.208738, val:  60.42%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "total_backward_count 1752410 real_backward_count 164281   9.375%\n",
      "lif layer 1 self.abs_max_v: 23491.0\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  0.390015/  1.071392, val:  64.17%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.85 seconds, 1.23 minutes\n",
      "total_backward_count 1762200 real_backward_count 165024   9.365%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  0.363163/  1.040420, val:  67.92%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.43 seconds, 1.22 minutes\n",
      "total_backward_count 1771990 real_backward_count 165797   9.357%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  0.375957/  1.058454, val:  68.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "total_backward_count 1781780 real_backward_count 166516   9.345%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  0.365184/  1.111795, val:  67.92%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.60 seconds, 1.21 minutes\n",
      "total_backward_count 1791570 real_backward_count 167285   9.337%\n",
      "fc layer 1 self.abs_max_out: 13077.0\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  0.357449/  1.345801, val:  54.17%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.18 seconds, 1.24 minutes\n",
      "total_backward_count 1801360 real_backward_count 168033   9.328%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  0.366938/  1.276978, val:  59.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.88 seconds, 1.25 minutes\n",
      "total_backward_count 1811150 real_backward_count 168742   9.317%\n",
      "lif layer 1 self.abs_max_v: 23550.0\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  0.398982/  1.194025, val:  65.42%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.64 seconds, 1.21 minutes\n",
      "total_backward_count 1820940 real_backward_count 169543   9.311%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  0.390492/  1.148422, val:  63.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.06 seconds, 1.25 minutes\n",
      "total_backward_count 1830730 real_backward_count 170290   9.302%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  0.376347/  1.176279, val:  62.08%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.21 seconds, 1.25 minutes\n",
      "total_backward_count 1840520 real_backward_count 171104   9.297%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  0.381149/  1.261983, val:  56.25%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.75 seconds, 1.21 minutes\n",
      "total_backward_count 1850310 real_backward_count 171858   9.288%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  0.367748/  1.119034, val:  60.42%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.71 seconds, 1.25 minutes\n",
      "total_backward_count 1860100 real_backward_count 172641   9.281%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  0.369885/  1.045732, val:  69.58%, val_best:  79.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "total_backward_count 1869890 real_backward_count 173402   9.273%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  0.375763/  1.149581, val:  62.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.57 seconds, 1.24 minutes\n",
      "total_backward_count 1879680 real_backward_count 174143   9.265%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  0.372278/  1.071042, val:  70.83%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.92 seconds, 1.23 minutes\n",
      "total_backward_count 1889470 real_backward_count 174867   9.255%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  0.377988/  1.244117, val:  58.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.64 seconds, 1.26 minutes\n",
      "total_backward_count 1899260 real_backward_count 175599   9.246%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  0.369555/  1.152561, val:  65.00%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.33 seconds, 1.24 minutes\n",
      "total_backward_count 1909050 real_backward_count 176272   9.233%\n",
      "lif layer 1 self.abs_max_v: 23595.0\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  0.371995/  1.105236, val:  68.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.67 seconds, 1.23 minutes\n",
      "total_backward_count 1918840 real_backward_count 177033   9.226%\n",
      "lif layer 1 self.abs_max_v: 23694.0\n",
      "fc layer 1 self.abs_max_out: 13118.0\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  0.377213/  1.189503, val:  62.50%, val_best:  79.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "total_backward_count 1928630 real_backward_count 177826   9.220%\n",
      "fc layer 1 self.abs_max_out: 13387.0\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  0.369887/  1.151457, val:  59.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.39 seconds, 1.24 minutes\n",
      "total_backward_count 1938420 real_backward_count 178589   9.213%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.363196/  1.050125, val:  67.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.26 seconds, 1.24 minutes\n",
      "total_backward_count 1948210 real_backward_count 179357   9.206%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.371675/  1.072873, val:  70.00%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356241c7e74043cc8b4c4b2a07f21c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÖ‚ñÅ‚ñÖ‚ñÖ‚ñÅ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÜ‚ñÜ‚ñá‚ñá‚ñÉ‚ñá‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÑ‚ñá‚ñÅ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñà‚ñá‚ñÜ‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñÖ‚ñá‚ñá‚ñà‚ñÜ‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÖ‚ñÅ‚ñÖ‚ñÖ‚ñÅ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÜ‚ñÜ‚ñá‚ñá‚ñÉ‚ñá‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñÖ‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÖ‚ñÉ‚ñÑ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.37167</td></tr><tr><td>val_acc_best</td><td>0.79167</td></tr><tr><td>val_acc_now</td><td>0.7</td></tr><tr><td>val_loss</td><td>1.07287</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">breezy-sweep-95</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hbfdnxsg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hbfdnxsg</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251110_154611-hbfdnxsg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mw2lqbmz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251110_195820-mw2lqbmz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mw2lqbmz' target=\"_blank\">confused-sweep-98</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hpjdvxst' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hpjdvxst</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hpjdvxst' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hpjdvxst</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mw2lqbmz' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mw2lqbmz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251110_195831_743', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 858.0\n",
      "lif layer 1 self.abs_max_v: 858.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 1130.0\n",
      "lif layer 1 self.abs_max_v: 1191.5\n",
      "fc layer 2 self.abs_max_out: 128.0\n",
      "lif layer 2 self.abs_max_v: 128.0\n",
      "fc layer 1 self.abs_max_out: 1200.0\n",
      "lif layer 1 self.abs_max_v: 1525.0\n",
      "fc layer 2 self.abs_max_out: 343.0\n",
      "lif layer 2 self.abs_max_v: 349.0\n",
      "lif layer 1 self.abs_max_v: 1564.5\n",
      "fc layer 2 self.abs_max_out: 590.0\n",
      "lif layer 2 self.abs_max_v: 669.0\n",
      "fc layer 1 self.abs_max_out: 1679.0\n",
      "lif layer 1 self.abs_max_v: 2039.0\n",
      "fc layer 2 self.abs_max_out: 751.0\n",
      "lif layer 2 self.abs_max_v: 894.5\n",
      "fc layer 1 self.abs_max_out: 1797.0\n",
      "fc layer 1 self.abs_max_out: 2336.0\n",
      "lif layer 1 self.abs_max_v: 2336.0\n",
      "fc layer 2 self.abs_max_out: 970.0\n",
      "lif layer 2 self.abs_max_v: 1214.5\n",
      "fc layer 3 self.abs_max_out: 92.0\n",
      "fc layer 1 self.abs_max_out: 2522.0\n",
      "lif layer 1 self.abs_max_v: 2522.0\n",
      "lif layer 2 self.abs_max_v: 1313.5\n",
      "fc layer 3 self.abs_max_out: 116.0\n",
      "fc layer 1 self.abs_max_out: 2824.0\n",
      "lif layer 1 self.abs_max_v: 2824.0\n",
      "lif layer 2 self.abs_max_v: 1470.5\n",
      "fc layer 1 self.abs_max_out: 3137.0\n",
      "lif layer 1 self.abs_max_v: 3137.0\n",
      "fc layer 2 self.abs_max_out: 1224.0\n",
      "fc layer 3 self.abs_max_out: 118.0\n",
      "lif layer 2 self.abs_max_v: 1578.5\n",
      "fc layer 3 self.abs_max_out: 176.0\n",
      "lif layer 2 self.abs_max_v: 1827.0\n",
      "fc layer 3 self.abs_max_out: 346.0\n",
      "lif layer 2 self.abs_max_v: 1890.0\n",
      "fc layer 2 self.abs_max_out: 1274.0\n",
      "lif layer 2 self.abs_max_v: 1942.5\n",
      "fc layer 1 self.abs_max_out: 3895.0\n",
      "lif layer 1 self.abs_max_v: 3895.0\n",
      "fc layer 2 self.abs_max_out: 1321.0\n",
      "lif layer 2 self.abs_max_v: 2114.5\n",
      "fc layer 1 self.abs_max_out: 5009.0\n",
      "lif layer 1 self.abs_max_v: 5009.0\n",
      "fc layer 2 self.abs_max_out: 1696.0\n",
      "lif layer 2 self.abs_max_v: 2254.5\n",
      "lif layer 2 self.abs_max_v: 2466.0\n",
      "fc layer 1 self.abs_max_out: 5130.0\n",
      "lif layer 1 self.abs_max_v: 5130.0\n",
      "fc layer 3 self.abs_max_out: 388.0\n",
      "fc layer 3 self.abs_max_out: 426.0\n",
      "fc layer 2 self.abs_max_out: 1758.0\n",
      "fc layer 2 self.abs_max_out: 2165.0\n",
      "lif layer 2 self.abs_max_v: 2649.0\n",
      "fc layer 3 self.abs_max_out: 445.0\n",
      "lif layer 2 self.abs_max_v: 2977.0\n",
      "fc layer 2 self.abs_max_out: 2183.0\n",
      "lif layer 2 self.abs_max_v: 3025.0\n",
      "fc layer 2 self.abs_max_out: 2207.0\n",
      "lif layer 2 self.abs_max_v: 3084.5\n",
      "fc layer 3 self.abs_max_out: 456.0\n",
      "fc layer 2 self.abs_max_out: 2357.0\n",
      "fc layer 1 self.abs_max_out: 5736.0\n",
      "lif layer 1 self.abs_max_v: 5736.0\n",
      "fc layer 3 self.abs_max_out: 477.0\n",
      "fc layer 3 self.abs_max_out: 511.0\n",
      "fc layer 3 self.abs_max_out: 628.0\n",
      "fc layer 2 self.abs_max_out: 2374.0\n",
      "fc layer 2 self.abs_max_out: 2416.0\n",
      "fc layer 3 self.abs_max_out: 639.0\n",
      "fc layer 3 self.abs_max_out: 675.0\n",
      "fc layer 2 self.abs_max_out: 2459.0\n",
      "fc layer 2 self.abs_max_out: 2604.0\n",
      "fc layer 2 self.abs_max_out: 2819.0\n",
      "lif layer 2 self.abs_max_v: 3101.5\n",
      "lif layer 2 self.abs_max_v: 3310.0\n",
      "fc layer 1 self.abs_max_out: 6035.0\n",
      "lif layer 1 self.abs_max_v: 6035.0\n",
      "fc layer 2 self.abs_max_out: 2924.0\n",
      "lif layer 2 self.abs_max_v: 3356.0\n",
      "lif layer 2 self.abs_max_v: 3367.5\n",
      "lif layer 2 self.abs_max_v: 3491.5\n",
      "lif layer 2 self.abs_max_v: 3563.5\n",
      "fc layer 3 self.abs_max_out: 750.0\n",
      "lif layer 2 self.abs_max_v: 3815.0\n",
      "fc layer 2 self.abs_max_out: 2966.0\n",
      "fc layer 3 self.abs_max_out: 768.0\n",
      "fc layer 1 self.abs_max_out: 6758.0\n",
      "lif layer 1 self.abs_max_v: 6758.0\n",
      "lif layer 2 self.abs_max_v: 3971.5\n",
      "fc layer 2 self.abs_max_out: 3070.0\n",
      "lif layer 2 self.abs_max_v: 3983.5\n",
      "lif layer 2 self.abs_max_v: 4111.0\n",
      "lif layer 2 self.abs_max_v: 4274.5\n",
      "fc layer 2 self.abs_max_out: 3310.0\n",
      "fc layer 2 self.abs_max_out: 3396.0\n",
      "fc layer 1 self.abs_max_out: 6888.0\n",
      "lif layer 1 self.abs_max_v: 6888.0\n",
      "fc layer 2 self.abs_max_out: 3467.0\n",
      "fc layer 1 self.abs_max_out: 7215.0\n",
      "lif layer 1 self.abs_max_v: 7215.0\n",
      "fc layer 2 self.abs_max_out: 3540.0\n",
      "fc layer 2 self.abs_max_out: 3849.0\n",
      "lif layer 2 self.abs_max_v: 4284.5\n",
      "lif layer 2 self.abs_max_v: 4460.0\n",
      "lif layer 2 self.abs_max_v: 4526.0\n",
      "lif layer 1 self.abs_max_v: 7225.0\n",
      "lif layer 1 self.abs_max_v: 8036.0\n",
      "lif layer 2 self.abs_max_v: 4915.0\n",
      "lif layer 2 self.abs_max_v: 4983.5\n",
      "fc layer 2 self.abs_max_out: 4075.0\n",
      "lif layer 2 self.abs_max_v: 5069.5\n",
      "fc layer 3 self.abs_max_out: 880.0\n",
      "fc layer 1 self.abs_max_out: 7280.0\n",
      "fc layer 1 self.abs_max_out: 7321.0\n",
      "fc layer 1 self.abs_max_out: 7438.0\n",
      "lif layer 1 self.abs_max_v: 8054.0\n",
      "lif layer 1 self.abs_max_v: 8464.0\n",
      "fc layer 2 self.abs_max_out: 4425.0\n",
      "lif layer 1 self.abs_max_v: 8799.0\n",
      "lif layer 1 self.abs_max_v: 10152.5\n",
      "fc layer 3 self.abs_max_out: 926.0\n",
      "fc layer 1 self.abs_max_out: 7639.0\n",
      "lif layer 1 self.abs_max_v: 10823.0\n",
      "lif layer 1 self.abs_max_v: 10838.5\n",
      "lif layer 1 self.abs_max_v: 11005.5\n",
      "fc layer 1 self.abs_max_out: 7818.0\n",
      "fc layer 1 self.abs_max_out: 8138.0\n",
      "fc layer 3 self.abs_max_out: 1022.0\n",
      "lif layer 1 self.abs_max_v: 11830.0\n",
      "fc layer 3 self.abs_max_out: 1057.0\n",
      "fc layer 3 self.abs_max_out: 1065.0\n",
      "lif layer 1 self.abs_max_v: 11867.0\n",
      "lif layer 1 self.abs_max_v: 11989.5\n",
      "lif layer 1 self.abs_max_v: 12130.0\n",
      "lif layer 1 self.abs_max_v: 13064.5\n",
      "lif layer 1 self.abs_max_v: 13672.5\n",
      "lif layer 1 self.abs_max_v: 14506.5\n",
      "lif layer 2 self.abs_max_v: 5172.0\n",
      "fc layer 1 self.abs_max_out: 8233.0\n",
      "lif layer 2 self.abs_max_v: 5256.5\n",
      "lif layer 1 self.abs_max_v: 14737.5\n",
      "lif layer 1 self.abs_max_v: 14970.0\n",
      "lif layer 1 self.abs_max_v: 14977.0\n",
      "fc layer 1 self.abs_max_out: 8569.0\n",
      "lif layer 2 self.abs_max_v: 5321.5\n",
      "lif layer 2 self.abs_max_v: 5375.0\n",
      "lif layer 2 self.abs_max_v: 5531.0\n",
      "lif layer 2 self.abs_max_v: 5591.0\n",
      "lif layer 2 self.abs_max_v: 5974.5\n",
      "fc layer 1 self.abs_max_out: 8903.0\n",
      "fc layer 1 self.abs_max_out: 9768.0\n",
      "lif layer 1 self.abs_max_v: 15394.0\n",
      "lif layer 1 self.abs_max_v: 15702.0\n",
      "lif layer 1 self.abs_max_v: 16729.0\n",
      "lif layer 1 self.abs_max_v: 17045.5\n",
      "fc layer 2 self.abs_max_out: 4498.0\n",
      "lif layer 1 self.abs_max_v: 17720.5\n",
      "fc layer 1 self.abs_max_out: 10013.0\n",
      "lif layer 1 self.abs_max_v: 18159.0\n",
      "fc layer 1 self.abs_max_out: 10431.0\n",
      "fc layer 1 self.abs_max_out: 10453.0\n",
      "fc layer 1 self.abs_max_out: 10584.0\n",
      "lif layer 1 self.abs_max_v: 18989.0\n",
      "lif layer 1 self.abs_max_v: 19848.5\n",
      "fc layer 1 self.abs_max_out: 10716.0\n",
      "fc layer 1 self.abs_max_out: 10771.0\n",
      "lif layer 1 self.abs_max_v: 20493.0\n",
      "fc layer 1 self.abs_max_out: 11360.0\n",
      "lif layer 1 self.abs_max_v: 21606.5\n",
      "fc layer 1 self.abs_max_out: 12158.0\n",
      "lif layer 1 self.abs_max_v: 22961.5\n",
      "lif layer 1 self.abs_max_v: 23481.0\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.999555/  2.105426, val:  33.33%, val_best:  33.33%, tr:  93.46%, tr_best:  93.46%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "total_backward_count 9790 real_backward_count 2824  28.846%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 6363.0\n",
      "lif layer 2 self.abs_max_v: 6739.5\n",
      "lif layer 2 self.abs_max_v: 6878.0\n",
      "lif layer 2 self.abs_max_v: 6934.0\n",
      "lif layer 2 self.abs_max_v: 7151.0\n",
      "lif layer 2 self.abs_max_v: 7479.5\n",
      "fc layer 2 self.abs_max_out: 4706.0\n",
      "fc layer 2 self.abs_max_out: 4747.0\n",
      "fc layer 1 self.abs_max_out: 12290.0\n",
      "lif layer 1 self.abs_max_v: 23761.5\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.987965/  2.112901, val:  37.50%, val_best:  37.50%, tr:  98.06%, tr_best:  98.06%, epoch time: 74.35 seconds, 1.24 minutes\n",
      "total_backward_count 19580 real_backward_count 4874  24.893%\n",
      "fc layer 1 self.abs_max_out: 12386.0\n",
      "lif layer 1 self.abs_max_v: 23977.0\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.988222/  2.102869, val:  48.75%, val_best:  48.75%, tr:  98.57%, tr_best:  98.57%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "total_backward_count 29370 real_backward_count 6777  23.075%\n",
      "fc layer 2 self.abs_max_out: 4762.0\n",
      "fc layer 1 self.abs_max_out: 12752.0\n",
      "fc layer 2 self.abs_max_out: 4878.0\n",
      "fc layer 2 self.abs_max_out: 4931.0\n",
      "fc layer 2 self.abs_max_out: 4985.0\n",
      "fc layer 2 self.abs_max_out: 5042.0\n",
      "lif layer 1 self.abs_max_v: 24006.0\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.986695/  2.107833, val:  33.75%, val_best:  48.75%, tr:  98.67%, tr_best:  98.67%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "total_backward_count 39160 real_backward_count 8592  21.941%\n",
      "fc layer 1 self.abs_max_out: 13224.0\n",
      "fc layer 1 self.abs_max_out: 13231.0\n",
      "fc layer 1 self.abs_max_out: 13632.0\n",
      "fc layer 3 self.abs_max_out: 1077.0\n",
      "lif layer 1 self.abs_max_v: 24089.5\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.988120/  2.111642, val:  40.83%, val_best:  48.75%, tr:  98.67%, tr_best:  98.67%, epoch time: 73.95 seconds, 1.23 minutes\n",
      "total_backward_count 48950 real_backward_count 10328  21.099%\n",
      "fc layer 1 self.abs_max_out: 14069.0\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.987484/  2.115554, val:  48.75%, val_best:  48.75%, tr:  99.28%, tr_best:  99.28%, epoch time: 75.30 seconds, 1.25 minutes\n",
      "total_backward_count 58740 real_backward_count 12164  20.708%\n",
      "fc layer 1 self.abs_max_out: 14268.0\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.995044/  2.097663, val:  47.92%, val_best:  48.75%, tr:  99.28%, tr_best:  99.28%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "total_backward_count 68530 real_backward_count 13888  20.266%\n",
      "fc layer 2 self.abs_max_out: 5086.0\n",
      "fc layer 2 self.abs_max_out: 5122.0\n",
      "fc layer 2 self.abs_max_out: 5173.0\n",
      "fc layer 2 self.abs_max_out: 5255.0\n",
      "fc layer 1 self.abs_max_out: 15841.0\n",
      "lif layer 1 self.abs_max_v: 24786.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.980595/  2.072346, val:  52.08%, val_best:  52.08%, tr:  98.98%, tr_best:  99.28%, epoch time: 75.17 seconds, 1.25 minutes\n",
      "total_backward_count 78320 real_backward_count 15539  19.840%\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.987458/  2.110505, val:  48.75%, val_best:  52.08%, tr:  98.98%, tr_best:  99.28%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "total_backward_count 88110 real_backward_count 17313  19.649%\n",
      "lif layer 2 self.abs_max_v: 7724.0\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.978878/  2.142601, val:  32.08%, val_best:  52.08%, tr:  98.98%, tr_best:  99.28%, epoch time: 75.62 seconds, 1.26 minutes\n",
      "total_backward_count 97900 real_backward_count 19045  19.454%\n",
      "lif layer 1 self.abs_max_v: 25001.0\n",
      "lif layer 1 self.abs_max_v: 25160.0\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.989713/  2.124010, val:  32.50%, val_best:  52.08%, tr:  98.77%, tr_best:  99.28%, epoch time: 74.48 seconds, 1.24 minutes\n",
      "total_backward_count 107690 real_backward_count 20818  19.331%\n",
      "lif layer 1 self.abs_max_v: 25788.5\n",
      "lif layer 1 self.abs_max_v: 26265.5\n",
      "lif layer 1 self.abs_max_v: 26368.5\n",
      "lif layer 2 self.abs_max_v: 8067.5\n",
      "fc layer 1 self.abs_max_out: 16074.0\n",
      "fc layer 2 self.abs_max_out: 5387.0\n",
      "fc layer 2 self.abs_max_out: 5542.0\n",
      "fc layer 3 self.abs_max_out: 1117.0\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.965340/  2.065827, val:  36.25%, val_best:  52.08%, tr:  99.49%, tr_best:  99.49%, epoch time: 73.97 seconds, 1.23 minutes\n",
      "total_backward_count 117480 real_backward_count 22486  19.140%\n",
      "lif layer 1 self.abs_max_v: 26633.0\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.962252/  2.113598, val:  36.67%, val_best:  52.08%, tr:  99.08%, tr_best:  99.49%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "total_backward_count 127270 real_backward_count 24192  19.008%\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.993151/  2.150833, val:  39.58%, val_best:  52.08%, tr:  99.08%, tr_best:  99.49%, epoch time: 75.20 seconds, 1.25 minutes\n",
      "total_backward_count 137060 real_backward_count 25932  18.920%\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.967768/  2.093726, val:  43.33%, val_best:  52.08%, tr:  98.88%, tr_best:  99.49%, epoch time: 74.58 seconds, 1.24 minutes\n",
      "total_backward_count 146850 real_backward_count 27559  18.767%\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.965034/  2.091097, val:  40.00%, val_best:  52.08%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "total_backward_count 156640 real_backward_count 29195  18.638%\n",
      "fc layer 3 self.abs_max_out: 1175.0\n",
      "fc layer 2 self.abs_max_out: 5561.0\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.944235/  2.103931, val:  45.83%, val_best:  52.08%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "total_backward_count 166430 real_backward_count 30826  18.522%\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.971104/  2.082785, val:  43.75%, val_best:  52.08%, tr:  99.18%, tr_best:  99.80%, epoch time: 74.73 seconds, 1.25 minutes\n",
      "total_backward_count 176220 real_backward_count 32509  18.448%\n",
      "fc layer 1 self.abs_max_out: 16647.0\n",
      "lif layer 1 self.abs_max_v: 26816.5\n",
      "fc layer 3 self.abs_max_out: 1203.0\n",
      "fc layer 3 self.abs_max_out: 1299.0\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.947220/  2.077318, val:  44.17%, val_best:  52.08%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "total_backward_count 186010 real_backward_count 34263  18.420%\n",
      "fc layer 2 self.abs_max_out: 5868.0\n",
      "fc layer 2 self.abs_max_out: 5907.0\n",
      "fc layer 3 self.abs_max_out: 1304.0\n",
      "fc layer 2 self.abs_max_out: 6121.0\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.915398/  2.105369, val:  33.33%, val_best:  52.08%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "total_backward_count 195800 real_backward_count 35849  18.309%\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.935117/  2.078529, val:  41.25%, val_best:  52.08%, tr:  99.08%, tr_best:  99.80%, epoch time: 74.64 seconds, 1.24 minutes\n",
      "total_backward_count 205590 real_backward_count 37480  18.230%\n",
      "lif layer 1 self.abs_max_v: 27166.0\n",
      "fc layer 3 self.abs_max_out: 1312.0\n",
      "lif layer 2 self.abs_max_v: 8223.0\n",
      "lif layer 2 self.abs_max_v: 8572.0\n",
      "lif layer 2 self.abs_max_v: 8692.0\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.954836/  2.110816, val:  40.83%, val_best:  52.08%, tr:  98.47%, tr_best:  99.80%, epoch time: 75.27 seconds, 1.25 minutes\n",
      "total_backward_count 215380 real_backward_count 39277  18.236%\n",
      "lif layer 1 self.abs_max_v: 27258.5\n",
      "lif layer 1 self.abs_max_v: 27493.5\n",
      "lif layer 1 self.abs_max_v: 28343.0\n",
      "lif layer 1 self.abs_max_v: 28458.0\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.974586/  2.072222, val:  56.25%, val_best:  56.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "total_backward_count 225170 real_backward_count 40981  18.200%\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.955076/  2.072604, val:  50.83%, val_best:  56.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 74.64 seconds, 1.24 minutes\n",
      "total_backward_count 234960 real_backward_count 42663  18.158%\n",
      "lif layer 1 self.abs_max_v: 28630.0\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.963212/  2.095557, val:  43.75%, val_best:  56.25%, tr:  99.49%, tr_best:  99.80%, epoch time: 74.56 seconds, 1.24 minutes\n",
      "total_backward_count 244750 real_backward_count 44224  18.069%\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.953462/  2.100201, val:  47.08%, val_best:  56.25%, tr:  99.08%, tr_best:  99.80%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "total_backward_count 254540 real_backward_count 45953  18.053%\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.939321/  2.079541, val:  50.83%, val_best:  56.25%, tr:  99.18%, tr_best:  99.80%, epoch time: 74.27 seconds, 1.24 minutes\n",
      "total_backward_count 264330 real_backward_count 47588  18.003%\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.949141/  2.080726, val:  50.42%, val_best:  56.25%, tr:  98.98%, tr_best:  99.80%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "total_backward_count 274120 real_backward_count 49320  17.992%\n",
      "fc layer 3 self.abs_max_out: 1351.0\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.927768/  2.074444, val:  40.83%, val_best:  56.25%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "total_backward_count 283910 real_backward_count 50985  17.958%\n",
      "lif layer 1 self.abs_max_v: 28838.0\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.915129/  2.087338, val:  29.17%, val_best:  56.25%, tr:  99.49%, tr_best:  99.80%, epoch time: 74.73 seconds, 1.25 minutes\n",
      "total_backward_count 293700 real_backward_count 52621  17.917%\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.931963/  2.067928, val:  41.25%, val_best:  56.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 74.36 seconds, 1.24 minutes\n",
      "total_backward_count 303490 real_backward_count 54201  17.859%\n",
      "fc layer 3 self.abs_max_out: 1388.0\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.928864/  2.098621, val:  34.17%, val_best:  56.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "total_backward_count 313280 real_backward_count 55860  17.831%\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.943848/  2.089748, val:  38.75%, val_best:  56.25%, tr:  99.18%, tr_best:  99.80%, epoch time: 74.00 seconds, 1.23 minutes\n",
      "total_backward_count 323070 real_backward_count 57497  17.797%\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.960808/  2.111341, val:  44.58%, val_best:  56.25%, tr:  98.98%, tr_best:  99.80%, epoch time: 75.35 seconds, 1.26 minutes\n",
      "total_backward_count 332860 real_backward_count 59213  17.789%\n",
      "lif layer 1 self.abs_max_v: 29147.5\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.961804/  2.132989, val:  47.92%, val_best:  56.25%, tr:  98.77%, tr_best:  99.80%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "total_backward_count 342650 real_backward_count 60886  17.769%\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.987238/  2.091278, val:  43.33%, val_best:  56.25%, tr:  98.88%, tr_best:  99.80%, epoch time: 74.19 seconds, 1.24 minutes\n",
      "total_backward_count 352440 real_backward_count 62636  17.772%\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.960797/  2.082092, val:  53.33%, val_best:  56.25%, tr:  99.08%, tr_best:  99.80%, epoch time: 74.19 seconds, 1.24 minutes\n",
      "total_backward_count 362230 real_backward_count 64313  17.755%\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.971275/  2.111222, val:  43.75%, val_best:  56.25%, tr:  98.98%, tr_best:  99.80%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "total_backward_count 372020 real_backward_count 65912  17.717%\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.975394/  2.114462, val:  45.00%, val_best:  56.25%, tr:  98.16%, tr_best:  99.80%, epoch time: 75.12 seconds, 1.25 minutes\n",
      "total_backward_count 381810 real_backward_count 67667  17.723%\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.961558/  2.101375, val:  36.67%, val_best:  56.25%, tr:  98.98%, tr_best:  99.80%, epoch time: 75.76 seconds, 1.26 minutes\n",
      "total_backward_count 391600 real_backward_count 69337  17.706%\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.973757/  2.107931, val:  48.75%, val_best:  56.25%, tr:  98.98%, tr_best:  99.80%, epoch time: 75.63 seconds, 1.26 minutes\n",
      "total_backward_count 401390 real_backward_count 71015  17.692%\n",
      "lif layer 1 self.abs_max_v: 29543.0\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.965141/  2.116068, val:  50.83%, val_best:  56.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 74.11 seconds, 1.24 minutes\n",
      "total_backward_count 411180 real_backward_count 72700  17.681%\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.985818/  2.127283, val:  36.25%, val_best:  56.25%, tr:  99.49%, tr_best:  99.80%, epoch time: 74.28 seconds, 1.24 minutes\n",
      "total_backward_count 420970 real_backward_count 74352  17.662%\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.977103/  2.117935, val:  45.83%, val_best:  56.25%, tr:  99.49%, tr_best:  99.80%, epoch time: 70.36 seconds, 1.17 minutes\n",
      "total_backward_count 430760 real_backward_count 76085  17.663%\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.962902/  2.093195, val:  37.08%, val_best:  56.25%, tr:  99.08%, tr_best:  99.80%, epoch time: 62.12 seconds, 1.04 minutes\n",
      "total_backward_count 440550 real_backward_count 77788  17.657%\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.953374/  2.121704, val:  39.17%, val_best:  56.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 60.83 seconds, 1.01 minutes\n",
      "total_backward_count 450340 real_backward_count 79406  17.632%\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.958325/  2.114200, val:  37.92%, val_best:  56.25%, tr:  98.57%, tr_best:  99.80%, epoch time: 61.71 seconds, 1.03 minutes\n",
      "total_backward_count 460130 real_backward_count 81074  17.620%\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.982679/  2.128927, val:  32.08%, val_best:  56.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 62.28 seconds, 1.04 minutes\n",
      "total_backward_count 469920 real_backward_count 82736  17.606%\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.973559/  2.115028, val:  47.92%, val_best:  56.25%, tr:  98.98%, tr_best:  99.80%, epoch time: 60.71 seconds, 1.01 minutes\n",
      "total_backward_count 479710 real_backward_count 84416  17.597%\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.978378/  2.098759, val:  52.50%, val_best:  56.25%, tr:  98.88%, tr_best:  99.80%, epoch time: 59.94 seconds, 1.00 minutes\n",
      "total_backward_count 489500 real_backward_count 86091  17.588%\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.959327/  2.081023, val:  44.58%, val_best:  56.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 60.52 seconds, 1.01 minutes\n",
      "total_backward_count 499290 real_backward_count 87747  17.574%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.951765/  2.123704, val:  39.58%, val_best:  56.25%, tr:  98.88%, tr_best:  99.80%, epoch time: 60.94 seconds, 1.02 minutes\n",
      "total_backward_count 509080 real_backward_count 89417  17.564%\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.961225/  2.093340, val:  53.75%, val_best:  56.25%, tr:  99.49%, tr_best:  99.80%, epoch time: 61.62 seconds, 1.03 minutes\n",
      "total_backward_count 518870 real_backward_count 91006  17.539%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.956918/  2.104765, val:  43.33%, val_best:  56.25%, tr:  99.18%, tr_best:  99.80%, epoch time: 60.60 seconds, 1.01 minutes\n",
      "total_backward_count 528660 real_backward_count 92698  17.535%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.957602/  2.077216, val:  50.83%, val_best:  56.25%, tr:  98.67%, tr_best:  99.80%, epoch time: 61.52 seconds, 1.03 minutes\n",
      "total_backward_count 538450 real_backward_count 94424  17.536%\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.969588/  2.111033, val:  39.58%, val_best:  56.25%, tr:  99.18%, tr_best:  99.80%, epoch time: 61.69 seconds, 1.03 minutes\n",
      "total_backward_count 548240 real_backward_count 96095  17.528%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.947403/  2.118156, val:  41.67%, val_best:  56.25%, tr:  99.18%, tr_best:  99.80%, epoch time: 60.70 seconds, 1.01 minutes\n",
      "total_backward_count 558030 real_backward_count 97736  17.514%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.951787/  2.106854, val:  45.83%, val_best:  56.25%, tr:  98.88%, tr_best:  99.80%, epoch time: 61.03 seconds, 1.02 minutes\n",
      "total_backward_count 567820 real_backward_count 99400  17.506%\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.956820/  2.071895, val:  42.92%, val_best:  56.25%, tr:  98.88%, tr_best:  99.80%, epoch time: 61.64 seconds, 1.03 minutes\n",
      "total_backward_count 577610 real_backward_count 101053  17.495%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.945515/  2.078788, val:  42.92%, val_best:  56.25%, tr:  98.88%, tr_best:  99.80%, epoch time: 60.81 seconds, 1.01 minutes\n",
      "total_backward_count 587400 real_backward_count 102701  17.484%\n",
      "fc layer 1 self.abs_max_out: 16866.0\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.946773/  2.101164, val:  45.83%, val_best:  56.25%, tr:  98.98%, tr_best:  99.80%, epoch time: 61.24 seconds, 1.02 minutes\n",
      "total_backward_count 597190 real_backward_count 104283  17.462%\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.945738/  2.063378, val:  48.75%, val_best:  56.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 61.06 seconds, 1.02 minutes\n",
      "total_backward_count 606980 real_backward_count 105908  17.448%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.956277/  2.059589, val:  38.75%, val_best:  56.25%, tr:  98.88%, tr_best:  99.80%, epoch time: 61.51 seconds, 1.03 minutes\n",
      "total_backward_count 616770 real_backward_count 107568  17.441%\n",
      "lif layer 2 self.abs_max_v: 9073.5\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.953242/  2.105201, val:  37.92%, val_best:  56.25%, tr:  98.98%, tr_best:  99.80%, epoch time: 60.78 seconds, 1.01 minutes\n",
      "total_backward_count 626560 real_backward_count 109184  17.426%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.953990/  2.074324, val:  27.50%, val_best:  56.25%, tr:  99.18%, tr_best:  99.80%, epoch time: 62.13 seconds, 1.04 minutes\n",
      "total_backward_count 636350 real_backward_count 110748  17.404%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.951922/  2.113215, val:  44.58%, val_best:  56.25%, tr:  98.98%, tr_best:  99.80%, epoch time: 62.27 seconds, 1.04 minutes\n",
      "total_backward_count 646140 real_backward_count 112402  17.396%\n",
      "lif layer 2 self.abs_max_v: 9131.0\n",
      "fc layer 1 self.abs_max_out: 17650.0\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.991230/  2.097170, val:  50.00%, val_best:  56.25%, tr:  98.77%, tr_best:  99.80%, epoch time: 61.26 seconds, 1.02 minutes\n",
      "total_backward_count 655930 real_backward_count 114150  17.403%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.971554/  2.130076, val:  43.33%, val_best:  56.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 62.18 seconds, 1.04 minutes\n",
      "total_backward_count 665720 real_backward_count 115824  17.398%\n",
      "fc layer 1 self.abs_max_out: 18039.0\n",
      "lif layer 1 self.abs_max_v: 29991.0\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.961357/  2.059045, val:  55.42%, val_best:  56.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 61.42 seconds, 1.02 minutes\n",
      "total_backward_count 675510 real_backward_count 117450  17.387%\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.942531/  2.124620, val:  31.67%, val_best:  56.25%, tr:  99.18%, tr_best:  99.80%, epoch time: 61.47 seconds, 1.02 minutes\n",
      "total_backward_count 685300 real_backward_count 119057  17.373%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.961108/  2.110548, val:  57.08%, val_best:  57.08%, tr:  98.98%, tr_best:  99.80%, epoch time: 62.03 seconds, 1.03 minutes\n",
      "total_backward_count 695090 real_backward_count 120744  17.371%\n",
      "fc layer 1 self.abs_max_out: 18046.0\n",
      "lif layer 1 self.abs_max_v: 30212.0\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.944852/  2.094190, val:  35.42%, val_best:  57.08%, tr:  99.28%, tr_best:  99.80%, epoch time: 62.66 seconds, 1.04 minutes\n",
      "total_backward_count 704880 real_backward_count 122346  17.357%\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.932582/  2.052391, val:  52.92%, val_best:  57.08%, tr:  98.98%, tr_best:  99.80%, epoch time: 63.07 seconds, 1.05 minutes\n",
      "total_backward_count 714670 real_backward_count 123959  17.345%\n",
      "lif layer 2 self.abs_max_v: 9158.5\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.919858/  2.110331, val:  40.42%, val_best:  57.08%, tr:  99.49%, tr_best:  99.80%, epoch time: 62.42 seconds, 1.04 minutes\n",
      "total_backward_count 724460 real_backward_count 125559  17.331%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.931858/  2.082718, val:  41.25%, val_best:  57.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 61.87 seconds, 1.03 minutes\n",
      "total_backward_count 734250 real_backward_count 127198  17.324%\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.959825/  2.070188, val:  52.08%, val_best:  57.08%, tr:  99.08%, tr_best:  99.80%, epoch time: 61.58 seconds, 1.03 minutes\n",
      "total_backward_count 744040 real_backward_count 128880  17.322%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.952883/  2.095163, val:  35.83%, val_best:  57.08%, tr:  99.18%, tr_best:  99.80%, epoch time: 62.25 seconds, 1.04 minutes\n",
      "total_backward_count 753830 real_backward_count 130545  17.318%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.927841/  2.128149, val:  37.92%, val_best:  57.08%, tr:  99.08%, tr_best:  99.80%, epoch time: 63.33 seconds, 1.06 minutes\n",
      "total_backward_count 763620 real_backward_count 132150  17.306%\n",
      "lif layer 1 self.abs_max_v: 30447.0\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.971471/  2.080751, val:  35.00%, val_best:  57.08%, tr:  99.28%, tr_best:  99.80%, epoch time: 62.55 seconds, 1.04 minutes\n",
      "total_backward_count 773410 real_backward_count 133809  17.301%\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.946672/  2.071880, val:  46.25%, val_best:  57.08%, tr:  98.98%, tr_best:  99.80%, epoch time: 63.00 seconds, 1.05 minutes\n",
      "total_backward_count 783200 real_backward_count 135418  17.290%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.943812/  2.094249, val:  48.75%, val_best:  57.08%, tr:  99.28%, tr_best:  99.80%, epoch time: 63.03 seconds, 1.05 minutes\n",
      "total_backward_count 792990 real_backward_count 136982  17.274%\n",
      "lif layer 1 self.abs_max_v: 32249.5\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.973984/  2.110621, val:  37.08%, val_best:  57.08%, tr:  99.18%, tr_best:  99.80%, epoch time: 60.77 seconds, 1.01 minutes\n",
      "total_backward_count 802780 real_backward_count 138537  17.257%\n",
      "fc layer 1 self.abs_max_out: 18519.0\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  1.966041/  2.074397, val:  49.58%, val_best:  57.08%, tr:  98.77%, tr_best:  99.80%, epoch time: 61.79 seconds, 1.03 minutes\n",
      "total_backward_count 812570 real_backward_count 140248  17.260%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  1.953280/  2.127045, val:  35.00%, val_best:  57.08%, tr:  99.18%, tr_best:  99.80%, epoch time: 61.09 seconds, 1.02 minutes\n",
      "total_backward_count 822360 real_backward_count 141891  17.254%\n",
      "fc layer 1 self.abs_max_out: 18935.0\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  1.981686/  2.130151, val:  37.50%, val_best:  57.08%, tr:  99.28%, tr_best:  99.80%, epoch time: 61.67 seconds, 1.03 minutes\n",
      "total_backward_count 832150 real_backward_count 143581  17.254%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  1.985324/  2.104235, val:  45.00%, val_best:  57.08%, tr:  98.98%, tr_best:  99.80%, epoch time: 61.64 seconds, 1.03 minutes\n",
      "total_backward_count 841940 real_backward_count 145219  17.248%\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  1.975048/  2.114936, val:  57.08%, val_best:  57.08%, tr:  99.28%, tr_best:  99.80%, epoch time: 60.70 seconds, 1.01 minutes\n",
      "total_backward_count 851730 real_backward_count 146888  17.246%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  1.998630/  2.128089, val:  52.08%, val_best:  57.08%, tr:  99.08%, tr_best:  99.80%, epoch time: 62.16 seconds, 1.04 minutes\n",
      "total_backward_count 861520 real_backward_count 148571  17.245%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  1.981040/  2.086548, val:  51.67%, val_best:  57.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 61.05 seconds, 1.02 minutes\n",
      "total_backward_count 871310 real_backward_count 150189  17.237%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  1.985688/  2.098903, val:  38.33%, val_best:  57.08%, tr:  99.08%, tr_best:  99.80%, epoch time: 60.60 seconds, 1.01 minutes\n",
      "total_backward_count 881100 real_backward_count 151889  17.239%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  1.984913/  2.138692, val:  42.50%, val_best:  57.08%, tr:  98.37%, tr_best:  99.80%, epoch time: 60.66 seconds, 1.01 minutes\n",
      "total_backward_count 890890 real_backward_count 153578  17.239%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  1.971929/  2.074606, val:  54.17%, val_best:  57.08%, tr:  99.18%, tr_best:  99.80%, epoch time: 61.53 seconds, 1.03 minutes\n",
      "total_backward_count 900680 real_backward_count 155214  17.233%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  1.978896/  2.120081, val:  36.25%, val_best:  57.08%, tr:  98.98%, tr_best:  99.80%, epoch time: 60.86 seconds, 1.01 minutes\n",
      "total_backward_count 910470 real_backward_count 156931  17.236%\n",
      "lif layer 2 self.abs_max_v: 9182.5\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  1.968737/  2.086437, val:  47.92%, val_best:  57.08%, tr:  98.67%, tr_best:  99.80%, epoch time: 61.56 seconds, 1.03 minutes\n",
      "total_backward_count 920260 real_backward_count 158578  17.232%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  1.954494/  2.058645, val:  51.67%, val_best:  57.08%, tr:  99.59%, tr_best:  99.80%, epoch time: 61.74 seconds, 1.03 minutes\n",
      "total_backward_count 930050 real_backward_count 160214  17.226%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  1.950273/  2.089600, val:  50.42%, val_best:  57.08%, tr:  99.28%, tr_best:  99.80%, epoch time: 61.40 seconds, 1.02 minutes\n",
      "total_backward_count 939840 real_backward_count 161887  17.225%\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  1.965195/  2.106195, val:  50.00%, val_best:  57.08%, tr:  99.18%, tr_best:  99.80%, epoch time: 61.11 seconds, 1.02 minutes\n",
      "total_backward_count 949630 real_backward_count 163498  17.217%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  1.977568/  2.061072, val:  54.58%, val_best:  57.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 60.35 seconds, 1.01 minutes\n",
      "total_backward_count 959420 real_backward_count 165146  17.213%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  1.973574/  2.126346, val:  40.42%, val_best:  57.08%, tr:  99.08%, tr_best:  99.80%, epoch time: 61.98 seconds, 1.03 minutes\n",
      "total_backward_count 969210 real_backward_count 166781  17.208%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  1.960200/  2.109864, val:  54.17%, val_best:  57.08%, tr:  99.08%, tr_best:  99.80%, epoch time: 61.17 seconds, 1.02 minutes\n",
      "total_backward_count 979000 real_backward_count 168440  17.205%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  1.964216/  2.134980, val:  35.83%, val_best:  57.08%, tr:  99.28%, tr_best:  99.80%, epoch time: 59.93 seconds, 1.00 minutes\n",
      "total_backward_count 988790 real_backward_count 170025  17.195%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  1.951664/  2.059051, val:  51.67%, val_best:  57.08%, tr:  99.59%, tr_best:  99.80%, epoch time: 61.23 seconds, 1.02 minutes\n",
      "total_backward_count 998580 real_backward_count 171668  17.191%\n",
      "lif layer 1 self.abs_max_v: 33532.0\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  1.948331/  2.101111, val:  45.00%, val_best:  57.08%, tr:  99.08%, tr_best:  99.80%, epoch time: 61.92 seconds, 1.03 minutes\n",
      "total_backward_count 1008370 real_backward_count 173256  17.182%\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  1.956003/  2.065304, val:  47.50%, val_best:  57.08%, tr:  98.98%, tr_best:  99.80%, epoch time: 61.36 seconds, 1.02 minutes\n",
      "total_backward_count 1018160 real_backward_count 174902  17.178%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  1.958728/  2.078157, val:  47.50%, val_best:  57.08%, tr:  98.57%, tr_best:  99.80%, epoch time: 61.34 seconds, 1.02 minutes\n",
      "total_backward_count 1027950 real_backward_count 176607  17.181%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  1.950141/  2.107592, val:  45.00%, val_best:  57.08%, tr:  98.57%, tr_best:  99.80%, epoch time: 61.75 seconds, 1.03 minutes\n",
      "total_backward_count 1037740 real_backward_count 178354  17.187%\n",
      "lif layer 2 self.abs_max_v: 9304.0\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  1.969064/  2.125216, val:  35.83%, val_best:  57.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 62.03 seconds, 1.03 minutes\n",
      "total_backward_count 1047530 real_backward_count 180046  17.188%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  1.989071/  2.082991, val:  47.08%, val_best:  57.08%, tr:  98.47%, tr_best:  99.80%, epoch time: 60.66 seconds, 1.01 minutes\n",
      "total_backward_count 1057320 real_backward_count 181743  17.189%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  1.969367/  2.092945, val:  50.83%, val_best:  57.08%, tr:  99.18%, tr_best:  99.80%, epoch time: 60.95 seconds, 1.02 minutes\n",
      "total_backward_count 1067110 real_backward_count 183422  17.189%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  1.944727/  2.083019, val:  39.17%, val_best:  57.08%, tr:  99.49%, tr_best:  99.80%, epoch time: 61.69 seconds, 1.03 minutes\n",
      "total_backward_count 1076900 real_backward_count 184994  17.178%\n",
      "lif layer 2 self.abs_max_v: 9336.0\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  1.951397/  2.074272, val:  61.67%, val_best:  61.67%, tr:  99.49%, tr_best:  99.80%, epoch time: 60.30 seconds, 1.00 minutes\n",
      "total_backward_count 1086690 real_backward_count 186626  17.174%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  1.970260/  2.089395, val:  49.58%, val_best:  61.67%, tr:  99.18%, tr_best:  99.80%, epoch time: 61.28 seconds, 1.02 minutes\n",
      "total_backward_count 1096480 real_backward_count 188239  17.168%\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  1.971761/  2.124801, val:  58.33%, val_best:  61.67%, tr:  98.88%, tr_best:  99.80%, epoch time: 62.10 seconds, 1.03 minutes\n",
      "total_backward_count 1106270 real_backward_count 189944  17.170%\n",
      "fc layer 3 self.abs_max_out: 1486.0\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  1.962696/  2.089040, val:  47.08%, val_best:  61.67%, tr:  98.77%, tr_best:  99.80%, epoch time: 60.41 seconds, 1.01 minutes\n",
      "total_backward_count 1116060 real_backward_count 191585  17.166%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  1.955911/  2.119576, val:  34.58%, val_best:  61.67%, tr:  98.77%, tr_best:  99.80%, epoch time: 61.35 seconds, 1.02 minutes\n",
      "total_backward_count 1125850 real_backward_count 193272  17.167%\n",
      "lif layer 2 self.abs_max_v: 9575.5\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  1.943628/  2.110407, val:  41.25%, val_best:  61.67%, tr:  98.67%, tr_best:  99.80%, epoch time: 60.95 seconds, 1.02 minutes\n",
      "total_backward_count 1135640 real_backward_count 194975  17.169%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  1.955799/  2.122476, val:  47.50%, val_best:  61.67%, tr:  99.18%, tr_best:  99.80%, epoch time: 61.56 seconds, 1.03 minutes\n",
      "total_backward_count 1145430 real_backward_count 196665  17.170%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  1.993536/  2.092238, val:  51.67%, val_best:  61.67%, tr:  99.08%, tr_best:  99.80%, epoch time: 60.80 seconds, 1.01 minutes\n",
      "total_backward_count 1155220 real_backward_count 198410  17.175%\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  1.964856/  2.095876, val:  45.00%, val_best:  61.67%, tr:  99.18%, tr_best:  99.80%, epoch time: 60.31 seconds, 1.01 minutes\n",
      "total_backward_count 1165010 real_backward_count 200091  17.175%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  1.972311/  2.115397, val:  47.08%, val_best:  61.67%, tr:  98.98%, tr_best:  99.80%, epoch time: 60.66 seconds, 1.01 minutes\n",
      "total_backward_count 1174800 real_backward_count 201696  17.169%\n",
      "lif layer 2 self.abs_max_v: 9660.0\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  1.969241/  2.120992, val:  50.83%, val_best:  61.67%, tr:  98.77%, tr_best:  99.80%, epoch time: 61.91 seconds, 1.03 minutes\n",
      "total_backward_count 1184590 real_backward_count 203395  17.170%\n",
      "fc layer 1 self.abs_max_out: 19598.0\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  1.966958/  2.115472, val:  47.92%, val_best:  61.67%, tr:  99.28%, tr_best:  99.80%, epoch time: 60.99 seconds, 1.02 minutes\n",
      "total_backward_count 1194380 real_backward_count 205062  17.169%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  1.961861/  2.071373, val:  51.25%, val_best:  61.67%, tr:  98.98%, tr_best:  99.80%, epoch time: 60.94 seconds, 1.02 minutes\n",
      "total_backward_count 1204170 real_backward_count 206696  17.165%\n",
      "lif layer 2 self.abs_max_v: 9754.5\n",
      "lif layer 2 self.abs_max_v: 9986.5\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  1.956487/  2.077822, val:  51.67%, val_best:  61.67%, tr:  99.49%, tr_best:  99.80%, epoch time: 61.26 seconds, 1.02 minutes\n",
      "total_backward_count 1213960 real_backward_count 208320  17.160%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  1.960687/  2.081826, val:  54.58%, val_best:  61.67%, tr:  98.98%, tr_best:  99.80%, epoch time: 61.62 seconds, 1.03 minutes\n",
      "total_backward_count 1223750 real_backward_count 209973  17.158%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  1.963636/  2.074649, val:  40.83%, val_best:  61.67%, tr:  98.98%, tr_best:  99.80%, epoch time: 60.64 seconds, 1.01 minutes\n",
      "total_backward_count 1233540 real_backward_count 211664  17.159%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  1.967844/  2.118202, val:  49.17%, val_best:  61.67%, tr:  99.28%, tr_best:  99.80%, epoch time: 60.58 seconds, 1.01 minutes\n",
      "total_backward_count 1243330 real_backward_count 213321  17.157%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  1.985903/  2.086389, val:  51.25%, val_best:  61.67%, tr:  99.39%, tr_best:  99.80%, epoch time: 62.65 seconds, 1.04 minutes\n",
      "total_backward_count 1253120 real_backward_count 214926  17.151%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  1.972704/  2.093369, val:  52.50%, val_best:  61.67%, tr:  98.98%, tr_best:  99.80%, epoch time: 60.13 seconds, 1.00 minutes\n",
      "total_backward_count 1262910 real_backward_count 216595  17.150%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  1.957930/  2.118289, val:  50.00%, val_best:  61.67%, tr:  99.80%, tr_best:  99.80%, epoch time: 60.74 seconds, 1.01 minutes\n",
      "total_backward_count 1272700 real_backward_count 218187  17.144%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  1.975645/  2.131920, val:  42.92%, val_best:  61.67%, tr:  99.49%, tr_best:  99.80%, epoch time: 61.88 seconds, 1.03 minutes\n",
      "total_backward_count 1282490 real_backward_count 219832  17.141%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  1.960504/  2.076416, val:  48.33%, val_best:  61.67%, tr:  98.98%, tr_best:  99.80%, epoch time: 60.71 seconds, 1.01 minutes\n",
      "total_backward_count 1292280 real_backward_count 221446  17.136%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  1.942729/  2.070074, val:  48.33%, val_best:  61.67%, tr:  99.49%, tr_best:  99.80%, epoch time: 61.14 seconds, 1.02 minutes\n",
      "total_backward_count 1302070 real_backward_count 223038  17.129%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  1.926107/  2.090480, val:  40.83%, val_best:  61.67%, tr:  98.98%, tr_best:  99.80%, epoch time: 60.51 seconds, 1.01 minutes\n",
      "total_backward_count 1311860 real_backward_count 224737  17.131%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  1.950878/  2.138278, val:  42.08%, val_best:  61.67%, tr:  98.98%, tr_best:  99.80%, epoch time: 61.78 seconds, 1.03 minutes\n",
      "total_backward_count 1321650 real_backward_count 226347  17.126%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  1.975198/  2.097137, val:  50.42%, val_best:  61.67%, tr:  98.37%, tr_best:  99.80%, epoch time: 61.48 seconds, 1.02 minutes\n",
      "total_backward_count 1331440 real_backward_count 228067  17.129%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  1.964899/  2.094907, val:  45.83%, val_best:  61.67%, tr:  99.39%, tr_best:  99.80%, epoch time: 60.56 seconds, 1.01 minutes\n",
      "total_backward_count 1341230 real_backward_count 229695  17.126%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  1.981753/  2.136081, val:  42.92%, val_best:  61.67%, tr:  98.77%, tr_best:  99.80%, epoch time: 61.34 seconds, 1.02 minutes\n",
      "total_backward_count 1351020 real_backward_count 231313  17.121%\n",
      "fc layer 1 self.abs_max_out: 20210.0\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  1.979687/  2.091311, val:  48.75%, val_best:  61.67%, tr:  98.77%, tr_best:  99.80%, epoch time: 60.56 seconds, 1.01 minutes\n",
      "total_backward_count 1360810 real_backward_count 232970  17.120%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  1.990483/  2.100395, val:  47.08%, val_best:  61.67%, tr:  99.08%, tr_best:  99.80%, epoch time: 60.80 seconds, 1.01 minutes\n",
      "total_backward_count 1370600 real_backward_count 234674  17.122%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  1.978187/  2.104230, val:  33.33%, val_best:  61.67%, tr:  99.59%, tr_best:  99.80%, epoch time: 60.55 seconds, 1.01 minutes\n",
      "total_backward_count 1380390 real_backward_count 236275  17.117%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  1.990855/  2.119323, val:  46.25%, val_best:  61.67%, tr:  99.39%, tr_best:  99.80%, epoch time: 61.01 seconds, 1.02 minutes\n",
      "total_backward_count 1390180 real_backward_count 237948  17.116%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  1.962827/  2.096289, val:  46.67%, val_best:  61.67%, tr:  99.18%, tr_best:  99.80%, epoch time: 60.92 seconds, 1.02 minutes\n",
      "total_backward_count 1399970 real_backward_count 239663  17.119%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  1.944469/  2.042345, val:  48.33%, val_best:  61.67%, tr:  99.59%, tr_best:  99.80%, epoch time: 59.97 seconds, 1.00 minutes\n",
      "total_backward_count 1409760 real_backward_count 241338  17.119%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  1.948860/  2.108952, val:  45.83%, val_best:  61.67%, tr:  99.28%, tr_best:  99.80%, epoch time: 61.01 seconds, 1.02 minutes\n",
      "total_backward_count 1419550 real_backward_count 242929  17.113%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  1.958330/  2.095967, val:  36.25%, val_best:  61.67%, tr:  99.08%, tr_best:  99.80%, epoch time: 61.53 seconds, 1.03 minutes\n",
      "total_backward_count 1429340 real_backward_count 244547  17.109%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  1.961735/  2.108220, val:  41.67%, val_best:  61.67%, tr:  98.67%, tr_best:  99.80%, epoch time: 60.51 seconds, 1.01 minutes\n",
      "total_backward_count 1439130 real_backward_count 246187  17.107%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  1.987110/  2.139218, val:  39.17%, val_best:  61.67%, tr:  99.59%, tr_best:  99.80%, epoch time: 61.25 seconds, 1.02 minutes\n",
      "total_backward_count 1448920 real_backward_count 247807  17.103%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  1.988459/  2.077331, val:  50.83%, val_best:  61.67%, tr:  99.18%, tr_best:  99.80%, epoch time: 61.12 seconds, 1.02 minutes\n",
      "total_backward_count 1458710 real_backward_count 249416  17.098%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  1.983035/  2.106850, val:  47.92%, val_best:  61.67%, tr:  99.39%, tr_best:  99.80%, epoch time: 60.70 seconds, 1.01 minutes\n",
      "total_backward_count 1468500 real_backward_count 251057  17.096%\n",
      "lif layer 2 self.abs_max_v: 10252.5\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  1.988161/  2.092091, val:  53.75%, val_best:  61.67%, tr:  99.18%, tr_best:  99.80%, epoch time: 61.42 seconds, 1.02 minutes\n",
      "total_backward_count 1478290 real_backward_count 252645  17.090%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  1.993826/  2.102707, val:  55.42%, val_best:  61.67%, tr:  98.57%, tr_best:  99.80%, epoch time: 61.46 seconds, 1.02 minutes\n",
      "total_backward_count 1488080 real_backward_count 254301  17.089%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  1.994054/  2.113505, val:  53.33%, val_best:  61.67%, tr:  99.18%, tr_best:  99.80%, epoch time: 60.67 seconds, 1.01 minutes\n",
      "total_backward_count 1497870 real_backward_count 255896  17.084%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  1.973123/  2.067666, val:  52.50%, val_best:  61.67%, tr:  99.18%, tr_best:  99.80%, epoch time: 60.74 seconds, 1.01 minutes\n",
      "total_backward_count 1507660 real_backward_count 257498  17.079%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  1.976637/  2.096848, val:  50.83%, val_best:  61.67%, tr:  98.88%, tr_best:  99.80%, epoch time: 60.77 seconds, 1.01 minutes\n",
      "total_backward_count 1517450 real_backward_count 259120  17.076%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  1.988056/  2.118665, val:  45.00%, val_best:  61.67%, tr:  99.08%, tr_best:  99.80%, epoch time: 61.82 seconds, 1.03 minutes\n",
      "total_backward_count 1527240 real_backward_count 260789  17.076%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  1.957594/  2.092088, val:  44.58%, val_best:  61.67%, tr:  98.98%, tr_best:  99.80%, epoch time: 61.42 seconds, 1.02 minutes\n",
      "total_backward_count 1537030 real_backward_count 262459  17.076%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  1.959002/  2.079790, val:  63.33%, val_best:  63.33%, tr:  99.59%, tr_best:  99.80%, epoch time: 60.13 seconds, 1.00 minutes\n",
      "total_backward_count 1546820 real_backward_count 264050  17.071%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  1.976039/  2.144796, val:  41.25%, val_best:  63.33%, tr:  99.18%, tr_best:  99.80%, epoch time: 61.55 seconds, 1.03 minutes\n",
      "total_backward_count 1556610 real_backward_count 265657  17.066%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  1.968421/  2.124052, val:  42.50%, val_best:  63.33%, tr:  98.98%, tr_best:  99.80%, epoch time: 60.56 seconds, 1.01 minutes\n",
      "total_backward_count 1566400 real_backward_count 267249  17.061%\n",
      "fc layer 1 self.abs_max_out: 21991.0\n",
      "lif layer 1 self.abs_max_v: 37553.5\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  1.969563/  2.096473, val:  53.33%, val_best:  63.33%, tr:  98.47%, tr_best:  99.80%, epoch time: 60.99 seconds, 1.02 minutes\n",
      "total_backward_count 1576190 real_backward_count 268905  17.060%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  1.984156/  2.090343, val:  51.67%, val_best:  63.33%, tr:  99.28%, tr_best:  99.80%, epoch time: 61.59 seconds, 1.03 minutes\n",
      "total_backward_count 1585980 real_backward_count 270529  17.058%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  1.992203/  2.142061, val:  41.67%, val_best:  63.33%, tr:  98.88%, tr_best:  99.80%, epoch time: 60.95 seconds, 1.02 minutes\n",
      "total_backward_count 1595770 real_backward_count 272163  17.055%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  1.984976/  2.112806, val:  50.00%, val_best:  63.33%, tr:  99.39%, tr_best:  99.80%, epoch time: 61.02 seconds, 1.02 minutes\n",
      "total_backward_count 1605560 real_backward_count 273753  17.050%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  1.965614/  2.073837, val:  46.67%, val_best:  63.33%, tr:  99.39%, tr_best:  99.80%, epoch time: 60.86 seconds, 1.01 minutes\n",
      "total_backward_count 1615350 real_backward_count 275395  17.049%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  1.975399/  2.106473, val:  47.92%, val_best:  63.33%, tr:  98.77%, tr_best:  99.80%, epoch time: 60.80 seconds, 1.01 minutes\n",
      "total_backward_count 1625140 real_backward_count 277108  17.051%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  1.985203/  2.146466, val:  45.83%, val_best:  63.33%, tr:  99.18%, tr_best:  99.80%, epoch time: 61.17 seconds, 1.02 minutes\n",
      "total_backward_count 1634930 real_backward_count 278695  17.046%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  1.990211/  2.113271, val:  38.75%, val_best:  63.33%, tr:  98.67%, tr_best:  99.80%, epoch time: 60.79 seconds, 1.01 minutes\n",
      "total_backward_count 1644720 real_backward_count 280393  17.048%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  1.982354/  2.138303, val:  33.33%, val_best:  63.33%, tr:  99.80%, tr_best:  99.80%, epoch time: 61.79 seconds, 1.03 minutes\n",
      "total_backward_count 1654510 real_backward_count 281968  17.042%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  1.970234/  2.109305, val:  52.92%, val_best:  63.33%, tr:  99.90%, tr_best:  99.90%, epoch time: 60.72 seconds, 1.01 minutes\n",
      "total_backward_count 1664300 real_backward_count 283527  17.036%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  1.982372/  2.095615, val:  32.92%, val_best:  63.33%, tr:  99.28%, tr_best:  99.90%, epoch time: 60.20 seconds, 1.00 minutes\n",
      "total_backward_count 1674090 real_backward_count 285092  17.030%\n",
      "lif layer 2 self.abs_max_v: 10256.0\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  1.991756/  2.097688, val:  42.92%, val_best:  63.33%, tr:  99.28%, tr_best:  99.90%, epoch time: 60.85 seconds, 1.01 minutes\n",
      "total_backward_count 1683880 real_backward_count 286662  17.024%\n",
      "lif layer 2 self.abs_max_v: 10304.5\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  1.967629/  2.077094, val:  60.83%, val_best:  63.33%, tr:  99.08%, tr_best:  99.90%, epoch time: 61.68 seconds, 1.03 minutes\n",
      "total_backward_count 1693670 real_backward_count 288386  17.027%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  1.972247/  2.102109, val:  50.00%, val_best:  63.33%, tr:  98.67%, tr_best:  99.90%, epoch time: 61.20 seconds, 1.02 minutes\n",
      "total_backward_count 1703460 real_backward_count 289961  17.022%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  1.979948/  2.094452, val:  54.17%, val_best:  63.33%, tr:  99.39%, tr_best:  99.90%, epoch time: 61.72 seconds, 1.03 minutes\n",
      "total_backward_count 1713250 real_backward_count 291635  17.022%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  1.995819/  2.104111, val:  54.17%, val_best:  63.33%, tr:  99.08%, tr_best:  99.90%, epoch time: 61.72 seconds, 1.03 minutes\n",
      "total_backward_count 1723040 real_backward_count 293289  17.022%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  1.983953/  2.105796, val:  40.42%, val_best:  63.33%, tr:  99.39%, tr_best:  99.90%, epoch time: 60.77 seconds, 1.01 minutes\n",
      "total_backward_count 1732830 real_backward_count 294898  17.018%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  1.978160/  2.120561, val:  54.58%, val_best:  63.33%, tr:  99.08%, tr_best:  99.90%, epoch time: 61.22 seconds, 1.02 minutes\n",
      "total_backward_count 1742620 real_backward_count 296472  17.013%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  1.982133/  2.121877, val:  48.75%, val_best:  63.33%, tr:  99.28%, tr_best:  99.90%, epoch time: 60.41 seconds, 1.01 minutes\n",
      "total_backward_count 1752410 real_backward_count 298098  17.011%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  2.011285/  2.117225, val:  45.42%, val_best:  63.33%, tr:  98.57%, tr_best:  99.90%, epoch time: 61.10 seconds, 1.02 minutes\n",
      "total_backward_count 1762200 real_backward_count 299801  17.013%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  1.990386/  2.110348, val:  48.75%, val_best:  63.33%, tr:  99.28%, tr_best:  99.90%, epoch time: 60.12 seconds, 1.00 minutes\n",
      "total_backward_count 1771990 real_backward_count 301429  17.011%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  1.977185/  2.110593, val:  46.67%, val_best:  63.33%, tr:  99.49%, tr_best:  99.90%, epoch time: 60.44 seconds, 1.01 minutes\n",
      "total_backward_count 1781780 real_backward_count 303034  17.007%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  1.983171/  2.105485, val:  53.75%, val_best:  63.33%, tr:  98.67%, tr_best:  99.90%, epoch time: 61.21 seconds, 1.02 minutes\n",
      "total_backward_count 1791570 real_backward_count 304702  17.008%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  1.970845/  2.093605, val:  49.17%, val_best:  63.33%, tr:  99.08%, tr_best:  99.90%, epoch time: 60.11 seconds, 1.00 minutes\n",
      "total_backward_count 1801360 real_backward_count 306350  17.007%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  1.952702/  2.089177, val:  47.50%, val_best:  63.33%, tr:  98.88%, tr_best:  99.90%, epoch time: 61.18 seconds, 1.02 minutes\n",
      "total_backward_count 1811150 real_backward_count 307956  17.003%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  1.973144/  2.125017, val:  31.67%, val_best:  63.33%, tr:  98.98%, tr_best:  99.90%, epoch time: 60.34 seconds, 1.01 minutes\n",
      "total_backward_count 1820940 real_backward_count 309657  17.005%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  1.992478/  2.082651, val:  59.58%, val_best:  63.33%, tr:  98.98%, tr_best:  99.90%, epoch time: 60.33 seconds, 1.01 minutes\n",
      "total_backward_count 1830730 real_backward_count 311365  17.008%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  1.997685/  2.139924, val:  34.17%, val_best:  63.33%, tr:  99.08%, tr_best:  99.90%, epoch time: 61.99 seconds, 1.03 minutes\n",
      "total_backward_count 1840520 real_backward_count 313042  17.008%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  1.982829/  2.118351, val:  45.42%, val_best:  63.33%, tr:  99.08%, tr_best:  99.90%, epoch time: 60.35 seconds, 1.01 minutes\n",
      "total_backward_count 1850310 real_backward_count 314718  17.009%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  1.964261/  2.045835, val:  52.92%, val_best:  63.33%, tr:  99.28%, tr_best:  99.90%, epoch time: 61.11 seconds, 1.02 minutes\n",
      "total_backward_count 1860100 real_backward_count 316337  17.006%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  1.961928/  2.077155, val:  53.33%, val_best:  63.33%, tr:  99.18%, tr_best:  99.90%, epoch time: 60.29 seconds, 1.00 minutes\n",
      "total_backward_count 1869890 real_backward_count 317909  17.001%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  1.970274/  2.090713, val:  41.67%, val_best:  63.33%, tr:  98.98%, tr_best:  99.90%, epoch time: 60.45 seconds, 1.01 minutes\n",
      "total_backward_count 1879680 real_backward_count 319567  17.001%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  1.959092/  2.110023, val:  44.58%, val_best:  63.33%, tr:  99.39%, tr_best:  99.90%, epoch time: 60.76 seconds, 1.01 minutes\n",
      "total_backward_count 1889470 real_backward_count 321137  16.996%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  1.965638/  2.132975, val:  37.50%, val_best:  63.33%, tr:  99.28%, tr_best:  99.90%, epoch time: 61.11 seconds, 1.02 minutes\n",
      "total_backward_count 1899260 real_backward_count 322752  16.994%\n",
      "lif layer 2 self.abs_max_v: 10551.0\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  1.977477/  2.075907, val:  37.92%, val_best:  63.33%, tr:  99.08%, tr_best:  99.90%, epoch time: 61.10 seconds, 1.02 minutes\n",
      "total_backward_count 1909050 real_backward_count 324374  16.991%\n",
      "lif layer 2 self.abs_max_v: 10566.5\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  1.954503/  2.072178, val:  49.58%, val_best:  63.33%, tr:  99.18%, tr_best:  99.90%, epoch time: 59.65 seconds, 0.99 minutes\n",
      "total_backward_count 1918840 real_backward_count 325999  16.989%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  1.967108/  2.117083, val:  46.25%, val_best:  63.33%, tr:  98.98%, tr_best:  99.90%, epoch time: 60.56 seconds, 1.01 minutes\n",
      "total_backward_count 1928630 real_backward_count 327565  16.984%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  1.975983/  2.097176, val:  46.25%, val_best:  63.33%, tr:  98.88%, tr_best:  99.90%, epoch time: 60.40 seconds, 1.01 minutes\n",
      "total_backward_count 1938420 real_backward_count 329223  16.984%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  1.990015/  2.136619, val:  43.33%, val_best:  63.33%, tr:  98.77%, tr_best:  99.90%, epoch time: 60.35 seconds, 1.01 minutes\n",
      "total_backward_count 1948210 real_backward_count 330854  16.982%\n",
      "lif layer 2 self.abs_max_v: 10578.5\n",
      "lif layer 2 self.abs_max_v: 10952.5\n",
      "fc layer 2 self.abs_max_out: 6201.0\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  1.975124/  2.103646, val:  52.50%, val_best:  63.33%, tr:  98.67%, tr_best:  99.90%, epoch time: 61.50 seconds, 1.02 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df3ffbcd3244eb1bddd127cba40deef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÇ‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÉ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñà‚ñÖ‚ñÇ‚ñÖ‚ñÜ‚ñÖ‚ñÅ‚ñÇ‚ñÖ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñá‚ñà‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÉ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñá‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÅ‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÅ‚ñÜ‚ñà‚ñÖ‚ñÜ‚ñÉ‚ñá‚ñÖ‚ñà‚ñÖ‚ñá‚ñÅ‚ñÜ‚ñÉ‚ñá‚ñÜ‚ñÑ‚ñá‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñà‚ñÖ‚ñÜ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÇ‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÉ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñà‚ñÖ‚ñÇ‚ñÖ‚ñÜ‚ñÖ‚ñÅ‚ñÇ‚ñÖ</td></tr><tr><td>val_loss</td><td>‚ñÜ‚ñÜ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÉ‚ñá‚ñÖ‚ñá‚ñÜ‚ñá‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñà‚ñÅ‚ñà‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñà‚ñà‚ñÖ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.98672</td></tr><tr><td>tr_epoch_loss</td><td>1.97512</td></tr><tr><td>val_acc_best</td><td>0.63333</td></tr><tr><td>val_acc_now</td><td>0.525</td></tr><tr><td>val_loss</td><td>2.10365</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">confused-sweep-98</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mw2lqbmz' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mw2lqbmz</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251110_195820-mw2lqbmz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jl8i3zxz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251110_233342-jl8i3zxz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jl8i3zxz' target=\"_blank\">denim-sweep-101</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hpjdvxst' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hpjdvxst</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hpjdvxst' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hpjdvxst</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jl8i3zxz' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jl8i3zxz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251110_233350_548', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 213.0\n",
      "lif layer 1 self.abs_max_v: 213.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 359.0\n",
      "lif layer 2 self.abs_max_v: 359.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 145.0\n",
      "fc layer 1 self.abs_max_out: 281.0\n",
      "lif layer 1 self.abs_max_v: 295.0\n",
      "fc layer 2 self.abs_max_out: 450.0\n",
      "lif layer 2 self.abs_max_v: 555.0\n",
      "fc layer 3 self.abs_max_out: 163.0\n",
      "fc layer 1 self.abs_max_out: 305.0\n",
      "lif layer 1 self.abs_max_v: 389.5\n",
      "lif layer 2 self.abs_max_v: 706.0\n",
      "lif layer 1 self.abs_max_v: 399.0\n",
      "fc layer 3 self.abs_max_out: 181.0\n",
      "lif layer 1 self.abs_max_v: 411.5\n",
      "fc layer 3 self.abs_max_out: 192.0\n",
      "lif layer 1 self.abs_max_v: 414.0\n",
      "fc layer 2 self.abs_max_out: 488.0\n",
      "lif layer 2 self.abs_max_v: 802.0\n",
      "fc layer 1 self.abs_max_out: 334.0\n",
      "lif layer 1 self.abs_max_v: 456.0\n",
      "fc layer 3 self.abs_max_out: 273.0\n",
      "lif layer 1 self.abs_max_v: 479.0\n",
      "fc layer 2 self.abs_max_out: 527.0\n",
      "lif layer 1 self.abs_max_v: 486.5\n",
      "fc layer 1 self.abs_max_out: 463.0\n",
      "lif layer 1 self.abs_max_v: 498.0\n",
      "lif layer 1 self.abs_max_v: 520.0\n",
      "fc layer 2 self.abs_max_out: 626.0\n",
      "lif layer 2 self.abs_max_v: 873.0\n",
      "lif layer 2 self.abs_max_v: 902.5\n",
      "lif layer 1 self.abs_max_v: 545.0\n",
      "lif layer 1 self.abs_max_v: 579.5\n",
      "lif layer 1 self.abs_max_v: 656.0\n",
      "lif layer 1 self.abs_max_v: 682.5\n",
      "fc layer 1 self.abs_max_out: 481.0\n",
      "lif layer 1 self.abs_max_v: 772.5\n",
      "lif layer 1 self.abs_max_v: 774.5\n",
      "fc layer 2 self.abs_max_out: 677.0\n",
      "lif layer 2 self.abs_max_v: 956.0\n",
      "lif layer 1 self.abs_max_v: 822.5\n",
      "lif layer 1 self.abs_max_v: 857.5\n",
      "fc layer 1 self.abs_max_out: 484.0\n",
      "fc layer 1 self.abs_max_out: 593.0\n",
      "lif layer 1 self.abs_max_v: 930.0\n",
      "fc layer 2 self.abs_max_out: 693.0\n",
      "lif layer 2 self.abs_max_v: 965.5\n",
      "fc layer 3 self.abs_max_out: 334.0\n",
      "lif layer 2 self.abs_max_v: 1042.0\n",
      "fc layer 1 self.abs_max_out: 626.0\n",
      "lif layer 1 self.abs_max_v: 967.5\n",
      "fc layer 1 self.abs_max_out: 634.0\n",
      "lif layer 2 self.abs_max_v: 1118.5\n",
      "fc layer 3 self.abs_max_out: 339.0\n",
      "fc layer 2 self.abs_max_out: 718.0\n",
      "fc layer 2 self.abs_max_out: 789.0\n",
      "lif layer 2 self.abs_max_v: 1153.5\n",
      "lif layer 1 self.abs_max_v: 1004.0\n",
      "lif layer 1 self.abs_max_v: 1008.0\n",
      "lif layer 1 self.abs_max_v: 1023.5\n",
      "lif layer 1 self.abs_max_v: 1028.0\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  2.334120/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.63%, tr_best:  15.63%, epoch time: 60.66 seconds, 1.01 minutes\n",
      "total_backward_count 9790 real_backward_count 8481  86.629%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 1 self.abs_max_v: 1059.0\n",
      "fc layer 1 self.abs_max_out: 652.0\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  2.333260/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.24%, tr_best:  16.24%, epoch time: 61.02 seconds, 1.02 minutes\n",
      "total_backward_count 19580 real_backward_count 16947  86.553%\n",
      "fc layer 2 self.abs_max_out: 817.0\n",
      "lif layer 1 self.abs_max_v: 1117.5\n",
      "lif layer 2 self.abs_max_v: 1191.0\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  2.334816/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.32%, tr_best:  16.24%, epoch time: 59.86 seconds, 1.00 minutes\n",
      "total_backward_count 29370 real_backward_count 25434  86.599%\n",
      "lif layer 1 self.abs_max_v: 1139.0\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  2.336932/  2.339790, val:  15.42%, val_best:  15.42%, tr:  14.81%, tr_best:  16.24%, epoch time: 60.67 seconds, 1.01 minutes\n",
      "total_backward_count 39160 real_backward_count 33940  86.670%\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  2.336821/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.73%, tr_best:  16.24%, epoch time: 60.28 seconds, 1.00 minutes\n",
      "total_backward_count 48950 real_backward_count 42452  86.725%\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  2.334077/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.73%, tr_best:  16.24%, epoch time: 60.55 seconds, 1.01 minutes\n",
      "total_backward_count 58740 real_backward_count 50947  86.733%\n",
      "fc layer 3 self.abs_max_out: 343.0\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  2.336844/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.63%, tr_best:  16.24%, epoch time: 60.33 seconds, 1.01 minutes\n",
      "total_backward_count 68530 real_backward_count 59438  86.733%\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  2.336697/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.42%, tr_best:  16.24%, epoch time: 60.72 seconds, 1.01 minutes\n",
      "total_backward_count 78320 real_backward_count 67952  86.762%\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  2.337156/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.73%, tr_best:  16.24%, epoch time: 61.00 seconds, 1.02 minutes\n",
      "total_backward_count 88110 real_backward_count 76474  86.794%\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  2.334628/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.83%, tr_best:  16.24%, epoch time: 60.96 seconds, 1.02 minutes\n",
      "total_backward_count 97900 real_backward_count 84946  86.768%\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  2.338066/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.83%, tr_best:  16.24%, epoch time: 61.78 seconds, 1.03 minutes\n",
      "total_backward_count 107690 real_backward_count 93459  86.785%\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  2.333129/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.04%, tr_best:  16.24%, epoch time: 61.72 seconds, 1.03 minutes\n",
      "total_backward_count 117480 real_backward_count 101904  86.742%\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  2.336455/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.22%, tr_best:  16.24%, epoch time: 62.90 seconds, 1.05 minutes\n",
      "total_backward_count 127270 real_backward_count 110396  86.742%\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  2.334589/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.42%, tr_best:  16.24%, epoch time: 63.51 seconds, 1.06 minutes\n",
      "total_backward_count 137060 real_backward_count 118892  86.744%\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  2.336972/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.22%, tr_best:  16.24%, epoch time: 62.50 seconds, 1.04 minutes\n",
      "total_backward_count 146850 real_backward_count 127386  86.746%\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  2.338608/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.02%, tr_best:  16.24%, epoch time: 63.01 seconds, 1.05 minutes\n",
      "total_backward_count 156640 real_backward_count 135913  86.768%\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  2.337663/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.53%, tr_best:  16.24%, epoch time: 62.19 seconds, 1.04 minutes\n",
      "total_backward_count 166430 real_backward_count 144384  86.754%\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  2.338185/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.83%, tr_best:  16.24%, epoch time: 63.77 seconds, 1.06 minutes\n",
      "total_backward_count 176220 real_backward_count 152896  86.764%\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  2.336869/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.02%, tr_best:  16.24%, epoch time: 63.70 seconds, 1.06 minutes\n",
      "total_backward_count 186010 real_backward_count 161443  86.793%\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  2.336118/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.73%, tr_best:  16.24%, epoch time: 63.17 seconds, 1.05 minutes\n",
      "total_backward_count 195800 real_backward_count 169944  86.795%\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  2.335250/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.22%, tr_best:  16.24%, epoch time: 63.31 seconds, 1.06 minutes\n",
      "total_backward_count 205590 real_backward_count 178449  86.798%\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  2.339286/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.22%, tr_best:  16.24%, epoch time: 62.97 seconds, 1.05 minutes\n",
      "total_backward_count 215380 real_backward_count 187009  86.827%\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  2.335916/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.93%, tr_best:  16.24%, epoch time: 63.51 seconds, 1.06 minutes\n",
      "total_backward_count 225170 real_backward_count 195487  86.818%\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  2.337102/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.73%, tr_best:  16.24%, epoch time: 63.93 seconds, 1.07 minutes\n",
      "total_backward_count 234960 real_backward_count 204016  86.830%\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  2.332256/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.63%, tr_best:  16.24%, epoch time: 63.72 seconds, 1.06 minutes\n",
      "total_backward_count 244750 real_backward_count 212508  86.827%\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  2.335910/  2.339790, val:  15.42%, val_best:  15.42%, tr:  14.50%, tr_best:  16.24%, epoch time: 63.82 seconds, 1.06 minutes\n",
      "total_backward_count 254540 real_backward_count 221010  86.827%\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  2.335562/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.34%, tr_best:  16.34%, epoch time: 63.66 seconds, 1.06 minutes\n",
      "total_backward_count 264330 real_backward_count 229484  86.817%\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  2.336825/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.32%, tr_best:  16.34%, epoch time: 64.10 seconds, 1.07 minutes\n",
      "total_backward_count 274120 real_backward_count 237978  86.815%\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  2.337441/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.32%, tr_best:  16.34%, epoch time: 64.07 seconds, 1.07 minutes\n",
      "total_backward_count 283910 real_backward_count 246502  86.824%\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  2.336761/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.24%, tr_best:  16.34%, epoch time: 63.13 seconds, 1.05 minutes\n",
      "total_backward_count 293700 real_backward_count 255007  86.826%\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  2.334676/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.73%, tr_best:  16.34%, epoch time: 62.82 seconds, 1.05 minutes\n",
      "total_backward_count 303490 real_backward_count 263495  86.822%\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  2.333834/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.42%, tr_best:  16.34%, epoch time: 63.74 seconds, 1.06 minutes\n",
      "total_backward_count 313280 real_backward_count 271988  86.819%\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  2.337042/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.83%, tr_best:  16.34%, epoch time: 63.73 seconds, 1.06 minutes\n",
      "total_backward_count 323070 real_backward_count 280477  86.816%\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  2.335079/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.32%, tr_best:  16.34%, epoch time: 63.50 seconds, 1.06 minutes\n",
      "total_backward_count 332860 real_backward_count 289002  86.824%\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  2.338473/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.83%, tr_best:  16.34%, epoch time: 62.77 seconds, 1.05 minutes\n",
      "total_backward_count 342650 real_backward_count 297508  86.826%\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  2.337529/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.73%, tr_best:  16.34%, epoch time: 64.76 seconds, 1.08 minutes\n",
      "total_backward_count 352440 real_backward_count 305993  86.821%\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  2.337773/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.42%, tr_best:  16.34%, epoch time: 64.19 seconds, 1.07 minutes\n",
      "total_backward_count 362230 real_backward_count 314511  86.826%\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  2.336957/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.93%, tr_best:  16.34%, epoch time: 62.91 seconds, 1.05 minutes\n",
      "total_backward_count 372020 real_backward_count 322975  86.817%\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  2.337661/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.12%, tr_best:  16.34%, epoch time: 63.55 seconds, 1.06 minutes\n",
      "total_backward_count 381810 real_backward_count 331470  86.815%\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  2.334541/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.63%, tr_best:  16.34%, epoch time: 63.28 seconds, 1.05 minutes\n",
      "total_backward_count 391600 real_backward_count 339934  86.806%\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  2.337318/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.14%, tr_best:  16.34%, epoch time: 64.11 seconds, 1.07 minutes\n",
      "total_backward_count 401390 real_backward_count 348413  86.802%\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  2.335212/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.63%, tr_best:  16.34%, epoch time: 63.36 seconds, 1.06 minutes\n",
      "total_backward_count 411180 real_backward_count 356915  86.803%\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  2.335915/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.42%, tr_best:  16.34%, epoch time: 63.51 seconds, 1.06 minutes\n",
      "total_backward_count 420970 real_backward_count 365388  86.797%\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  2.335700/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.14%, tr_best:  16.34%, epoch time: 62.61 seconds, 1.04 minutes\n",
      "total_backward_count 430760 real_backward_count 373881  86.796%\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  2.336637/  2.339790, val:  15.42%, val_best:  15.42%, tr:  14.81%, tr_best:  16.34%, epoch time: 63.25 seconds, 1.05 minutes\n",
      "total_backward_count 440550 real_backward_count 382363  86.792%\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  2.337630/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.12%, tr_best:  16.34%, epoch time: 63.12 seconds, 1.05 minutes\n",
      "total_backward_count 450340 real_backward_count 390857  86.792%\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  2.332172/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.42%, tr_best:  16.34%, epoch time: 63.37 seconds, 1.06 minutes\n",
      "total_backward_count 460130 real_backward_count 399337  86.788%\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  2.337544/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.24%, tr_best:  16.34%, epoch time: 63.74 seconds, 1.06 minutes\n",
      "total_backward_count 469920 real_backward_count 407860  86.793%\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  2.336965/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.24%, tr_best:  16.34%, epoch time: 63.30 seconds, 1.06 minutes\n",
      "total_backward_count 479710 real_backward_count 416356  86.793%\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  2.336256/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.53%, tr_best:  16.34%, epoch time: 63.77 seconds, 1.06 minutes\n",
      "total_backward_count 489500 real_backward_count 424853  86.793%\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  2.339982/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.12%, tr_best:  16.34%, epoch time: 63.18 seconds, 1.05 minutes\n",
      "total_backward_count 499290 real_backward_count 433368  86.797%\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  2.337053/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.04%, tr_best:  16.34%, epoch time: 62.54 seconds, 1.04 minutes\n",
      "total_backward_count 509080 real_backward_count 441870  86.798%\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  2.335029/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.63%, tr_best:  16.34%, epoch time: 63.16 seconds, 1.05 minutes\n",
      "total_backward_count 518870 real_backward_count 450364  86.797%\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  2.339625/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.22%, tr_best:  16.34%, epoch time: 63.23 seconds, 1.05 minutes\n",
      "total_backward_count 528660 real_backward_count 458865  86.798%\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  2.337845/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.93%, tr_best:  16.34%, epoch time: 63.90 seconds, 1.06 minutes\n",
      "total_backward_count 538450 real_backward_count 467351  86.796%\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  2.338061/  2.339790, val:  15.42%, val_best:  15.42%, tr:  14.91%, tr_best:  16.34%, epoch time: 63.74 seconds, 1.06 minutes\n",
      "total_backward_count 548240 real_backward_count 475832  86.793%\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  2.334863/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.93%, tr_best:  16.34%, epoch time: 63.00 seconds, 1.05 minutes\n",
      "total_backward_count 558030 real_backward_count 484352  86.797%\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  2.335682/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.22%, tr_best:  16.34%, epoch time: 63.35 seconds, 1.06 minutes\n",
      "total_backward_count 567820 real_backward_count 492854  86.798%\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  2.337928/  2.339790, val:  15.42%, val_best:  15.42%, tr:  14.81%, tr_best:  16.34%, epoch time: 63.72 seconds, 1.06 minutes\n",
      "total_backward_count 577610 real_backward_count 501373  86.801%\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  2.338082/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.04%, tr_best:  16.34%, epoch time: 62.60 seconds, 1.04 minutes\n",
      "total_backward_count 587400 real_backward_count 509832  86.795%\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  2.336562/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.12%, tr_best:  16.34%, epoch time: 62.90 seconds, 1.05 minutes\n",
      "total_backward_count 597190 real_backward_count 518355  86.799%\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  2.334482/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.24%, tr_best:  16.34%, epoch time: 63.03 seconds, 1.05 minutes\n",
      "total_backward_count 606980 real_backward_count 526825  86.794%\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  2.335332/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.83%, tr_best:  16.34%, epoch time: 63.20 seconds, 1.05 minutes\n",
      "total_backward_count 616770 real_backward_count 535325  86.795%\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  2.336583/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.53%, tr_best:  16.34%, epoch time: 63.97 seconds, 1.07 minutes\n",
      "total_backward_count 626560 real_backward_count 543845  86.799%\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  2.338400/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.42%, tr_best:  16.34%, epoch time: 63.47 seconds, 1.06 minutes\n",
      "total_backward_count 636350 real_backward_count 552323  86.795%\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  2.334169/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.73%, tr_best:  16.34%, epoch time: 63.73 seconds, 1.06 minutes\n",
      "total_backward_count 646140 real_backward_count 560824  86.796%\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  2.337765/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.83%, tr_best:  16.34%, epoch time: 63.93 seconds, 1.07 minutes\n",
      "total_backward_count 655930 real_backward_count 569327  86.797%\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  2.336627/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.53%, tr_best:  16.34%, epoch time: 64.33 seconds, 1.07 minutes\n",
      "total_backward_count 665720 real_backward_count 577805  86.794%\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  2.335931/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.93%, tr_best:  16.34%, epoch time: 62.84 seconds, 1.05 minutes\n",
      "total_backward_count 675510 real_backward_count 586320  86.797%\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  2.335676/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.53%, tr_best:  16.34%, epoch time: 63.75 seconds, 1.06 minutes\n",
      "total_backward_count 685300 real_backward_count 594827  86.798%\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  2.335828/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.42%, tr_best:  16.34%, epoch time: 63.62 seconds, 1.06 minutes\n",
      "total_backward_count 695090 real_backward_count 603327  86.798%\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  2.335339/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.24%, tr_best:  16.34%, epoch time: 63.79 seconds, 1.06 minutes\n",
      "total_backward_count 704880 real_backward_count 611811  86.796%\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  2.338017/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.34%, tr_best:  16.34%, epoch time: 63.89 seconds, 1.06 minutes\n",
      "total_backward_count 714670 real_backward_count 620320  86.798%\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  2.337819/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.04%, tr_best:  16.34%, epoch time: 63.01 seconds, 1.05 minutes\n",
      "total_backward_count 724460 real_backward_count 628812  86.797%\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  2.332725/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.04%, tr_best:  16.34%, epoch time: 63.41 seconds, 1.06 minutes\n",
      "total_backward_count 734250 real_backward_count 637260  86.791%\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  2.337319/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.63%, tr_best:  16.34%, epoch time: 63.73 seconds, 1.06 minutes\n",
      "total_backward_count 744040 real_backward_count 645771  86.793%\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  2.336018/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.04%, tr_best:  16.34%, epoch time: 63.56 seconds, 1.06 minutes\n",
      "total_backward_count 753830 real_backward_count 654280  86.794%\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  2.337929/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.02%, tr_best:  16.34%, epoch time: 62.93 seconds, 1.05 minutes\n",
      "total_backward_count 763620 real_backward_count 662791  86.796%\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  2.336921/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.83%, tr_best:  16.34%, epoch time: 63.90 seconds, 1.07 minutes\n",
      "total_backward_count 773410 real_backward_count 671279  86.795%\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  2.338359/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.63%, tr_best:  16.34%, epoch time: 62.92 seconds, 1.05 minutes\n",
      "total_backward_count 783200 real_backward_count 679795  86.797%\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  2.335750/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.83%, tr_best:  16.34%, epoch time: 63.66 seconds, 1.06 minutes\n",
      "total_backward_count 792990 real_backward_count 688311  86.799%\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  2.337675/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.83%, tr_best:  16.34%, epoch time: 62.90 seconds, 1.05 minutes\n",
      "total_backward_count 802780 real_backward_count 696826  86.802%\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  2.336825/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.53%, tr_best:  16.34%, epoch time: 62.94 seconds, 1.05 minutes\n",
      "total_backward_count 812570 real_backward_count 705345  86.804%\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  2.333846/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.32%, tr_best:  16.34%, epoch time: 62.98 seconds, 1.05 minutes\n",
      "total_backward_count 822360 real_backward_count 713840  86.804%\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  2.337137/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.83%, tr_best:  16.34%, epoch time: 62.82 seconds, 1.05 minutes\n",
      "total_backward_count 832150 real_backward_count 722330  86.803%\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  2.336639/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.32%, tr_best:  16.34%, epoch time: 63.87 seconds, 1.06 minutes\n",
      "total_backward_count 841940 real_backward_count 730848  86.805%\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  2.336210/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.42%, tr_best:  16.34%, epoch time: 63.77 seconds, 1.06 minutes\n",
      "total_backward_count 851730 real_backward_count 739360  86.807%\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  2.336429/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.63%, tr_best:  16.34%, epoch time: 62.33 seconds, 1.04 minutes\n",
      "total_backward_count 861520 real_backward_count 747848  86.806%\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  2.340056/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.04%, tr_best:  16.34%, epoch time: 63.90 seconds, 1.07 minutes\n",
      "total_backward_count 871310 real_backward_count 756348  86.806%\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  2.334118/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.04%, tr_best:  16.34%, epoch time: 64.24 seconds, 1.07 minutes\n",
      "total_backward_count 881100 real_backward_count 764807  86.801%\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  2.336409/  2.339790, val:  15.42%, val_best:  15.42%, tr:  14.81%, tr_best:  16.34%, epoch time: 63.47 seconds, 1.06 minutes\n",
      "total_backward_count 890890 real_backward_count 773324  86.804%\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  2.337291/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.42%, tr_best:  16.34%, epoch time: 62.68 seconds, 1.04 minutes\n",
      "total_backward_count 900680 real_backward_count 781813  86.803%\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  2.340801/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.22%, tr_best:  16.34%, epoch time: 62.47 seconds, 1.04 minutes\n",
      "total_backward_count 910470 real_backward_count 790331  86.805%\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  2.335600/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.45%, tr_best:  16.45%, epoch time: 63.55 seconds, 1.06 minutes\n",
      "total_backward_count 920260 real_backward_count 798790  86.800%\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  2.337963/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.32%, tr_best:  16.45%, epoch time: 63.25 seconds, 1.05 minutes\n",
      "total_backward_count 930050 real_backward_count 807283  86.800%\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  2.335073/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.93%, tr_best:  16.45%, epoch time: 63.37 seconds, 1.06 minutes\n",
      "total_backward_count 939840 real_backward_count 815743  86.796%\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  2.337390/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.63%, tr_best:  16.45%, epoch time: 63.64 seconds, 1.06 minutes\n",
      "total_backward_count 949630 real_backward_count 824270  86.799%\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  2.336918/  2.339790, val:  15.42%, val_best:  15.42%, tr:  14.81%, tr_best:  16.45%, epoch time: 62.74 seconds, 1.05 minutes\n",
      "total_backward_count 959420 real_backward_count 832764  86.799%\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  2.336482/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.12%, tr_best:  16.45%, epoch time: 63.29 seconds, 1.05 minutes\n",
      "total_backward_count 969210 real_backward_count 841284  86.801%\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  2.339618/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.42%, tr_best:  16.45%, epoch time: 60.65 seconds, 1.01 minutes\n",
      "total_backward_count 979000 real_backward_count 849810  86.804%\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  2.333164/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.73%, tr_best:  16.45%, epoch time: 63.61 seconds, 1.06 minutes\n",
      "total_backward_count 988790 real_backward_count 858251  86.798%\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  2.335429/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.83%, tr_best:  16.45%, epoch time: 65.23 seconds, 1.09 minutes\n",
      "total_backward_count 998580 real_backward_count 866726  86.796%\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  2.333357/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.34%, tr_best:  16.45%, epoch time: 64.38 seconds, 1.07 minutes\n",
      "total_backward_count 1008370 real_backward_count 875224  86.796%\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  2.337463/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.42%, tr_best:  16.45%, epoch time: 65.12 seconds, 1.09 minutes\n",
      "total_backward_count 1018160 real_backward_count 883727  86.796%\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  2.336037/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.53%, tr_best:  16.45%, epoch time: 63.40 seconds, 1.06 minutes\n",
      "total_backward_count 1027950 real_backward_count 892249  86.799%\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  2.336167/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.32%, tr_best:  16.45%, epoch time: 64.28 seconds, 1.07 minutes\n",
      "total_backward_count 1037740 real_backward_count 900751  86.799%\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  2.340539/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.32%, tr_best:  16.45%, epoch time: 62.92 seconds, 1.05 minutes\n",
      "total_backward_count 1047530 real_backward_count 909256  86.800%\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  2.339979/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.12%, tr_best:  16.45%, epoch time: 64.52 seconds, 1.08 minutes\n",
      "total_backward_count 1057320 real_backward_count 917779  86.802%\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  2.337796/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.83%, tr_best:  16.45%, epoch time: 64.73 seconds, 1.08 minutes\n",
      "total_backward_count 1067110 real_backward_count 926266  86.801%\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  2.339761/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.02%, tr_best:  16.45%, epoch time: 64.10 seconds, 1.07 minutes\n",
      "total_backward_count 1076900 real_backward_count 934798  86.805%\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  2.334543/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.14%, tr_best:  16.45%, epoch time: 64.15 seconds, 1.07 minutes\n",
      "total_backward_count 1086690 real_backward_count 943244  86.800%\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  2.337501/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.93%, tr_best:  16.45%, epoch time: 63.76 seconds, 1.06 minutes\n",
      "total_backward_count 1096480 real_backward_count 951744  86.800%\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  2.335824/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.02%, tr_best:  16.45%, epoch time: 64.47 seconds, 1.07 minutes\n",
      "total_backward_count 1106270 real_backward_count 960257  86.801%\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  2.338549/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.22%, tr_best:  16.45%, epoch time: 64.10 seconds, 1.07 minutes\n",
      "total_backward_count 1116060 real_backward_count 968798  86.805%\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  2.339044/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.53%, tr_best:  16.45%, epoch time: 64.33 seconds, 1.07 minutes\n",
      "total_backward_count 1125850 real_backward_count 977283  86.804%\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  2.339487/  2.339790, val:  15.42%, val_best:  15.42%, tr:  14.91%, tr_best:  16.45%, epoch time: 63.95 seconds, 1.07 minutes\n",
      "total_backward_count 1135640 real_backward_count 985797  86.805%\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  2.333304/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.45%, tr_best:  16.45%, epoch time: 64.06 seconds, 1.07 minutes\n",
      "total_backward_count 1145430 real_backward_count 994273  86.803%\n",
      "epoch-117 lr=['0.0009766'], tr/val_loss:  2.335162/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.42%, tr_best:  16.45%, epoch time: 63.77 seconds, 1.06 minutes\n",
      "total_backward_count 1155220 real_backward_count 1002740  86.801%\n",
      "epoch-118 lr=['0.0009766'], tr/val_loss:  2.337522/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.22%, tr_best:  16.45%, epoch time: 63.88 seconds, 1.06 minutes\n",
      "total_backward_count 1165010 real_backward_count 1011287  86.805%\n",
      "epoch-119 lr=['0.0009766'], tr/val_loss:  2.338323/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.22%, tr_best:  16.45%, epoch time: 63.85 seconds, 1.06 minutes\n",
      "total_backward_count 1174800 real_backward_count 1019789  86.805%\n",
      "epoch-120 lr=['0.0009766'], tr/val_loss:  2.337442/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.73%, tr_best:  16.45%, epoch time: 63.07 seconds, 1.05 minutes\n",
      "total_backward_count 1184590 real_backward_count 1028297  86.806%\n",
      "epoch-121 lr=['0.0009766'], tr/val_loss:  2.336985/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.63%, tr_best:  16.45%, epoch time: 63.98 seconds, 1.07 minutes\n",
      "total_backward_count 1194380 real_backward_count 1036806  86.807%\n",
      "epoch-122 lr=['0.0009766'], tr/val_loss:  2.335181/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.73%, tr_best:  16.45%, epoch time: 64.24 seconds, 1.07 minutes\n",
      "total_backward_count 1204170 real_backward_count 1045269  86.804%\n",
      "epoch-123 lr=['0.0009766'], tr/val_loss:  2.338550/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.53%, tr_best:  16.45%, epoch time: 64.05 seconds, 1.07 minutes\n",
      "total_backward_count 1213960 real_backward_count 1053789  86.806%\n",
      "epoch-124 lr=['0.0009766'], tr/val_loss:  2.334129/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.73%, tr_best:  16.45%, epoch time: 64.42 seconds, 1.07 minutes\n",
      "total_backward_count 1223750 real_backward_count 1062279  86.805%\n",
      "epoch-125 lr=['0.0009766'], tr/val_loss:  2.337037/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.32%, tr_best:  16.45%, epoch time: 64.09 seconds, 1.07 minutes\n",
      "total_backward_count 1233540 real_backward_count 1070772  86.805%\n",
      "epoch-126 lr=['0.0009766'], tr/val_loss:  2.340448/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.22%, tr_best:  16.45%, epoch time: 64.78 seconds, 1.08 minutes\n",
      "total_backward_count 1243330 real_backward_count 1079274  86.805%\n",
      "epoch-127 lr=['0.0009766'], tr/val_loss:  2.334591/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.24%, tr_best:  16.45%, epoch time: 64.43 seconds, 1.07 minutes\n",
      "total_backward_count 1253120 real_backward_count 1087762  86.804%\n",
      "epoch-128 lr=['0.0009766'], tr/val_loss:  2.338352/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.12%, tr_best:  16.45%, epoch time: 63.91 seconds, 1.07 minutes\n",
      "total_backward_count 1262910 real_backward_count 1096274  86.805%\n",
      "epoch-129 lr=['0.0009766'], tr/val_loss:  2.337989/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.73%, tr_best:  16.45%, epoch time: 63.65 seconds, 1.06 minutes\n",
      "total_backward_count 1272700 real_backward_count 1104761  86.805%\n",
      "epoch-130 lr=['0.0009766'], tr/val_loss:  2.339378/  2.339790, val:  15.42%, val_best:  15.42%, tr:  14.91%, tr_best:  16.45%, epoch time: 63.32 seconds, 1.06 minutes\n",
      "total_backward_count 1282490 real_backward_count 1113262  86.805%\n",
      "epoch-131 lr=['0.0009766'], tr/val_loss:  2.336673/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.53%, tr_best:  16.45%, epoch time: 64.27 seconds, 1.07 minutes\n",
      "total_backward_count 1292280 real_backward_count 1121786  86.807%\n",
      "epoch-132 lr=['0.0009766'], tr/val_loss:  2.335772/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.14%, tr_best:  16.45%, epoch time: 63.60 seconds, 1.06 minutes\n",
      "total_backward_count 1302070 real_backward_count 1130269  86.806%\n",
      "epoch-133 lr=['0.0009766'], tr/val_loss:  2.335706/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.63%, tr_best:  16.45%, epoch time: 65.05 seconds, 1.08 minutes\n",
      "total_backward_count 1311860 real_backward_count 1138746  86.804%\n",
      "epoch-134 lr=['0.0009766'], tr/val_loss:  2.337155/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.22%, tr_best:  16.45%, epoch time: 63.75 seconds, 1.06 minutes\n",
      "total_backward_count 1321650 real_backward_count 1147228  86.803%\n",
      "epoch-135 lr=['0.0009766'], tr/val_loss:  2.337773/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.14%, tr_best:  16.45%, epoch time: 63.98 seconds, 1.07 minutes\n",
      "total_backward_count 1331440 real_backward_count 1155712  86.802%\n",
      "epoch-136 lr=['0.0009766'], tr/val_loss:  2.336693/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.12%, tr_best:  16.45%, epoch time: 63.20 seconds, 1.05 minutes\n",
      "total_backward_count 1341230 real_backward_count 1164223  86.803%\n",
      "epoch-137 lr=['0.0009766'], tr/val_loss:  2.339618/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.83%, tr_best:  16.45%, epoch time: 64.19 seconds, 1.07 minutes\n",
      "total_backward_count 1351020 real_backward_count 1172709  86.802%\n",
      "epoch-138 lr=['0.0009766'], tr/val_loss:  2.339058/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.63%, tr_best:  16.45%, epoch time: 63.44 seconds, 1.06 minutes\n",
      "total_backward_count 1360810 real_backward_count 1181189  86.800%\n",
      "epoch-139 lr=['0.0009766'], tr/val_loss:  2.336363/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.02%, tr_best:  16.45%, epoch time: 63.78 seconds, 1.06 minutes\n",
      "total_backward_count 1370600 real_backward_count 1189684  86.800%\n",
      "epoch-140 lr=['0.0009766'], tr/val_loss:  2.337901/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.63%, tr_best:  16.45%, epoch time: 64.62 seconds, 1.08 minutes\n",
      "total_backward_count 1380390 real_backward_count 1198203  86.802%\n",
      "epoch-141 lr=['0.0009766'], tr/val_loss:  2.334248/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.53%, tr_best:  16.45%, epoch time: 63.40 seconds, 1.06 minutes\n",
      "total_backward_count 1390180 real_backward_count 1206683  86.800%\n",
      "epoch-142 lr=['0.0009766'], tr/val_loss:  2.335492/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.02%, tr_best:  16.45%, epoch time: 64.04 seconds, 1.07 minutes\n",
      "total_backward_count 1399970 real_backward_count 1215213  86.803%\n",
      "epoch-143 lr=['0.0009766'], tr/val_loss:  2.333241/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.04%, tr_best:  16.45%, epoch time: 63.37 seconds, 1.06 minutes\n",
      "total_backward_count 1409760 real_backward_count 1223707  86.803%\n",
      "epoch-144 lr=['0.0009766'], tr/val_loss:  2.338724/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.02%, tr_best:  16.45%, epoch time: 65.04 seconds, 1.08 minutes\n",
      "total_backward_count 1419550 real_backward_count 1232208  86.803%\n",
      "epoch-145 lr=['0.0009766'], tr/val_loss:  2.333733/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.12%, tr_best:  16.45%, epoch time: 63.51 seconds, 1.06 minutes\n",
      "total_backward_count 1429340 real_backward_count 1240687  86.801%\n",
      "epoch-146 lr=['0.0009766'], tr/val_loss:  2.335002/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.63%, tr_best:  16.45%, epoch time: 63.55 seconds, 1.06 minutes\n",
      "total_backward_count 1439130 real_backward_count 1249182  86.801%\n",
      "epoch-147 lr=['0.0009766'], tr/val_loss:  2.338075/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.02%, tr_best:  16.45%, epoch time: 63.56 seconds, 1.06 minutes\n",
      "total_backward_count 1448920 real_backward_count 1257694  86.802%\n",
      "epoch-148 lr=['0.0009766'], tr/val_loss:  2.335497/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.63%, tr_best:  16.45%, epoch time: 63.54 seconds, 1.06 minutes\n",
      "total_backward_count 1458710 real_backward_count 1266181  86.801%\n",
      "epoch-149 lr=['0.0009766'], tr/val_loss:  2.336296/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.32%, tr_best:  16.45%, epoch time: 63.87 seconds, 1.06 minutes\n",
      "total_backward_count 1468500 real_backward_count 1274709  86.803%\n",
      "epoch-150 lr=['0.0009766'], tr/val_loss:  2.336092/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.42%, tr_best:  16.45%, epoch time: 64.13 seconds, 1.07 minutes\n",
      "total_backward_count 1478290 real_backward_count 1283247  86.806%\n",
      "epoch-151 lr=['0.0009766'], tr/val_loss:  2.335749/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.83%, tr_best:  16.45%, epoch time: 63.91 seconds, 1.07 minutes\n",
      "total_backward_count 1488080 real_backward_count 1291738  86.806%\n",
      "epoch-152 lr=['0.0009766'], tr/val_loss:  2.337641/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.42%, tr_best:  16.45%, epoch time: 63.87 seconds, 1.06 minutes\n",
      "total_backward_count 1497870 real_backward_count 1300265  86.808%\n",
      "epoch-153 lr=['0.0009766'], tr/val_loss:  2.335666/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.42%, tr_best:  16.45%, epoch time: 63.86 seconds, 1.06 minutes\n",
      "total_backward_count 1507660 real_backward_count 1308770  86.808%\n",
      "epoch-154 lr=['0.0009766'], tr/val_loss:  2.338585/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.32%, tr_best:  16.45%, epoch time: 64.26 seconds, 1.07 minutes\n",
      "total_backward_count 1517450 real_backward_count 1317282  86.809%\n",
      "epoch-155 lr=['0.0009766'], tr/val_loss:  2.336233/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.53%, tr_best:  16.45%, epoch time: 63.86 seconds, 1.06 minutes\n",
      "total_backward_count 1527240 real_backward_count 1325774  86.808%\n",
      "epoch-156 lr=['0.0009766'], tr/val_loss:  2.334562/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.83%, tr_best:  16.45%, epoch time: 63.85 seconds, 1.06 minutes\n",
      "total_backward_count 1537030 real_backward_count 1334285  86.809%\n",
      "epoch-157 lr=['0.0009766'], tr/val_loss:  2.340225/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.53%, tr_best:  16.45%, epoch time: 63.93 seconds, 1.07 minutes\n",
      "total_backward_count 1546820 real_backward_count 1342799  86.810%\n",
      "epoch-158 lr=['0.0009766'], tr/val_loss:  2.335612/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.83%, tr_best:  16.45%, epoch time: 63.20 seconds, 1.05 minutes\n",
      "total_backward_count 1556610 real_backward_count 1351290  86.810%\n",
      "epoch-159 lr=['0.0009766'], tr/val_loss:  2.336347/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.02%, tr_best:  16.45%, epoch time: 63.66 seconds, 1.06 minutes\n",
      "total_backward_count 1566400 real_backward_count 1359827  86.812%\n",
      "epoch-160 lr=['0.0009766'], tr/val_loss:  2.333679/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.32%, tr_best:  16.45%, epoch time: 63.82 seconds, 1.06 minutes\n",
      "total_backward_count 1576190 real_backward_count 1368332  86.813%\n",
      "epoch-161 lr=['0.0009766'], tr/val_loss:  2.335567/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.53%, tr_best:  16.45%, epoch time: 64.58 seconds, 1.08 minutes\n",
      "total_backward_count 1585980 real_backward_count 1376828  86.812%\n",
      "epoch-162 lr=['0.0009766'], tr/val_loss:  2.336171/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.34%, tr_best:  16.45%, epoch time: 63.57 seconds, 1.06 minutes\n",
      "total_backward_count 1595770 real_backward_count 1385302  86.811%\n",
      "epoch-163 lr=['0.0009766'], tr/val_loss:  2.337410/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.83%, tr_best:  16.45%, epoch time: 65.46 seconds, 1.09 minutes\n",
      "total_backward_count 1605560 real_backward_count 1393824  86.812%\n",
      "epoch-164 lr=['0.0009766'], tr/val_loss:  2.337739/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.93%, tr_best:  16.45%, epoch time: 62.09 seconds, 1.03 minutes\n",
      "total_backward_count 1615350 real_backward_count 1402331  86.813%\n",
      "epoch-165 lr=['0.0009766'], tr/val_loss:  2.334809/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.83%, tr_best:  16.45%, epoch time: 63.46 seconds, 1.06 minutes\n",
      "total_backward_count 1625140 real_backward_count 1410832  86.813%\n",
      "epoch-166 lr=['0.0009766'], tr/val_loss:  2.337217/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.32%, tr_best:  16.45%, epoch time: 62.97 seconds, 1.05 minutes\n",
      "total_backward_count 1634930 real_backward_count 1419368  86.815%\n",
      "epoch-167 lr=['0.0009766'], tr/val_loss:  2.339158/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.22%, tr_best:  16.45%, epoch time: 63.57 seconds, 1.06 minutes\n",
      "total_backward_count 1644720 real_backward_count 1427883  86.816%\n",
      "epoch-168 lr=['0.0009766'], tr/val_loss:  2.335099/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.53%, tr_best:  16.45%, epoch time: 64.10 seconds, 1.07 minutes\n",
      "total_backward_count 1654510 real_backward_count 1436350  86.814%\n",
      "epoch-169 lr=['0.0009766'], tr/val_loss:  2.338108/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.73%, tr_best:  16.45%, epoch time: 63.39 seconds, 1.06 minutes\n",
      "total_backward_count 1664300 real_backward_count 1444872  86.816%\n",
      "epoch-170 lr=['0.0009766'], tr/val_loss:  2.336232/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.63%, tr_best:  16.45%, epoch time: 63.89 seconds, 1.06 minutes\n",
      "total_backward_count 1674090 real_backward_count 1453361  86.815%\n",
      "epoch-171 lr=['0.0009766'], tr/val_loss:  2.335886/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.83%, tr_best:  16.45%, epoch time: 62.85 seconds, 1.05 minutes\n",
      "total_backward_count 1683880 real_backward_count 1461868  86.815%\n",
      "epoch-172 lr=['0.0009766'], tr/val_loss:  2.336428/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.04%, tr_best:  16.45%, epoch time: 63.44 seconds, 1.06 minutes\n",
      "total_backward_count 1693670 real_backward_count 1470368  86.815%\n",
      "epoch-173 lr=['0.0009766'], tr/val_loss:  2.340504/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.63%, tr_best:  16.45%, epoch time: 64.10 seconds, 1.07 minutes\n",
      "total_backward_count 1703460 real_backward_count 1478867  86.815%\n",
      "epoch-174 lr=['0.0009766'], tr/val_loss:  2.339252/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.32%, tr_best:  16.45%, epoch time: 64.30 seconds, 1.07 minutes\n",
      "total_backward_count 1713250 real_backward_count 1487372  86.816%\n",
      "epoch-175 lr=['0.0009766'], tr/val_loss:  2.338949/  2.339790, val:  15.42%, val_best:  15.42%, tr:  14.91%, tr_best:  16.45%, epoch time: 63.93 seconds, 1.07 minutes\n",
      "total_backward_count 1723040 real_backward_count 1495900  86.817%\n",
      "epoch-176 lr=['0.0009766'], tr/val_loss:  2.336491/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.42%, tr_best:  16.45%, epoch time: 63.50 seconds, 1.06 minutes\n",
      "total_backward_count 1732830 real_backward_count 1504422  86.819%\n",
      "epoch-177 lr=['0.0009766'], tr/val_loss:  2.337985/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.42%, tr_best:  16.45%, epoch time: 63.31 seconds, 1.06 minutes\n",
      "total_backward_count 1742620 real_backward_count 1512923  86.819%\n",
      "epoch-178 lr=['0.0009766'], tr/val_loss:  2.339763/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.04%, tr_best:  16.45%, epoch time: 63.36 seconds, 1.06 minutes\n",
      "total_backward_count 1752410 real_backward_count 1521413  86.818%\n",
      "epoch-179 lr=['0.0009766'], tr/val_loss:  2.335148/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.73%, tr_best:  16.45%, epoch time: 63.68 seconds, 1.06 minutes\n",
      "total_backward_count 1762200 real_backward_count 1529930  86.819%\n",
      "epoch-180 lr=['0.0009766'], tr/val_loss:  2.337254/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.42%, tr_best:  16.45%, epoch time: 63.33 seconds, 1.06 minutes\n",
      "total_backward_count 1771990 real_backward_count 1538482  86.822%\n",
      "epoch-181 lr=['0.0009766'], tr/val_loss:  2.339008/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.73%, tr_best:  16.45%, epoch time: 63.93 seconds, 1.07 minutes\n",
      "total_backward_count 1781780 real_backward_count 1546964  86.821%\n",
      "epoch-182 lr=['0.0009766'], tr/val_loss:  2.337491/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.53%, tr_best:  16.45%, epoch time: 63.46 seconds, 1.06 minutes\n",
      "total_backward_count 1791570 real_backward_count 1555482  86.822%\n",
      "epoch-183 lr=['0.0009766'], tr/val_loss:  2.335920/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.22%, tr_best:  16.45%, epoch time: 63.83 seconds, 1.06 minutes\n",
      "total_backward_count 1801360 real_backward_count 1563949  86.820%\n",
      "epoch-184 lr=['0.0009766'], tr/val_loss:  2.336497/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.73%, tr_best:  16.45%, epoch time: 63.60 seconds, 1.06 minutes\n",
      "total_backward_count 1811150 real_backward_count 1572418  86.819%\n",
      "epoch-185 lr=['0.0009766'], tr/val_loss:  2.337574/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.53%, tr_best:  16.45%, epoch time: 63.99 seconds, 1.07 minutes\n",
      "total_backward_count 1820940 real_backward_count 1580911  86.818%\n",
      "epoch-186 lr=['0.0009766'], tr/val_loss:  2.337791/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.22%, tr_best:  16.45%, epoch time: 63.81 seconds, 1.06 minutes\n",
      "total_backward_count 1830730 real_backward_count 1589393  86.817%\n",
      "epoch-187 lr=['0.0009766'], tr/val_loss:  2.339028/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.32%, tr_best:  16.45%, epoch time: 63.66 seconds, 1.06 minutes\n",
      "total_backward_count 1840520 real_backward_count 1597936  86.820%\n",
      "epoch-188 lr=['0.0009766'], tr/val_loss:  2.338382/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.14%, tr_best:  16.45%, epoch time: 63.82 seconds, 1.06 minutes\n",
      "total_backward_count 1850310 real_backward_count 1606440  86.820%\n",
      "epoch-189 lr=['0.0009766'], tr/val_loss:  2.333725/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.12%, tr_best:  16.45%, epoch time: 64.25 seconds, 1.07 minutes\n",
      "total_backward_count 1860100 real_backward_count 1614955  86.821%\n",
      "epoch-190 lr=['0.0009766'], tr/val_loss:  2.337346/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.63%, tr_best:  16.45%, epoch time: 63.75 seconds, 1.06 minutes\n",
      "total_backward_count 1869890 real_backward_count 1623452  86.821%\n",
      "epoch-191 lr=['0.0009766'], tr/val_loss:  2.338063/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.83%, tr_best:  16.45%, epoch time: 64.15 seconds, 1.07 minutes\n",
      "total_backward_count 1879680 real_backward_count 1631953  86.821%\n",
      "epoch-192 lr=['0.0009766'], tr/val_loss:  2.339396/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.12%, tr_best:  16.45%, epoch time: 62.78 seconds, 1.05 minutes\n",
      "total_backward_count 1889470 real_backward_count 1640476  86.822%\n",
      "epoch-193 lr=['0.0009766'], tr/val_loss:  2.337704/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.24%, tr_best:  16.45%, epoch time: 63.42 seconds, 1.06 minutes\n",
      "total_backward_count 1899260 real_backward_count 1648956  86.821%\n",
      "epoch-194 lr=['0.0009766'], tr/val_loss:  2.337538/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.73%, tr_best:  16.45%, epoch time: 62.60 seconds, 1.04 minutes\n",
      "total_backward_count 1909050 real_backward_count 1657438  86.820%\n",
      "epoch-195 lr=['0.0009766'], tr/val_loss:  2.334978/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.93%, tr_best:  16.45%, epoch time: 64.20 seconds, 1.07 minutes\n",
      "total_backward_count 1918840 real_backward_count 1665910  86.819%\n",
      "epoch-196 lr=['0.0009766'], tr/val_loss:  2.339672/  2.339790, val:  15.42%, val_best:  15.42%, tr:  16.04%, tr_best:  16.45%, epoch time: 62.92 seconds, 1.05 minutes\n",
      "total_backward_count 1928630 real_backward_count 1674434  86.820%\n",
      "epoch-197 lr=['0.0009766'], tr/val_loss:  2.339024/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.53%, tr_best:  16.45%, epoch time: 63.42 seconds, 1.06 minutes\n",
      "total_backward_count 1938420 real_backward_count 1682978  86.822%\n",
      "epoch-198 lr=['0.0009766'], tr/val_loss:  2.332648/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.53%, tr_best:  16.45%, epoch time: 63.65 seconds, 1.06 minutes\n",
      "total_backward_count 1948210 real_backward_count 1691483  86.822%\n",
      "epoch-199 lr=['0.0009766'], tr/val_loss:  2.338150/  2.339790, val:  15.42%, val_best:  15.42%, tr:  15.93%, tr_best:  16.45%, epoch time: 63.30 seconds, 1.06 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75038b4d82154aa292dd67984a8ef18e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñà‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>tr_acc</td><td>‚ñà‚ñÜ‚ñá‚ñÉ‚ñÑ‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñÉ‚ñá‚ñÉ‚ñà‚ñÖ‚ñà‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÇ‚ñÜ‚ñÉ‚ñá‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñá‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñà‚ñá</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÅ‚ñÇ‚ñÅ‚ñÜ‚ñá‚ñÑ‚ñÇ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÇ‚ñÑ‚ñÉ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñà‚ñÖ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñá‚ñÅ‚ñÜ‚ñÉ‚ñà‚ñÖ‚ñá‚ñà‚ñÜ‚ñÑ‚ñá‚ñÖ‚ñÜ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>0.0</td></tr><tr><td>tr_acc</td><td>0.15935</td></tr><tr><td>tr_epoch_loss</td><td>2.33815</td></tr><tr><td>val_acc_best</td><td>0.15417</td></tr><tr><td>val_acc_now</td><td>0.15417</td></tr><tr><td>val_loss</td><td>2.33979</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">denim-sweep-101</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jl8i3zxz' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jl8i3zxz</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251110_233342-jl8i3zxz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: eyqhf604 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251111_030555-eyqhf604</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/eyqhf604' target=\"_blank\">deft-sweep-105</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hpjdvxst' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hpjdvxst</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hpjdvxst' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hpjdvxst</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/eyqhf604' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/eyqhf604</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251111_030603_619', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 427.0\n",
      "lif layer 1 self.abs_max_v: 427.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 915.0\n",
      "lif layer 2 self.abs_max_v: 915.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 449.0\n",
      "fc layer 1 self.abs_max_out: 562.0\n",
      "lif layer 1 self.abs_max_v: 591.0\n",
      "fc layer 2 self.abs_max_out: 962.0\n",
      "lif layer 2 self.abs_max_v: 1419.5\n",
      "fc layer 3 self.abs_max_out: 499.0\n",
      "fc layer 1 self.abs_max_out: 601.0\n",
      "lif layer 1 self.abs_max_v: 764.5\n",
      "fc layer 2 self.abs_max_out: 1127.0\n",
      "lif layer 2 self.abs_max_v: 1837.0\n",
      "lif layer 1 self.abs_max_v: 785.5\n",
      "fc layer 1 self.abs_max_out: 621.0\n",
      "lif layer 1 self.abs_max_v: 810.0\n",
      "lif layer 1 self.abs_max_v: 815.0\n",
      "fc layer 2 self.abs_max_out: 1139.0\n",
      "lif layer 2 self.abs_max_v: 1846.0\n",
      "fc layer 1 self.abs_max_out: 653.0\n",
      "lif layer 1 self.abs_max_v: 882.0\n",
      "lif layer 2 self.abs_max_v: 1893.0\n",
      "lif layer 1 self.abs_max_v: 949.5\n",
      "lif layer 2 self.abs_max_v: 2072.5\n",
      "lif layer 1 self.abs_max_v: 962.0\n",
      "fc layer 3 self.abs_max_out: 583.0\n",
      "fc layer 1 self.abs_max_out: 941.0\n",
      "lif layer 1 self.abs_max_v: 992.5\n",
      "lif layer 1 self.abs_max_v: 1052.0\n",
      "fc layer 2 self.abs_max_out: 1276.0\n",
      "lif layer 1 self.abs_max_v: 1100.0\n",
      "fc layer 3 self.abs_max_out: 590.0\n",
      "fc layer 3 self.abs_max_out: 591.0\n",
      "lif layer 1 self.abs_max_v: 1101.5\n",
      "lif layer 1 self.abs_max_v: 1124.0\n",
      "lif layer 1 self.abs_max_v: 1138.5\n",
      "lif layer 1 self.abs_max_v: 1319.0\n",
      "fc layer 1 self.abs_max_out: 1132.0\n",
      "lif layer 1 self.abs_max_v: 1384.0\n",
      "lif layer 1 self.abs_max_v: 1518.0\n",
      "lif layer 1 self.abs_max_v: 1524.0\n",
      "lif layer 2 self.abs_max_v: 2087.0\n",
      "fc layer 2 self.abs_max_out: 1373.0\n",
      "lif layer 2 self.abs_max_v: 2366.5\n",
      "fc layer 3 self.abs_max_out: 613.0\n",
      "fc layer 1 self.abs_max_out: 1158.0\n",
      "fc layer 1 self.abs_max_out: 1164.0\n",
      "lif layer 1 self.abs_max_v: 1606.0\n",
      "lif layer 1 self.abs_max_v: 1666.0\n",
      "fc layer 1 self.abs_max_out: 1222.0\n",
      "fc layer 2 self.abs_max_out: 1478.0\n",
      "lif layer 2 self.abs_max_v: 2580.0\n",
      "lif layer 2 self.abs_max_v: 2716.0\n",
      "fc layer 3 self.abs_max_out: 656.0\n",
      "fc layer 2 self.abs_max_out: 1689.0\n",
      "lif layer 2 self.abs_max_v: 2741.0\n",
      "lif layer 1 self.abs_max_v: 1718.0\n",
      "fc layer 1 self.abs_max_out: 1228.0\n",
      "lif layer 1 self.abs_max_v: 1973.0\n",
      "fc layer 1 self.abs_max_out: 1370.0\n",
      "lif layer 1 self.abs_max_v: 2010.5\n",
      "fc layer 1 self.abs_max_out: 1376.0\n",
      "lif layer 1 self.abs_max_v: 2056.0\n",
      "fc layer 2 self.abs_max_out: 1812.0\n",
      "lif layer 2 self.abs_max_v: 2893.0\n",
      "lif layer 1 self.abs_max_v: 2095.0\n",
      "fc layer 1 self.abs_max_out: 1455.0\n",
      "lif layer 1 self.abs_max_v: 2149.0\n",
      "lif layer 1 self.abs_max_v: 2152.5\n",
      "lif layer 1 self.abs_max_v: 2270.0\n",
      "lif layer 1 self.abs_max_v: 2337.5\n",
      "fc layer 1 self.abs_max_out: 1480.0\n",
      "lif layer 1 self.abs_max_v: 2506.0\n",
      "fc layer 1 self.abs_max_out: 1524.0\n",
      "fc layer 1 self.abs_max_out: 1676.0\n",
      "lif layer 1 self.abs_max_v: 2553.5\n",
      "lif layer 1 self.abs_max_v: 2744.0\n",
      "lif layer 1 self.abs_max_v: 2764.5\n",
      "lif layer 1 self.abs_max_v: 2802.5\n",
      "fc layer 1 self.abs_max_out: 1791.0\n",
      "fc layer 3 self.abs_max_out: 685.0\n",
      "lif layer 1 self.abs_max_v: 2826.0\n",
      "lif layer 1 self.abs_max_v: 2892.0\n",
      "lif layer 1 self.abs_max_v: 2998.0\n",
      "lif layer 1 self.abs_max_v: 3094.0\n",
      "fc layer 1 self.abs_max_out: 1850.0\n",
      "fc layer 3 self.abs_max_out: 707.0\n",
      "lif layer 1 self.abs_max_v: 3115.0\n",
      "fc layer 1 self.abs_max_out: 1859.0\n",
      "fc layer 1 self.abs_max_out: 1869.0\n",
      "lif layer 1 self.abs_max_v: 3144.0\n",
      "lif layer 1 self.abs_max_v: 3171.5\n",
      "fc layer 1 self.abs_max_out: 1884.0\n",
      "lif layer 1 self.abs_max_v: 3470.0\n",
      "fc layer 1 self.abs_max_out: 1893.0\n",
      "fc layer 1 self.abs_max_out: 1911.0\n",
      "fc layer 1 self.abs_max_out: 1954.0\n",
      "fc layer 1 self.abs_max_out: 1971.0\n",
      "lif layer 1 self.abs_max_v: 3476.5\n",
      "fc layer 1 self.abs_max_out: 2040.0\n",
      "lif layer 1 self.abs_max_v: 3564.0\n",
      "fc layer 1 self.abs_max_out: 2070.0\n",
      "lif layer 1 self.abs_max_v: 3594.0\n",
      "lif layer 1 self.abs_max_v: 3645.0\n",
      "lif layer 1 self.abs_max_v: 3758.0\n",
      "fc layer 1 self.abs_max_out: 2078.0\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  1.850516/  2.020126, val:  35.83%, val_best:  35.83%, tr:  95.61%, tr_best:  95.61%, epoch time: 63.60 seconds, 1.06 minutes\n",
      "total_backward_count 9790 real_backward_count 2569  26.241%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 2166.0\n",
      "lif layer 1 self.abs_max_v: 3890.5\n",
      "fc layer 3 self.abs_max_out: 712.0\n",
      "fc layer 3 self.abs_max_out: 715.0\n",
      "fc layer 3 self.abs_max_out: 864.0\n",
      "fc layer 1 self.abs_max_out: 2301.0\n",
      "fc layer 1 self.abs_max_out: 2513.0\n",
      "lif layer 1 self.abs_max_v: 4095.5\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  1.809232/  2.000774, val:  42.08%, val_best:  42.08%, tr:  98.37%, tr_best:  98.37%, epoch time: 64.24 seconds, 1.07 minutes\n",
      "total_backward_count 19580 real_backward_count 4286  21.890%\n",
      "lif layer 1 self.abs_max_v: 4122.5\n",
      "lif layer 1 self.abs_max_v: 4325.5\n",
      "lif layer 1 self.abs_max_v: 4335.0\n",
      "lif layer 1 self.abs_max_v: 4369.0\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  1.782221/  1.976699, val:  46.67%, val_best:  46.67%, tr:  99.08%, tr_best:  99.08%, epoch time: 64.09 seconds, 1.07 minutes\n",
      "total_backward_count 29370 real_backward_count 5897  20.078%\n",
      "fc layer 2 self.abs_max_out: 1844.0\n",
      "fc layer 1 self.abs_max_out: 2715.0\n",
      "lif layer 1 self.abs_max_v: 4485.5\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  1.782183/  1.995004, val:  33.75%, val_best:  46.67%, tr:  99.28%, tr_best:  99.28%, epoch time: 63.75 seconds, 1.06 minutes\n",
      "total_backward_count 39160 real_backward_count 7432  18.979%\n",
      "lif layer 1 self.abs_max_v: 4529.0\n",
      "lif layer 1 self.abs_max_v: 4686.5\n",
      "fc layer 1 self.abs_max_out: 2749.0\n",
      "lif layer 1 self.abs_max_v: 4719.0\n",
      "lif layer 1 self.abs_max_v: 4757.5\n",
      "lif layer 1 self.abs_max_v: 5000.5\n",
      "fc layer 1 self.abs_max_out: 2803.0\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  1.794726/  1.986516, val:  42.50%, val_best:  46.67%, tr:  99.80%, tr_best:  99.80%, epoch time: 63.43 seconds, 1.06 minutes\n",
      "total_backward_count 48950 real_backward_count 8843  18.065%\n",
      "fc layer 1 self.abs_max_out: 2867.0\n",
      "fc layer 1 self.abs_max_out: 2928.0\n",
      "lif layer 1 self.abs_max_v: 5134.5\n",
      "lif layer 1 self.abs_max_v: 5164.0\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  1.796148/  1.982132, val:  40.42%, val_best:  46.67%, tr:  99.39%, tr_best:  99.80%, epoch time: 63.52 seconds, 1.06 minutes\n",
      "total_backward_count 58740 real_backward_count 10201  17.366%\n",
      "lif layer 2 self.abs_max_v: 2942.0\n",
      "fc layer 2 self.abs_max_out: 1854.0\n",
      "lif layer 1 self.abs_max_v: 5172.0\n",
      "lif layer 1 self.abs_max_v: 5386.0\n",
      "lif layer 2 self.abs_max_v: 2949.5\n",
      "lif layer 2 self.abs_max_v: 3101.0\n",
      "lif layer 2 self.abs_max_v: 3230.5\n",
      "fc layer 2 self.abs_max_out: 1855.0\n",
      "fc layer 2 self.abs_max_out: 1961.0\n",
      "fc layer 2 self.abs_max_out: 1971.0\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  1.795565/  1.977081, val:  53.33%, val_best:  53.33%, tr:  99.69%, tr_best:  99.80%, epoch time: 63.59 seconds, 1.06 minutes\n",
      "total_backward_count 68530 real_backward_count 11564  16.874%\n",
      "lif layer 1 self.abs_max_v: 5443.5\n",
      "fc layer 1 self.abs_max_out: 2961.0\n",
      "fc layer 1 self.abs_max_out: 2967.0\n",
      "lif layer 1 self.abs_max_v: 5509.5\n",
      "fc layer 1 self.abs_max_out: 3028.0\n",
      "lif layer 1 self.abs_max_v: 5523.5\n",
      "lif layer 1 self.abs_max_v: 5786.0\n",
      "fc layer 1 self.abs_max_out: 3099.0\n",
      "fc layer 1 self.abs_max_out: 3124.0\n",
      "lif layer 1 self.abs_max_v: 5950.0\n",
      "lif layer 1 self.abs_max_v: 5970.0\n",
      "lif layer 1 self.abs_max_v: 6094.0\n",
      "fc layer 1 self.abs_max_out: 3149.0\n",
      "lif layer 1 self.abs_max_v: 6196.0\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  1.788970/  1.940920, val:  54.17%, val_best:  54.17%, tr:  99.49%, tr_best:  99.80%, epoch time: 63.32 seconds, 1.06 minutes\n",
      "total_backward_count 78320 real_backward_count 12889  16.457%\n",
      "fc layer 1 self.abs_max_out: 3244.0\n",
      "fc layer 1 self.abs_max_out: 3521.0\n",
      "lif layer 1 self.abs_max_v: 6398.5\n",
      "fc layer 1 self.abs_max_out: 3529.0\n",
      "lif layer 1 self.abs_max_v: 6426.0\n",
      "fc layer 1 self.abs_max_out: 3570.0\n",
      "lif layer 1 self.abs_max_v: 6783.0\n",
      "lif layer 1 self.abs_max_v: 6861.5\n",
      "fc layer 1 self.abs_max_out: 3615.0\n",
      "lif layer 1 self.abs_max_v: 7046.0\n",
      "fc layer 1 self.abs_max_out: 3682.0\n",
      "lif layer 1 self.abs_max_v: 7205.0\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  1.802202/  1.968619, val:  52.08%, val_best:  54.17%, tr:  99.39%, tr_best:  99.80%, epoch time: 64.87 seconds, 1.08 minutes\n",
      "total_backward_count 88110 real_backward_count 14224  16.143%\n",
      "fc layer 1 self.abs_max_out: 3692.0\n",
      "fc layer 1 self.abs_max_out: 3734.0\n",
      "lif layer 1 self.abs_max_v: 7283.5\n",
      "fc layer 1 self.abs_max_out: 3812.0\n",
      "lif layer 1 self.abs_max_v: 7454.0\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  1.807456/  1.999344, val:  39.58%, val_best:  54.17%, tr:  99.69%, tr_best:  99.80%, epoch time: 63.45 seconds, 1.06 minutes\n",
      "total_backward_count 97900 real_backward_count 15501  15.834%\n",
      "fc layer 1 self.abs_max_out: 3843.0\n",
      "lif layer 1 self.abs_max_v: 7517.0\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  1.807224/  1.967424, val:  51.25%, val_best:  54.17%, tr:  99.69%, tr_best:  99.80%, epoch time: 64.33 seconds, 1.07 minutes\n",
      "total_backward_count 107690 real_backward_count 16738  15.543%\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  1.805708/  1.969893, val:  57.50%, val_best:  57.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 63.42 seconds, 1.06 minutes\n",
      "total_backward_count 117480 real_backward_count 17985  15.309%\n",
      "lif layer 2 self.abs_max_v: 3297.0\n",
      "lif layer 2 self.abs_max_v: 3409.5\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  1.790372/  1.944590, val:  51.25%, val_best:  57.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 63.74 seconds, 1.06 minutes\n",
      "total_backward_count 127270 real_backward_count 19188  15.077%\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  1.775839/  1.952987, val:  47.08%, val_best:  57.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 63.46 seconds, 1.06 minutes\n",
      "total_backward_count 137060 real_backward_count 20420  14.899%\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  1.779341/  1.949820, val:  52.92%, val_best:  57.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 64.04 seconds, 1.07 minutes\n",
      "total_backward_count 146850 real_backward_count 21588  14.701%\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  1.764978/  1.924334, val:  46.67%, val_best:  57.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 64.06 seconds, 1.07 minutes\n",
      "total_backward_count 156640 real_backward_count 22781  14.544%\n",
      "fc layer 2 self.abs_max_out: 2002.0\n",
      "fc layer 2 self.abs_max_out: 2010.0\n",
      "fc layer 2 self.abs_max_out: 2105.0\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  1.749239/  1.921458, val:  50.00%, val_best:  57.50%, tr:  99.49%, tr_best: 100.00%, epoch time: 62.90 seconds, 1.05 minutes\n",
      "total_backward_count 166430 real_backward_count 23938  14.383%\n",
      "fc layer 2 self.abs_max_out: 2113.0\n",
      "fc layer 2 self.abs_max_out: 2120.0\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  1.753572/  1.910994, val:  60.00%, val_best:  60.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 64.09 seconds, 1.07 minutes\n",
      "total_backward_count 176220 real_backward_count 25143  14.268%\n",
      "fc layer 1 self.abs_max_out: 3914.0\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  1.760173/  1.935303, val:  51.25%, val_best:  60.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 63.29 seconds, 1.05 minutes\n",
      "total_backward_count 186010 real_backward_count 26319  14.149%\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  1.759340/  1.941461, val:  47.92%, val_best:  60.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 64.42 seconds, 1.07 minutes\n",
      "total_backward_count 195800 real_backward_count 27406  13.997%\n",
      "fc layer 2 self.abs_max_out: 2137.0\n",
      "lif layer 2 self.abs_max_v: 3455.0\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  1.763487/  1.954867, val:  52.08%, val_best:  60.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 63.70 seconds, 1.06 minutes\n",
      "total_backward_count 205590 real_backward_count 28534  13.879%\n",
      "lif layer 2 self.abs_max_v: 3467.0\n",
      "lif layer 2 self.abs_max_v: 3510.5\n",
      "lif layer 2 self.abs_max_v: 3606.0\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  1.773396/  1.951156, val:  53.33%, val_best:  60.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 63.85 seconds, 1.06 minutes\n",
      "total_backward_count 215380 real_backward_count 29746  13.811%\n",
      "fc layer 1 self.abs_max_out: 4046.0\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  1.772741/  1.927685, val:  55.00%, val_best:  60.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 64.15 seconds, 1.07 minutes\n",
      "total_backward_count 225170 real_backward_count 30884  13.716%\n",
      "fc layer 2 self.abs_max_out: 2196.0\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  1.753410/  1.902843, val:  57.08%, val_best:  60.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 63.50 seconds, 1.06 minutes\n",
      "total_backward_count 234960 real_backward_count 32022  13.629%\n",
      "lif layer 2 self.abs_max_v: 3721.0\n",
      "fc layer 1 self.abs_max_out: 4048.0\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  1.743957/  1.930126, val:  51.67%, val_best:  60.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 64.12 seconds, 1.07 minutes\n",
      "total_backward_count 244750 real_backward_count 33160  13.549%\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  1.777746/  1.937607, val:  63.75%, val_best:  63.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 63.97 seconds, 1.07 minutes\n",
      "total_backward_count 254540 real_backward_count 34376  13.505%\n",
      "fc layer 1 self.abs_max_out: 4188.0\n",
      "lif layer 1 self.abs_max_v: 7533.5\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  1.778874/  1.940808, val:  55.83%, val_best:  63.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 64.18 seconds, 1.07 minutes\n",
      "total_backward_count 264330 real_backward_count 35483  13.424%\n",
      "lif layer 1 self.abs_max_v: 7717.5\n",
      "lif layer 1 self.abs_max_v: 7842.0\n",
      "fc layer 1 self.abs_max_out: 4302.0\n",
      "fc layer 1 self.abs_max_out: 4394.0\n",
      "lif layer 1 self.abs_max_v: 8209.5\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  1.766326/  1.935817, val:  67.50%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 63.31 seconds, 1.06 minutes\n",
      "total_backward_count 274120 real_backward_count 36637  13.365%\n",
      "lif layer 2 self.abs_max_v: 3796.0\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  1.764153/  1.929352, val:  51.67%, val_best:  67.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 63.84 seconds, 1.06 minutes\n",
      "total_backward_count 283910 real_backward_count 37744  13.294%\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  1.760019/  1.922496, val:  52.50%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 63.84 seconds, 1.06 minutes\n",
      "total_backward_count 293700 real_backward_count 38821  13.218%\n",
      "fc layer 1 self.abs_max_out: 4620.0\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  1.757306/  1.922387, val:  53.75%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 63.85 seconds, 1.06 minutes\n",
      "total_backward_count 303490 real_backward_count 39937  13.159%\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  1.748765/  1.903943, val:  57.50%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 64.13 seconds, 1.07 minutes\n",
      "total_backward_count 313280 real_backward_count 41044  13.101%\n",
      "lif layer 1 self.abs_max_v: 8261.0\n",
      "lif layer 1 self.abs_max_v: 8420.5\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  1.743117/  1.908377, val:  46.67%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 63.46 seconds, 1.06 minutes\n",
      "total_backward_count 323070 real_backward_count 42123  13.038%\n",
      "lif layer 1 self.abs_max_v: 8627.5\n",
      "fc layer 1 self.abs_max_out: 4644.0\n",
      "fc layer 1 self.abs_max_out: 4684.0\n",
      "lif layer 1 self.abs_max_v: 8888.5\n",
      "lif layer 1 self.abs_max_v: 8957.0\n",
      "lif layer 1 self.abs_max_v: 9153.5\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  1.735157/  1.899654, val:  60.42%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 62.20 seconds, 1.04 minutes\n",
      "total_backward_count 332860 real_backward_count 43202  12.979%\n",
      "fc layer 1 self.abs_max_out: 4711.0\n",
      "fc layer 1 self.abs_max_out: 4738.0\n",
      "lif layer 1 self.abs_max_v: 9256.5\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  1.721111/  1.897458, val:  55.83%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 64.08 seconds, 1.07 minutes\n",
      "total_backward_count 342650 real_backward_count 44219  12.905%\n",
      "fc layer 1 self.abs_max_out: 4759.0\n",
      "fc layer 1 self.abs_max_out: 5006.0\n",
      "fc layer 1 self.abs_max_out: 5054.0\n",
      "lif layer 1 self.abs_max_v: 9572.0\n",
      "lif layer 1 self.abs_max_v: 9576.0\n",
      "lif layer 1 self.abs_max_v: 9768.0\n",
      "fc layer 1 self.abs_max_out: 5141.0\n",
      "lif layer 1 self.abs_max_v: 10025.0\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  1.717229/  1.899226, val:  46.67%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 63.45 seconds, 1.06 minutes\n",
      "total_backward_count 352440 real_backward_count 45240  12.836%\n",
      "fc layer 1 self.abs_max_out: 5166.0\n",
      "lif layer 1 self.abs_max_v: 10071.0\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  1.708349/  1.883716, val:  60.42%, val_best:  67.50%, tr:  99.49%, tr_best: 100.00%, epoch time: 62.88 seconds, 1.05 minutes\n",
      "total_backward_count 362230 real_backward_count 46313  12.786%\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  1.718953/  1.885549, val:  64.58%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 64.41 seconds, 1.07 minutes\n",
      "total_backward_count 372020 real_backward_count 47293  12.712%\n",
      "fc layer 3 self.abs_max_out: 893.0\n",
      "fc layer 1 self.abs_max_out: 5194.0\n",
      "fc layer 1 self.abs_max_out: 5296.0\n",
      "lif layer 1 self.abs_max_v: 10318.5\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  1.729214/  1.895240, val:  54.58%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.80 seconds, 1.03 minutes\n",
      "total_backward_count 381810 real_backward_count 48371  12.669%\n",
      "lif layer 2 self.abs_max_v: 3812.0\n",
      "fc layer 2 self.abs_max_out: 2205.0\n",
      "lif layer 2 self.abs_max_v: 4009.0\n",
      "lif layer 2 self.abs_max_v: 4084.5\n",
      "lif layer 2 self.abs_max_v: 4186.5\n",
      "fc layer 2 self.abs_max_out: 2398.0\n",
      "lif layer 2 self.abs_max_v: 4238.0\n",
      "fc layer 2 self.abs_max_out: 2416.0\n",
      "lif layer 2 self.abs_max_v: 4375.5\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  1.711524/  1.912852, val:  48.75%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.00 seconds, 1.03 minutes\n",
      "total_backward_count 391600 real_backward_count 49385  12.611%\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  1.721939/  1.875634, val:  57.92%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 63.68 seconds, 1.06 minutes\n",
      "total_backward_count 401390 real_backward_count 50472  12.574%\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  1.716334/  1.869193, val:  61.25%, val_best:  67.50%, tr:  99.49%, tr_best: 100.00%, epoch time: 63.07 seconds, 1.05 minutes\n",
      "total_backward_count 411180 real_backward_count 51520  12.530%\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  1.703578/  1.879599, val:  55.83%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.89 seconds, 1.05 minutes\n",
      "total_backward_count 420970 real_backward_count 52526  12.477%\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  1.706127/  1.874264, val:  63.33%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 62.37 seconds, 1.04 minutes\n",
      "total_backward_count 430760 real_backward_count 53587  12.440%\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  1.721249/  1.911510, val:  51.25%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 63.98 seconds, 1.07 minutes\n",
      "total_backward_count 440550 real_backward_count 54614  12.397%\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  1.717266/  1.869912, val:  55.83%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 63.16 seconds, 1.05 minutes\n",
      "total_backward_count 450340 real_backward_count 55604  12.347%\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  1.693252/  1.875930, val:  61.25%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 63.15 seconds, 1.05 minutes\n",
      "total_backward_count 460130 real_backward_count 56661  12.314%\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  1.705133/  1.871212, val:  52.50%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 63.63 seconds, 1.06 minutes\n",
      "total_backward_count 469920 real_backward_count 57695  12.278%\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  1.685919/  1.843355, val:  61.25%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 63.31 seconds, 1.06 minutes\n",
      "total_backward_count 479710 real_backward_count 58726  12.242%\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  1.691448/  1.871597, val:  53.75%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 63.00 seconds, 1.05 minutes\n",
      "total_backward_count 489500 real_backward_count 59771  12.211%\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  1.691981/  1.867932, val:  51.67%, val_best:  67.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 62.41 seconds, 1.04 minutes\n",
      "total_backward_count 499290 real_backward_count 60766  12.170%\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  1.688034/  1.860782, val:  62.92%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 63.66 seconds, 1.06 minutes\n",
      "total_backward_count 509080 real_backward_count 61761  12.132%\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  1.699726/  1.864681, val:  60.83%, val_best:  67.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 64.18 seconds, 1.07 minutes\n",
      "total_backward_count 518870 real_backward_count 62797  12.103%\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  1.701814/  1.874417, val:  61.67%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 62.92 seconds, 1.05 minutes\n",
      "total_backward_count 528660 real_backward_count 63791  12.067%\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  1.709108/  1.883772, val:  62.92%, val_best:  67.50%, tr:  99.39%, tr_best: 100.00%, epoch time: 62.91 seconds, 1.05 minutes\n",
      "total_backward_count 538450 real_backward_count 64835  12.041%\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  1.704457/  1.878084, val:  62.50%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 63.28 seconds, 1.05 minutes\n",
      "total_backward_count 548240 real_backward_count 65860  12.013%\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  1.693393/  1.898192, val:  54.58%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.46 seconds, 1.04 minutes\n",
      "total_backward_count 558030 real_backward_count 66923  11.993%\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  1.697862/  1.860315, val:  58.33%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.70 seconds, 1.05 minutes\n",
      "total_backward_count 567820 real_backward_count 67859  11.951%\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  1.691756/  1.873169, val:  64.17%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 62.97 seconds, 1.05 minutes\n",
      "total_backward_count 577610 real_backward_count 68822  11.915%\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  1.694909/  1.867359, val:  57.92%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.70 seconds, 1.04 minutes\n",
      "total_backward_count 587400 real_backward_count 69789  11.881%\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  1.685534/  1.872064, val:  48.75%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 63.27 seconds, 1.05 minutes\n",
      "total_backward_count 597190 real_backward_count 70757  11.848%\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  1.692128/  1.853483, val:  65.00%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 63.11 seconds, 1.05 minutes\n",
      "total_backward_count 606980 real_backward_count 71814  11.831%\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  1.686163/  1.862566, val:  54.17%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 63.23 seconds, 1.05 minutes\n",
      "total_backward_count 616770 real_backward_count 72829  11.808%\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  1.689322/  1.860495, val:  60.00%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.32 seconds, 1.04 minutes\n",
      "total_backward_count 626560 real_backward_count 73806  11.780%\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  1.690679/  1.865707, val:  55.42%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.54 seconds, 1.04 minutes\n",
      "total_backward_count 636350 real_backward_count 74752  11.747%\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  1.661543/  1.852816, val:  63.33%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 62.85 seconds, 1.05 minutes\n",
      "total_backward_count 646140 real_backward_count 75703  11.716%\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  1.673162/  1.844383, val:  55.83%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 62.50 seconds, 1.04 minutes\n",
      "total_backward_count 655930 real_backward_count 76650  11.686%\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  1.664860/  1.832517, val:  62.08%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 63.06 seconds, 1.05 minutes\n",
      "total_backward_count 665720 real_backward_count 77631  11.661%\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  1.661358/  1.825585, val:  58.75%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 63.16 seconds, 1.05 minutes\n",
      "total_backward_count 675510 real_backward_count 78587  11.634%\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  1.657745/  1.845626, val:  57.50%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 62.39 seconds, 1.04 minutes\n",
      "total_backward_count 685300 real_backward_count 79532  11.605%\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  1.667879/  1.839563, val:  62.08%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.87 seconds, 1.03 minutes\n",
      "total_backward_count 695090 real_backward_count 80549  11.588%\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  1.669006/  1.850489, val:  57.92%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 62.75 seconds, 1.05 minutes\n",
      "total_backward_count 704880 real_backward_count 81550  11.569%\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  1.669653/  1.843676, val:  62.50%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.67 seconds, 1.04 minutes\n",
      "total_backward_count 714670 real_backward_count 82504  11.544%\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  1.663988/  1.843252, val:  61.67%, val_best:  67.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 63.24 seconds, 1.05 minutes\n",
      "total_backward_count 724460 real_backward_count 83467  11.521%\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  1.654997/  1.813915, val:  59.58%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 63.51 seconds, 1.06 minutes\n",
      "total_backward_count 734250 real_backward_count 84471  11.504%\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  1.644870/  1.826382, val:  66.25%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 63.35 seconds, 1.06 minutes\n",
      "total_backward_count 744040 real_backward_count 85393  11.477%\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  1.645165/  1.824110, val:  56.67%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 63.50 seconds, 1.06 minutes\n",
      "total_backward_count 753830 real_backward_count 86407  11.462%\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  1.640476/  1.838119, val:  47.50%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 63.32 seconds, 1.06 minutes\n",
      "total_backward_count 763620 real_backward_count 87332  11.437%\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  1.646490/  1.828886, val:  53.75%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.91 seconds, 1.05 minutes\n",
      "total_backward_count 773410 real_backward_count 88286  11.415%\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  1.648672/  1.836562, val:  64.58%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.72 seconds, 1.05 minutes\n",
      "total_backward_count 783200 real_backward_count 89164  11.385%\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  1.654551/  1.842652, val:  54.17%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 63.24 seconds, 1.05 minutes\n",
      "total_backward_count 792990 real_backward_count 90058  11.357%\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  1.638292/  1.829252, val:  54.58%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 63.32 seconds, 1.06 minutes\n",
      "total_backward_count 802780 real_backward_count 90942  11.328%\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  1.635801/  1.830498, val:  52.50%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.77 seconds, 1.05 minutes\n",
      "total_backward_count 812570 real_backward_count 91865  11.305%\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  1.640290/  1.823164, val:  59.58%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 62.71 seconds, 1.05 minutes\n",
      "total_backward_count 822360 real_backward_count 92775  11.282%\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  1.635621/  1.808280, val:  65.42%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 63.32 seconds, 1.06 minutes\n",
      "total_backward_count 832150 real_backward_count 93698  11.260%\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  1.636405/  1.810826, val:  70.83%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 63.95 seconds, 1.07 minutes\n",
      "total_backward_count 841940 real_backward_count 94625  11.239%\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  1.639768/  1.837692, val:  57.08%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.88 seconds, 1.03 minutes\n",
      "total_backward_count 851730 real_backward_count 95531  11.216%\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  1.643059/  1.814393, val:  64.17%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 63.28 seconds, 1.05 minutes\n",
      "total_backward_count 861520 real_backward_count 96497  11.201%\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  1.648019/  1.826920, val:  65.83%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 63.26 seconds, 1.05 minutes\n",
      "total_backward_count 871310 real_backward_count 97371  11.175%\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  1.647575/  1.816792, val:  70.00%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.93 seconds, 1.05 minutes\n",
      "total_backward_count 881100 real_backward_count 98316  11.158%\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  1.660720/  1.848902, val:  61.67%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 63.13 seconds, 1.05 minutes\n",
      "total_backward_count 890890 real_backward_count 99258  11.141%\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  1.668079/  1.834246, val:  75.42%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.87 seconds, 1.05 minutes\n",
      "total_backward_count 900680 real_backward_count 100118  11.116%\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  1.670596/  1.841235, val:  57.08%, val_best:  75.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 62.52 seconds, 1.04 minutes\n",
      "total_backward_count 910470 real_backward_count 101021  11.095%\n",
      "fc layer 1 self.abs_max_out: 5431.0\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  1.667016/  1.841476, val:  58.33%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.97 seconds, 1.05 minutes\n",
      "total_backward_count 920260 real_backward_count 101915  11.075%\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  1.646864/  1.828703, val:  65.83%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 63.12 seconds, 1.05 minutes\n",
      "total_backward_count 930050 real_backward_count 102810  11.054%\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  1.641180/  1.801554, val:  73.75%, val_best:  75.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 63.58 seconds, 1.06 minutes\n",
      "total_backward_count 939840 real_backward_count 103672  11.031%\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  1.635052/  1.816403, val:  64.58%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 63.14 seconds, 1.05 minutes\n",
      "total_backward_count 949630 real_backward_count 104549  11.009%\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  1.622221/  1.796653, val:  71.25%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.62 seconds, 1.04 minutes\n",
      "total_backward_count 959420 real_backward_count 105380  10.984%\n",
      "fc layer 1 self.abs_max_out: 5493.0\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  1.617000/  1.824447, val:  61.25%, val_best:  75.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 62.57 seconds, 1.04 minutes\n",
      "total_backward_count 969210 real_backward_count 106225  10.960%\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  1.626938/  1.815192, val:  65.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 63.27 seconds, 1.05 minutes\n",
      "total_backward_count 979000 real_backward_count 107098  10.940%\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  1.634104/  1.840649, val:  57.08%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 63.22 seconds, 1.05 minutes\n",
      "total_backward_count 988790 real_backward_count 107943  10.917%\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  1.637675/  1.816734, val:  62.92%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.66 seconds, 1.04 minutes\n",
      "total_backward_count 998580 real_backward_count 108814  10.897%\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  1.623295/  1.803155, val:  66.67%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.66 seconds, 1.04 minutes\n",
      "total_backward_count 1008370 real_backward_count 109635  10.872%\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  1.608537/  1.813722, val:  58.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 63.90 seconds, 1.07 minutes\n",
      "total_backward_count 1018160 real_backward_count 110500  10.853%\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  1.624716/  1.818547, val:  65.42%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 63.01 seconds, 1.05 minutes\n",
      "total_backward_count 1027950 real_backward_count 111392  10.836%\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  1.623301/  1.792693, val:  69.58%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 63.10 seconds, 1.05 minutes\n",
      "total_backward_count 1037740 real_backward_count 112274  10.819%\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  1.619895/  1.818804, val:  55.83%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.87 seconds, 1.05 minutes\n",
      "total_backward_count 1047530 real_backward_count 113079  10.795%\n",
      "fc layer 1 self.abs_max_out: 5500.0\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  1.624290/  1.816687, val:  57.92%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 63.25 seconds, 1.05 minutes\n",
      "total_backward_count 1057320 real_backward_count 113948  10.777%\n",
      "fc layer 1 self.abs_max_out: 5630.0\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  1.620160/  1.805671, val:  67.08%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 63.49 seconds, 1.06 minutes\n",
      "total_backward_count 1067110 real_backward_count 114787  10.757%\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  1.608709/  1.798959, val:  60.00%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.69 seconds, 1.04 minutes\n",
      "total_backward_count 1076900 real_backward_count 115705  10.744%\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  1.622455/  1.803941, val:  67.92%, val_best:  75.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 63.45 seconds, 1.06 minutes\n",
      "total_backward_count 1086690 real_backward_count 116581  10.728%\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  1.618111/  1.809361, val:  60.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.58 seconds, 1.04 minutes\n",
      "total_backward_count 1096480 real_backward_count 117443  10.711%\n",
      "fc layer 2 self.abs_max_out: 2451.0\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  1.623294/  1.798091, val:  72.50%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 63.14 seconds, 1.05 minutes\n",
      "total_backward_count 1106270 real_backward_count 118281  10.692%\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  1.626004/  1.816111, val:  64.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 63.01 seconds, 1.05 minutes\n",
      "total_backward_count 1116060 real_backward_count 119105  10.672%\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  1.627782/  1.824861, val:  70.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 63.01 seconds, 1.05 minutes\n",
      "total_backward_count 1125850 real_backward_count 119959  10.655%\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  1.618225/  1.806621, val:  66.67%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.84 seconds, 1.03 minutes\n",
      "total_backward_count 1135640 real_backward_count 120780  10.635%\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  1.623280/  1.800208, val:  62.92%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 63.44 seconds, 1.06 minutes\n",
      "total_backward_count 1145430 real_backward_count 121626  10.618%\n",
      "epoch-117 lr=['0.0009766'], tr/val_loss:  1.613039/  1.790347, val:  70.83%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 63.54 seconds, 1.06 minutes\n",
      "total_backward_count 1155220 real_backward_count 122471  10.602%\n",
      "epoch-118 lr=['0.0009766'], tr/val_loss:  1.608230/  1.802414, val:  57.08%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.91 seconds, 1.05 minutes\n",
      "total_backward_count 1165010 real_backward_count 123335  10.587%\n",
      "epoch-119 lr=['0.0009766'], tr/val_loss:  1.615379/  1.799553, val:  65.83%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.98 seconds, 1.05 minutes\n",
      "total_backward_count 1174800 real_backward_count 124195  10.572%\n",
      "epoch-120 lr=['0.0009766'], tr/val_loss:  1.618557/  1.808462, val:  65.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 63.27 seconds, 1.05 minutes\n",
      "total_backward_count 1184590 real_backward_count 124992  10.551%\n",
      "fc layer 2 self.abs_max_out: 2459.0\n",
      "epoch-121 lr=['0.0009766'], tr/val_loss:  1.618797/  1.805746, val:  61.67%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 63.32 seconds, 1.06 minutes\n",
      "total_backward_count 1194380 real_backward_count 125884  10.540%\n",
      "epoch-122 lr=['0.0009766'], tr/val_loss:  1.619905/  1.797257, val:  77.08%, val_best:  77.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 62.79 seconds, 1.05 minutes\n",
      "total_backward_count 1204170 real_backward_count 126739  10.525%\n",
      "fc layer 3 self.abs_max_out: 907.0\n",
      "epoch-123 lr=['0.0009766'], tr/val_loss:  1.614774/  1.787269, val:  68.75%, val_best:  77.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 63.27 seconds, 1.05 minutes\n",
      "total_backward_count 1213960 real_backward_count 127590  10.510%\n",
      "fc layer 1 self.abs_max_out: 5657.0\n",
      "epoch-124 lr=['0.0009766'], tr/val_loss:  1.606538/  1.779316, val:  72.92%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 63.08 seconds, 1.05 minutes\n",
      "total_backward_count 1223750 real_backward_count 128468  10.498%\n",
      "fc layer 1 self.abs_max_out: 5721.0\n",
      "epoch-125 lr=['0.0009766'], tr/val_loss:  1.605747/  1.776731, val:  71.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.97 seconds, 1.05 minutes\n",
      "total_backward_count 1233540 real_backward_count 129329  10.484%\n",
      "fc layer 2 self.abs_max_out: 2463.0\n",
      "fc layer 2 self.abs_max_out: 2504.0\n",
      "fc layer 1 self.abs_max_out: 5850.0\n",
      "epoch-126 lr=['0.0009766'], tr/val_loss:  1.597973/  1.778432, val:  62.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.08 seconds, 1.03 minutes\n",
      "total_backward_count 1243330 real_backward_count 130157  10.468%\n",
      "fc layer 1 self.abs_max_out: 5887.0\n",
      "epoch-127 lr=['0.0009766'], tr/val_loss:  1.596939/  1.781638, val:  66.25%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.80 seconds, 1.05 minutes\n",
      "total_backward_count 1253120 real_backward_count 130967  10.451%\n",
      "epoch-128 lr=['0.0009766'], tr/val_loss:  1.604128/  1.775108, val:  74.58%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.95 seconds, 1.05 minutes\n",
      "total_backward_count 1262910 real_backward_count 131800  10.436%\n",
      "fc layer 2 self.abs_max_out: 2611.0\n",
      "epoch-129 lr=['0.0009766'], tr/val_loss:  1.586084/  1.768126, val:  74.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.72 seconds, 1.05 minutes\n",
      "total_backward_count 1272700 real_backward_count 132599  10.419%\n",
      "epoch-130 lr=['0.0009766'], tr/val_loss:  1.580486/  1.764417, val:  68.75%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.40 seconds, 1.04 minutes\n",
      "total_backward_count 1282490 real_backward_count 133438  10.405%\n",
      "epoch-131 lr=['0.0009766'], tr/val_loss:  1.599057/  1.777905, val:  58.33%, val_best:  77.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 63.59 seconds, 1.06 minutes\n",
      "total_backward_count 1292280 real_backward_count 134306  10.393%\n",
      "epoch-132 lr=['0.0009766'], tr/val_loss:  1.604133/  1.799495, val:  65.83%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 60.15 seconds, 1.00 minutes\n",
      "total_backward_count 1302070 real_backward_count 135136  10.379%\n",
      "lif layer 2 self.abs_max_v: 4416.5\n",
      "epoch-133 lr=['0.0009766'], tr/val_loss:  1.610255/  1.779714, val:  65.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.60 seconds, 1.03 minutes\n",
      "total_backward_count 1311860 real_backward_count 135928  10.361%\n",
      "epoch-134 lr=['0.0009766'], tr/val_loss:  1.601658/  1.773355, val:  67.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.43 seconds, 1.02 minutes\n",
      "total_backward_count 1321650 real_backward_count 136726  10.345%\n",
      "epoch-135 lr=['0.0009766'], tr/val_loss:  1.602999/  1.787430, val:  65.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.13 seconds, 1.04 minutes\n",
      "total_backward_count 1331440 real_backward_count 137589  10.334%\n",
      "epoch-136 lr=['0.0009766'], tr/val_loss:  1.593346/  1.772616, val:  68.75%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.43 seconds, 1.02 minutes\n",
      "total_backward_count 1341230 real_backward_count 138367  10.316%\n",
      "epoch-137 lr=['0.0009766'], tr/val_loss:  1.593176/  1.781446, val:  61.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.36 seconds, 1.02 minutes\n",
      "total_backward_count 1351020 real_backward_count 139144  10.299%\n",
      "epoch-138 lr=['0.0009766'], tr/val_loss:  1.592219/  1.759056, val:  73.75%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 60.76 seconds, 1.01 minutes\n",
      "total_backward_count 1360810 real_backward_count 139967  10.286%\n",
      "epoch-139 lr=['0.0009766'], tr/val_loss:  1.586972/  1.762728, val:  63.33%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.48 seconds, 1.02 minutes\n",
      "total_backward_count 1370600 real_backward_count 140756  10.270%\n",
      "epoch-140 lr=['0.0009766'], tr/val_loss:  1.592281/  1.781735, val:  55.00%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.22 seconds, 1.02 minutes\n",
      "total_backward_count 1380390 real_backward_count 141572  10.256%\n",
      "epoch-141 lr=['0.0009766'], tr/val_loss:  1.594756/  1.774906, val:  73.75%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.69 seconds, 1.03 minutes\n",
      "total_backward_count 1390180 real_backward_count 142348  10.240%\n",
      "epoch-142 lr=['0.0009766'], tr/val_loss:  1.601284/  1.786066, val:  66.25%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.06 seconds, 1.02 minutes\n",
      "total_backward_count 1399970 real_backward_count 143112  10.223%\n",
      "epoch-143 lr=['0.0009766'], tr/val_loss:  1.602205/  1.788811, val:  74.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.56 seconds, 1.03 minutes\n",
      "total_backward_count 1409760 real_backward_count 143901  10.207%\n",
      "epoch-144 lr=['0.0009766'], tr/val_loss:  1.616688/  1.797971, val:  64.58%, val_best:  77.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 61.96 seconds, 1.03 minutes\n",
      "total_backward_count 1419550 real_backward_count 144726  10.195%\n",
      "epoch-145 lr=['0.0009766'], tr/val_loss:  1.613497/  1.780219, val:  66.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.81 seconds, 1.03 minutes\n",
      "total_backward_count 1429340 real_backward_count 145511  10.180%\n",
      "epoch-146 lr=['0.0009766'], tr/val_loss:  1.608816/  1.778630, val:  69.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.31 seconds, 1.02 minutes\n",
      "total_backward_count 1439130 real_backward_count 146302  10.166%\n",
      "epoch-147 lr=['0.0009766'], tr/val_loss:  1.597352/  1.790155, val:  56.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.90 seconds, 1.03 minutes\n",
      "total_backward_count 1448920 real_backward_count 147046  10.149%\n",
      "epoch-148 lr=['0.0009766'], tr/val_loss:  1.612391/  1.793313, val:  63.33%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.94 seconds, 1.03 minutes\n",
      "total_backward_count 1458710 real_backward_count 147784  10.131%\n",
      "epoch-149 lr=['0.0009766'], tr/val_loss:  1.604276/  1.793082, val:  70.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.68 seconds, 1.03 minutes\n",
      "total_backward_count 1468500 real_backward_count 148534  10.115%\n",
      "epoch-150 lr=['0.0009766'], tr/val_loss:  1.604753/  1.766763, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.46 seconds, 1.02 minutes\n",
      "total_backward_count 1478290 real_backward_count 149248  10.096%\n",
      "epoch-151 lr=['0.0009766'], tr/val_loss:  1.604998/  1.787280, val:  65.00%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.52 seconds, 1.03 minutes\n",
      "total_backward_count 1488080 real_backward_count 150016  10.081%\n",
      "lif layer 2 self.abs_max_v: 4501.0\n",
      "lif layer 2 self.abs_max_v: 4538.5\n",
      "epoch-152 lr=['0.0009766'], tr/val_loss:  1.615463/  1.794136, val:  63.33%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.36 seconds, 1.02 minutes\n",
      "total_backward_count 1497870 real_backward_count 150734  10.063%\n",
      "epoch-153 lr=['0.0009766'], tr/val_loss:  1.610484/  1.772324, val:  73.75%, val_best:  77.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 60.84 seconds, 1.01 minutes\n",
      "total_backward_count 1507660 real_backward_count 151571  10.053%\n",
      "epoch-154 lr=['0.0009766'], tr/val_loss:  1.614028/  1.795525, val:  67.50%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.15 seconds, 1.02 minutes\n",
      "total_backward_count 1517450 real_backward_count 152349  10.040%\n",
      "fc layer 1 self.abs_max_out: 6008.0\n",
      "epoch-155 lr=['0.0009766'], tr/val_loss:  1.617154/  1.811900, val:  59.17%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.46 seconds, 1.02 minutes\n",
      "total_backward_count 1527240 real_backward_count 153109  10.025%\n",
      "epoch-156 lr=['0.0009766'], tr/val_loss:  1.614142/  1.782697, val:  67.08%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 60.89 seconds, 1.01 minutes\n",
      "total_backward_count 1537030 real_backward_count 153868  10.011%\n",
      "epoch-157 lr=['0.0009766'], tr/val_loss:  1.613653/  1.777035, val:  70.00%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.46 seconds, 1.02 minutes\n",
      "total_backward_count 1546820 real_backward_count 154609   9.995%\n",
      "epoch-158 lr=['0.0009766'], tr/val_loss:  1.604319/  1.780506, val:  75.42%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.70 seconds, 1.03 minutes\n",
      "total_backward_count 1556610 real_backward_count 155372   9.981%\n",
      "epoch-159 lr=['0.0009766'], tr/val_loss:  1.595288/  1.776659, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.25 seconds, 1.02 minutes\n",
      "total_backward_count 1566400 real_backward_count 156099   9.965%\n",
      "fc layer 1 self.abs_max_out: 6026.0\n",
      "epoch-160 lr=['0.0009766'], tr/val_loss:  1.584809/  1.752565, val:  75.00%, val_best:  79.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.53 seconds, 1.03 minutes\n",
      "total_backward_count 1576190 real_backward_count 156816   9.949%\n",
      "fc layer 1 self.abs_max_out: 6098.0\n",
      "epoch-161 lr=['0.0009766'], tr/val_loss:  1.609430/  1.777321, val:  72.50%, val_best:  79.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.90 seconds, 1.03 minutes\n",
      "total_backward_count 1585980 real_backward_count 157589   9.936%\n",
      "fc layer 1 self.abs_max_out: 6209.0\n",
      "epoch-162 lr=['0.0009766'], tr/val_loss:  1.609254/  1.795631, val:  68.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.50 seconds, 1.02 minutes\n",
      "total_backward_count 1595770 real_backward_count 158330   9.922%\n",
      "lif layer 1 self.abs_max_v: 10333.0\n",
      "epoch-163 lr=['0.0009766'], tr/val_loss:  1.616668/  1.783987, val:  77.50%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.43 seconds, 1.02 minutes\n",
      "total_backward_count 1605560 real_backward_count 159084   9.908%\n",
      "epoch-164 lr=['0.0009766'], tr/val_loss:  1.597571/  1.772556, val:  73.75%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 60.88 seconds, 1.01 minutes\n",
      "total_backward_count 1615350 real_backward_count 159805   9.893%\n",
      "epoch-165 lr=['0.0009766'], tr/val_loss:  1.604464/  1.777201, val:  62.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.51 seconds, 1.03 minutes\n",
      "total_backward_count 1625140 real_backward_count 160536   9.878%\n",
      "epoch-166 lr=['0.0009766'], tr/val_loss:  1.580760/  1.755205, val:  70.00%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 60.96 seconds, 1.02 minutes\n",
      "total_backward_count 1634930 real_backward_count 161302   9.866%\n",
      "epoch-167 lr=['0.0009766'], tr/val_loss:  1.583830/  1.770249, val:  64.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 60.53 seconds, 1.01 minutes\n",
      "total_backward_count 1644720 real_backward_count 162088   9.855%\n",
      "lif layer 1 self.abs_max_v: 10598.0\n",
      "epoch-168 lr=['0.0009766'], tr/val_loss:  1.586335/  1.772262, val:  62.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.06 seconds, 1.03 minutes\n",
      "total_backward_count 1654510 real_backward_count 162842   9.842%\n",
      "epoch-169 lr=['0.0009766'], tr/val_loss:  1.576999/  1.762418, val:  60.42%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 60.96 seconds, 1.02 minutes\n",
      "total_backward_count 1664300 real_backward_count 163616   9.831%\n",
      "epoch-170 lr=['0.0009766'], tr/val_loss:  1.574093/  1.759430, val:  66.25%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.43 seconds, 1.02 minutes\n",
      "total_backward_count 1674090 real_backward_count 164305   9.815%\n",
      "epoch-171 lr=['0.0009766'], tr/val_loss:  1.584190/  1.771581, val:  67.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.34 seconds, 1.02 minutes\n",
      "total_backward_count 1683880 real_backward_count 165034   9.801%\n",
      "epoch-172 lr=['0.0009766'], tr/val_loss:  1.595593/  1.765476, val:  69.58%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.19 seconds, 1.04 minutes\n",
      "total_backward_count 1693670 real_backward_count 165823   9.791%\n",
      "lif layer 1 self.abs_max_v: 10651.5\n",
      "epoch-173 lr=['0.0009766'], tr/val_loss:  1.583791/  1.751339, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.29 seconds, 1.02 minutes\n",
      "total_backward_count 1703460 real_backward_count 166526   9.776%\n",
      "fc layer 2 self.abs_max_out: 2672.0\n",
      "epoch-174 lr=['0.0009766'], tr/val_loss:  1.569274/  1.755829, val:  71.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.15 seconds, 1.02 minutes\n",
      "total_backward_count 1713250 real_backward_count 167240   9.762%\n",
      "epoch-175 lr=['0.0009766'], tr/val_loss:  1.566423/  1.744713, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.16 seconds, 1.02 minutes\n",
      "total_backward_count 1723040 real_backward_count 167960   9.748%\n",
      "epoch-176 lr=['0.0009766'], tr/val_loss:  1.566845/  1.752305, val:  73.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.33 seconds, 1.02 minutes\n",
      "total_backward_count 1732830 real_backward_count 168716   9.736%\n",
      "epoch-177 lr=['0.0009766'], tr/val_loss:  1.564702/  1.726236, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.47 seconds, 1.04 minutes\n",
      "total_backward_count 1742620 real_backward_count 169406   9.721%\n",
      "epoch-178 lr=['0.0009766'], tr/val_loss:  1.552996/  1.735309, val:  73.33%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 60.99 seconds, 1.02 minutes\n",
      "total_backward_count 1752410 real_backward_count 170093   9.706%\n",
      "epoch-179 lr=['0.0009766'], tr/val_loss:  1.559768/  1.743819, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.05 seconds, 1.02 minutes\n",
      "total_backward_count 1762200 real_backward_count 170796   9.692%\n",
      "fc layer 2 self.abs_max_out: 2675.0\n",
      "epoch-180 lr=['0.0009766'], tr/val_loss:  1.570885/  1.744565, val:  72.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.65 seconds, 1.03 minutes\n",
      "total_backward_count 1771990 real_backward_count 171511   9.679%\n",
      "epoch-181 lr=['0.0009766'], tr/val_loss:  1.567377/  1.743490, val:  72.50%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.31 seconds, 1.02 minutes\n",
      "total_backward_count 1781780 real_backward_count 172159   9.662%\n",
      "epoch-182 lr=['0.0009766'], tr/val_loss:  1.576006/  1.750273, val:  76.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.08 seconds, 1.02 minutes\n",
      "total_backward_count 1791570 real_backward_count 172855   9.648%\n",
      "epoch-183 lr=['0.0009766'], tr/val_loss:  1.587403/  1.756971, val:  63.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 60.77 seconds, 1.01 minutes\n",
      "total_backward_count 1801360 real_backward_count 173522   9.633%\n",
      "fc layer 2 self.abs_max_out: 2676.0\n",
      "epoch-184 lr=['0.0009766'], tr/val_loss:  1.583538/  1.775907, val:  66.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.73 seconds, 1.03 minutes\n",
      "total_backward_count 1811150 real_backward_count 174192   9.618%\n",
      "epoch-185 lr=['0.0009766'], tr/val_loss:  1.580902/  1.761842, val:  72.92%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.34 seconds, 1.02 minutes\n",
      "total_backward_count 1820940 real_backward_count 174909   9.605%\n",
      "epoch-186 lr=['0.0009766'], tr/val_loss:  1.577302/  1.762204, val:  69.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.21 seconds, 1.02 minutes\n",
      "total_backward_count 1830730 real_backward_count 175651   9.595%\n",
      "epoch-187 lr=['0.0009766'], tr/val_loss:  1.561819/  1.750379, val:  72.50%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.89 seconds, 1.03 minutes\n",
      "total_backward_count 1840520 real_backward_count 176406   9.585%\n",
      "epoch-188 lr=['0.0009766'], tr/val_loss:  1.563007/  1.749757, val:  72.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.71 seconds, 1.03 minutes\n",
      "total_backward_count 1850310 real_backward_count 177091   9.571%\n",
      "epoch-189 lr=['0.0009766'], tr/val_loss:  1.564177/  1.743404, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.22 seconds, 1.02 minutes\n",
      "total_backward_count 1860100 real_backward_count 177774   9.557%\n",
      "epoch-190 lr=['0.0009766'], tr/val_loss:  1.562005/  1.728131, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 60.98 seconds, 1.02 minutes\n",
      "total_backward_count 1869890 real_backward_count 178436   9.543%\n",
      "epoch-191 lr=['0.0009766'], tr/val_loss:  1.569879/  1.764774, val:  60.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.50 seconds, 1.03 minutes\n",
      "total_backward_count 1879680 real_backward_count 179108   9.529%\n",
      "epoch-192 lr=['0.0009766'], tr/val_loss:  1.570950/  1.758745, val:  73.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.58 seconds, 1.03 minutes\n",
      "total_backward_count 1889470 real_backward_count 179761   9.514%\n",
      "epoch-193 lr=['0.0009766'], tr/val_loss:  1.571645/  1.759481, val:  74.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.68 seconds, 1.03 minutes\n",
      "total_backward_count 1899260 real_backward_count 180427   9.500%\n",
      "epoch-194 lr=['0.0009766'], tr/val_loss:  1.570355/  1.746037, val:  71.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.57 seconds, 1.03 minutes\n",
      "total_backward_count 1909050 real_backward_count 181080   9.485%\n",
      "epoch-195 lr=['0.0009766'], tr/val_loss:  1.551958/  1.737661, val:  73.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.63 seconds, 1.03 minutes\n",
      "total_backward_count 1918840 real_backward_count 181733   9.471%\n",
      "epoch-196 lr=['0.0009766'], tr/val_loss:  1.555362/  1.741011, val:  71.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.68 seconds, 1.03 minutes\n",
      "total_backward_count 1928630 real_backward_count 182396   9.457%\n",
      "epoch-197 lr=['0.0009766'], tr/val_loss:  1.557583/  1.736090, val:  69.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.81 seconds, 1.03 minutes\n",
      "total_backward_count 1938420 real_backward_count 183076   9.445%\n",
      "epoch-198 lr=['0.0009766'], tr/val_loss:  1.550347/  1.754591, val:  65.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.20 seconds, 1.02 minutes\n",
      "total_backward_count 1948210 real_backward_count 183734   9.431%\n",
      "epoch-199 lr=['0.0009766'], tr/val_loss:  1.550470/  1.753089, val:  65.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 60.98 seconds, 1.02 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0addba4fb9469f86880f2d547d079e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñá‚ñÜ‚ñá‚ñÖ‚ñà‚ñà‚ñÖ‚ñÜ‚ñá‚ñÖ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñá‚ñÜ‚ñá‚ñÖ‚ñà‚ñà‚ñÖ‚ñÜ‚ñá‚ñÖ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.55047</td></tr><tr><td>val_acc_best</td><td>0.81667</td></tr><tr><td>val_acc_now</td><td>0.65417</td></tr><tr><td>val_loss</td><td>1.75309</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">deft-sweep-105</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/eyqhf604' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/eyqhf604</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251111_030555-eyqhf604/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: g0jqsrsw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251111_063519-g0jqsrsw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/g0jqsrsw' target=\"_blank\">generous-sweep-109</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hpjdvxst' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hpjdvxst</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hpjdvxst' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hpjdvxst</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/g0jqsrsw' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/g0jqsrsw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251111_063527_265', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 427.0\n",
      "lif layer 1 self.abs_max_v: 427.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 562.0\n",
      "lif layer 1 self.abs_max_v: 591.0\n",
      "fc layer 2 self.abs_max_out: 72.0\n",
      "lif layer 2 self.abs_max_v: 72.0\n",
      "fc layer 1 self.abs_max_out: 601.0\n",
      "lif layer 1 self.abs_max_v: 764.5\n",
      "fc layer 2 self.abs_max_out: 178.0\n",
      "lif layer 2 self.abs_max_v: 184.0\n",
      "lif layer 1 self.abs_max_v: 785.5\n",
      "fc layer 2 self.abs_max_out: 278.0\n",
      "lif layer 2 self.abs_max_v: 295.5\n",
      "fc layer 1 self.abs_max_out: 719.0\n",
      "lif layer 1 self.abs_max_v: 879.5\n",
      "fc layer 2 self.abs_max_out: 363.0\n",
      "lif layer 2 self.abs_max_v: 409.5\n",
      "fc layer 1 self.abs_max_out: 897.0\n",
      "lif layer 1 self.abs_max_v: 897.0\n",
      "fc layer 2 self.abs_max_out: 433.0\n",
      "lif layer 2 self.abs_max_v: 550.5\n",
      "fc layer 3 self.abs_max_out: 35.0\n",
      "fc layer 1 self.abs_max_out: 941.0\n",
      "lif layer 1 self.abs_max_v: 949.5\n",
      "lif layer 2 self.abs_max_v: 570.5\n",
      "fc layer 1 self.abs_max_out: 1126.0\n",
      "lif layer 1 self.abs_max_v: 1126.0\n",
      "fc layer 2 self.abs_max_out: 466.0\n",
      "lif layer 2 self.abs_max_v: 659.5\n",
      "fc layer 3 self.abs_max_out: 57.0\n",
      "fc layer 2 self.abs_max_out: 472.0\n",
      "fc layer 2 self.abs_max_out: 497.0\n",
      "lif layer 1 self.abs_max_v: 1210.5\n",
      "fc layer 2 self.abs_max_out: 572.0\n",
      "lif layer 2 self.abs_max_v: 770.0\n",
      "fc layer 3 self.abs_max_out: 130.0\n",
      "lif layer 2 self.abs_max_v: 782.0\n",
      "fc layer 1 self.abs_max_out: 1172.0\n",
      "lif layer 2 self.abs_max_v: 811.0\n",
      "fc layer 1 self.abs_max_out: 1780.0\n",
      "lif layer 1 self.abs_max_v: 1780.0\n",
      "fc layer 2 self.abs_max_out: 583.0\n",
      "lif layer 2 self.abs_max_v: 971.5\n",
      "fc layer 3 self.abs_max_out: 146.0\n",
      "fc layer 1 self.abs_max_out: 2130.0\n",
      "lif layer 1 self.abs_max_v: 2130.0\n",
      "fc layer 2 self.abs_max_out: 769.0\n",
      "lif layer 2 self.abs_max_v: 998.5\n",
      "lif layer 2 self.abs_max_v: 1151.5\n",
      "lif layer 2 self.abs_max_v: 1209.5\n",
      "fc layer 3 self.abs_max_out: 279.0\n",
      "fc layer 1 self.abs_max_out: 2368.0\n",
      "lif layer 1 self.abs_max_v: 2368.0\n",
      "lif layer 2 self.abs_max_v: 1233.5\n",
      "fc layer 2 self.abs_max_out: 787.0\n",
      "lif layer 2 self.abs_max_v: 1262.0\n",
      "lif layer 2 self.abs_max_v: 1342.5\n",
      "fc layer 2 self.abs_max_out: 816.0\n",
      "fc layer 2 self.abs_max_out: 848.0\n",
      "lif layer 2 self.abs_max_v: 1388.5\n",
      "fc layer 2 self.abs_max_out: 1010.0\n",
      "fc layer 2 self.abs_max_out: 1108.0\n",
      "lif layer 2 self.abs_max_v: 1411.5\n",
      "fc layer 2 self.abs_max_out: 1134.0\n",
      "lif layer 2 self.abs_max_v: 1537.0\n",
      "fc layer 2 self.abs_max_out: 1179.0\n",
      "fc layer 2 self.abs_max_out: 1291.0\n",
      "fc layer 2 self.abs_max_out: 1342.0\n",
      "lif layer 2 self.abs_max_v: 1597.0\n",
      "fc layer 1 self.abs_max_out: 2395.0\n",
      "lif layer 1 self.abs_max_v: 2395.0\n",
      "fc layer 1 self.abs_max_out: 2657.0\n",
      "lif layer 1 self.abs_max_v: 2657.0\n",
      "lif layer 2 self.abs_max_v: 1656.0\n",
      "lif layer 2 self.abs_max_v: 1681.5\n",
      "lif layer 2 self.abs_max_v: 1733.5\n",
      "fc layer 2 self.abs_max_out: 1361.0\n",
      "fc layer 2 self.abs_max_out: 1626.0\n",
      "fc layer 1 self.abs_max_out: 2880.0\n",
      "lif layer 1 self.abs_max_v: 2880.0\n",
      "lif layer 2 self.abs_max_v: 2032.0\n",
      "lif layer 2 self.abs_max_v: 2085.0\n",
      "lif layer 2 self.abs_max_v: 2092.5\n",
      "fc layer 1 self.abs_max_out: 2916.0\n",
      "lif layer 1 self.abs_max_v: 2916.0\n",
      "lif layer 2 self.abs_max_v: 2160.5\n",
      "lif layer 2 self.abs_max_v: 2224.5\n",
      "fc layer 3 self.abs_max_out: 356.0\n",
      "fc layer 2 self.abs_max_out: 1756.0\n",
      "fc layer 2 self.abs_max_out: 1880.0\n",
      "fc layer 3 self.abs_max_out: 362.0\n",
      "lif layer 2 self.abs_max_v: 2271.5\n",
      "fc layer 2 self.abs_max_out: 1962.0\n",
      "lif layer 2 self.abs_max_v: 2308.0\n",
      "fc layer 1 self.abs_max_out: 3241.0\n",
      "lif layer 1 self.abs_max_v: 3241.0\n",
      "fc layer 2 self.abs_max_out: 2046.0\n",
      "lif layer 2 self.abs_max_v: 2325.0\n",
      "fc layer 3 self.abs_max_out: 402.0\n",
      "lif layer 1 self.abs_max_v: 3593.0\n",
      "lif layer 2 self.abs_max_v: 2343.0\n",
      "lif layer 2 self.abs_max_v: 2446.5\n",
      "lif layer 2 self.abs_max_v: 2508.5\n",
      "fc layer 2 self.abs_max_out: 2077.0\n",
      "lif layer 1 self.abs_max_v: 3948.0\n",
      "fc layer 2 self.abs_max_out: 2093.0\n",
      "fc layer 2 self.abs_max_out: 2320.0\n",
      "fc layer 2 self.abs_max_out: 2420.0\n",
      "fc layer 1 self.abs_max_out: 3303.0\n",
      "lif layer 1 self.abs_max_v: 4460.0\n",
      "lif layer 1 self.abs_max_v: 4817.0\n",
      "fc layer 1 self.abs_max_out: 3443.0\n",
      "fc layer 3 self.abs_max_out: 412.0\n",
      "fc layer 3 self.abs_max_out: 427.0\n",
      "fc layer 3 self.abs_max_out: 431.0\n",
      "lif layer 2 self.abs_max_v: 2516.5\n",
      "fc layer 1 self.abs_max_out: 3564.0\n",
      "fc layer 2 self.abs_max_out: 2486.0\n",
      "fc layer 2 self.abs_max_out: 2498.0\n",
      "lif layer 1 self.abs_max_v: 4892.5\n",
      "fc layer 3 self.abs_max_out: 453.0\n",
      "fc layer 3 self.abs_max_out: 465.0\n",
      "fc layer 1 self.abs_max_out: 3648.0\n",
      "lif layer 1 self.abs_max_v: 5007.5\n",
      "lif layer 1 self.abs_max_v: 5139.5\n",
      "lif layer 2 self.abs_max_v: 2633.5\n",
      "fc layer 1 self.abs_max_out: 3970.0\n",
      "lif layer 1 self.abs_max_v: 5362.0\n",
      "fc layer 1 self.abs_max_out: 4001.0\n",
      "fc layer 1 self.abs_max_out: 4348.0\n",
      "fc layer 2 self.abs_max_out: 2598.0\n",
      "fc layer 2 self.abs_max_out: 2616.0\n",
      "fc layer 2 self.abs_max_out: 2620.0\n",
      "fc layer 1 self.abs_max_out: 4664.0\n",
      "lif layer 1 self.abs_max_v: 5632.0\n",
      "fc layer 3 self.abs_max_out: 470.0\n",
      "lif layer 2 self.abs_max_v: 2649.0\n",
      "lif layer 2 self.abs_max_v: 2657.0\n",
      "lif layer 2 self.abs_max_v: 2674.5\n",
      "lif layer 2 self.abs_max_v: 2845.5\n",
      "lif layer 2 self.abs_max_v: 2879.0\n",
      "fc layer 2 self.abs_max_out: 2705.0\n",
      "lif layer 2 self.abs_max_v: 2998.0\n",
      "fc layer 1 self.abs_max_out: 4690.0\n",
      "fc layer 1 self.abs_max_out: 4786.0\n",
      "lif layer 1 self.abs_max_v: 5799.5\n",
      "lif layer 1 self.abs_max_v: 6118.0\n",
      "lif layer 1 self.abs_max_v: 6277.0\n",
      "fc layer 1 self.abs_max_out: 4853.0\n",
      "fc layer 1 self.abs_max_out: 4939.0\n",
      "fc layer 1 self.abs_max_out: 5101.0\n",
      "lif layer 1 self.abs_max_v: 6706.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  2.022359/  2.091696, val:  31.67%, val_best:  31.67%, tr:  86.21%, tr_best:  86.21%, epoch time: 62.14 seconds, 1.04 minutes\n",
      "total_backward_count 9790 real_backward_count 3506  35.812%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 3151.0\n",
      "lif layer 1 self.abs_max_v: 7403.5\n",
      "fc layer 1 self.abs_max_out: 5114.0\n",
      "lif layer 2 self.abs_max_v: 3425.5\n",
      "fc layer 1 self.abs_max_out: 5161.0\n",
      "fc layer 1 self.abs_max_out: 5308.0\n",
      "fc layer 1 self.abs_max_out: 5766.0\n",
      "fc layer 3 self.abs_max_out: 479.0\n",
      "fc layer 2 self.abs_max_out: 2802.0\n",
      "lif layer 1 self.abs_max_v: 7763.5\n",
      "fc layer 2 self.abs_max_out: 2971.0\n",
      "fc layer 1 self.abs_max_out: 5818.0\n",
      "lif layer 1 self.abs_max_v: 7886.5\n",
      "fc layer 2 self.abs_max_out: 3074.0\n",
      "fc layer 2 self.abs_max_out: 3129.0\n",
      "lif layer 1 self.abs_max_v: 8070.0\n",
      "lif layer 1 self.abs_max_v: 8852.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.985855/  2.094426, val:  37.50%, val_best:  37.50%, tr:  97.14%, tr_best:  97.14%, epoch time: 61.50 seconds, 1.02 minutes\n",
      "total_backward_count 19580 real_backward_count 5738  29.305%\n",
      "lif layer 2 self.abs_max_v: 3448.5\n",
      "lif layer 2 self.abs_max_v: 3577.5\n",
      "fc layer 2 self.abs_max_out: 3184.0\n",
      "fc layer 3 self.abs_max_out: 490.0\n",
      "lif layer 2 self.abs_max_v: 3642.5\n",
      "lif layer 2 self.abs_max_v: 3720.5\n",
      "lif layer 2 self.abs_max_v: 3730.5\n",
      "fc layer 1 self.abs_max_out: 6272.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.981413/  2.085394, val:  39.17%, val_best:  39.17%, tr:  98.88%, tr_best:  98.88%, epoch time: 62.19 seconds, 1.04 minutes\n",
      "total_backward_count 29370 real_backward_count 7705  26.234%\n",
      "lif layer 2 self.abs_max_v: 3764.5\n",
      "lif layer 2 self.abs_max_v: 3904.0\n",
      "lif layer 2 self.abs_max_v: 3915.0\n",
      "lif layer 1 self.abs_max_v: 9005.0\n",
      "lif layer 2 self.abs_max_v: 3919.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.986632/  2.091526, val:  39.58%, val_best:  39.58%, tr:  98.77%, tr_best:  98.88%, epoch time: 61.72 seconds, 1.03 minutes\n",
      "total_backward_count 39160 real_backward_count 9560  24.413%\n",
      "lif layer 2 self.abs_max_v: 4022.5\n",
      "fc layer 1 self.abs_max_out: 6320.0\n",
      "fc layer 1 self.abs_max_out: 6347.0\n",
      "lif layer 1 self.abs_max_v: 9773.0\n",
      "lif layer 1 self.abs_max_v: 9838.0\n",
      "lif layer 1 self.abs_max_v: 9920.5\n",
      "lif layer 1 self.abs_max_v: 10557.5\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  2.000618/  2.108247, val:  42.92%, val_best:  42.92%, tr:  99.69%, tr_best:  99.69%, epoch time: 62.05 seconds, 1.03 minutes\n",
      "total_backward_count 48950 real_backward_count 11253  22.989%\n",
      "fc layer 1 self.abs_max_out: 6760.0\n",
      "fc layer 2 self.abs_max_out: 3345.0\n",
      "lif layer 2 self.abs_max_v: 4093.5\n",
      "lif layer 2 self.abs_max_v: 4103.0\n",
      "lif layer 2 self.abs_max_v: 4180.0\n",
      "fc layer 2 self.abs_max_out: 3354.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  2.003634/  2.089125, val:  53.75%, val_best:  53.75%, tr:  99.49%, tr_best:  99.69%, epoch time: 61.74 seconds, 1.03 minutes\n",
      "total_backward_count 58740 real_backward_count 12945  22.038%\n",
      "lif layer 2 self.abs_max_v: 4269.0\n",
      "lif layer 2 self.abs_max_v: 4500.5\n",
      "lif layer 2 self.abs_max_v: 4524.5\n",
      "fc layer 2 self.abs_max_out: 3446.0\n",
      "fc layer 2 self.abs_max_out: 3500.0\n",
      "fc layer 1 self.abs_max_out: 7167.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.994958/  2.096136, val:  50.42%, val_best:  53.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 61.51 seconds, 1.03 minutes\n",
      "total_backward_count 68530 real_backward_count 14606  21.313%\n",
      "fc layer 3 self.abs_max_out: 500.0\n",
      "fc layer 2 self.abs_max_out: 3596.0\n",
      "fc layer 1 self.abs_max_out: 7370.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.978294/  2.070955, val:  50.83%, val_best:  53.75%, tr:  99.28%, tr_best:  99.80%, epoch time: 61.49 seconds, 1.02 minutes\n",
      "total_backward_count 78320 real_backward_count 16171  20.647%\n",
      "lif layer 1 self.abs_max_v: 10732.5\n",
      "lif layer 1 self.abs_max_v: 10747.5\n",
      "lif layer 1 self.abs_max_v: 11354.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.979025/  2.079860, val:  52.08%, val_best:  53.75%, tr:  99.39%, tr_best:  99.80%, epoch time: 61.88 seconds, 1.03 minutes\n",
      "total_backward_count 88110 real_backward_count 17760  20.157%\n",
      "fc layer 1 self.abs_max_out: 7434.0\n",
      "fc layer 3 self.abs_max_out: 507.0\n",
      "fc layer 2 self.abs_max_out: 3761.0\n",
      "lif layer 1 self.abs_max_v: 11525.5\n",
      "fc layer 2 self.abs_max_out: 3827.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.974191/  2.099949, val:  41.25%, val_best:  53.75%, tr:  99.28%, tr_best:  99.80%, epoch time: 61.31 seconds, 1.02 minutes\n",
      "total_backward_count 97900 real_backward_count 19265  19.678%\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.972591/  2.092363, val:  45.42%, val_best:  53.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 61.82 seconds, 1.03 minutes\n",
      "total_backward_count 107690 real_backward_count 20733  19.252%\n",
      "fc layer 3 self.abs_max_out: 513.0\n",
      "fc layer 1 self.abs_max_out: 7799.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.974343/  2.073072, val:  44.58%, val_best:  53.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 62.09 seconds, 1.03 minutes\n",
      "total_backward_count 117480 real_backward_count 22184  18.883%\n",
      "fc layer 2 self.abs_max_out: 3941.0\n",
      "fc layer 2 self.abs_max_out: 4016.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.960773/  2.071825, val:  48.33%, val_best:  53.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 62.08 seconds, 1.03 minutes\n",
      "total_backward_count 127270 real_backward_count 23670  18.598%\n",
      "fc layer 3 self.abs_max_out: 519.0\n",
      "fc layer 3 self.abs_max_out: 544.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.979144/  2.098528, val:  41.25%, val_best:  53.75%, tr:  99.28%, tr_best:  99.80%, epoch time: 62.75 seconds, 1.05 minutes\n",
      "total_backward_count 137060 real_backward_count 25136  18.339%\n",
      "fc layer 1 self.abs_max_out: 7811.0\n",
      "lif layer 2 self.abs_max_v: 4604.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.975382/  2.064357, val:  49.58%, val_best:  53.75%, tr:  99.28%, tr_best:  99.80%, epoch time: 62.44 seconds, 1.04 minutes\n",
      "total_backward_count 146850 real_backward_count 26554  18.082%\n",
      "lif layer 1 self.abs_max_v: 11752.5\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.977588/  2.078305, val:  44.58%, val_best:  53.75%, tr:  99.69%, tr_best:  99.80%, epoch time: 62.28 seconds, 1.04 minutes\n",
      "total_backward_count 156640 real_backward_count 28065  17.917%\n",
      "fc layer 3 self.abs_max_out: 546.0\n",
      "fc layer 1 self.abs_max_out: 8108.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.977007/  2.077284, val:  49.58%, val_best:  53.75%, tr:  99.90%, tr_best:  99.90%, epoch time: 62.37 seconds, 1.04 minutes\n",
      "total_backward_count 166430 real_backward_count 29469  17.707%\n",
      "fc layer 1 self.abs_max_out: 8148.0\n",
      "lif layer 2 self.abs_max_v: 4743.0\n",
      "lif layer 2 self.abs_max_v: 4859.5\n",
      "lif layer 2 self.abs_max_v: 4937.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.988155/  2.081768, val:  58.75%, val_best:  58.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 62.02 seconds, 1.03 minutes\n",
      "total_backward_count 176220 real_backward_count 30905  17.538%\n",
      "lif layer 1 self.abs_max_v: 12411.5\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.983398/  2.076226, val:  51.67%, val_best:  58.75%, tr:  99.18%, tr_best:  99.90%, epoch time: 62.72 seconds, 1.05 minutes\n",
      "total_backward_count 186010 real_backward_count 32378  17.407%\n",
      "lif layer 1 self.abs_max_v: 12626.5\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.969580/  2.085462, val:  45.00%, val_best:  58.75%, tr:  99.28%, tr_best:  99.90%, epoch time: 62.34 seconds, 1.04 minutes\n",
      "total_backward_count 195800 real_backward_count 33747  17.235%\n",
      "fc layer 3 self.abs_max_out: 564.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.979485/  2.085757, val:  48.33%, val_best:  58.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 62.95 seconds, 1.05 minutes\n",
      "total_backward_count 205590 real_backward_count 35117  17.081%\n",
      "fc layer 1 self.abs_max_out: 8478.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.966861/  2.062681, val:  56.67%, val_best:  58.75%, tr:  99.80%, tr_best:  99.90%, epoch time: 61.94 seconds, 1.03 minutes\n",
      "total_backward_count 215380 real_backward_count 36531  16.961%\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.964989/  2.057497, val:  56.67%, val_best:  58.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 62.59 seconds, 1.04 minutes\n",
      "total_backward_count 225170 real_backward_count 37903  16.833%\n",
      "lif layer 1 self.abs_max_v: 12655.5\n",
      "fc layer 1 self.abs_max_out: 8547.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.960847/  2.067602, val:  60.83%, val_best:  60.83%, tr:  99.69%, tr_best:  99.90%, epoch time: 61.87 seconds, 1.03 minutes\n",
      "total_backward_count 234960 real_backward_count 39281  16.718%\n",
      "lif layer 2 self.abs_max_v: 5037.0\n",
      "lif layer 2 self.abs_max_v: 5241.5\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.972881/  2.050685, val:  53.75%, val_best:  60.83%, tr:  99.49%, tr_best:  99.90%, epoch time: 62.03 seconds, 1.03 minutes\n",
      "total_backward_count 244750 real_backward_count 40633  16.602%\n",
      "lif layer 1 self.abs_max_v: 13053.0\n",
      "lif layer 1 self.abs_max_v: 13163.5\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.964503/  2.061740, val:  64.17%, val_best:  64.17%, tr:  99.59%, tr_best:  99.90%, epoch time: 62.56 seconds, 1.04 minutes\n",
      "total_backward_count 254540 real_backward_count 42044  16.518%\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.966866/  2.073807, val:  51.67%, val_best:  64.17%, tr:  99.39%, tr_best:  99.90%, epoch time: 62.28 seconds, 1.04 minutes\n",
      "total_backward_count 264330 real_backward_count 43455  16.440%\n",
      "fc layer 3 self.abs_max_out: 568.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.966658/  2.049226, val:  66.25%, val_best:  66.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 62.51 seconds, 1.04 minutes\n",
      "total_backward_count 274120 real_backward_count 44856  16.364%\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.958058/  2.064486, val:  58.33%, val_best:  66.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 62.73 seconds, 1.05 minutes\n",
      "total_backward_count 283910 real_backward_count 46156  16.257%\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.964242/  2.067489, val:  51.25%, val_best:  66.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 61.99 seconds, 1.03 minutes\n",
      "total_backward_count 293700 real_backward_count 47496  16.172%\n",
      "fc layer 3 self.abs_max_out: 574.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.964427/  2.041948, val:  61.25%, val_best:  66.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 61.96 seconds, 1.03 minutes\n",
      "total_backward_count 303490 real_backward_count 48851  16.096%\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.954725/  2.056135, val:  52.92%, val_best:  66.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 61.97 seconds, 1.03 minutes\n",
      "total_backward_count 313280 real_backward_count 50174  16.016%\n",
      "lif layer 2 self.abs_max_v: 5287.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.956496/  2.051834, val:  48.75%, val_best:  66.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 61.41 seconds, 1.02 minutes\n",
      "total_backward_count 323070 real_backward_count 51447  15.924%\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.946563/  2.035359, val:  64.17%, val_best:  66.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 62.36 seconds, 1.04 minutes\n",
      "total_backward_count 332860 real_backward_count 52813  15.866%\n",
      "fc layer 1 self.abs_max_out: 8751.0\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.944502/  2.062806, val:  48.75%, val_best:  66.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 62.05 seconds, 1.03 minutes\n",
      "total_backward_count 342650 real_backward_count 54131  15.798%\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.949203/  2.061507, val:  48.33%, val_best:  66.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 62.19 seconds, 1.04 minutes\n",
      "total_backward_count 352440 real_backward_count 55408  15.721%\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.958420/  2.037656, val:  64.58%, val_best:  66.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 61.51 seconds, 1.03 minutes\n",
      "total_backward_count 362230 real_backward_count 56706  15.655%\n",
      "lif layer 1 self.abs_max_v: 13684.0\n",
      "lif layer 2 self.abs_max_v: 5315.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.950644/  2.062703, val:  55.83%, val_best:  66.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 61.70 seconds, 1.03 minutes\n",
      "total_backward_count 372020 real_backward_count 57938  15.574%\n",
      "fc layer 1 self.abs_max_out: 8906.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.962858/  2.050871, val:  60.83%, val_best:  66.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 62.11 seconds, 1.04 minutes\n",
      "total_backward_count 381810 real_backward_count 59248  15.518%\n",
      "lif layer 2 self.abs_max_v: 5456.0\n",
      "lif layer 2 self.abs_max_v: 5516.5\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.947888/  2.058055, val:  51.67%, val_best:  66.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 62.37 seconds, 1.04 minutes\n",
      "total_backward_count 391600 real_backward_count 60503  15.450%\n",
      "lif layer 2 self.abs_max_v: 5621.5\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.952609/  2.044997, val:  61.67%, val_best:  66.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 61.75 seconds, 1.03 minutes\n",
      "total_backward_count 401390 real_backward_count 61772  15.390%\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.944644/  2.032580, val:  58.33%, val_best:  66.25%, tr:  99.39%, tr_best:  99.90%, epoch time: 62.40 seconds, 1.04 minutes\n",
      "total_backward_count 411180 real_backward_count 63037  15.331%\n",
      "fc layer 1 self.abs_max_out: 9032.0\n",
      "fc layer 1 self.abs_max_out: 9044.0\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.937798/  2.039500, val:  55.00%, val_best:  66.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 61.48 seconds, 1.02 minutes\n",
      "total_backward_count 420970 real_backward_count 64265  15.266%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.955202/  2.043339, val:  57.92%, val_best:  66.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 61.90 seconds, 1.03 minutes\n",
      "total_backward_count 430760 real_backward_count 65549  15.217%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.950002/  2.049369, val:  57.08%, val_best:  66.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 62.03 seconds, 1.03 minutes\n",
      "total_backward_count 440550 real_backward_count 66858  15.176%\n",
      "lif layer 1 self.abs_max_v: 13708.5\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.944257/  2.038060, val:  55.83%, val_best:  66.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 61.97 seconds, 1.03 minutes\n",
      "total_backward_count 450340 real_backward_count 68137  15.130%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.936588/  2.046840, val:  58.33%, val_best:  66.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 61.82 seconds, 1.03 minutes\n",
      "total_backward_count 460130 real_backward_count 69413  15.086%\n",
      "fc layer 1 self.abs_max_out: 9072.0\n",
      "lif layer 2 self.abs_max_v: 5734.0\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.938659/  2.051368, val:  46.67%, val_best:  66.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 62.59 seconds, 1.04 minutes\n",
      "total_backward_count 469920 real_backward_count 70677  15.040%\n",
      "lif layer 2 self.abs_max_v: 5893.0\n",
      "fc layer 1 self.abs_max_out: 9600.0\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.942144/  2.052857, val:  57.50%, val_best:  66.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 62.64 seconds, 1.04 minutes\n",
      "total_backward_count 479710 real_backward_count 71894  14.987%\n",
      "lif layer 1 self.abs_max_v: 13813.5\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.945164/  2.046663, val:  59.17%, val_best:  66.25%, tr:  99.39%, tr_best:  99.90%, epoch time: 61.91 seconds, 1.03 minutes\n",
      "total_backward_count 489500 real_backward_count 73191  14.952%\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.940644/  2.032496, val:  57.92%, val_best:  66.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 61.58 seconds, 1.03 minutes\n",
      "total_backward_count 499290 real_backward_count 74453  14.912%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.938017/  2.046520, val:  57.92%, val_best:  66.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 61.87 seconds, 1.03 minutes\n",
      "total_backward_count 509080 real_backward_count 75711  14.872%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.948322/  2.036666, val:  64.17%, val_best:  66.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 61.23 seconds, 1.02 minutes\n",
      "total_backward_count 518870 real_backward_count 77026  14.845%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.943192/  2.053214, val:  60.83%, val_best:  66.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 62.49 seconds, 1.04 minutes\n",
      "total_backward_count 528660 real_backward_count 78301  14.811%\n",
      "lif layer 1 self.abs_max_v: 14561.0\n",
      "fc layer 3 self.abs_max_out: 588.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.949187/  2.049447, val:  63.33%, val_best:  66.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 62.89 seconds, 1.05 minutes\n",
      "total_backward_count 538450 real_backward_count 79552  14.774%\n",
      "lif layer 1 self.abs_max_v: 14883.5\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.951109/  2.063675, val:  52.08%, val_best:  66.25%, tr:  99.39%, tr_best:  99.90%, epoch time: 62.55 seconds, 1.04 minutes\n",
      "total_backward_count 548240 real_backward_count 80820  14.742%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.948995/  2.049466, val:  50.83%, val_best:  66.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 62.40 seconds, 1.04 minutes\n",
      "total_backward_count 558030 real_backward_count 82065  14.706%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.941281/  2.041407, val:  60.00%, val_best:  66.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 61.75 seconds, 1.03 minutes\n",
      "total_backward_count 567820 real_backward_count 83300  14.670%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.936556/  2.031125, val:  61.25%, val_best:  66.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 62.42 seconds, 1.04 minutes\n",
      "total_backward_count 577610 real_backward_count 84513  14.631%\n",
      "lif layer 1 self.abs_max_v: 15598.5\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.941313/  2.039617, val:  62.50%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.24 seconds, 1.04 minutes\n",
      "total_backward_count 587400 real_backward_count 85749  14.598%\n",
      "fc layer 2 self.abs_max_out: 4213.0\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.931307/  2.037410, val:  52.50%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.43 seconds, 1.02 minutes\n",
      "total_backward_count 597190 real_backward_count 87009  14.570%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.939533/  2.029057, val:  68.33%, val_best:  68.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 62.75 seconds, 1.05 minutes\n",
      "total_backward_count 606980 real_backward_count 88282  14.544%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.951254/  2.062253, val:  56.67%, val_best:  68.33%, tr:  99.49%, tr_best: 100.00%, epoch time: 62.29 seconds, 1.04 minutes\n",
      "total_backward_count 616770 real_backward_count 89512  14.513%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.940973/  2.046569, val:  55.00%, val_best:  68.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 62.56 seconds, 1.04 minutes\n",
      "total_backward_count 626560 real_backward_count 90702  14.476%\n",
      "fc layer 1 self.abs_max_out: 9727.0\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.929582/  2.015159, val:  60.83%, val_best:  68.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 62.32 seconds, 1.04 minutes\n",
      "total_backward_count 636350 real_backward_count 91905  14.443%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.920157/  2.027707, val:  62.92%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.71 seconds, 1.03 minutes\n",
      "total_backward_count 646140 real_backward_count 93125  14.413%\n",
      "lif layer 1 self.abs_max_v: 16174.5\n",
      "fc layer 1 self.abs_max_out: 10041.0\n",
      "lif layer 1 self.abs_max_v: 16776.5\n",
      "lif layer 1 self.abs_max_v: 17272.5\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.927210/  2.038648, val:  57.08%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 62.13 seconds, 1.04 minutes\n",
      "total_backward_count 655930 real_backward_count 94388  14.390%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.930520/  2.025285, val:  62.08%, val_best:  68.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 62.46 seconds, 1.04 minutes\n",
      "total_backward_count 665720 real_backward_count 95611  14.362%\n",
      "lif layer 2 self.abs_max_v: 5983.5\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.924222/  2.026110, val:  61.67%, val_best:  68.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 61.99 seconds, 1.03 minutes\n",
      "total_backward_count 675510 real_backward_count 96845  14.337%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.923652/  2.038515, val:  55.83%, val_best:  68.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.83 seconds, 1.03 minutes\n",
      "total_backward_count 685300 real_backward_count 98011  14.302%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.932650/  2.024678, val:  70.42%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.63 seconds, 1.04 minutes\n",
      "total_backward_count 695090 real_backward_count 99222  14.275%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.930173/  2.045695, val:  55.42%, val_best:  70.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 62.03 seconds, 1.03 minutes\n",
      "total_backward_count 704880 real_backward_count 100424  14.247%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.945283/  2.038675, val:  63.75%, val_best:  70.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 61.52 seconds, 1.03 minutes\n",
      "total_backward_count 714670 real_backward_count 101579  14.213%\n",
      "lif layer 2 self.abs_max_v: 6075.5\n",
      "lif layer 2 self.abs_max_v: 6166.0\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.931490/  2.026261, val:  60.42%, val_best:  70.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 62.46 seconds, 1.04 minutes\n",
      "total_backward_count 724460 real_backward_count 102691  14.175%\n",
      "lif layer 1 self.abs_max_v: 17664.0\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.930427/  2.027318, val:  62.08%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.37 seconds, 1.04 minutes\n",
      "total_backward_count 734250 real_backward_count 103915  14.153%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.926757/  2.023371, val:  63.33%, val_best:  70.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 62.75 seconds, 1.05 minutes\n",
      "total_backward_count 744040 real_backward_count 105014  14.114%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.917088/  2.025505, val:  53.33%, val_best:  70.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 62.60 seconds, 1.04 minutes\n",
      "total_backward_count 753830 real_backward_count 106275  14.098%\n",
      "fc layer 1 self.abs_max_out: 10298.0\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.920261/  2.024930, val:  51.25%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.97 seconds, 1.03 minutes\n",
      "total_backward_count 763620 real_backward_count 107467  14.073%\n",
      "fc layer 3 self.abs_max_out: 589.0\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.920096/  2.006540, val:  57.08%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.51 seconds, 1.04 minutes\n",
      "total_backward_count 773410 real_backward_count 108684  14.053%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.916949/  2.011648, val:  71.67%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.71 seconds, 1.05 minutes\n",
      "total_backward_count 783200 real_backward_count 109816  14.021%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.915105/  2.016384, val:  61.25%, val_best:  71.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 62.78 seconds, 1.05 minutes\n",
      "total_backward_count 792990 real_backward_count 111023  14.001%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.907761/  2.022265, val:  52.08%, val_best:  71.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 63.31 seconds, 1.06 minutes\n",
      "total_backward_count 802780 real_backward_count 112162  13.972%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.920199/  2.013889, val:  58.33%, val_best:  71.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 62.41 seconds, 1.04 minutes\n",
      "total_backward_count 812570 real_backward_count 113374  13.953%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.904861/  2.003910, val:  64.58%, val_best:  71.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 62.64 seconds, 1.04 minutes\n",
      "total_backward_count 822360 real_backward_count 114561  13.931%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.899264/  2.000088, val:  70.83%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.37 seconds, 1.04 minutes\n",
      "total_backward_count 832150 real_backward_count 115733  13.908%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.910701/  2.017874, val:  61.67%, val_best:  71.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 63.33 seconds, 1.06 minutes\n",
      "total_backward_count 841940 real_backward_count 116916  13.887%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.905790/  2.003809, val:  64.58%, val_best:  71.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 62.70 seconds, 1.05 minutes\n",
      "total_backward_count 851730 real_backward_count 118092  13.865%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.917722/  2.025505, val:  53.75%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.53 seconds, 1.04 minutes\n",
      "total_backward_count 861520 real_backward_count 119300  13.848%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.904536/  2.014142, val:  62.50%, val_best:  71.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 62.55 seconds, 1.04 minutes\n",
      "total_backward_count 871310 real_backward_count 120519  13.832%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.915014/  2.003699, val:  62.92%, val_best:  71.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 62.93 seconds, 1.05 minutes\n",
      "total_backward_count 881100 real_backward_count 121705  13.813%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.908514/  2.018302, val:  57.50%, val_best:  71.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 62.26 seconds, 1.04 minutes\n",
      "total_backward_count 890890 real_backward_count 122894  13.795%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.909896/  1.995214, val:  71.25%, val_best:  71.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 62.67 seconds, 1.04 minutes\n",
      "total_backward_count 900680 real_backward_count 124000  13.767%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.919723/  2.022872, val:  57.08%, val_best:  71.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 62.79 seconds, 1.05 minutes\n",
      "total_backward_count 910470 real_backward_count 125187  13.750%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.918934/  2.021007, val:  58.33%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.38 seconds, 1.04 minutes\n",
      "total_backward_count 920260 real_backward_count 126349  13.730%\n",
      "fc layer 1 self.abs_max_out: 10322.0\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.916225/  2.017528, val:  62.08%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.69 seconds, 1.04 minutes\n",
      "total_backward_count 930050 real_backward_count 127456  13.704%\n",
      "fc layer 1 self.abs_max_out: 10340.0\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.912973/  2.027335, val:  64.17%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.49 seconds, 1.04 minutes\n",
      "total_backward_count 939840 real_backward_count 128648  13.688%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.911302/  2.005493, val:  55.83%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.88 seconds, 1.05 minutes\n",
      "total_backward_count 949630 real_backward_count 129790  13.667%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.899708/  2.012864, val:  75.83%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.23 seconds, 1.04 minutes\n",
      "total_backward_count 959420 real_backward_count 130879  13.641%\n",
      "fc layer 1 self.abs_max_out: 10510.0\n",
      "fc layer 1 self.abs_max_out: 10737.0\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.911494/  2.020686, val:  65.00%, val_best:  75.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 62.42 seconds, 1.04 minutes\n",
      "total_backward_count 969210 real_backward_count 131982  13.617%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.905891/  2.016712, val:  62.50%, val_best:  75.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 61.89 seconds, 1.03 minutes\n",
      "total_backward_count 979000 real_backward_count 133120  13.598%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.907053/  2.026680, val:  59.58%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.31 seconds, 1.04 minutes\n",
      "total_backward_count 988790 real_backward_count 134282  13.580%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.917147/  2.007979, val:  65.42%, val_best:  75.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 62.36 seconds, 1.04 minutes\n",
      "total_backward_count 998580 real_backward_count 135437  13.563%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.914738/  2.012445, val:  60.42%, val_best:  75.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 62.96 seconds, 1.05 minutes\n",
      "total_backward_count 1008370 real_backward_count 136589  13.546%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.906473/  2.016062, val:  56.67%, val_best:  75.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 62.72 seconds, 1.05 minutes\n",
      "total_backward_count 1018160 real_backward_count 137701  13.524%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.909350/  2.003965, val:  65.00%, val_best:  75.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 63.15 seconds, 1.05 minutes\n",
      "total_backward_count 1027950 real_backward_count 138831  13.506%\n",
      "fc layer 1 self.abs_max_out: 11175.0\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.894059/  2.012630, val:  61.25%, val_best:  75.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.50 seconds, 1.03 minutes\n",
      "total_backward_count 1037740 real_backward_count 140001  13.491%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.891057/  2.000502, val:  58.75%, val_best:  75.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 62.24 seconds, 1.04 minutes\n",
      "total_backward_count 1047530 real_backward_count 141130  13.473%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.898958/  1.996419, val:  63.33%, val_best:  75.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 62.86 seconds, 1.05 minutes\n",
      "total_backward_count 1057320 real_backward_count 142285  13.457%\n",
      "lif layer 1 self.abs_max_v: 17932.0\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.890506/  1.991869, val:  69.17%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.08 seconds, 1.03 minutes\n",
      "total_backward_count 1067110 real_backward_count 143448  13.443%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.878416/  1.989485, val:  57.92%, val_best:  75.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 62.01 seconds, 1.03 minutes\n",
      "total_backward_count 1076900 real_backward_count 144526  13.421%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.881683/  1.981797, val:  65.00%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.68 seconds, 1.04 minutes\n",
      "total_backward_count 1086690 real_backward_count 145608  13.399%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.875237/  1.992849, val:  60.83%, val_best:  75.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 62.47 seconds, 1.04 minutes\n",
      "total_backward_count 1096480 real_backward_count 146732  13.382%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.887232/  1.987619, val:  68.33%, val_best:  75.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 61.77 seconds, 1.03 minutes\n",
      "total_backward_count 1106270 real_backward_count 147844  13.364%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.881265/  1.982148, val:  66.67%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.28 seconds, 1.02 minutes\n",
      "total_backward_count 1116060 real_backward_count 148979  13.349%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.876373/  2.000312, val:  53.33%, val_best:  75.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 60.83 seconds, 1.01 minutes\n",
      "total_backward_count 1125850 real_backward_count 150110  13.333%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.875209/  1.995276, val:  64.17%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.75 seconds, 1.03 minutes\n",
      "total_backward_count 1135640 real_backward_count 151212  13.315%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.892458/  2.001227, val:  62.50%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.98 seconds, 1.03 minutes\n",
      "total_backward_count 1145430 real_backward_count 152313  13.297%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.889544/  1.990766, val:  61.67%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.96 seconds, 1.03 minutes\n",
      "total_backward_count 1155220 real_backward_count 153385  13.278%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.882710/  1.988801, val:  60.00%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 60.89 seconds, 1.01 minutes\n",
      "total_backward_count 1165010 real_backward_count 154509  13.262%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.879021/  1.991885, val:  67.50%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.71 seconds, 1.03 minutes\n",
      "total_backward_count 1174800 real_backward_count 155612  13.246%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.874477/  1.987106, val:  68.75%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.44 seconds, 1.02 minutes\n",
      "total_backward_count 1184590 real_backward_count 156711  13.229%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.879102/  1.989458, val:  64.17%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.15 seconds, 1.02 minutes\n",
      "total_backward_count 1194380 real_backward_count 157835  13.215%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.889085/  2.005398, val:  71.67%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.07 seconds, 1.02 minutes\n",
      "total_backward_count 1204170 real_backward_count 158980  13.202%\n",
      "lif layer 1 self.abs_max_v: 18625.5\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.887922/  1.969471, val:  68.33%, val_best:  75.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 62.56 seconds, 1.04 minutes\n",
      "total_backward_count 1213960 real_backward_count 160131  13.191%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.882488/  1.996168, val:  72.50%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.89 seconds, 1.03 minutes\n",
      "total_backward_count 1223750 real_backward_count 161229  13.175%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.887312/  1.990583, val:  62.08%, val_best:  75.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 62.42 seconds, 1.04 minutes\n",
      "total_backward_count 1233540 real_backward_count 162393  13.165%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.890335/  2.005571, val:  60.00%, val_best:  75.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 61.91 seconds, 1.03 minutes\n",
      "total_backward_count 1243330 real_backward_count 163505  13.151%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.884373/  1.993828, val:  70.00%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.49 seconds, 1.02 minutes\n",
      "total_backward_count 1253120 real_backward_count 164535  13.130%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.878433/  1.995029, val:  61.25%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.03 seconds, 1.03 minutes\n",
      "total_backward_count 1262910 real_backward_count 165624  13.114%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.873560/  1.985184, val:  68.33%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.50 seconds, 1.04 minutes\n",
      "total_backward_count 1272700 real_backward_count 166692  13.098%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.877757/  1.991890, val:  63.33%, val_best:  75.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 62.73 seconds, 1.05 minutes\n",
      "total_backward_count 1282490 real_backward_count 167770  13.082%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.893497/  1.986482, val:  73.33%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.36 seconds, 1.02 minutes\n",
      "total_backward_count 1292280 real_backward_count 168967  13.075%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.873044/  1.991505, val:  58.33%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.03 seconds, 1.03 minutes\n",
      "total_backward_count 1302070 real_backward_count 170040  13.059%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.874986/  1.978954, val:  68.75%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.46 seconds, 1.04 minutes\n",
      "total_backward_count 1311860 real_backward_count 171093  13.042%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.874407/  1.998681, val:  55.00%, val_best:  75.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 62.10 seconds, 1.03 minutes\n",
      "total_backward_count 1321650 real_backward_count 172173  13.027%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.888330/  1.979667, val:  80.00%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 62.58 seconds, 1.04 minutes\n",
      "total_backward_count 1331440 real_backward_count 173239  13.011%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.891650/  1.978876, val:  71.25%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.43 seconds, 1.02 minutes\n",
      "total_backward_count 1341230 real_backward_count 174288  12.995%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.879899/  1.991529, val:  61.25%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 62.37 seconds, 1.04 minutes\n",
      "total_backward_count 1351020 real_backward_count 175339  12.978%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.879991/  1.979050, val:  69.58%, val_best:  80.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 62.21 seconds, 1.04 minutes\n",
      "total_backward_count 1360810 real_backward_count 176409  12.964%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.876805/  1.969708, val:  72.08%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.75 seconds, 1.03 minutes\n",
      "total_backward_count 1370600 real_backward_count 177535  12.953%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.868073/  1.975405, val:  57.50%, val_best:  80.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 62.29 seconds, 1.04 minutes\n",
      "total_backward_count 1380390 real_backward_count 178595  12.938%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.859434/  1.964406, val:  69.58%, val_best:  80.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 61.77 seconds, 1.03 minutes\n",
      "total_backward_count 1390180 real_backward_count 179650  12.923%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.859656/  1.968844, val:  65.00%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.84 seconds, 1.03 minutes\n",
      "total_backward_count 1399970 real_backward_count 180737  12.910%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.852660/  1.972181, val:  66.67%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.90 seconds, 1.03 minutes\n",
      "total_backward_count 1409760 real_backward_count 181800  12.896%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.864689/  1.971773, val:  61.67%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.57 seconds, 1.03 minutes\n",
      "total_backward_count 1419550 real_backward_count 182904  12.885%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.866657/  1.968805, val:  67.92%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.91 seconds, 1.03 minutes\n",
      "total_backward_count 1429340 real_backward_count 183941  12.869%\n",
      "fc layer 1 self.abs_max_out: 11342.0\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.864257/  1.978988, val:  60.83%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.74 seconds, 1.03 minutes\n",
      "total_backward_count 1439130 real_backward_count 184999  12.855%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.851919/  1.985323, val:  60.00%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.09 seconds, 1.02 minutes\n",
      "total_backward_count 1448920 real_backward_count 186022  12.839%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.845944/  1.957177, val:  62.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.20 seconds, 1.04 minutes\n",
      "total_backward_count 1458710 real_backward_count 187048  12.823%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.837711/  1.958886, val:  68.33%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.18 seconds, 1.04 minutes\n",
      "total_backward_count 1468500 real_backward_count 188104  12.809%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.838109/  1.971611, val:  67.08%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.58 seconds, 1.04 minutes\n",
      "total_backward_count 1478290 real_backward_count 189113  12.793%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.853712/  1.957640, val:  65.42%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.34 seconds, 1.04 minutes\n",
      "total_backward_count 1488080 real_backward_count 190209  12.782%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.850151/  1.966007, val:  58.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.96 seconds, 1.03 minutes\n",
      "total_backward_count 1497870 real_backward_count 191168  12.763%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.861671/  1.964697, val:  73.33%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 62.17 seconds, 1.04 minutes\n",
      "total_backward_count 1507660 real_backward_count 192244  12.751%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.871113/  1.984002, val:  67.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.79 seconds, 1.03 minutes\n",
      "total_backward_count 1517450 real_backward_count 193296  12.738%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.856234/  1.975256, val:  63.33%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.47 seconds, 1.02 minutes\n",
      "total_backward_count 1527240 real_backward_count 194314  12.723%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.857106/  1.965302, val:  67.08%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.80 seconds, 1.03 minutes\n",
      "total_backward_count 1537030 real_backward_count 195366  12.711%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.856041/  1.979453, val:  71.67%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.39 seconds, 1.02 minutes\n",
      "total_backward_count 1546820 real_backward_count 196381  12.696%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.865848/  1.990802, val:  66.25%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.88 seconds, 1.05 minutes\n",
      "total_backward_count 1556610 real_backward_count 197426  12.683%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.864076/  1.981457, val:  76.25%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.82 seconds, 1.05 minutes\n",
      "total_backward_count 1566400 real_backward_count 198463  12.670%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.855260/  1.954203, val:  70.00%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.88 seconds, 1.03 minutes\n",
      "total_backward_count 1576190 real_backward_count 199458  12.654%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.846768/  1.958813, val:  75.83%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.32 seconds, 1.04 minutes\n",
      "total_backward_count 1585980 real_backward_count 200516  12.643%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.848267/  1.975940, val:  65.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.31 seconds, 1.02 minutes\n",
      "total_backward_count 1595770 real_backward_count 201595  12.633%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.843129/  1.966029, val:  67.50%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 60.94 seconds, 1.02 minutes\n",
      "total_backward_count 1605560 real_backward_count 202613  12.619%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.851521/  1.965757, val:  72.50%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.40 seconds, 1.04 minutes\n",
      "total_backward_count 1615350 real_backward_count 203623  12.606%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.848053/  1.959362, val:  66.25%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.04 seconds, 1.02 minutes\n",
      "total_backward_count 1625140 real_backward_count 204626  12.591%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.837486/  1.958320, val:  73.75%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.04 seconds, 1.02 minutes\n",
      "total_backward_count 1634930 real_backward_count 205600  12.575%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.848726/  1.953573, val:  61.25%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.25 seconds, 1.02 minutes\n",
      "total_backward_count 1644720 real_backward_count 206627  12.563%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.835146/  1.965845, val:  61.67%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.05 seconds, 1.02 minutes\n",
      "total_backward_count 1654510 real_backward_count 207632  12.549%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.839469/  1.957554, val:  66.67%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.88 seconds, 1.03 minutes\n",
      "total_backward_count 1664300 real_backward_count 208665  12.538%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.848822/  1.967870, val:  75.83%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 60.68 seconds, 1.01 minutes\n",
      "total_backward_count 1674090 real_backward_count 209649  12.523%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.856585/  1.962579, val:  62.50%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.69 seconds, 1.03 minutes\n",
      "total_backward_count 1683880 real_backward_count 210655  12.510%\n",
      "fc layer 3 self.abs_max_out: 600.0\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.846491/  1.944572, val:  71.25%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.58 seconds, 1.03 minutes\n",
      "total_backward_count 1693670 real_backward_count 211695  12.499%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.830069/  1.950879, val:  66.25%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.23 seconds, 1.04 minutes\n",
      "total_backward_count 1703460 real_backward_count 212669  12.485%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.837769/  1.963437, val:  67.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.02 seconds, 1.03 minutes\n",
      "total_backward_count 1713250 real_backward_count 213645  12.470%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.844911/  1.960134, val:  65.83%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.43 seconds, 1.02 minutes\n",
      "total_backward_count 1723040 real_backward_count 214662  12.458%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.840461/  1.956297, val:  69.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.21 seconds, 1.02 minutes\n",
      "total_backward_count 1732830 real_backward_count 215636  12.444%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.828122/  1.947599, val:  64.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.58 seconds, 1.03 minutes\n",
      "total_backward_count 1742620 real_backward_count 216612  12.430%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.836093/  1.966937, val:  59.58%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.19 seconds, 1.02 minutes\n",
      "total_backward_count 1752410 real_backward_count 217578  12.416%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.845798/  1.967289, val:  71.25%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.70 seconds, 1.03 minutes\n",
      "total_backward_count 1762200 real_backward_count 218611  12.406%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.845093/  1.959529, val:  62.92%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 60.68 seconds, 1.01 minutes\n",
      "total_backward_count 1771990 real_backward_count 219559  12.391%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.834402/  1.959925, val:  69.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.72 seconds, 1.03 minutes\n",
      "total_backward_count 1781780 real_backward_count 220506  12.376%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.841246/  1.961291, val:  73.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.46 seconds, 1.02 minutes\n",
      "total_backward_count 1791570 real_backward_count 221493  12.363%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.836649/  1.957687, val:  59.17%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 60.40 seconds, 1.01 minutes\n",
      "total_backward_count 1801360 real_backward_count 222444  12.349%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.833078/  1.964012, val:  66.25%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.84 seconds, 1.03 minutes\n",
      "total_backward_count 1811150 real_backward_count 223398  12.335%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.838718/  1.956587, val:  65.42%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 60.93 seconds, 1.02 minutes\n",
      "total_backward_count 1820940 real_backward_count 224409  12.324%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.845645/  1.952184, val:  75.42%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.13 seconds, 1.04 minutes\n",
      "total_backward_count 1830730 real_backward_count 225426  12.313%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.840513/  1.966760, val:  68.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 60.90 seconds, 1.02 minutes\n",
      "total_backward_count 1840520 real_backward_count 226405  12.301%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.838378/  1.957516, val:  71.25%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.44 seconds, 1.02 minutes\n",
      "total_backward_count 1850310 real_backward_count 227354  12.287%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.820139/  1.936045, val:  65.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.28 seconds, 1.04 minutes\n",
      "total_backward_count 1860100 real_backward_count 228298  12.273%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.829934/  1.928157, val:  77.92%, val_best:  80.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 61.65 seconds, 1.03 minutes\n",
      "total_backward_count 1869890 real_backward_count 229228  12.259%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.815579/  1.936803, val:  61.25%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.33 seconds, 1.04 minutes\n",
      "total_backward_count 1879680 real_backward_count 230132  12.243%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.823796/  1.949255, val:  68.75%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.22 seconds, 1.02 minutes\n",
      "total_backward_count 1889470 real_backward_count 231064  12.229%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.821475/  1.951821, val:  60.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.29 seconds, 1.02 minutes\n",
      "total_backward_count 1899260 real_backward_count 231954  12.213%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.813877/  1.935215, val:  62.50%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 60.40 seconds, 1.01 minutes\n",
      "total_backward_count 1909050 real_backward_count 232909  12.200%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.819157/  1.939806, val:  64.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 60.79 seconds, 1.01 minutes\n",
      "total_backward_count 1918840 real_backward_count 233835  12.186%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.819932/  1.950537, val:  76.67%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 60.96 seconds, 1.02 minutes\n",
      "total_backward_count 1928630 real_backward_count 234813  12.175%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.832888/  1.949857, val:  62.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.04 seconds, 1.02 minutes\n",
      "total_backward_count 1938420 real_backward_count 235774  12.163%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.826942/  1.954498, val:  66.67%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 60.73 seconds, 1.01 minutes\n",
      "total_backward_count 1948210 real_backward_count 236695  12.149%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.833016/  1.957433, val:  63.75%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.08 seconds, 1.03 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf87c4834d80402a99249f9579971545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñá‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99898</td></tr><tr><td>tr_epoch_loss</td><td>1.83302</td></tr><tr><td>val_acc_best</td><td>0.8</td></tr><tr><td>val_acc_now</td><td>0.6375</td></tr><tr><td>val_loss</td><td>1.95743</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">generous-sweep-109</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/g0jqsrsw' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/g0jqsrsw</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251111_063519-g0jqsrsw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vx0hhz8j with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251111_100253-vx0hhz8j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vx0hhz8j' target=\"_blank\">atomic-sweep-112</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hpjdvxst' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hpjdvxst</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hpjdvxst' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hpjdvxst</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vx0hhz8j' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vx0hhz8j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251111_100300_797', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 427.0\n",
      "lif layer 1 self.abs_max_v: 427.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 251.0\n",
      "lif layer 2 self.abs_max_v: 251.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 562.0\n",
      "lif layer 1 self.abs_max_v: 591.0\n",
      "fc layer 2 self.abs_max_out: 565.0\n",
      "lif layer 2 self.abs_max_v: 594.0\n",
      "fc layer 3 self.abs_max_out: 206.0\n",
      "fc layer 1 self.abs_max_out: 601.0\n",
      "lif layer 1 self.abs_max_v: 764.5\n",
      "lif layer 2 self.abs_max_v: 740.5\n",
      "lif layer 1 self.abs_max_v: 785.5\n",
      "fc layer 2 self.abs_max_out: 688.0\n",
      "lif layer 2 self.abs_max_v: 1024.5\n",
      "fc layer 1 self.abs_max_out: 670.0\n",
      "lif layer 1 self.abs_max_v: 810.0\n",
      "lif layer 2 self.abs_max_v: 1102.5\n",
      "lif layer 1 self.abs_max_v: 815.0\n",
      "lif layer 1 self.abs_max_v: 882.0\n",
      "fc layer 2 self.abs_max_out: 951.0\n",
      "fc layer 3 self.abs_max_out: 238.0\n",
      "lif layer 1 self.abs_max_v: 949.5\n",
      "lif layer 2 self.abs_max_v: 1163.5\n",
      "fc layer 1 self.abs_max_out: 824.0\n",
      "lif layer 1 self.abs_max_v: 962.0\n",
      "fc layer 3 self.abs_max_out: 302.0\n",
      "fc layer 1 self.abs_max_out: 941.0\n",
      "lif layer 1 self.abs_max_v: 1072.5\n",
      "lif layer 1 self.abs_max_v: 1134.0\n",
      "fc layer 2 self.abs_max_out: 1092.0\n",
      "lif layer 1 self.abs_max_v: 1230.0\n",
      "lif layer 2 self.abs_max_v: 1210.0\n",
      "lif layer 1 self.abs_max_v: 1384.0\n",
      "fc layer 1 self.abs_max_out: 1103.0\n",
      "fc layer 1 self.abs_max_out: 1437.0\n",
      "lif layer 1 self.abs_max_v: 1463.5\n",
      "lif layer 2 self.abs_max_v: 1249.5\n",
      "fc layer 1 self.abs_max_out: 1492.0\n",
      "lif layer 1 self.abs_max_v: 1638.0\n",
      "lif layer 2 self.abs_max_v: 1313.0\n",
      "fc layer 3 self.abs_max_out: 330.0\n",
      "lif layer 2 self.abs_max_v: 1335.0\n",
      "lif layer 2 self.abs_max_v: 1451.5\n",
      "fc layer 1 self.abs_max_out: 1493.0\n",
      "fc layer 1 self.abs_max_out: 1754.0\n",
      "lif layer 1 self.abs_max_v: 1754.0\n",
      "fc layer 3 self.abs_max_out: 375.0\n",
      "lif layer 2 self.abs_max_v: 1523.5\n",
      "lif layer 1 self.abs_max_v: 1862.0\n",
      "fc layer 3 self.abs_max_out: 400.0\n",
      "lif layer 1 self.abs_max_v: 1869.0\n",
      "fc layer 3 self.abs_max_out: 525.0\n",
      "lif layer 2 self.abs_max_v: 1588.5\n",
      "lif layer 2 self.abs_max_v: 1803.5\n",
      "fc layer 1 self.abs_max_out: 1774.0\n",
      "fc layer 2 self.abs_max_out: 1114.0\n",
      "fc layer 2 self.abs_max_out: 1133.0\n",
      "lif layer 1 self.abs_max_v: 1877.5\n",
      "fc layer 2 self.abs_max_out: 1274.0\n",
      "lif layer 2 self.abs_max_v: 1861.0\n",
      "fc layer 1 self.abs_max_out: 1796.0\n",
      "fc layer 1 self.abs_max_out: 2178.0\n",
      "lif layer 1 self.abs_max_v: 2178.0\n",
      "lif layer 2 self.abs_max_v: 1936.5\n",
      "lif layer 1 self.abs_max_v: 2370.0\n",
      "lif layer 1 self.abs_max_v: 2473.5\n",
      "lif layer 1 self.abs_max_v: 2607.0\n",
      "fc layer 1 self.abs_max_out: 2424.0\n",
      "lif layer 1 self.abs_max_v: 2975.0\n",
      "lif layer 2 self.abs_max_v: 1975.0\n",
      "lif layer 1 self.abs_max_v: 3023.5\n",
      "lif layer 2 self.abs_max_v: 1986.5\n",
      "fc layer 2 self.abs_max_out: 1299.0\n",
      "fc layer 2 self.abs_max_out: 1304.0\n",
      "lif layer 2 self.abs_max_v: 2161.5\n",
      "fc layer 2 self.abs_max_out: 1432.0\n",
      "lif layer 2 self.abs_max_v: 2339.5\n",
      "lif layer 2 self.abs_max_v: 2446.0\n",
      "lif layer 2 self.abs_max_v: 2602.0\n",
      "lif layer 1 self.abs_max_v: 3177.5\n",
      "lif layer 1 self.abs_max_v: 3443.0\n",
      "lif layer 1 self.abs_max_v: 3518.5\n",
      "lif layer 1 self.abs_max_v: 3665.5\n",
      "lif layer 1 self.abs_max_v: 3725.0\n",
      "fc layer 2 self.abs_max_out: 1511.0\n",
      "lif layer 2 self.abs_max_v: 2682.0\n",
      "lif layer 2 self.abs_max_v: 2767.0\n",
      "fc layer 3 self.abs_max_out: 566.0\n",
      "fc layer 3 self.abs_max_out: 583.0\n",
      "fc layer 2 self.abs_max_out: 1629.0\n",
      "fc layer 2 self.abs_max_out: 1666.0\n",
      "fc layer 1 self.abs_max_out: 2526.0\n",
      "lif layer 1 self.abs_max_v: 3976.5\n",
      "fc layer 2 self.abs_max_out: 1683.0\n",
      "fc layer 1 self.abs_max_out: 2589.0\n",
      "lif layer 1 self.abs_max_v: 4065.5\n",
      "fc layer 1 self.abs_max_out: 2864.0\n",
      "lif layer 1 self.abs_max_v: 4897.0\n",
      "lif layer 1 self.abs_max_v: 5111.5\n",
      "fc layer 2 self.abs_max_out: 1711.0\n",
      "lif layer 2 self.abs_max_v: 2838.0\n",
      "fc layer 2 self.abs_max_out: 1730.0\n",
      "fc layer 2 self.abs_max_out: 1756.0\n",
      "fc layer 3 self.abs_max_out: 584.0\n",
      "fc layer 2 self.abs_max_out: 1802.0\n",
      "fc layer 2 self.abs_max_out: 1808.0\n",
      "fc layer 3 self.abs_max_out: 662.0\n",
      "fc layer 1 self.abs_max_out: 2924.0\n",
      "lif layer 2 self.abs_max_v: 2920.0\n",
      "lif layer 2 self.abs_max_v: 2970.5\n",
      "fc layer 2 self.abs_max_out: 1945.0\n",
      "fc layer 1 self.abs_max_out: 3182.0\n",
      "lif layer 1 self.abs_max_v: 5150.0\n",
      "lif layer 1 self.abs_max_v: 5369.0\n",
      "fc layer 2 self.abs_max_out: 1979.0\n",
      "fc layer 2 self.abs_max_out: 2012.0\n",
      "fc layer 2 self.abs_max_out: 2166.0\n",
      "fc layer 2 self.abs_max_out: 2224.0\n",
      "fc layer 1 self.abs_max_out: 3247.0\n",
      "fc layer 1 self.abs_max_out: 3392.0\n",
      "lif layer 1 self.abs_max_v: 5400.0\n",
      "lif layer 1 self.abs_max_v: 5980.0\n",
      "fc layer 1 self.abs_max_out: 3594.0\n",
      "lif layer 2 self.abs_max_v: 2995.0\n",
      "lif layer 2 self.abs_max_v: 2999.5\n",
      "fc layer 2 self.abs_max_out: 2336.0\n",
      "lif layer 2 self.abs_max_v: 3055.5\n",
      "lif layer 1 self.abs_max_v: 5999.0\n",
      "lif layer 1 self.abs_max_v: 6194.5\n",
      "fc layer 1 self.abs_max_out: 3684.0\n",
      "fc layer 1 self.abs_max_out: 3691.0\n",
      "fc layer 2 self.abs_max_out: 2352.0\n",
      "fc layer 2 self.abs_max_out: 2401.0\n",
      "fc layer 1 self.abs_max_out: 3735.0\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  1.894970/  2.048833, val:  36.25%, val_best:  36.25%, tr:  89.68%, tr_best:  89.68%, epoch time: 61.89 seconds, 1.03 minutes\n",
      "total_backward_count 9790 real_backward_count 3088  31.542%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 2520.0\n",
      "fc layer 2 self.abs_max_out: 2624.0\n",
      "fc layer 1 self.abs_max_out: 3923.0\n",
      "lif layer 1 self.abs_max_v: 6963.5\n",
      "lif layer 2 self.abs_max_v: 3271.5\n",
      "lif layer 2 self.abs_max_v: 3374.0\n",
      "lif layer 2 self.abs_max_v: 3407.0\n",
      "fc layer 1 self.abs_max_out: 3967.0\n",
      "lif layer 2 self.abs_max_v: 3471.0\n",
      "lif layer 2 self.abs_max_v: 3566.5\n",
      "lif layer 2 self.abs_max_v: 3666.5\n",
      "fc layer 1 self.abs_max_out: 4083.0\n",
      "fc layer 1 self.abs_max_out: 4120.0\n",
      "fc layer 1 self.abs_max_out: 4256.0\n",
      "fc layer 3 self.abs_max_out: 705.0\n",
      "lif layer 2 self.abs_max_v: 3790.0\n",
      "lif layer 2 self.abs_max_v: 3824.0\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  1.855148/  2.036196, val:  40.83%, val_best:  40.83%, tr:  97.96%, tr_best:  97.96%, epoch time: 61.27 seconds, 1.02 minutes\n",
      "total_backward_count 19580 real_backward_count 5156  26.333%\n",
      "fc layer 1 self.abs_max_out: 4408.0\n",
      "fc layer 1 self.abs_max_out: 4496.0\n",
      "lif layer 1 self.abs_max_v: 7204.5\n",
      "lif layer 1 self.abs_max_v: 7225.0\n",
      "fc layer 1 self.abs_max_out: 4511.0\n",
      "lif layer 1 self.abs_max_v: 7388.0\n",
      "fc layer 2 self.abs_max_out: 2658.0\n",
      "lif layer 2 self.abs_max_v: 3898.0\n",
      "fc layer 2 self.abs_max_out: 2686.0\n",
      "fc layer 2 self.abs_max_out: 2714.0\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  1.858229/  2.040789, val:  45.00%, val_best:  45.00%, tr:  98.26%, tr_best:  98.26%, epoch time: 61.47 seconds, 1.02 minutes\n",
      "total_backward_count 29370 real_backward_count 7025  23.919%\n",
      "lif layer 1 self.abs_max_v: 7655.0\n",
      "fc layer 2 self.abs_max_out: 2864.0\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  1.885797/  2.046824, val:  38.33%, val_best:  45.00%, tr:  98.57%, tr_best:  98.57%, epoch time: 61.15 seconds, 1.02 minutes\n",
      "total_backward_count 39160 real_backward_count 8753  22.352%\n",
      "fc layer 1 self.abs_max_out: 4602.0\n",
      "fc layer 1 self.abs_max_out: 4761.0\n",
      "fc layer 2 self.abs_max_out: 3128.0\n",
      "lif layer 1 self.abs_max_v: 7809.5\n",
      "lif layer 1 self.abs_max_v: 8503.0\n",
      "fc layer 1 self.abs_max_out: 4785.0\n",
      "fc layer 1 self.abs_max_out: 5380.0\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  1.874724/  2.030077, val:  49.17%, val_best:  49.17%, tr:  99.49%, tr_best:  99.49%, epoch time: 61.20 seconds, 1.02 minutes\n",
      "total_backward_count 48950 real_backward_count 10266  20.972%\n",
      "fc layer 1 self.abs_max_out: 5433.0\n",
      "fc layer 2 self.abs_max_out: 3277.0\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  1.870008/  2.030566, val:  48.33%, val_best:  49.17%, tr:  99.28%, tr_best:  99.49%, epoch time: 61.78 seconds, 1.03 minutes\n",
      "total_backward_count 58740 real_backward_count 11857  20.186%\n",
      "lif layer 1 self.abs_max_v: 8602.0\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  1.869246/  2.017918, val:  46.25%, val_best:  49.17%, tr:  99.28%, tr_best:  99.49%, epoch time: 61.56 seconds, 1.03 minutes\n",
      "total_backward_count 68530 real_backward_count 13338  19.463%\n",
      "fc layer 2 self.abs_max_out: 3450.0\n",
      "fc layer 2 self.abs_max_out: 3469.0\n",
      "fc layer 2 self.abs_max_out: 3478.0\n",
      "lif layer 2 self.abs_max_v: 3925.0\n",
      "lif layer 1 self.abs_max_v: 8725.5\n",
      "lif layer 2 self.abs_max_v: 4067.0\n",
      "lif layer 2 self.abs_max_v: 4070.0\n",
      "lif layer 1 self.abs_max_v: 8742.0\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  1.861405/  2.010096, val:  51.25%, val_best:  51.25%, tr:  99.28%, tr_best:  99.49%, epoch time: 61.86 seconds, 1.03 minutes\n",
      "total_backward_count 78320 real_backward_count 14812  18.912%\n",
      "lif layer 1 self.abs_max_v: 8958.5\n",
      "lif layer 1 self.abs_max_v: 9185.5\n",
      "lif layer 1 self.abs_max_v: 9205.0\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  1.861873/  2.006135, val:  57.50%, val_best:  57.50%, tr:  99.08%, tr_best:  99.49%, epoch time: 61.18 seconds, 1.02 minutes\n",
      "total_backward_count 88110 real_backward_count 16297  18.496%\n",
      "lif layer 2 self.abs_max_v: 4093.0\n",
      "lif layer 2 self.abs_max_v: 4186.5\n",
      "fc layer 1 self.abs_max_out: 5649.0\n",
      "lif layer 1 self.abs_max_v: 9484.0\n",
      "lif layer 1 self.abs_max_v: 9535.5\n",
      "lif layer 1 self.abs_max_v: 9766.0\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  1.861487/  2.040337, val:  34.58%, val_best:  57.50%, tr:  99.28%, tr_best:  99.49%, epoch time: 62.12 seconds, 1.04 minutes\n",
      "total_backward_count 97900 real_backward_count 17764  18.145%\n",
      "fc layer 2 self.abs_max_out: 3547.0\n",
      "fc layer 1 self.abs_max_out: 5770.0\n",
      "lif layer 1 self.abs_max_v: 10292.0\n",
      "fc layer 1 self.abs_max_out: 5824.0\n",
      "lif layer 1 self.abs_max_v: 10420.0\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  1.866716/  2.049675, val:  35.83%, val_best:  57.50%, tr:  99.18%, tr_best:  99.49%, epoch time: 60.79 seconds, 1.01 minutes\n",
      "total_backward_count 107690 real_backward_count 19149  17.782%\n",
      "fc layer 2 self.abs_max_out: 3664.0\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  1.857342/  1.990036, val:  47.92%, val_best:  57.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 61.35 seconds, 1.02 minutes\n",
      "total_backward_count 117480 real_backward_count 20531  17.476%\n",
      "fc layer 1 self.abs_max_out: 5955.0\n",
      "lif layer 1 self.abs_max_v: 10536.0\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  1.854152/  2.014514, val:  46.25%, val_best:  57.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 62.46 seconds, 1.04 minutes\n",
      "total_backward_count 127270 real_backward_count 21953  17.249%\n",
      "lif layer 1 self.abs_max_v: 10622.0\n",
      "lif layer 2 self.abs_max_v: 4221.5\n",
      "lif layer 2 self.abs_max_v: 4263.5\n",
      "lif layer 2 self.abs_max_v: 4293.0\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  1.861223/  2.030575, val:  42.92%, val_best:  57.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 60.83 seconds, 1.01 minutes\n",
      "total_backward_count 137060 real_backward_count 23318  17.013%\n",
      "lif layer 2 self.abs_max_v: 4354.5\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  1.850488/  2.012656, val:  47.08%, val_best:  57.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 62.16 seconds, 1.04 minutes\n",
      "total_backward_count 146850 real_backward_count 24679  16.806%\n",
      "lif layer 2 self.abs_max_v: 4472.0\n",
      "fc layer 2 self.abs_max_out: 3746.0\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  1.863685/  2.017644, val:  48.75%, val_best:  57.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 61.38 seconds, 1.02 minutes\n",
      "total_backward_count 156640 real_backward_count 26017  16.609%\n",
      "fc layer 1 self.abs_max_out: 6189.0\n",
      "lif layer 1 self.abs_max_v: 10873.5\n",
      "lif layer 1 self.abs_max_v: 10998.0\n",
      "lif layer 1 self.abs_max_v: 11159.0\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  1.845869/  1.999034, val:  55.83%, val_best:  57.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 61.18 seconds, 1.02 minutes\n",
      "total_backward_count 166430 real_backward_count 27279  16.391%\n",
      "lif layer 2 self.abs_max_v: 4627.0\n",
      "lif layer 2 self.abs_max_v: 4745.5\n",
      "fc layer 1 self.abs_max_out: 6274.0\n",
      "lif layer 2 self.abs_max_v: 4771.0\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  1.845089/  1.989020, val:  57.92%, val_best:  57.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 61.97 seconds, 1.03 minutes\n",
      "total_backward_count 176220 real_backward_count 28600  16.230%\n",
      "lif layer 2 self.abs_max_v: 4781.5\n",
      "lif layer 2 self.abs_max_v: 4796.0\n",
      "fc layer 1 self.abs_max_out: 6730.0\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  1.846671/  2.004833, val:  39.58%, val_best:  57.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.07 seconds, 1.02 minutes\n",
      "total_backward_count 186010 real_backward_count 30003  16.130%\n",
      "fc layer 2 self.abs_max_out: 3774.0\n",
      "lif layer 2 self.abs_max_v: 4951.5\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  1.851156/  2.012813, val:  49.17%, val_best:  57.92%, tr:  99.39%, tr_best: 100.00%, epoch time: 61.62 seconds, 1.03 minutes\n",
      "total_backward_count 195800 real_backward_count 31264  15.967%\n",
      "lif layer 1 self.abs_max_v: 11501.0\n",
      "fc layer 1 self.abs_max_out: 6743.0\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  1.835809/  2.013955, val:  44.17%, val_best:  57.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 60.88 seconds, 1.01 minutes\n",
      "total_backward_count 205590 real_backward_count 32549  15.832%\n",
      "fc layer 1 self.abs_max_out: 6802.0\n",
      "fc layer 1 self.abs_max_out: 7009.0\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  1.846840/  2.038787, val:  47.50%, val_best:  57.92%, tr:  99.39%, tr_best: 100.00%, epoch time: 60.29 seconds, 1.00 minutes\n",
      "total_backward_count 215380 real_backward_count 33951  15.763%\n",
      "lif layer 2 self.abs_max_v: 4972.5\n",
      "lif layer 2 self.abs_max_v: 5018.5\n",
      "fc layer 1 self.abs_max_out: 7010.0\n",
      "fc layer 1 self.abs_max_out: 7028.0\n",
      "fc layer 1 self.abs_max_out: 7045.0\n",
      "fc layer 1 self.abs_max_out: 7289.0\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  1.847297/  1.979405, val:  53.75%, val_best:  57.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.87 seconds, 1.03 minutes\n",
      "total_backward_count 225170 real_backward_count 35240  15.650%\n",
      "lif layer 1 self.abs_max_v: 12253.5\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  1.847296/  2.001215, val:  55.42%, val_best:  57.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 60.47 seconds, 1.01 minutes\n",
      "total_backward_count 234960 real_backward_count 36532  15.548%\n",
      "fc layer 1 self.abs_max_out: 7395.0\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  1.852086/  1.997452, val:  55.00%, val_best:  57.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.18 seconds, 1.04 minutes\n",
      "total_backward_count 244750 real_backward_count 37853  15.466%\n",
      "lif layer 1 self.abs_max_v: 12418.0\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  1.863030/  1.999959, val:  54.17%, val_best:  57.92%, tr:  99.49%, tr_best: 100.00%, epoch time: 61.86 seconds, 1.03 minutes\n",
      "total_backward_count 254540 real_backward_count 39235  15.414%\n",
      "fc layer 1 self.abs_max_out: 7509.0\n",
      "fc layer 1 self.abs_max_out: 7630.0\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  1.862127/  1.994206, val:  57.50%, val_best:  57.92%, tr:  99.28%, tr_best: 100.00%, epoch time: 61.54 seconds, 1.03 minutes\n",
      "total_backward_count 264330 real_backward_count 40515  15.327%\n",
      "fc layer 1 self.abs_max_out: 7664.0\n",
      "lif layer 2 self.abs_max_v: 5098.0\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  1.865786/  2.012304, val:  53.75%, val_best:  57.92%, tr:  99.39%, tr_best: 100.00%, epoch time: 61.97 seconds, 1.03 minutes\n",
      "total_backward_count 274120 real_backward_count 41842  15.264%\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  1.867322/  2.020293, val:  55.42%, val_best:  57.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 60.59 seconds, 1.01 minutes\n",
      "total_backward_count 283910 real_backward_count 43099  15.181%\n",
      "fc layer 1 self.abs_max_out: 7665.0\n",
      "fc layer 1 self.abs_max_out: 7880.0\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  1.864480/  1.997845, val:  53.33%, val_best:  57.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.90 seconds, 1.03 minutes\n",
      "total_backward_count 293700 real_backward_count 44439  15.131%\n",
      "lif layer 2 self.abs_max_v: 5099.5\n",
      "lif layer 2 self.abs_max_v: 5159.5\n",
      "lif layer 2 self.abs_max_v: 5169.0\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  1.853806/  2.003338, val:  61.25%, val_best:  61.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 60.38 seconds, 1.01 minutes\n",
      "total_backward_count 303490 real_backward_count 45722  15.065%\n",
      "lif layer 2 self.abs_max_v: 5243.5\n",
      "lif layer 2 self.abs_max_v: 5284.5\n",
      "lif layer 1 self.abs_max_v: 12553.5\n",
      "fc layer 1 self.abs_max_out: 8101.0\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  1.865997/  2.006311, val:  50.42%, val_best:  61.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.31 seconds, 1.02 minutes\n",
      "total_backward_count 313280 real_backward_count 47017  15.008%\n",
      "lif layer 2 self.abs_max_v: 5293.0\n",
      "lif layer 2 self.abs_max_v: 5356.0\n",
      "fc layer 2 self.abs_max_out: 3895.0\n",
      "lif layer 2 self.abs_max_v: 5422.0\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  1.867055/  2.018403, val:  47.92%, val_best:  61.25%, tr:  99.39%, tr_best: 100.00%, epoch time: 62.16 seconds, 1.04 minutes\n",
      "total_backward_count 323070 real_backward_count 48279  14.944%\n",
      "fc layer 1 self.abs_max_out: 8225.0\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  1.858093/  1.991223, val:  56.67%, val_best:  61.25%, tr:  99.39%, tr_best: 100.00%, epoch time: 60.90 seconds, 1.02 minutes\n",
      "total_backward_count 332860 real_backward_count 49592  14.899%\n",
      "fc layer 1 self.abs_max_out: 8290.0\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  1.864753/  1.997013, val:  57.50%, val_best:  61.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.94 seconds, 1.03 minutes\n",
      "total_backward_count 342650 real_backward_count 50881  14.849%\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  1.868295/  1.996819, val:  55.00%, val_best:  61.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 60.68 seconds, 1.01 minutes\n",
      "total_backward_count 352440 real_backward_count 52091  14.780%\n",
      "lif layer 1 self.abs_max_v: 12744.0\n",
      "lif layer 2 self.abs_max_v: 5469.0\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  1.853349/  1.984038, val:  60.00%, val_best:  61.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 61.45 seconds, 1.02 minutes\n",
      "total_backward_count 362230 real_backward_count 53313  14.718%\n",
      "lif layer 1 self.abs_max_v: 13013.5\n",
      "fc layer 1 self.abs_max_out: 8549.0\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  1.854283/  1.989867, val:  57.92%, val_best:  61.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.70 seconds, 1.03 minutes\n",
      "total_backward_count 372020 real_backward_count 54507  14.652%\n",
      "lif layer 1 self.abs_max_v: 13180.5\n",
      "lif layer 1 self.abs_max_v: 13255.0\n",
      "fc layer 2 self.abs_max_out: 3914.0\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  1.870136/  2.003318, val:  64.58%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 60.54 seconds, 1.01 minutes\n",
      "total_backward_count 381810 real_backward_count 55763  14.605%\n",
      "fc layer 2 self.abs_max_out: 3928.0\n",
      "fc layer 2 self.abs_max_out: 3962.0\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  1.865787/  1.994554, val:  56.67%, val_best:  64.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 60.54 seconds, 1.01 minutes\n",
      "total_backward_count 391600 real_backward_count 56985  14.552%\n",
      "fc layer 1 self.abs_max_out: 8586.0\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  1.860951/  1.994847, val:  58.75%, val_best:  64.58%, tr:  99.49%, tr_best: 100.00%, epoch time: 61.21 seconds, 1.02 minutes\n",
      "total_backward_count 401390 real_backward_count 58256  14.514%\n",
      "fc layer 2 self.abs_max_out: 4001.0\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  1.882033/  2.020206, val:  51.67%, val_best:  64.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 60.95 seconds, 1.02 minutes\n",
      "total_backward_count 411180 real_backward_count 59507  14.472%\n",
      "lif layer 2 self.abs_max_v: 5582.0\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  1.881040/  2.008130, val:  53.75%, val_best:  64.58%, tr:  99.49%, tr_best: 100.00%, epoch time: 61.73 seconds, 1.03 minutes\n",
      "total_backward_count 420970 real_backward_count 60710  14.421%\n",
      "fc layer 1 self.abs_max_out: 8649.0\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  1.873994/  1.997222, val:  58.75%, val_best:  64.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.24 seconds, 1.02 minutes\n",
      "total_backward_count 430760 real_backward_count 61987  14.390%\n",
      "lif layer 2 self.abs_max_v: 5634.0\n",
      "fc layer 1 self.abs_max_out: 9451.0\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  1.873680/  2.013662, val:  55.42%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.58 seconds, 1.03 minutes\n",
      "total_backward_count 440550 real_backward_count 63246  14.356%\n",
      "fc layer 2 self.abs_max_out: 4074.0\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  1.893167/  1.998990, val:  59.58%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.06 seconds, 1.02 minutes\n",
      "total_backward_count 450340 real_backward_count 64517  14.326%\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  1.883466/  2.035072, val:  55.42%, val_best:  64.58%, tr:  99.49%, tr_best: 100.00%, epoch time: 61.48 seconds, 1.02 minutes\n",
      "total_backward_count 460130 real_backward_count 65740  14.287%\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  1.904944/  2.036289, val:  57.50%, val_best:  64.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 61.98 seconds, 1.03 minutes\n",
      "total_backward_count 469920 real_backward_count 66956  14.248%\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  1.917120/  2.022371, val:  57.92%, val_best:  64.58%, tr:  99.18%, tr_best: 100.00%, epoch time: 61.65 seconds, 1.03 minutes\n",
      "total_backward_count 479710 real_backward_count 68159  14.208%\n",
      "lif layer 1 self.abs_max_v: 13356.5\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  1.904726/  1.995464, val:  53.75%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.18 seconds, 1.02 minutes\n",
      "total_backward_count 489500 real_backward_count 69387  14.175%\n",
      "lif layer 2 self.abs_max_v: 5680.0\n",
      "lif layer 1 self.abs_max_v: 13717.0\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  1.896921/  2.012940, val:  53.75%, val_best:  64.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 61.55 seconds, 1.03 minutes\n",
      "total_backward_count 499290 real_backward_count 70589  14.138%\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  1.895164/  2.016695, val:  55.00%, val_best:  64.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 60.55 seconds, 1.01 minutes\n",
      "total_backward_count 509080 real_backward_count 71781  14.100%\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  1.905090/  2.018787, val:  62.92%, val_best:  64.58%, tr:  99.49%, tr_best: 100.00%, epoch time: 61.71 seconds, 1.03 minutes\n",
      "total_backward_count 518870 real_backward_count 73024  14.074%\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  1.914557/  2.026028, val:  52.50%, val_best:  64.58%, tr:  99.49%, tr_best: 100.00%, epoch time: 61.18 seconds, 1.02 minutes\n",
      "total_backward_count 528660 real_backward_count 74292  14.053%\n",
      "fc layer 1 self.abs_max_out: 9535.0\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  1.917973/  2.033874, val:  53.33%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.43 seconds, 1.04 minutes\n",
      "total_backward_count 538450 real_backward_count 75522  14.026%\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  1.922884/  2.050430, val:  63.33%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.65 seconds, 1.03 minutes\n",
      "total_backward_count 548240 real_backward_count 76741  13.998%\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  1.929607/  2.051169, val:  52.50%, val_best:  64.58%, tr:  99.49%, tr_best: 100.00%, epoch time: 61.75 seconds, 1.03 minutes\n",
      "total_backward_count 558030 real_backward_count 77975  13.973%\n",
      "fc layer 1 self.abs_max_out: 9590.0\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  1.924661/  2.034439, val:  56.67%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.51 seconds, 1.03 minutes\n",
      "total_backward_count 567820 real_backward_count 79134  13.936%\n",
      "lif layer 1 self.abs_max_v: 13866.0\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  1.932888/  2.043294, val:  62.08%, val_best:  64.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 60.50 seconds, 1.01 minutes\n",
      "total_backward_count 577610 real_backward_count 80317  13.905%\n",
      "fc layer 2 self.abs_max_out: 4222.0\n",
      "lif layer 1 self.abs_max_v: 13929.5\n",
      "lif layer 1 self.abs_max_v: 14191.0\n",
      "lif layer 1 self.abs_max_v: 14486.0\n",
      "fc layer 1 self.abs_max_out: 9618.0\n",
      "fc layer 1 self.abs_max_out: 9995.0\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  1.930147/  2.025476, val:  59.58%, val_best:  64.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.86 seconds, 1.03 minutes\n",
      "total_backward_count 587400 real_backward_count 81424  13.862%\n",
      "lif layer 2 self.abs_max_v: 5685.0\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  1.930546/  2.047271, val:  51.67%, val_best:  64.58%, tr:  99.49%, tr_best: 100.00%, epoch time: 60.81 seconds, 1.01 minutes\n",
      "total_backward_count 597190 real_backward_count 82576  13.827%\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  1.944379/  2.033945, val:  64.58%, val_best:  64.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 61.52 seconds, 1.03 minutes\n",
      "total_backward_count 606980 real_backward_count 83793  13.805%\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  1.938694/  2.061612, val:  55.42%, val_best:  64.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 61.53 seconds, 1.03 minutes\n",
      "total_backward_count 616770 real_backward_count 84999  13.781%\n",
      "fc layer 1 self.abs_max_out: 10498.0\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  1.931327/  2.045918, val:  60.42%, val_best:  64.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 61.10 seconds, 1.02 minutes\n",
      "total_backward_count 626560 real_backward_count 86144  13.749%\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  1.928049/  2.031891, val:  49.58%, val_best:  64.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 60.89 seconds, 1.01 minutes\n",
      "total_backward_count 636350 real_backward_count 87281  13.716%\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  1.923349/  2.055695, val:  56.67%, val_best:  64.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 61.85 seconds, 1.03 minutes\n",
      "total_backward_count 646140 real_backward_count 88406  13.682%\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  1.939048/  2.047413, val:  61.25%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 62.20 seconds, 1.04 minutes\n",
      "total_backward_count 655930 real_backward_count 89593  13.659%\n",
      "fc layer 1 self.abs_max_out: 10555.0\n",
      "lif layer 1 self.abs_max_v: 14747.0\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  1.930369/  2.044835, val:  57.92%, val_best:  64.58%, tr:  99.49%, tr_best: 100.00%, epoch time: 60.96 seconds, 1.02 minutes\n",
      "total_backward_count 665720 real_backward_count 90809  13.641%\n",
      "fc layer 1 self.abs_max_out: 10801.0\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  1.911571/  2.021977, val:  63.75%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.46 seconds, 1.02 minutes\n",
      "total_backward_count 675510 real_backward_count 92021  13.622%\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  1.924515/  2.066439, val:  53.33%, val_best:  64.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 62.09 seconds, 1.03 minutes\n",
      "total_backward_count 685300 real_backward_count 93126  13.589%\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  1.927518/  2.049077, val:  60.42%, val_best:  64.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 61.59 seconds, 1.03 minutes\n",
      "total_backward_count 695090 real_backward_count 94263  13.561%\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  1.919352/  2.049966, val:  54.17%, val_best:  64.58%, tr:  99.49%, tr_best: 100.00%, epoch time: 61.07 seconds, 1.02 minutes\n",
      "total_backward_count 704880 real_backward_count 95411  13.536%\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  1.929391/  2.045285, val:  67.08%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.23 seconds, 1.02 minutes\n",
      "total_backward_count 714670 real_backward_count 96539  13.508%\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  1.918884/  2.041701, val:  59.58%, val_best:  67.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 61.71 seconds, 1.03 minutes\n",
      "total_backward_count 724460 real_backward_count 97622  13.475%\n",
      "lif layer 2 self.abs_max_v: 5771.5\n",
      "lif layer 1 self.abs_max_v: 14774.5\n",
      "lif layer 1 self.abs_max_v: 15109.5\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  1.909910/  2.023166, val:  52.92%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 62.15 seconds, 1.04 minutes\n",
      "total_backward_count 734250 real_backward_count 98789  13.454%\n",
      "fc layer 1 self.abs_max_out: 10864.0\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  1.914082/  2.031186, val:  62.08%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.53 seconds, 1.03 minutes\n",
      "total_backward_count 744040 real_backward_count 99832  13.418%\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  1.940616/  2.031784, val:  60.00%, val_best:  67.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 61.53 seconds, 1.03 minutes\n",
      "total_backward_count 753830 real_backward_count 101026  13.402%\n",
      "lif layer 1 self.abs_max_v: 15270.5\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  1.933696/  2.041072, val:  56.67%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.43 seconds, 1.04 minutes\n",
      "total_backward_count 763620 real_backward_count 102190  13.382%\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  1.949623/  2.046764, val:  54.58%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.34 seconds, 1.02 minutes\n",
      "total_backward_count 773410 real_backward_count 103355  13.364%\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  1.945853/  2.030897, val:  60.83%, val_best:  67.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 61.58 seconds, 1.03 minutes\n",
      "total_backward_count 783200 real_backward_count 104462  13.338%\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  1.936703/  2.033798, val:  60.42%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.82 seconds, 1.03 minutes\n",
      "total_backward_count 792990 real_backward_count 105569  13.313%\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  1.936930/  2.052696, val:  55.42%, val_best:  67.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 61.93 seconds, 1.03 minutes\n",
      "total_backward_count 802780 real_backward_count 106650  13.285%\n",
      "lif layer 1 self.abs_max_v: 15589.5\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  1.957561/  2.052588, val:  57.08%, val_best:  67.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 61.80 seconds, 1.03 minutes\n",
      "total_backward_count 812570 real_backward_count 107824  13.270%\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  1.945952/  2.055156, val:  57.08%, val_best:  67.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 61.17 seconds, 1.02 minutes\n",
      "total_backward_count 822360 real_backward_count 108999  13.254%\n",
      "lif layer 2 self.abs_max_v: 5794.0\n",
      "lif layer 2 self.abs_max_v: 5942.0\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  1.949254/  2.055563, val:  62.92%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 62.08 seconds, 1.03 minutes\n",
      "total_backward_count 832150 real_backward_count 110164  13.238%\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  1.955494/  2.046964, val:  64.17%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 60.91 seconds, 1.02 minutes\n",
      "total_backward_count 841940 real_backward_count 111277  13.217%\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  1.949583/  2.045631, val:  61.67%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 62.19 seconds, 1.04 minutes\n",
      "total_backward_count 851730 real_backward_count 112400  13.197%\n",
      "fc layer 1 self.abs_max_out: 11224.0\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  1.957221/  2.049010, val:  60.42%, val_best:  67.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 61.75 seconds, 1.03 minutes\n",
      "total_backward_count 861520 real_backward_count 113557  13.181%\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  1.943745/  2.031978, val:  62.50%, val_best:  67.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 61.90 seconds, 1.03 minutes\n",
      "total_backward_count 871310 real_backward_count 114675  13.161%\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  1.944213/  2.051993, val:  64.58%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.39 seconds, 1.02 minutes\n",
      "total_backward_count 881100 real_backward_count 115854  13.149%\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  1.952017/  2.052325, val:  57.92%, val_best:  67.08%, tr:  99.39%, tr_best: 100.00%, epoch time: 61.07 seconds, 1.02 minutes\n",
      "total_backward_count 890890 real_backward_count 117035  13.137%\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  1.946133/  2.051513, val:  68.75%, val_best:  68.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 61.15 seconds, 1.02 minutes\n",
      "total_backward_count 900680 real_backward_count 118075  13.110%\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  1.954781/  2.040289, val:  56.67%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.54 seconds, 1.03 minutes\n",
      "total_backward_count 910470 real_backward_count 119173  13.089%\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  1.933343/  2.034041, val:  58.75%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.30 seconds, 1.02 minutes\n",
      "total_backward_count 920260 real_backward_count 120248  13.067%\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  1.924606/  2.044059, val:  59.58%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.66 seconds, 1.03 minutes\n",
      "total_backward_count 930050 real_backward_count 121332  13.046%\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  1.926937/  2.028424, val:  73.75%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.60 seconds, 1.03 minutes\n",
      "total_backward_count 939840 real_backward_count 122388  13.022%\n",
      "fc layer 1 self.abs_max_out: 11305.0\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  1.932102/  2.035833, val:  52.92%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.79 seconds, 1.03 minutes\n",
      "total_backward_count 949630 real_backward_count 123478  13.003%\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  1.917969/  2.013992, val:  64.17%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 60.86 seconds, 1.01 minutes\n",
      "total_backward_count 959420 real_backward_count 124495  12.976%\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  1.919391/  2.043193, val:  53.75%, val_best:  73.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 61.31 seconds, 1.02 minutes\n",
      "total_backward_count 969210 real_backward_count 125541  12.953%\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  1.928066/  2.036413, val:  64.58%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.70 seconds, 1.03 minutes\n",
      "total_backward_count 979000 real_backward_count 126616  12.933%\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  1.945366/  2.044785, val:  65.42%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.13 seconds, 1.02 minutes\n",
      "total_backward_count 988790 real_backward_count 127645  12.909%\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  1.949086/  2.042847, val:  61.25%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.50 seconds, 1.04 minutes\n",
      "total_backward_count 998580 real_backward_count 128754  12.894%\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  1.941868/  2.037320, val:  65.00%, val_best:  73.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 61.87 seconds, 1.03 minutes\n",
      "total_backward_count 1008370 real_backward_count 129831  12.875%\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  1.951718/  2.050895, val:  64.17%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.31 seconds, 1.02 minutes\n",
      "total_backward_count 1018160 real_backward_count 130901  12.857%\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  1.951548/  2.039041, val:  60.83%, val_best:  73.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 61.93 seconds, 1.03 minutes\n",
      "total_backward_count 1027950 real_backward_count 131948  12.836%\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  1.942435/  2.038377, val:  58.33%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.32 seconds, 1.02 minutes\n",
      "total_backward_count 1037740 real_backward_count 133045  12.821%\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  1.941869/  2.042659, val:  44.17%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.98 seconds, 1.03 minutes\n",
      "total_backward_count 1047530 real_backward_count 134054  12.797%\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  1.927263/  2.026106, val:  55.42%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 60.88 seconds, 1.01 minutes\n",
      "total_backward_count 1057320 real_backward_count 135130  12.780%\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  1.927201/  2.034009, val:  59.58%, val_best:  73.75%, tr:  99.39%, tr_best: 100.00%, epoch time: 61.23 seconds, 1.02 minutes\n",
      "total_backward_count 1067110 real_backward_count 136195  12.763%\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  1.931127/  2.033658, val:  59.58%, val_best:  73.75%, tr:  99.28%, tr_best: 100.00%, epoch time: 61.55 seconds, 1.03 minutes\n",
      "total_backward_count 1076900 real_backward_count 137255  12.745%\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  1.937479/  2.027452, val:  62.08%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.51 seconds, 1.03 minutes\n",
      "total_backward_count 1086690 real_backward_count 138335  12.730%\n",
      "lif layer 1 self.abs_max_v: 15717.0\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  1.942503/  2.059765, val:  57.50%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.47 seconds, 1.02 minutes\n",
      "total_backward_count 1096480 real_backward_count 139378  12.711%\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  1.952079/  2.028969, val:  69.17%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.29 seconds, 1.02 minutes\n",
      "total_backward_count 1106270 real_backward_count 140445  12.695%\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  1.942431/  2.045253, val:  58.75%, val_best:  73.75%, tr:  99.49%, tr_best: 100.00%, epoch time: 61.47 seconds, 1.02 minutes\n",
      "total_backward_count 1116060 real_backward_count 141488  12.677%\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  1.945470/  2.043726, val:  56.25%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.18 seconds, 1.02 minutes\n",
      "total_backward_count 1125850 real_backward_count 142528  12.660%\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  1.948996/  2.033884, val:  55.83%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.41 seconds, 1.02 minutes\n",
      "total_backward_count 1135640 real_backward_count 143609  12.646%\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  1.940286/  2.026069, val:  59.58%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.61 seconds, 1.03 minutes\n",
      "total_backward_count 1145430 real_backward_count 144643  12.628%\n",
      "lif layer 1 self.abs_max_v: 15975.0\n",
      "epoch-117 lr=['0.0009766'], tr/val_loss:  1.933074/  2.027415, val:  63.33%, val_best:  73.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 61.60 seconds, 1.03 minutes\n",
      "total_backward_count 1155220 real_backward_count 145677  12.610%\n",
      "epoch-118 lr=['0.0009766'], tr/val_loss:  1.932309/  2.033589, val:  61.67%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.68 seconds, 1.03 minutes\n",
      "total_backward_count 1165010 real_backward_count 146729  12.595%\n",
      "lif layer 1 self.abs_max_v: 16610.5\n",
      "epoch-119 lr=['0.0009766'], tr/val_loss:  1.928569/  2.020002, val:  72.92%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.78 seconds, 1.03 minutes\n",
      "total_backward_count 1174800 real_backward_count 147728  12.575%\n",
      "epoch-120 lr=['0.0009766'], tr/val_loss:  1.925701/  2.025475, val:  67.92%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 60.96 seconds, 1.02 minutes\n",
      "total_backward_count 1184590 real_backward_count 148696  12.553%\n",
      "epoch-121 lr=['0.0009766'], tr/val_loss:  1.919471/  2.030970, val:  57.50%, val_best:  73.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 61.63 seconds, 1.03 minutes\n",
      "total_backward_count 1194380 real_backward_count 149774  12.540%\n",
      "epoch-122 lr=['0.0009766'], tr/val_loss:  1.932165/  2.028914, val:  57.92%, val_best:  73.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 61.41 seconds, 1.02 minutes\n",
      "total_backward_count 1204170 real_backward_count 150847  12.527%\n",
      "epoch-123 lr=['0.0009766'], tr/val_loss:  1.929566/  2.019468, val:  72.50%, val_best:  73.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 61.43 seconds, 1.02 minutes\n",
      "total_backward_count 1213960 real_backward_count 151925  12.515%\n",
      "epoch-124 lr=['0.0009766'], tr/val_loss:  1.933453/  2.017177, val:  67.08%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 60.87 seconds, 1.01 minutes\n",
      "total_backward_count 1223750 real_backward_count 152949  12.498%\n",
      "epoch-125 lr=['0.0009766'], tr/val_loss:  1.931606/  2.027529, val:  62.50%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.26 seconds, 1.02 minutes\n",
      "total_backward_count 1233540 real_backward_count 154004  12.485%\n",
      "epoch-126 lr=['0.0009766'], tr/val_loss:  1.934673/  2.033928, val:  68.33%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.35 seconds, 1.02 minutes\n",
      "total_backward_count 1243330 real_backward_count 155002  12.467%\n",
      "epoch-127 lr=['0.0009766'], tr/val_loss:  1.927359/  2.030573, val:  67.08%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.98 seconds, 1.03 minutes\n",
      "total_backward_count 1253120 real_backward_count 156010  12.450%\n",
      "epoch-128 lr=['0.0009766'], tr/val_loss:  1.941480/  2.035285, val:  64.58%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.06 seconds, 1.02 minutes\n",
      "total_backward_count 1262910 real_backward_count 157034  12.434%\n",
      "epoch-129 lr=['0.0009766'], tr/val_loss:  1.933465/  2.026235, val:  72.50%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 60.76 seconds, 1.01 minutes\n",
      "total_backward_count 1272700 real_backward_count 158015  12.416%\n",
      "epoch-130 lr=['0.0009766'], tr/val_loss:  1.929908/  1.999809, val:  58.33%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.42 seconds, 1.02 minutes\n",
      "total_backward_count 1282490 real_backward_count 159002  12.398%\n",
      "epoch-131 lr=['0.0009766'], tr/val_loss:  1.929206/  2.036626, val:  61.25%, val_best:  73.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 61.49 seconds, 1.02 minutes\n",
      "total_backward_count 1292280 real_backward_count 160102  12.389%\n",
      "epoch-132 lr=['0.0009766'], tr/val_loss:  1.940833/  2.034359, val:  54.58%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.34 seconds, 1.02 minutes\n",
      "total_backward_count 1302070 real_backward_count 161065  12.370%\n",
      "epoch-133 lr=['0.0009766'], tr/val_loss:  1.936495/  2.022867, val:  66.25%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 62.15 seconds, 1.04 minutes\n",
      "total_backward_count 1311860 real_backward_count 162088  12.356%\n",
      "epoch-134 lr=['0.0009766'], tr/val_loss:  1.929998/  2.026778, val:  60.42%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.53 seconds, 1.04 minutes\n",
      "total_backward_count 1321650 real_backward_count 163073  12.339%\n",
      "epoch-135 lr=['0.0009766'], tr/val_loss:  1.936656/  2.031611, val:  68.75%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.24 seconds, 1.02 minutes\n",
      "total_backward_count 1331440 real_backward_count 164118  12.326%\n",
      "epoch-136 lr=['0.0009766'], tr/val_loss:  1.946133/  2.033840, val:  70.83%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.38 seconds, 1.02 minutes\n",
      "total_backward_count 1341230 real_backward_count 165111  12.310%\n",
      "epoch-137 lr=['0.0009766'], tr/val_loss:  1.946410/  2.030529, val:  66.67%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.29 seconds, 1.02 minutes\n",
      "total_backward_count 1351020 real_backward_count 166084  12.293%\n",
      "epoch-138 lr=['0.0009766'], tr/val_loss:  1.942546/  2.025490, val:  81.25%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.94 seconds, 1.03 minutes\n",
      "total_backward_count 1360810 real_backward_count 167076  12.278%\n",
      "epoch-139 lr=['0.0009766'], tr/val_loss:  1.948921/  2.036201, val:  67.92%, val_best:  81.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 61.26 seconds, 1.02 minutes\n",
      "total_backward_count 1370600 real_backward_count 168078  12.263%\n",
      "epoch-140 lr=['0.0009766'], tr/val_loss:  1.951081/  2.033859, val:  69.17%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 60.35 seconds, 1.01 minutes\n",
      "total_backward_count 1380390 real_backward_count 169114  12.251%\n",
      "epoch-141 lr=['0.0009766'], tr/val_loss:  1.941379/  2.036642, val:  59.58%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.16 seconds, 1.02 minutes\n",
      "total_backward_count 1390180 real_backward_count 170012  12.229%\n",
      "epoch-142 lr=['0.0009766'], tr/val_loss:  1.953277/  2.034909, val:  65.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.32 seconds, 1.02 minutes\n",
      "total_backward_count 1399970 real_backward_count 171010  12.215%\n",
      "fc layer 1 self.abs_max_out: 11361.0\n",
      "epoch-143 lr=['0.0009766'], tr/val_loss:  1.948539/  2.031040, val:  68.33%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 60.98 seconds, 1.02 minutes\n",
      "total_backward_count 1409760 real_backward_count 171985  12.200%\n",
      "epoch-144 lr=['0.0009766'], tr/val_loss:  1.942393/  2.026093, val:  71.67%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.58 seconds, 1.03 minutes\n",
      "total_backward_count 1419550 real_backward_count 172969  12.185%\n",
      "epoch-145 lr=['0.0009766'], tr/val_loss:  1.939412/  2.021674, val:  61.25%, val_best:  81.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 61.17 seconds, 1.02 minutes\n",
      "total_backward_count 1429340 real_backward_count 173907  12.167%\n",
      "epoch-146 lr=['0.0009766'], tr/val_loss:  1.948269/  2.052783, val:  64.58%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.30 seconds, 1.02 minutes\n",
      "total_backward_count 1439130 real_backward_count 174917  12.154%\n",
      "epoch-147 lr=['0.0009766'], tr/val_loss:  1.941170/  2.048729, val:  62.50%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.52 seconds, 1.03 minutes\n",
      "total_backward_count 1448920 real_backward_count 175855  12.137%\n",
      "epoch-148 lr=['0.0009766'], tr/val_loss:  1.949963/  2.023902, val:  72.92%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.71 seconds, 1.03 minutes\n",
      "total_backward_count 1458710 real_backward_count 176868  12.125%\n",
      "epoch-149 lr=['0.0009766'], tr/val_loss:  1.945821/  2.042519, val:  70.42%, val_best:  81.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 61.56 seconds, 1.03 minutes\n",
      "total_backward_count 1468500 real_backward_count 177858  12.112%\n",
      "fc layer 1 self.abs_max_out: 11572.0\n",
      "lif layer 2 self.abs_max_v: 6010.5\n",
      "epoch-150 lr=['0.0009766'], tr/val_loss:  1.943053/  2.051743, val:  69.17%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 61.63 seconds, 1.03 minutes\n",
      "total_backward_count 1478290 real_backward_count 178799  12.095%\n",
      "lif layer 1 self.abs_max_v: 16631.0\n",
      "epoch-151 lr=['0.0009766'], tr/val_loss:  1.964820/  2.056511, val:  57.08%, val_best:  81.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 61.75 seconds, 1.03 minutes\n",
      "total_backward_count 1488080 real_backward_count 179751  12.079%\n",
      "epoch-152 lr=['0.0009766'], tr/val_loss:  1.952662/  2.039589, val:  64.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.83 seconds, 1.03 minutes\n",
      "total_backward_count 1497870 real_backward_count 180694  12.063%\n",
      "epoch-153 lr=['0.0009766'], tr/val_loss:  1.953345/  2.035644, val:  73.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.29 seconds, 1.02 minutes\n",
      "total_backward_count 1507660 real_backward_count 181729  12.054%\n",
      "epoch-154 lr=['0.0009766'], tr/val_loss:  1.955924/  2.038573, val:  63.75%, val_best:  81.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 61.00 seconds, 1.02 minutes\n",
      "total_backward_count 1517450 real_backward_count 182689  12.039%\n",
      "epoch-155 lr=['0.0009766'], tr/val_loss:  1.953102/  2.050910, val:  68.33%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.80 seconds, 1.03 minutes\n",
      "total_backward_count 1527240 real_backward_count 183657  12.025%\n",
      "epoch-156 lr=['0.0009766'], tr/val_loss:  1.962502/  2.045551, val:  57.92%, val_best:  81.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 61.01 seconds, 1.02 minutes\n",
      "total_backward_count 1537030 real_backward_count 184663  12.014%\n",
      "epoch-157 lr=['0.0009766'], tr/val_loss:  1.945903/  2.032924, val:  64.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.07 seconds, 1.02 minutes\n",
      "total_backward_count 1546820 real_backward_count 185602  11.999%\n",
      "epoch-158 lr=['0.0009766'], tr/val_loss:  1.955338/  2.044351, val:  70.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 61.12 seconds, 1.02 minutes\n",
      "total_backward_count 1556610 real_backward_count 186534  11.983%\n",
      "epoch-159 lr=['0.0009766'], tr/val_loss:  1.952145/  2.044543, val:  69.17%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.36 seconds, 1.02 minutes\n",
      "total_backward_count 1566400 real_backward_count 187507  11.971%\n",
      "epoch-160 lr=['0.0009766'], tr/val_loss:  1.952513/  2.028281, val:  73.75%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 60.16 seconds, 1.00 minutes\n",
      "total_backward_count 1576190 real_backward_count 188401  11.953%\n",
      "epoch-161 lr=['0.0009766'], tr/val_loss:  1.964200/  2.041632, val:  70.42%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 63.04 seconds, 1.05 minutes\n",
      "total_backward_count 1585980 real_backward_count 189365  11.940%\n",
      "epoch-162 lr=['0.0009766'], tr/val_loss:  1.948619/  2.044052, val:  64.58%, val_best:  81.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 63.95 seconds, 1.07 minutes\n",
      "total_backward_count 1595770 real_backward_count 190309  11.926%\n",
      "lif layer 2 self.abs_max_v: 6070.5\n",
      "epoch-163 lr=['0.0009766'], tr/val_loss:  1.960284/  2.057607, val:  70.83%, val_best:  81.25%, tr:  99.39%, tr_best: 100.00%, epoch time: 63.88 seconds, 1.06 minutes\n",
      "total_backward_count 1605560 real_backward_count 191256  11.912%\n",
      "epoch-164 lr=['0.0009766'], tr/val_loss:  1.957053/  2.025321, val:  67.92%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 63.42 seconds, 1.06 minutes\n",
      "total_backward_count 1615350 real_backward_count 192134  11.894%\n",
      "epoch-165 lr=['0.0009766'], tr/val_loss:  1.951849/  2.029187, val:  70.83%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 63.22 seconds, 1.05 minutes\n",
      "total_backward_count 1625140 real_backward_count 193105  11.882%\n",
      "epoch-166 lr=['0.0009766'], tr/val_loss:  1.949243/  2.035859, val:  69.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 63.60 seconds, 1.06 minutes\n",
      "total_backward_count 1634930 real_backward_count 194062  11.870%\n",
      "epoch-167 lr=['0.0009766'], tr/val_loss:  1.959174/  2.049667, val:  59.58%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 63.22 seconds, 1.05 minutes\n",
      "total_backward_count 1644720 real_backward_count 194985  11.855%\n",
      "epoch-168 lr=['0.0009766'], tr/val_loss:  1.953520/  2.061501, val:  63.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 63.20 seconds, 1.05 minutes\n",
      "total_backward_count 1654510 real_backward_count 195896  11.840%\n",
      "fc layer 1 self.abs_max_out: 11648.0\n",
      "epoch-169 lr=['0.0009766'], tr/val_loss:  1.952498/  2.026996, val:  70.42%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 62.86 seconds, 1.05 minutes\n",
      "total_backward_count 1664300 real_backward_count 196818  11.826%\n",
      "lif layer 2 self.abs_max_v: 6211.5\n",
      "epoch-170 lr=['0.0009766'], tr/val_loss:  1.952390/  2.038033, val:  67.50%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 63.93 seconds, 1.07 minutes\n",
      "total_backward_count 1674090 real_backward_count 197712  11.810%\n",
      "lif layer 2 self.abs_max_v: 6412.0\n",
      "epoch-171 lr=['0.0009766'], tr/val_loss:  1.947944/  2.042466, val:  60.83%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 63.20 seconds, 1.05 minutes\n",
      "total_backward_count 1683880 real_backward_count 198618  11.795%\n",
      "epoch-172 lr=['0.0009766'], tr/val_loss:  1.962251/  2.039372, val:  67.50%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 62.32 seconds, 1.04 minutes\n",
      "total_backward_count 1693670 real_backward_count 199582  11.784%\n",
      "epoch-173 lr=['0.0009766'], tr/val_loss:  1.957421/  2.043738, val:  68.75%, val_best:  81.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 62.05 seconds, 1.03 minutes\n",
      "total_backward_count 1703460 real_backward_count 200463  11.768%\n",
      "epoch-174 lr=['0.0009766'], tr/val_loss:  1.951301/  2.036535, val:  70.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.07 seconds, 1.03 minutes\n",
      "total_backward_count 1713250 real_backward_count 201386  11.755%\n",
      "epoch-175 lr=['0.0009766'], tr/val_loss:  1.950859/  2.035302, val:  70.42%, val_best:  81.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 61.78 seconds, 1.03 minutes\n",
      "total_backward_count 1723040 real_backward_count 202285  11.740%\n",
      "epoch-176 lr=['0.0009766'], tr/val_loss:  1.959142/  2.044825, val:  61.25%, val_best:  81.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 62.29 seconds, 1.04 minutes\n",
      "total_backward_count 1732830 real_backward_count 203178  11.725%\n",
      "epoch-177 lr=['0.0009766'], tr/val_loss:  1.960060/  2.036005, val:  71.67%, val_best:  81.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 61.92 seconds, 1.03 minutes\n",
      "total_backward_count 1742620 real_backward_count 204081  11.711%\n",
      "epoch-178 lr=['0.0009766'], tr/val_loss:  1.950020/  2.033303, val:  70.42%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 61.88 seconds, 1.03 minutes\n",
      "total_backward_count 1752410 real_backward_count 204976  11.697%\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        # \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.5, 0.25, 0.125, 0.0625]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [2, 4, 6, 8, 10]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "        \"learning_rate\": {\"values\": [1/128, 1/256, 1/512, 1/1024]}, \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [14]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [-1]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [True]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [-11,-10,-9]},\n",
    "        # \"scale_exp_1w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        # \"scale_exp_2w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        # \"scale_exp_3w\": {\"values\": [-9]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"5\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w + 1,wandb.config.scale_exp_1w + 1]],\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'hpjdvxst'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
