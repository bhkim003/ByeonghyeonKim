{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.7834769413661389\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:32\n",
    "# learning_rate:0.007176761798504128\n",
    "# pre_spike_weight:5.165214142219577\n",
    "# rate_coding:true\n",
    "# TIME_STEP:9\n",
    "# time_step:9\n",
    "# v_decay:0.7834769413661389\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"CIFAR10\"\n",
    "\n",
    "\n",
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.38993471232202725\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.06285718352377828\n",
    "# pre_spike_weight:6.21970124592063\n",
    "# rate_coding:true\n",
    "# TIME_STEP:16\n",
    "# time_step:16\n",
    "# v_decay:0.38993471232202725\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"MNIST\"\n",
    "\n",
    "# BATCH:64\n",
    "# batch_size:64\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.9266077968579136\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.07732456724854177\n",
    "# pre_spike_weight:1.5377416716615555\n",
    "# rate_coding:true\n",
    "# TIME_STEP:7\n",
    "# time_step:7\n",
    "# v_decay:0.9266077968579136\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"FASHION_MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from snntorch import spikegen\n",
    "\n",
    " \n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# my module import\n",
    "from modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class RESERVOIR(nn.Module):\n",
    "    def __init__ (self, TIME_STEP=8, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1,\n",
    "                  FC_RESERVOIR=False):\n",
    "        super(RESERVOIR, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.in_spike_size = in_spike_size\n",
    "        self.in_channel = in_channel\n",
    "        self.receptive_size = receptive_size #3\n",
    "        self.v_init = v_init\n",
    "        self.v_decay = v_decay\n",
    "        self.v_threshold = v_threshold\n",
    "        self.v_reset = v_reset\n",
    "        self.hard_reset = hard_reset\n",
    "        self.pre_spike_weight = pre_spike_weight\n",
    "        self.FC_RESERVOIR = FC_RESERVOIR\n",
    "\n",
    "        self.out_channel = 1\n",
    "\n",
    "        # 파라미터 \n",
    "        if self.FC_RESERVOIR == True:\n",
    "            self.reservoir = nn.Linear(in_features=self.in_channel*self.in_spike_size*self.in_spike_size, out_features=self.in_channel*self.in_spike_size*self.in_spike_size, bias=True)\n",
    "        else:\n",
    "            self.reservoir = nn.Conv2d(in_channels=self.in_channel, out_channels=self.in_channel, \n",
    "                                            kernel_size=self.receptive_size, \n",
    "                                            stride=1, padding=1, groups=self.in_channel)\n",
    "\n",
    "        # kaiming 초기화\n",
    "        nn.init.kaiming_normal_(self.reservoir.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.reservoir.bias, 0)\n",
    "\n",
    "        # membrane potential 초기화\n",
    "        self.v = torch.full((self.in_channel, self.in_spike_size, self.in_spike_size), fill_value=self.v_init, requires_grad=False)\n",
    "\n",
    "        \n",
    "    def forward(self, pre_spike):    \n",
    "        # pre_spike [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        v = torch.full_like(pre_spike[0], fill_value=self.v_init, requires_grad=False)\n",
    "        post_spike = torch.zeros_like(pre_spike[0], requires_grad=False)\n",
    "        # v [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "        # recurrent [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        # timestep 안 맞으면 종료\n",
    "        assert pre_spike.size(0) == self.TIME_STEP, f\"Time step mismatch: {pre_spike.size(0)} vs {self.TIME_STEP}\"\n",
    "\n",
    "        output = []\n",
    "        for t in range (self.TIME_STEP):\n",
    "            # depthwise conv reservoir: pre_spike[t] [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "            # fc conv reservoir: pre_spike[t] [batch_size, in_channel*in_spike_size*in_spike_size]\n",
    "            input_current = self.pre_spike_weight * pre_spike[t]\n",
    "                \n",
    "            recurrent_current = self.reservoir(post_spike)\n",
    "            current = input_current + recurrent_current\n",
    "            # current [batch_size, in_channel, in_spike_size, in_spike_size] # kernel size 3이니까 사이즈 유지\n",
    "            \n",
    "            # decay and itegrate\n",
    "            v = v*self.v_decay + current\n",
    "\n",
    "            # post spike\n",
    "            post_spike = (v >= self.v_threshold).float()\n",
    "\n",
    "            output.append(post_spike)\n",
    "            \n",
    "            #reset\n",
    "            if self.hard_reset: # hard reset\n",
    "                v = (1 - post_spike)*v + post_spike*self.v_reset \n",
    "            else: # soft reset\n",
    "                v = v - post_spike*self.v_threshold\n",
    "\n",
    "        output = torch.stack(output, dim=0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RESERVOIR_NET(nn.Module):\n",
    "    def __init__(self, TIME_STEP=8, CLASS_NUM=10, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1,\n",
    "                 no_reservoir = False, FC_RESERVOIR=False):\n",
    "        super(RESERVOIR_NET, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.no_reservoir = no_reservoir\n",
    "        self.FC_RESERVOIR = FC_RESERVOIR\n",
    "\n",
    "        if self.no_reservoir == False:\n",
    "            self.reservoir = RESERVOIR(TIME_STEP = self.TIME_STEP, in_spike_size=in_spike_size, in_channel=in_channel, receptive_size=receptive_size, v_init=v_init, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight,\n",
    "                                       FC_RESERVOIR=FC_RESERVOIR)\n",
    "        \n",
    "        self.classifier = nn.Linear(in_features=in_channel*in_spike_size*in_spike_size, out_features=CLASS_NUM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.TIME_STEP == x.size(1), f\"Time step mismatch: {x.size(1)} vs {self.TIME_STEP}\"\n",
    "\n",
    "        # x size [batch_size, TIME_STEP, in_channel, in_spike_size, in_spike_size]\n",
    "        x = x.permute(1,0,2,3,4)\n",
    "        # x size [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        if (self.FC_RESERVOIR == True):\n",
    "            x = x.reshape(x.size(0), x.size(1), -1)\n",
    "\n",
    "        if self.no_reservoir == False:\n",
    "            with torch.no_grad():\n",
    "                x = self.reservoir(x) # reservoir weight는 학습 안함\n",
    "\n",
    "        T, B, *spatial_dims = x.shape\n",
    "\n",
    "        x = x.reshape(T * B, -1) # time,batch 축은 합쳐서 FC에 삽입\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        x = x.view(T , B, -1).contiguous() \n",
    "        \n",
    "        x = x.mean(dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(which_data, data_path, rate_coding, BATCH, IMAGE_SIZE, TIME, dvs_duration, dvs_clipping):\n",
    "    if which_data == 'MNIST':\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0,), (1,))])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    elif (which_data == 'CIFAR10'):\n",
    "\n",
    "        if rate_coding :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor()])\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor()])\n",
    "            \n",
    "            transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.RandomHorizontalFlip(),\n",
    "                                                transforms.ToTensor()])\n",
    "                                            # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.ToTensor()])\n",
    "        \n",
    "        else :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            \n",
    "            # assert IMAGE_SIZE == 32, 'OTTT랑 맞짱뜰 때는 32로 ㄱ'\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.RandomCrop(IMAGE_SIZE, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "\n",
    "        trainset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform_train)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform_test)\n",
    "        \n",
    "        \n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        \n",
    "        synapse_conv_in_channels = 3\n",
    "        CLASS_NUM = 10\n",
    "        '''\n",
    "        classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "                'dog', 'frog', 'horse', 'ship', 'truck') \n",
    "        '''\n",
    "\n",
    "\n",
    "    elif (which_data == 'FASHION_MNIST'):\n",
    "\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor()])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "    elif (which_data == 'DVS_GESTURE'):\n",
    "        data_dir = data_path + '/gesture'\n",
    "        transform = None\n",
    "\n",
    "        # # spikingjelly.datasets.dvs128_gesture.DVS128Gesture(root: str, train: bool, use_frame=True, frames_num=10, split_by='number', normalization='max')\n",
    "       \n",
    "        #https://spikingjelly.readthedocs.io/zh-cn/latest/activation_based_en/neuromorphic_datasets.html\n",
    "        # 10ms마다 1개의 timestep하고 싶으면 위의 주소 참고. 근데 timestep이 각각 좀 다를 거임.\n",
    "\n",
    "        if dvs_duration > 0:\n",
    "            resize_shape = (IMAGE_SIZE, IMAGE_SIZE)\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(\n",
    "                data_dir, train=False, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "        else:\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(data_dir, train=False,\n",
    "                                            data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "        \n",
    "        ## 11번째 클래스 배제 ########################################################################\n",
    "        exclude_class = 10\n",
    "        if dvs_duration > 0:\n",
    "            train_file_name = f'modules/dvs_gesture_class_index/train_indices_dvsgesture_duration_{dvs_duration}'\n",
    "            test_file_name = f'modules/dvs_gesture_class_index/test_indices_dvsgesture_duration_{dvs_duration}'\n",
    "            if (os.path.isfile(train_file_name) and os.path.isfile(test_file_name)):\n",
    "                print('\\ndvsgestrue 10 class indices exist. we want to exclude the 11th class\\n')\n",
    "                with open(train_file_name, 'rb') as f:\n",
    "                    train_indices = pickle.load(f)\n",
    "                with open(test_file_name, 'rb') as f:\n",
    "                    test_indices = pickle.load(f)\n",
    "            else:\n",
    "                print('\\ndvsgestrue 10 class indices doesn\\'t exist. we want to exclude the 11th class\\n')\n",
    "                train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "                test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "                with open(train_file_name, 'wb') as f:\n",
    "                    pickle.dump(train_indices, f)\n",
    "                with open(test_file_name, 'wb') as f:\n",
    "                    pickle.dump(test_indices, f)\n",
    "        else:\n",
    "            train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "            test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "        ################################################################################################\n",
    "\n",
    "        # SubsetRandomSampler 생성\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        test_sampler = SequentialSampler(test_indices)\n",
    "\n",
    "        # ([B, T, 2, 128, 128]) \n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH, num_workers=2, sampler=train_sampler, collate_fn=pad_sequence_collate)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=BATCH, num_workers=2, sampler=test_sampler, collate_fn=pad_sequence_collate)\n",
    "        synapse_conv_in_channels = 2\n",
    "        CLASS_NUM = 10\n",
    "        # mapping = { 0 :'Hand Clapping'  1 :'Right Hand Wave'2 :'Left Hand Wave' 3 :'Right Arm CW'   4 :'Right Arm CCW'  5 :'Left Arm CW'    6 :'Left Arm CCW'   7 :'Arm Roll'       8 :'Air Drums'      9 :'Air Guitar'     10:'Other'}\n",
    "\n",
    "    else:\n",
    "        assert False, 'wrong dataset name'\n",
    "\n",
    "\n",
    "    \n",
    "    return train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    iterator = enumerate(train_loader, 0)\n",
    "    for i, data in iterator:\n",
    "    # for i, (inputs, labels) in enumerate(train_loader):\n",
    "        if len(data) == 2:\n",
    "            inputs, labels = data\n",
    "            # 처리 로직 작성\n",
    "        elif len(data) == 3:\n",
    "            inputs, labels, x_len = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # if rate_coding == True:\n",
    "        #     inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        # else:\n",
    "        #     inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        \n",
    "\n",
    "        ###########################################################################################################################        \n",
    "        if (which_data == 'n_tidigits'):\n",
    "            inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "            labels = labels[:, 0, :]\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "        elif (which_data == 'heidelberg'):\n",
    "            inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "            print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "        # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "        # print(labels)\n",
    "            \n",
    "        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        elif rate_coding == True :\n",
    "            inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        else :\n",
    "            inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "        ####################################################################################################################### \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        iter_correct = (predicted == labels).sum().item()\n",
    "        correct += iter_correct\n",
    "        # if i % 100 == 99:\n",
    "        # print(f\"[{i+1}] loss: {running_loss / 100:.3f}\")\n",
    "        # running_loss = 0.0\n",
    "        iter_accuracy = 100 * iter_correct / labels.size(0)\n",
    "        wandb.log({\"iter_accuracy\": iter_accuracy})\n",
    "    tr_accuracy = 100 * correct / total         \n",
    "    wandb.log({\"tr_accuracy\": tr_accuracy})\n",
    "    print(f\"Train Accuracy: {tr_accuracy:.2f}%\")\n",
    "    \n",
    "def test(model, test_loader, criterion, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "    iterator = enumerate(test_loader, 0)\n",
    "    with torch.no_grad():\n",
    "        for i, data in iterator:\n",
    "        # for inputs, labels in test_loader:\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "                \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # if rate_coding == True:\n",
    "            #     inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            # else:\n",
    "            #     inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "\n",
    "        \n",
    "\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'n_tidigits'):\n",
    "                inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "                labels = labels[:, 0, :]\n",
    "                labels = torch.argmax(labels, dim=1)\n",
    "            elif (which_data == 'heidelberg'):\n",
    "                inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "                print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "            # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "            # print(labels)\n",
    "                \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = 100 * correct / total\n",
    "    wandb.log({\"val_accuracy\": val_accuracy})\n",
    "    print(f\"Test loss: {test_loss / len(test_loader):.3f}, Val Accuracy: {val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_path='/data2', which_data='MNIST', gpu = '3',learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=10, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= gpu\n",
    "    # run = wandb.init(project=f'reservoir')\n",
    "\n",
    "    hyperparameters = locals()\n",
    "\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'{which_data}_sweeprun_epoch{EPOCH}'\n",
    "    wandb.run.log_code(\".\", include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"))\n",
    "\n",
    "    train_loader, test_loader, in_channel, CLASS_NUM = data_loader(\n",
    "        which_data=which_data, data_path=data_path, rate_coding=rate_coding, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME=TIME_STEP, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    net = RESERVOIR_NET(TIME_STEP=TIME_STEP, CLASS_NUM=CLASS_NUM, in_spike_size=IMAGE_SIZE, in_channel=in_channel, receptive_size=3, v_init=0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight, \n",
    "                            no_reservoir = no_reservoir, FC_RESERVOIR=FC_RESERVOIR)\n",
    "    net = net.to(device)\n",
    "    wandb.watch(net, log=\"all\", log_freq = 1) #gradient, parameter logging해줌\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "\n",
    "    print(net)\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        train(net, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data)\n",
    "        test(net, test_loader, criterion, device, rate_coding, TIME_STEP, which_data)\n",
    "        wandb.log({\"epoch\": epoch})\n",
    "        # torch.save(net.state_dict(), 'net_save/reservoir_net.pth')\n",
    "        # artifact = wandb.Artifact('model', type='model')\n",
    "        # artifact.add_file('net_save/reservoir_net.pth')\n",
    "        # run.log_artifact(artifact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep 하기 싫을 때\n",
    "# wandb.init(project=f'reservoir')\n",
    "# main(data_path='/data2', which_data='CIFAR10', gpu = '3', learning_rate = 0.0072, BATCH=256, IMAGE_SIZE=32, TIME_STEP=9, EPOCH=50, rate_coding=True, v_decay= 0.78,\n",
    "# v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=5.0, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 6b8la78n\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/6b8la78n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: r3319ejr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCH: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tFC_RESERVOIR: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 28\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 1000000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.09\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_reservoir: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_spike_weight: 5.244667191233999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttime_step: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: MNIST\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240726_221228-r3319ejr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/r3319ejr' target=\"_blank\">classic-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/6b8la78n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/6b8la78n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/6b8la78n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/6b8la78n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/r3319ejr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/r3319ejr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'EPOCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_spike_weight' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'no_reservoir' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'FC_RESERVOIR' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESERVOIR_NET(\n",
      "  (classifier): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1\n"
     ]
    }
   ],
   "source": [
    "# sweep하고싶을 때\n",
    "def sweep_cover(data_path='/data2', which_data='CIFAR10', gpu = '6', learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=3, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False):\n",
    "    \n",
    "    wandb.init(save_code = True)\n",
    "\n",
    "    learning_rate  =  wandb.config.learning_rate\n",
    "    BATCH  =  wandb.config.batch_size\n",
    "    TIME_STEP  =  wandb.config.time_step\n",
    "    v_decay  =  wandb.config.decay\n",
    "    pre_spike_weight  =  wandb.config.pre_spike_weight\n",
    "    which_data  =  wandb.config.which_data\n",
    "    data_path  =  wandb.config.data_path\n",
    "    rate_coding  =  wandb.config.rate_coding\n",
    "    EPOCH  =  wandb.config.EPOCH\n",
    "    IMAGE_SIZE  =  wandb.config.IMAGE_SIZE\n",
    "    dvs_duration  =  wandb.config.dvs_duration\n",
    "    dvs_clipping  =  wandb.config.dvs_clipping\n",
    "    no_reservoir  =  wandb.config.no_reservoir\n",
    "    FC_RESERVOIR  =  wandb.config.FC_RESERVOIR\n",
    "    main(data_path=data_path, which_data=which_data, gpu = gpu, learning_rate = learning_rate, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME_STEP=TIME_STEP, EPOCH=EPOCH, rate_coding=rate_coding, v_decay= v_decay,\n",
    "v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping, no_reservoir = no_reservoir, FC_RESERVOIR=FC_RESERVOIR)\n",
    "\n",
    "\n",
    "\n",
    "which_data_hyper = 'MNIST' # 'MNIST', 'CIFAR10' ', 'FASHION_MNIST', 'DVS_GESTURE'\n",
    "data_path_hyper = '/data2'\n",
    "\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes',\n",
    "    'name': f'{which_data_hyper} ba_reservoir',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_accuracy'},\n",
    "    'parameters': \n",
    "    {\n",
    "        \"learning_rate\": {\"values\": [0.09]},\n",
    "        \"batch_size\": {\"values\": [128, 256]},\n",
    "        \"time_step\": {\"values\": [4,5,6,7,8]},\n",
    "        \"decay\": {\"values\": [0.7]},\n",
    "        \"pre_spike_weight\": {\"min\": 0.5, \"max\": 10.0},\n",
    "        \"which_data\": {\"values\": [which_data_hyper]},\n",
    "        \"data_path\": {\"values\": [data_path_hyper]},\n",
    "        \"rate_coding\": {\"values\": [True, False]},\n",
    "        \"EPOCH\": {\"values\": [3]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [28]},\n",
    "        \"dvs_duration\": {\"values\": [1000000]},\n",
    "        \"dvs_clipping\": {\"values\": [True]},\n",
    "        \"no_reservoir\": {\"values\": [True, False]},\n",
    "        \"FC_RESERVOIR\": {\"values\": [False]},\n",
    "     }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'reservoir')\n",
    "wandb.agent(sweep_id, function=sweep_cover, count=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SAVE하기\n",
    "\n",
    "# # Import\n",
    "# import wandb\n",
    "# # Save your model.\n",
    "# torch.save(model.state_dict(), 'save/to/path/model.pth')\n",
    "# # Save as artifact for version control.\n",
    "# run = wandb.init(project='your-project-name')\n",
    "# artifact = wandb.Artifact('model', type='model')\n",
    "# artifact.add_file('save/to/path/model.pth')\n",
    "# run.log_artifact(artifact)\n",
    "# run.finish()\n",
    "\n",
    "\n",
    "# # LOAD 하기\n",
    "\n",
    "# import wandb\n",
    "# run = wandb.init()\n",
    "\n",
    "\n",
    "# artifact = run.use_artifact('entity/your-project-name/model:v0', type='model')\n",
    "# artifact_dir = artifact.download()\n",
    "\n",
    "\n",
    "# run.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
