{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.7834769413661389\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:32\n",
    "# learning_rate:0.007176761798504128\n",
    "# pre_spike_weight:5.165214142219577\n",
    "# rate_coding:true\n",
    "# TIME_STEP:9\n",
    "# time_step:9\n",
    "# v_decay:0.7834769413661389\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"CIFAR10\"\n",
    "\n",
    "\n",
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.38993471232202725\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.06285718352377828\n",
    "# pre_spike_weight:6.21970124592063\n",
    "# rate_coding:true\n",
    "# TIME_STEP:16\n",
    "# time_step:16\n",
    "# v_decay:0.38993471232202725\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"MNIST\"\n",
    "\n",
    "# BATCH:64\n",
    "# batch_size:64\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.9266077968579136\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.07732456724854177\n",
    "# pre_spike_weight:1.5377416716615555\n",
    "# rate_coding:true\n",
    "# TIME_STEP:7\n",
    "# time_step:7\n",
    "# v_decay:0.9266077968579136\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"FASHION_MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from snntorch import spikegen\n",
    "\n",
    " \n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# my module import\n",
    "from modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class RESERVOIR(nn.Module):\n",
    "    def __init__ (self, TIME_STEP=8, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1,\n",
    "                  FC_RESERVOIR=False):\n",
    "        super(RESERVOIR, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.in_spike_size = in_spike_size\n",
    "        self.in_channel = in_channel\n",
    "        self.receptive_size = receptive_size #3\n",
    "        self.v_init = v_init\n",
    "        self.v_decay = v_decay\n",
    "        self.v_threshold = v_threshold\n",
    "        self.v_reset = v_reset\n",
    "        self.hard_reset = hard_reset\n",
    "        self.pre_spike_weight = pre_spike_weight\n",
    "        self.FC_RESERVOIR = FC_RESERVOIR\n",
    "\n",
    "        self.out_channel = 1\n",
    "\n",
    "        # 파라미터 \n",
    "        if self.FC_RESERVOIR == True:\n",
    "            self.reservoir = nn.Linear(in_features=self.in_channel*self.in_spike_size*self.in_spike_size, out_features=self.in_channel*self.in_spike_size*self.in_spike_size, bias=True)\n",
    "        else:\n",
    "            self.reservoir = nn.Conv2d(in_channels=self.in_channel, out_channels=self.in_channel, \n",
    "                                            kernel_size=self.receptive_size, \n",
    "                                            stride=1, padding=1, groups=self.in_channel)\n",
    "\n",
    "        # kaiming 초기화\n",
    "        nn.init.kaiming_normal_(self.reservoir.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.reservoir.bias, 0)\n",
    "\n",
    "        # membrane potential 초기화\n",
    "        self.v = torch.full((self.in_channel, self.in_spike_size, self.in_spike_size), fill_value=self.v_init, requires_grad=False)\n",
    "\n",
    "        \n",
    "    def forward(self, pre_spike):    \n",
    "        # pre_spike [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        v = torch.full_like(pre_spike[0], fill_value=self.v_init, requires_grad=False)\n",
    "        post_spike = torch.zeros_like(pre_spike[0], requires_grad=False)\n",
    "        # v [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "        # recurrent [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        # timestep 안 맞으면 종료\n",
    "        assert pre_spike.size(0) == self.TIME_STEP, f\"Time step mismatch: {pre_spike.size(0)} vs {self.TIME_STEP}\"\n",
    "\n",
    "        output = []\n",
    "        for t in range (self.TIME_STEP):\n",
    "            # depthwise conv reservoir: pre_spike[t] [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "            # fc conv reservoir: pre_spike[t] [batch_size, in_channel*in_spike_size*in_spike_size]\n",
    "            input_current = self.pre_spike_weight * pre_spike[t]\n",
    "                \n",
    "            recurrent_current = self.reservoir(post_spike)\n",
    "            current = input_current + recurrent_current\n",
    "            # current [batch_size, in_channel, in_spike_size, in_spike_size] # kernel size 3이니까 사이즈 유지\n",
    "            \n",
    "            # decay and itegrate\n",
    "            v = v*self.v_decay + current\n",
    "\n",
    "            # post spike\n",
    "            post_spike = (v >= self.v_threshold).float()\n",
    "\n",
    "            output.append(post_spike)\n",
    "            \n",
    "            #reset\n",
    "            if self.hard_reset: # hard reset\n",
    "                v = (1 - post_spike)*v + post_spike*self.v_reset \n",
    "            else: # soft reset\n",
    "                v = v - post_spike*self.v_threshold\n",
    "\n",
    "        output = torch.stack(output, dim=0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RESERVOIR_NET(nn.Module):\n",
    "    def __init__(self, TIME_STEP=8, CLASS_NUM=10, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1,\n",
    "                 no_reservoir = False, FC_RESERVOIR=False):\n",
    "        super(RESERVOIR_NET, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.no_reservoir = no_reservoir\n",
    "        self.FC_RESERVOIR = FC_RESERVOIR\n",
    "\n",
    "        if self.no_reservoir == False:\n",
    "            self.reservoir = RESERVOIR(TIME_STEP = self.TIME_STEP, in_spike_size=in_spike_size, in_channel=in_channel, receptive_size=receptive_size, v_init=v_init, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight,\n",
    "                                       FC_RESERVOIR=FC_RESERVOIR)\n",
    "        \n",
    "        self.classifier = nn.Linear(in_features=in_channel*in_spike_size*in_spike_size, out_features=CLASS_NUM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.TIME_STEP == x.size(1), f\"Time step mismatch: {x.size(1)} vs {self.TIME_STEP}\"\n",
    "\n",
    "        # x size [batch_size, TIME_STEP, in_channel, in_spike_size, in_spike_size]\n",
    "        x = x.permute(1,0,2,3,4)\n",
    "        # x size [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        if (self.FC_RESERVOIR == True):\n",
    "            x = x.reshape(x.size(0), x.size(1), -1)\n",
    "\n",
    "        if self.no_reservoir == False:\n",
    "            with torch.no_grad():\n",
    "                x = self.reservoir(x) # reservoir weight는 학습 안함\n",
    "\n",
    "        T, B, *spatial_dims = x.shape\n",
    "\n",
    "        x = x.reshape(T * B, -1) # time,batch 축은 합쳐서 FC에 삽입\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        x = x.view(T , B, -1).contiguous() \n",
    "        \n",
    "        x = x.mean(dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(which_data, data_path, rate_coding, BATCH, IMAGE_SIZE, TIME, dvs_duration, dvs_clipping):\n",
    "    if which_data == 'MNIST':\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0,), (1,))])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    elif (which_data == 'CIFAR10'):\n",
    "\n",
    "        if rate_coding :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor()])\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor()])\n",
    "            \n",
    "            transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.RandomHorizontalFlip(),\n",
    "                                                transforms.ToTensor()])\n",
    "                                            # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.ToTensor()])\n",
    "        \n",
    "        else :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            \n",
    "            # assert IMAGE_SIZE == 32, 'OTTT랑 맞짱뜰 때는 32로 ㄱ'\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.RandomCrop(IMAGE_SIZE, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "\n",
    "        trainset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform_train)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform_test)\n",
    "        \n",
    "        \n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        \n",
    "        synapse_conv_in_channels = 3\n",
    "        CLASS_NUM = 10\n",
    "        '''\n",
    "        classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "                'dog', 'frog', 'horse', 'ship', 'truck') \n",
    "        '''\n",
    "\n",
    "\n",
    "    elif (which_data == 'FASHION_MNIST'):\n",
    "\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor()])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "    elif (which_data == 'DVS_GESTURE'):\n",
    "        data_dir = data_path + '/gesture'\n",
    "        transform = None\n",
    "\n",
    "        # # spikingjelly.datasets.dvs128_gesture.DVS128Gesture(root: str, train: bool, use_frame=True, frames_num=10, split_by='number', normalization='max')\n",
    "       \n",
    "        #https://spikingjelly.readthedocs.io/zh-cn/latest/activation_based_en/neuromorphic_datasets.html\n",
    "        # 10ms마다 1개의 timestep하고 싶으면 위의 주소 참고. 근데 timestep이 각각 좀 다를 거임.\n",
    "\n",
    "        if dvs_duration > 0:\n",
    "            resize_shape = (IMAGE_SIZE, IMAGE_SIZE)\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(\n",
    "                data_dir, train=False, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "        else:\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(data_dir, train=False,\n",
    "                                            data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "        \n",
    "        ## 11번째 클래스 배제 ########################################################################\n",
    "        exclude_class = 10\n",
    "        if dvs_duration > 0:\n",
    "            train_file_name = f'modules/dvs_gesture_class_index/train_indices_dvsgesture_duration_{dvs_duration}'\n",
    "            test_file_name = f'modules/dvs_gesture_class_index/test_indices_dvsgesture_duration_{dvs_duration}'\n",
    "            if (os.path.isfile(train_file_name) and os.path.isfile(test_file_name)):\n",
    "                print('\\ndvsgestrue 10 class indices exist. we want to exclude the 11th class\\n')\n",
    "                with open(train_file_name, 'rb') as f:\n",
    "                    train_indices = pickle.load(f)\n",
    "                with open(test_file_name, 'rb') as f:\n",
    "                    test_indices = pickle.load(f)\n",
    "            else:\n",
    "                print('\\ndvsgestrue 10 class indices doesn\\'t exist. we want to exclude the 11th class\\n')\n",
    "                train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "                test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "                with open(train_file_name, 'wb') as f:\n",
    "                    pickle.dump(train_indices, f)\n",
    "                with open(test_file_name, 'wb') as f:\n",
    "                    pickle.dump(test_indices, f)\n",
    "        else:\n",
    "            train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "            test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "        ################################################################################################\n",
    "\n",
    "        # SubsetRandomSampler 생성\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        test_sampler = SequentialSampler(test_indices)\n",
    "\n",
    "        # ([B, T, 2, 128, 128]) \n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH, num_workers=2, sampler=train_sampler, collate_fn=pad_sequence_collate)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=BATCH, num_workers=2, sampler=test_sampler, collate_fn=pad_sequence_collate)\n",
    "        synapse_conv_in_channels = 2\n",
    "        CLASS_NUM = 10\n",
    "        # mapping = { 0 :'Hand Clapping'  1 :'Right Hand Wave'2 :'Left Hand Wave' 3 :'Right Arm CW'   4 :'Right Arm CCW'  5 :'Left Arm CW'    6 :'Left Arm CCW'   7 :'Arm Roll'       8 :'Air Drums'      9 :'Air Guitar'     10:'Other'}\n",
    "\n",
    "    else:\n",
    "        assert False, 'wrong dataset name'\n",
    "\n",
    "\n",
    "    \n",
    "    return train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    iterator = enumerate(train_loader, 0)\n",
    "    for i, data in iterator:\n",
    "    # for i, (inputs, labels) in enumerate(train_loader):\n",
    "        if len(data) == 2:\n",
    "            inputs, labels = data\n",
    "            # 처리 로직 작성\n",
    "        elif len(data) == 3:\n",
    "            inputs, labels, x_len = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # if rate_coding == True:\n",
    "        #     inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        # else:\n",
    "        #     inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        \n",
    "\n",
    "        ###########################################################################################################################        \n",
    "        if (which_data == 'n_tidigits'):\n",
    "            inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "            labels = labels[:, 0, :]\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "        elif (which_data == 'heidelberg'):\n",
    "            inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "            print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "        # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "        # print(labels)\n",
    "            \n",
    "        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        elif rate_coding == True :\n",
    "            inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        else :\n",
    "            inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "        ####################################################################################################################### \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        iter_correct = (predicted == labels).sum().item()\n",
    "        correct += iter_correct\n",
    "        # if i % 100 == 99:\n",
    "        # print(f\"[{i+1}] loss: {running_loss / 100:.3f}\")\n",
    "        # running_loss = 0.0\n",
    "        iter_accuracy = 100 * iter_correct / labels.size(0)\n",
    "        wandb.log({\"iter_accuracy\": iter_accuracy})\n",
    "    tr_accuracy = 100 * correct / total         \n",
    "    wandb.log({\"tr_accuracy\": tr_accuracy})\n",
    "    print(f\"Train Accuracy: {tr_accuracy:.2f}%\")\n",
    "    \n",
    "def test(model, test_loader, criterion, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "    iterator = enumerate(test_loader, 0)\n",
    "    with torch.no_grad():\n",
    "        for i, data in iterator:\n",
    "        # for inputs, labels in test_loader:\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "                \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # if rate_coding == True:\n",
    "            #     inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            # else:\n",
    "            #     inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "\n",
    "        \n",
    "\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'n_tidigits'):\n",
    "                inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "                labels = labels[:, 0, :]\n",
    "                labels = torch.argmax(labels, dim=1)\n",
    "            elif (which_data == 'heidelberg'):\n",
    "                inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "                print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "            # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "            # print(labels)\n",
    "                \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = 100 * correct / total\n",
    "    wandb.log({\"val_accuracy\": val_accuracy})\n",
    "    print(f\"Test loss: {test_loss / len(test_loader):.3f}, Val Accuracy: {val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_path='/data2', which_data='MNIST', gpu = '3',learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=10, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= gpu\n",
    "    # run = wandb.init(project=f'reservoir')\n",
    "\n",
    "    hyperparameters = locals()\n",
    "\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'{which_data}_sweeprun_epoch{EPOCH}'\n",
    "    wandb.run.log_code(\".\", include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"))\n",
    "\n",
    "    train_loader, test_loader, in_channel, CLASS_NUM = data_loader(\n",
    "        which_data=which_data, data_path=data_path, rate_coding=rate_coding, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME=TIME_STEP, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    net = RESERVOIR_NET(TIME_STEP=TIME_STEP, CLASS_NUM=CLASS_NUM, in_spike_size=IMAGE_SIZE, in_channel=in_channel, receptive_size=3, v_init=0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight, \n",
    "                            no_reservoir = no_reservoir, FC_RESERVOIR=FC_RESERVOIR)\n",
    "    net = net.to(device)\n",
    "    wandb.watch(net, log=\"all\", log_freq = 1) #gradient, parameter logging해줌\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "\n",
    "    print(net)\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        train(net, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data)\n",
    "        test(net, test_loader, criterion, device, rate_coding, TIME_STEP, which_data)\n",
    "        wandb.log({\"epoch\": epoch})\n",
    "        # torch.save(net.state_dict(), 'net_save/reservoir_net.pth')\n",
    "        # artifact = wandb.Artifact('model', type='model')\n",
    "        # artifact.add_file('net_save/reservoir_net.pth')\n",
    "        # run.log_artifact(artifact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep 하기 싫을 때\n",
    "# wandb.init(project=f'reservoir')\n",
    "# main(data_path='/data2', which_data='CIFAR10', gpu = '3', learning_rate = 0.0072, BATCH=256, IMAGE_SIZE=32, TIME_STEP=9, EPOCH=50, rate_coding=True, v_decay= 0.78,\n",
    "# v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=5.0, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: zqowa3fy\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/zqowa3fy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: c1e2hxv6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCH: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tFC_RESERVOIR: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 28\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.8836500732945596\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 1000000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.09759183148163286\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_reservoir: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_spike_weight: 6.660148589910747\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttime_step: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: MNIST\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240726_191229-c1e2hxv6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/c1e2hxv6' target=\"_blank\">eager-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/zqowa3fy' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/zqowa3fy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/zqowa3fy' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/zqowa3fy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/c1e2hxv6' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/c1e2hxv6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'EPOCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_spike_weight' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'no_reservoir' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'FC_RESERVOIR' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /data2/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90737c0392f54b0d8e28c7e502e0f376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /data2/MNIST/raw/train-images-idx3-ubyte.gz to /data2/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /data2/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9282ed7018c241ccafd5ec212987a783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /data2/MNIST/raw/train-labels-idx1-ubyte.gz to /data2/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /data2/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc1cc2cf03d498486e43d25382f4d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sweep하고싶을 때\n",
    "def sweep_cover(data_path='/data2', which_data='CIFAR10', gpu = '6', learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=3, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False):\n",
    "    \n",
    "    wandb.init(save_code = True)\n",
    "\n",
    "    learning_rate  =  wandb.config.learning_rate\n",
    "    BATCH  =  wandb.config.batch_size\n",
    "    TIME_STEP  =  wandb.config.time_step\n",
    "    v_decay  =  wandb.config.decay\n",
    "    pre_spike_weight  =  wandb.config.pre_spike_weight\n",
    "    which_data  =  wandb.config.which_data\n",
    "    data_path  =  wandb.config.data_path\n",
    "    rate_coding  =  wandb.config.rate_coding\n",
    "    EPOCH  =  wandb.config.EPOCH\n",
    "    IMAGE_SIZE  =  wandb.config.IMAGE_SIZE\n",
    "    dvs_duration  =  wandb.config.dvs_duration\n",
    "    dvs_clipping  =  wandb.config.dvs_clipping\n",
    "    no_reservoir  =  wandb.config.no_reservoir\n",
    "    FC_RESERVOIR  =  wandb.config.FC_RESERVOIR\n",
    "    main(data_path=data_path, which_data=which_data, gpu = gpu, learning_rate = learning_rate, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME_STEP=TIME_STEP, EPOCH=EPOCH, rate_coding=rate_coding, v_decay= v_decay,\n",
    "v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping, no_reservoir = no_reservoir, FC_RESERVOIR=FC_RESERVOIR)\n",
    "\n",
    "\n",
    "\n",
    "which_data_hyper = 'MNIST' # 'MNIST', 'CIFAR10' ', 'FASHION_MNIST', 'DVS_GESTURE'\n",
    "data_path_hyper = '/data2'\n",
    "\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes',\n",
    "    'name': f'{which_data_hyper} ba_reservoir',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_accuracy'},\n",
    "    'parameters': \n",
    "    {\n",
    "        \"learning_rate\": {\"min\": 0.00001, \"max\": 0.1},\n",
    "        \"batch_size\": {\"values\": [16, 32, 64, 128, 256]},\n",
    "        \"time_step\": {\"values\": [4,5,6,7,8]},\n",
    "        \"decay\": {\"min\": 0.25, \"max\": 1.0},\n",
    "        \"pre_spike_weight\": {\"min\": 0.5, \"max\": 10.0},\n",
    "        \"which_data\": {\"values\": [which_data_hyper]},\n",
    "        \"data_path\": {\"values\": [data_path_hyper]},\n",
    "        \"rate_coding\": {\"values\": [True, False]},\n",
    "        \"EPOCH\": {\"values\": [20]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [28]},\n",
    "        \"dvs_duration\": {\"values\": [1000000]},\n",
    "        \"dvs_clipping\": {\"values\": [True]},\n",
    "        \"no_reservoir\": {\"values\": [True, False]},\n",
    "        \"FC_RESERVOIR\": {\"values\": [False]},\n",
    "     }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'reservoir')\n",
    "wandb.agent(sweep_id, function=sweep_cover, count=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SAVE하기\n",
    "\n",
    "# # Import\n",
    "# import wandb\n",
    "# # Save your model.\n",
    "# torch.save(model.state_dict(), 'save/to/path/model.pth')\n",
    "# # Save as artifact for version control.\n",
    "# run = wandb.init(project='your-project-name')\n",
    "# artifact = wandb.Artifact('model', type='model')\n",
    "# artifact.add_file('save/to/path/model.pth')\n",
    "# run.log_artifact(artifact)\n",
    "# run.finish()\n",
    "\n",
    "\n",
    "# # LOAD 하기\n",
    "\n",
    "# import wandb\n",
    "# run = wandb.init()\n",
    "\n",
    "\n",
    "# artifact = run.use_artifact('entity/your-project-name/model:v0', type='model')\n",
    "# artifact_dir = artifact.download()\n",
    "\n",
    "\n",
    "# run.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
