{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30253/4213678604.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os \n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "\n",
    "                    timestep_sums_threshold = 15,\n",
    "\n",
    "                    loser_encourage_mode = False, # True # False\n",
    "                    \n",
    "                    lif_layer_sg_width2 = None,\n",
    "                    lif_layer_v_threshold2 = None,\n",
    "                    learning_rate2 = None,\n",
    "                    init_scaling = None,\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    if which_data == 'n_tidigits_tonic':\n",
    "        assert merge_polarities == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "    synapse_fc_out_features = 10\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp,\n",
    "                    ANPI_MODE=False,\n",
    "                    lif_layer_sg_width2=lif_layer_sg_width2,\n",
    "                    lif_layer_v_threshold2=lif_layer_v_threshold2,\n",
    "                    init_scaling=init_scaling).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    # # wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    # ###########################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    # class CustomLossFunction(torch.autograd.Function):\n",
    "    #     @staticmethod\n",
    "    #     def forward(ctx, input, target):\n",
    "    #         ctx.save_for_backward(input, target)\n",
    "    #         return F.cross_entropy(input, target)\n",
    "\n",
    "    #     @staticmethod\n",
    "    #     def backward(ctx, grad_output):\n",
    "    #         # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "    #         input, target = ctx.saved_tensors\n",
    "    #         input_argmax = input.argmax(dim=1)\n",
    "    #         input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "    #         target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "    #         # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "    #         return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "    \n",
    "\n",
    "    print(\"ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\")\n",
    "    print(\"ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\")\n",
    "    print(\"ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\")\n",
    "    print(\"ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\")\n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            input, target = ctx.saved_tensors\n",
    "            assert input.shape[0] == 1 and target.shape[0] == 1, \"Batch size must be 1 for this custom loss function.\"\n",
    "            batch_size, num_classes = input.shape\n",
    "\n",
    "            target_0 = [0,1,2,3,4]\n",
    "            target_1 = [5,6,7,8,9]\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "\n",
    "            if (target.item() == 0) and (input_argmax.item() in target_0) or \\\n",
    "                (target.item() == 1) and (input_argmax.item() in target_1):\n",
    "                return input_one_hot - input_one_hot, None  \n",
    "            else:\n",
    "                if target.item() == 0:\n",
    "                    input_slice = input[:, 0:5]\n",
    "                    if loser_encourage_mode:\n",
    "                        input_argmin = input_slice.argmin(dim=1)\n",
    "                    else:\n",
    "                        input_argmin = input_slice.argmax(dim=1)\n",
    "                elif target.item() == 1:\n",
    "                    input_slice = input[:, 5:10] \n",
    "                    if loser_encourage_mode:\n",
    "                        input_argmin = input_slice.argmin(dim=1) + 5\n",
    "                    else:\n",
    "                        input_argmin = input_slice.argmax(dim=1) + 5\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected target: {target.item()}\")\n",
    "\n",
    "                # gradient Î∞©Ìñ•ÏùÑ argmin Ï™ΩÏúºÎ°ú\n",
    "                modified_target_one_hot = torch.zeros_like(input).scatter_(1, input_argmin.unsqueeze(1), 1.0)\n",
    "\n",
    "                return input_one_hot - modified_target_one_hot, None\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "            self.additional_dw_weight = 1.0\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                # lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        lr = learning_rate\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        lr = learning_rate2\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        lr = 1.0\n",
    "\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                    \n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                            \n",
    "                    \n",
    "                    dw = dw * self.additional_dw_weight\n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    max_activation_accul = 0\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "        # optimizer.additional_dw_weight = 1.0 if epoch % 2 ==0 else 0.0\n",
    "        optimizer.additional_dw_weight = 1.0\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        # if epoch %2 == 0:\n",
    "        #     iterator = enumerate(train_loader, 0)\n",
    "        # else:\n",
    "        #     iterator = enumerate(test_loader, 0)\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        train_spike_distribution = []\n",
    "        train_predicted_distribution = []\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "                \n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                # inputs: [Batch, Time, Channel, Height, Width]\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif (which_data == 'n_tidigits_tonic'):\n",
    "                inputs = inputs.unsqueeze(-1)\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "                # labels = torch.tensor(labels) \n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "\n",
    "                            \n",
    "            if i == 1:\n",
    "                # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                        \n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            \n",
    "            \n",
    "                        \n",
    "            ## Îç∞Ïù¥ÌÑ∞ÎßàÎã§ TIMESTEPSÎã§Î•¥Îã§ ########################################################\n",
    "            hetero_timesteps = True\n",
    "            if hetero_timesteps == True:\n",
    "                assert real_batch == 1\n",
    "                this_data_timesteps = inputs.shape[0]\n",
    "                TIME = this_data_timesteps//temporal_filter\n",
    "                net.module.change_timesteps(TIME) # netÏóê TIME ÏÑ§Ï†ï\n",
    "            ## Îç∞Ïù¥ÌÑ∞ÎßàÎã§ TIMESTEPSÎã§Î•¥Îã§ ########################################################\n",
    "            \n",
    "\n",
    "            \n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    # inputs # [Time, Batch, Channel, Height, Width]\n",
    "                    # inputs # [Batch, Channel, Height,Time, Width]\n",
    "                    # inputs # [Batch, Channel, Height,Time * Width]\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "            \n",
    "            # if hetero_timesteps == True:\n",
    "            #     assert real_batch == 1\n",
    "            #     # inputs # [Time, Batch, Channel, Height, Width]\n",
    "            #     # inputs timestpeÎ≥ÑÎ°ú sumÍ∞íÏù¥ 10ÎØ∏ÎßåÏùº Ïãú Ï†úÏô∏\n",
    "            #     # time stepÎ≥Ñ Ìï© Í≥ÑÏÇ∞: shape = [T]\n",
    "            #     timestep_sums = inputs.sum(dim=(1,2,3,4))  # sum over (B, C, H, W)\n",
    "\n",
    "            #     # 10 Ïù¥ÏÉÅÏù∏ ÌÉÄÏûÑÏä§ÌÖùÎßå ÏÑ†ÌÉù\n",
    "            #     valid_timesteps = timestep_sums >= timestep_sums_threshold\n",
    "            #     assert valid_timesteps.sum().item() != 0, \"No valid timesteps found. Check your data preprocessing.\"\n",
    "\n",
    "            #     # Ìï¥Îãπ ÌÉÄÏûÑÏä§ÌÖùÎßå Ï∂îÏ∂ú\n",
    "            #     inputs = inputs[valid_timesteps]\n",
    "            #     TIME = inputs.shape[0] # validÌïú time stepÏùò Í∞úÏàò\n",
    "            #     net.module.change_timesteps(TIME) # netÏóê TIME ÏÑ§Ï†ï\n",
    "            train_spike_distribution.append(TIME)\n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device).to(torch.float)\n",
    "            labels = labels.to(device).to(torch.long)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            bp_timestep = random.randint(0, TIME - 1)  # 0 ~ TIME-1 Ï§ë ÌïòÎÇò ÏÑ†ÌÉù\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = ((outputs_one_time.detach()).argmax(dim=1) >= 5).long()\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "                    # optimizer.additional_dw_weight = 1.0 if t == bp_timestep else 0.0\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            \n",
    "            # target_0 = [0,1,2,3,4]\n",
    "            # target_1 = [5,6,7,8,9]\n",
    "            predicted = (predicted >= 5).long()\n",
    "            train_predicted_distribution.append(predicted.cpu().numpy())\n",
    "\n",
    "\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            # if True :\n",
    "            if i == len(train_loader)-1 :\n",
    "                \n",
    "                \n",
    "                train_predicted_distribution = np.array(train_predicted_distribution)\n",
    "                unique_vals, counts = np.unique(train_predicted_distribution, return_counts=True)\n",
    "                for val, count in zip(unique_vals, counts):\n",
    "                    print(f\"train - Value {val}: {count} occurrences\")\n",
    "\n",
    "                print(f'train_spike_distribution.mean {np.mean(train_spike_distribution):.6f}, min {np.min(train_spike_distribution)}, max {np.max(train_spike_distribution)}')\n",
    "\n",
    "\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "                \n",
    "                test_spike_distribution = []\n",
    "                test_predicted_distribution = []\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    # for data_val in train_loader:\n",
    "                    for data_val in test_loader:\n",
    "                    # for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "                            \n",
    "                        ## batch ÌÅ¨Í∏∞ ######################################\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        ###########################################################\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif (which_data == 'n_tidigits_tonic'):\n",
    "                            inputs_val = inputs_val.unsqueeze(-1)\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                            # labels_val = torch.tensor(labels_val)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "                        \n",
    "                        ## Îç∞Ïù¥ÌÑ∞ÎßàÎã§ TIMESTEPSÎã§Î•¥Îã§ ########################################################\n",
    "                        hetero_timesteps = True\n",
    "                        if hetero_timesteps == True:\n",
    "                            assert real_batch == 1\n",
    "                            this_data_timesteps = inputs_val.shape[0]\n",
    "                            TIME = this_data_timesteps//temporal_filter\n",
    "                            net.module.change_timesteps(TIME) # netÏóê TIME ÏÑ§Ï†ï\n",
    "                        ## Îç∞Ïù¥ÌÑ∞ÎßàÎã§ TIMESTEPSÎã§Î•¥Îã§ ########################################################\n",
    "                        \n",
    "\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs_val = (inputs_val != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        \n",
    "                                    \n",
    "                        # if hetero_timesteps == True:\n",
    "                        #     assert real_batch == 1\n",
    "                        #     # inputs_val # [Time, Batch, Channel, Height, Width]\n",
    "                        #     # inputs_val timestpeÎ≥ÑÎ°ú sumÍ∞íÏù¥ 10ÎØ∏ÎßåÏùº Ïãú Ï†úÏô∏\n",
    "                        #     # time stepÎ≥Ñ Ìï© Í≥ÑÏÇ∞: shape = [T]\n",
    "                        #     timestep_sums = inputs_val.sum(dim=(1,2,3,4))  # sum over (B, C, H, W)\n",
    "\n",
    "                        #     # 10 Ïù¥ÏÉÅÏù∏ ÌÉÄÏûÑÏä§ÌÖùÎßå ÏÑ†ÌÉù\n",
    "                        #     valid_timesteps = timestep_sums >= timestep_sums_threshold\n",
    "                        #     assert valid_timesteps.sum().item() != 0, \"No valid timesteps found. Check your data preprocessing.\"\n",
    "\n",
    "                        #     # Ìï¥Îãπ ÌÉÄÏûÑÏä§ÌÖùÎßå Ï∂îÏ∂ú\n",
    "                        #     inputs_val = inputs_val[valid_timesteps]\n",
    "                        #     TIME = inputs_val.shape[0] # validÌïú time stepÏùò Í∞úÏàò\n",
    "                        #     net.module.change_timesteps(TIME) # netÏóê TIME ÏÑ§Ï†ï\n",
    "                        test_spike_distribution.append(TIME)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "                        # ##############################################################################################\n",
    "                        # dvs_visualization(inputs_val, labels_val, TIME, BATCH, my_seed)\n",
    "                        # #####################################################################################################\n",
    "\n",
    "                        inputs_val = inputs_val.to(torch.float).to(device)\n",
    "                        labels_val = labels_val.to(torch.long).to(device)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                            \n",
    "                            if max_activation_accul < outputs.abs().max().item() * TIME * (2**(-scale_exp[2][0])):\n",
    "                                max_activation_accul = outputs.abs().max().item() * TIME * (2**(-scale_exp[2][0]))\n",
    "                                print(f\"max_activation_accul updated: {max_activation_accul:.2f} at epoch {epoch}, iter {i}\")\n",
    "                       \n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                                    \n",
    "                        predicted = (predicted >= 5).long()\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "                        test_predicted_distribution.append(predicted.cpu().numpy())\n",
    "\n",
    "                    print(f'test_spike_distribution.mean {np.mean(test_spike_distribution):.6f}, min {np.min(test_spike_distribution)}, max {np.max(test_spike_distribution)}')\n",
    "\n",
    "                    test_predicted_distribution = np.array(test_predicted_distribution)\n",
    "                    unique_vals, counts = np.unique(test_predicted_distribution, return_counts=True)\n",
    "                    for val, count in zip(unique_vals, counts):\n",
    "                        print(f\"test - Value {val}: {count} occurrences\")\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "\n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    assert val_acc_best > 0.2\n",
    "                elif epoch > 10:\n",
    "                    assert val_acc_best > 0.4\n",
    "                elif epoch > 30:\n",
    "                    assert val_acc_best > 0.5\n",
    "                elif epoch > 100:\n",
    "                    assert val_acc_best > 0.6\n",
    "                    \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "\n",
    "# my_snn_system(  devices = \"5\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 42,\n",
    "#                 TIME = 4, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 8, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "#                 # n_tidigits_tonic 8\n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'n_tidigits_tonic',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','n_tidigits_tonic', 'DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.03125,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 6.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 # pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_20250704_185524_987.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 1/512, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 1000,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 1, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 0, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = 9, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = False, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 8, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[-10,-10],[-10,-10],[-9,-9]], \n",
    "#                 # quantize_bit_list=[],\n",
    "#                 # scale_exp=[], \n",
    "#                 timestep_sums_threshold = 0,\n",
    "\n",
    "#                 loser_encourage_mode = True,\n",
    "                \n",
    "#                 lif_layer_sg_width2 = 4.0,\n",
    "#                 lif_layer_v_threshold2 = 8,\n",
    "#                 learning_rate2 = 8,\n",
    "#                 init_scaling = [1/4,1/4,1/4],\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: qxn8dtpu\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/qxn8dtpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6l9rncdr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloser_encourage_mode: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 30241\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttimestep_sums_threshold: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: n_tidigits_tonic\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251222_124412-6l9rncdr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/6l9rncdr' target=\"_blank\">tough-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/qxn8dtpu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/qxn8dtpu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/qxn8dtpu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/qxn8dtpu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/6l9rncdr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/6l9rncdr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'timestep_sums_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'loser_encourage_mode' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251222_124419_224', 'my_seed': 30241, 'TIME': 4, 'BATCH': 1, 'IMAGE_SIZE': 8, 'which_data': 'n_tidigits_tonic', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 64, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 1, 'dvs_duration': 0, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': False, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 8, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'timestep_sums_threshold': 0, 'lif_layer_sg_width2': 2, 'lif_layer_v_threshold2': 64, 'init_scaling': [0.5, 0.25, 0.0625], 'learning_rate': 1, 'learning_rate2': 1, 'loser_encourage_mode': True} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target word: 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target word: 0\n",
      "\n",
      "\n",
      "\n",
      "train_dataset length = 4032, test_dataset length = 452\n",
      "\n",
      "len(train_loader): 4032 BATCH: 1 train_data_count: 4032\n",
      "len(test_loader): 452 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 4, self.v_threshold 64\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 2, self.v_threshold 64\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=512, out_features=200, TIME=4, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.25, 0.0625])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=64, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=4, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=4, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.25, 0.0625])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=64, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=4, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=4, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.25, 0.0625])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 144,400\n",
      "========================================================\n",
      "\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 708.0\n",
      "lif layer 1 self.abs_max_v: 708.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 514.0\n",
      "lif layer 2 self.abs_max_v: 514.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 32.0\n",
      "fc layer 1 self.abs_max_out: 2079.0\n",
      "lif layer 1 self.abs_max_v: 2079.0\n",
      "fc layer 2 self.abs_max_out: 528.0\n",
      "lif layer 2 self.abs_max_v: 653.0\n",
      "fc layer 3 self.abs_max_out: 43.0\n",
      "lif layer 1 self.abs_max_v: 2313.5\n",
      "fc layer 2 self.abs_max_out: 610.0\n",
      "lif layer 1 self.abs_max_v: 2419.0\n",
      "layer   1  Sparsity: 70.3613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 71.0\n",
      "lif layer 2 self.abs_max_v: 696.0\n",
      "fc layer 1 self.abs_max_out: 2080.0\n",
      "fc layer 2 self.abs_max_out: 619.0\n",
      "fc layer 3 self.abs_max_out: 73.0\n",
      "lif layer 2 self.abs_max_v: 763.5\n",
      "fc layer 3 self.abs_max_out: 76.0\n",
      "fc layer 3 self.abs_max_out: 95.0\n",
      "fc layer 2 self.abs_max_out: 630.0\n",
      "fc layer 1 self.abs_max_out: 2354.0\n",
      "lif layer 2 self.abs_max_v: 898.5\n",
      "fc layer 2 self.abs_max_out: 648.0\n",
      "fc layer 3 self.abs_max_out: 111.0\n",
      "fc layer 1 self.abs_max_out: 2488.0\n",
      "lif layer 1 self.abs_max_v: 2488.0\n",
      "lif layer 1 self.abs_max_v: 2517.5\n",
      "lif layer 1 self.abs_max_v: 2555.0\n",
      "lif layer 1 self.abs_max_v: 2583.0\n",
      "fc layer 2 self.abs_max_out: 665.0\n",
      "lif layer 1 self.abs_max_v: 2680.0\n",
      "fc layer 2 self.abs_max_out: 708.0\n",
      "lif layer 1 self.abs_max_v: 2716.0\n",
      "lif layer 1 self.abs_max_v: 2817.0\n",
      "fc layer 1 self.abs_max_out: 2500.0\n",
      "lif layer 1 self.abs_max_v: 2833.5\n",
      "fc layer 2 self.abs_max_out: 756.0\n",
      "lif layer 2 self.abs_max_v: 938.0\n",
      "fc layer 1 self.abs_max_out: 2509.0\n",
      "fc layer 2 self.abs_max_out: 763.0\n",
      "lif layer 2 self.abs_max_v: 993.5\n",
      "fc layer 2 self.abs_max_out: 775.0\n",
      "fc layer 1 self.abs_max_out: 2581.0\n",
      "fc layer 1 self.abs_max_out: 2734.0\n",
      "lif layer 2 self.abs_max_v: 1024.5\n",
      "lif layer 1 self.abs_max_v: 2834.5\n",
      "lif layer 2 self.abs_max_v: 1149.0\n",
      "lif layer 1 self.abs_max_v: 3027.5\n",
      "lif layer 2 self.abs_max_v: 1283.0\n",
      "fc layer 2 self.abs_max_out: 783.0\n",
      "fc layer 2 self.abs_max_out: 788.0\n",
      "fc layer 2 self.abs_max_out: 840.0\n",
      "lif layer 2 self.abs_max_v: 1337.0\n",
      "fc layer 2 self.abs_max_out: 887.0\n",
      "fc layer 1 self.abs_max_out: 2903.0\n",
      "lif layer 1 self.abs_max_v: 3029.0\n",
      "fc layer 2 self.abs_max_out: 893.0\n",
      "fc layer 2 self.abs_max_out: 908.0\n",
      "lif layer 2 self.abs_max_v: 1381.5\n",
      "lif layer 2 self.abs_max_v: 1387.5\n",
      "lif layer 2 self.abs_max_v: 1437.0\n",
      "fc layer 2 self.abs_max_out: 924.0\n",
      "fc layer 2 self.abs_max_out: 943.0\n",
      "lif layer 2 self.abs_max_v: 1575.5\n",
      "fc layer 2 self.abs_max_out: 951.0\n",
      "lif layer 1 self.abs_max_v: 3096.5\n",
      "lif layer 1 self.abs_max_v: 3205.0\n",
      "fc layer 1 self.abs_max_out: 2915.0\n",
      "fc layer 1 self.abs_max_out: 2942.0\n",
      "fc layer 2 self.abs_max_out: 998.0\n",
      "fc layer 2 self.abs_max_out: 1034.0\n",
      "lif layer 2 self.abs_max_v: 1609.5\n",
      "fc layer 1 self.abs_max_out: 3094.0\n",
      "fc layer 2 self.abs_max_out: 1045.0\n",
      "lif layer 2 self.abs_max_v: 1628.0\n",
      "lif layer 2 self.abs_max_v: 1632.0\n",
      "fc layer 2 self.abs_max_out: 1057.0\n",
      "lif layer 2 self.abs_max_v: 1674.0\n",
      "fc layer 2 self.abs_max_out: 1121.0\n",
      "lif layer 2 self.abs_max_v: 1697.5\n",
      "fc layer 2 self.abs_max_out: 1122.0\n",
      "lif layer 2 self.abs_max_v: 1720.0\n",
      "lif layer 2 self.abs_max_v: 1781.0\n",
      "lif layer 2 self.abs_max_v: 1842.5\n",
      "fc layer 2 self.abs_max_out: 1125.0\n",
      "fc layer 2 self.abs_max_out: 1157.0\n",
      "fc layer 2 self.abs_max_out: 1185.0\n",
      "fc layer 2 self.abs_max_out: 1212.0\n",
      "fc layer 2 self.abs_max_out: 1232.0\n",
      "fc layer 1 self.abs_max_out: 3125.0\n",
      "fc layer 2 self.abs_max_out: 1236.0\n",
      "fc layer 2 self.abs_max_out: 1262.0\n",
      "fc layer 2 self.abs_max_out: 1300.0\n",
      "fc layer 2 self.abs_max_out: 1318.0\n",
      "fc layer 2 self.abs_max_out: 1319.0\n",
      "fc layer 2 self.abs_max_out: 1380.0\n",
      "fc layer 2 self.abs_max_out: 1400.0\n",
      "fc layer 2 self.abs_max_out: 1482.0\n",
      "lif layer 2 self.abs_max_v: 1843.5\n",
      "lif layer 2 self.abs_max_v: 2030.0\n",
      "lif layer 2 self.abs_max_v: 2035.0\n",
      "fc layer 1 self.abs_max_out: 3201.0\n",
      "lif layer 1 self.abs_max_v: 3222.5\n",
      "fc layer 3 self.abs_max_out: 120.0\n",
      "lif layer 1 self.abs_max_v: 3254.0\n",
      "lif layer 2 self.abs_max_v: 2050.0\n",
      "lif layer 1 self.abs_max_v: 3460.0\n",
      "lif layer 1 self.abs_max_v: 3724.0\n",
      "lif layer 2 self.abs_max_v: 2113.5\n",
      "lif layer 2 self.abs_max_v: 2189.0\n",
      "lif layer 2 self.abs_max_v: 2199.5\n",
      "fc layer 3 self.abs_max_out: 125.0\n",
      "fc layer 1 self.abs_max_out: 3293.0\n",
      "lif layer 2 self.abs_max_v: 2243.5\n",
      "lif layer 1 self.abs_max_v: 4102.5\n",
      "lif layer 2 self.abs_max_v: 2261.0\n",
      "lif layer 2 self.abs_max_v: 2263.5\n",
      "lif layer 2 self.abs_max_v: 2294.5\n",
      "lif layer 2 self.abs_max_v: 2342.5\n",
      "fc layer 1 self.abs_max_out: 3314.0\n",
      "fc layer 3 self.abs_max_out: 128.0\n",
      "lif layer 1 self.abs_max_v: 4142.0\n",
      "fc layer 2 self.abs_max_out: 1485.0\n",
      "fc layer 2 self.abs_max_out: 1537.0\n",
      "fc layer 3 self.abs_max_out: 140.0\n",
      "lif layer 2 self.abs_max_v: 2351.5\n",
      "lif layer 2 self.abs_max_v: 2396.5\n",
      "fc layer 1 self.abs_max_out: 3385.0\n",
      "lif layer 1 self.abs_max_v: 4268.5\n",
      "fc layer 2 self.abs_max_out: 1543.0\n",
      "fc layer 2 self.abs_max_out: 1568.0\n",
      "fc layer 1 self.abs_max_out: 3497.0\n",
      "fc layer 2 self.abs_max_out: 1605.0\n",
      "fc layer 3 self.abs_max_out: 148.0\n",
      "fc layer 3 self.abs_max_out: 155.0\n",
      "fc layer 2 self.abs_max_out: 1713.0\n",
      "lif layer 2 self.abs_max_v: 2522.0\n",
      "train - Value 0: 1978 occurrences\n",
      "train - Value 1: 2054 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 204.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 226.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 238.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 257.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 314.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 321.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 326.00 at epoch 0, iter 4031\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 78 occurrences\n",
      "test - Value 1: 374 occurrences\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 46.093605/ 54.744530, val:  61.95%, val_best:  61.95%, tr:  78.67%, tr_best:  78.67%, epoch time: 116.86 seconds, 1.95 minutes\n",
      "layer   1  Sparsity: 79.4576%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.8020%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.5460%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 16128 real_backward_count 4318  26.773%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "layer   1  Sparsity: 87.2070%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 4309.0\n",
      "fc layer 2 self.abs_max_out: 1862.0\n",
      "fc layer 3 self.abs_max_out: 164.0\n",
      "fc layer 3 self.abs_max_out: 172.0\n",
      "lif layer 2 self.abs_max_v: 2529.0\n",
      "lif layer 1 self.abs_max_v: 4451.5\n",
      "fc layer 1 self.abs_max_out: 3509.0\n",
      "fc layer 2 self.abs_max_out: 1885.0\n",
      "fc layer 2 self.abs_max_out: 1903.0\n",
      "fc layer 2 self.abs_max_out: 1904.0\n",
      "fc layer 2 self.abs_max_out: 1954.0\n",
      "fc layer 2 self.abs_max_out: 1983.0\n",
      "fc layer 2 self.abs_max_out: 2209.0\n",
      "fc layer 1 self.abs_max_out: 3610.0\n",
      "lif layer 2 self.abs_max_v: 2583.0\n",
      "fc layer 2 self.abs_max_out: 2268.0\n",
      "fc layer 3 self.abs_max_out: 209.0\n",
      "lif layer 2 self.abs_max_v: 2586.5\n",
      "fc layer 2 self.abs_max_out: 2283.0\n",
      "fc layer 2 self.abs_max_out: 2305.0\n",
      "lif layer 2 self.abs_max_v: 2600.5\n",
      "lif layer 2 self.abs_max_v: 2690.5\n",
      "lif layer 2 self.abs_max_v: 2798.0\n",
      "lif layer 2 self.abs_max_v: 2839.0\n",
      "lif layer 1 self.abs_max_v: 4481.0\n",
      "lif layer 1 self.abs_max_v: 4596.5\n",
      "fc layer 1 self.abs_max_out: 3653.0\n",
      "fc layer 1 self.abs_max_out: 3765.0\n",
      "fc layer 1 self.abs_max_out: 3798.0\n",
      "fc layer 2 self.abs_max_out: 2310.0\n",
      "fc layer 2 self.abs_max_out: 2349.0\n",
      "train - Value 0: 1955 occurrences\n",
      "train - Value 1: 2077 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 376.00 at epoch 1, iter 4031\n",
      "max_activation_accul updated: 424.00 at epoch 1, iter 4031\n",
      "max_activation_accul updated: 443.00 at epoch 1, iter 4031\n",
      "max_activation_accul updated: 444.00 at epoch 1, iter 4031\n",
      "max_activation_accul updated: 483.00 at epoch 1, iter 4031\n",
      "max_activation_accul updated: 558.00 at epoch 1, iter 4031\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 373 occurrences\n",
      "test - Value 1: 79 occurrences\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 55.490967/ 53.339512, val:  66.59%, val_best:  66.59%, tr:  86.14%, tr_best:  86.14%, epoch time: 121.20 seconds, 2.02 minutes\n",
      "layer   1  Sparsity: 79.4539%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.2090%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.6060%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 32256 real_backward_count 7927  24.575%\n",
      "layer   1  Sparsity: 70.5078%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 2416.0\n",
      "fc layer 2 self.abs_max_out: 2494.0\n",
      "fc layer 1 self.abs_max_out: 3806.0\n",
      "fc layer 1 self.abs_max_out: 3835.0\n",
      "fc layer 2 self.abs_max_out: 2511.0\n",
      "fc layer 2 self.abs_max_out: 2536.0\n",
      "lif layer 1 self.abs_max_v: 4604.0\n",
      "fc layer 1 self.abs_max_out: 4003.0\n",
      "lif layer 1 self.abs_max_v: 4713.0\n",
      "lif layer 1 self.abs_max_v: 4849.0\n",
      "lif layer 1 self.abs_max_v: 5135.0\n",
      "fc layer 1 self.abs_max_out: 4023.0\n",
      "fc layer 2 self.abs_max_out: 2592.0\n",
      "lif layer 1 self.abs_max_v: 5168.5\n",
      "fc layer 1 self.abs_max_out: 4073.0\n",
      "fc layer 1 self.abs_max_out: 4079.0\n",
      "train - Value 0: 1982 occurrences\n",
      "train - Value 1: 2050 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 560.00 at epoch 2, iter 4031\n",
      "fc layer 1 self.abs_max_out: 4117.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 133 occurrences\n",
      "test - Value 1: 319 occurrences\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 67.008354/105.855751, val:  76.77%, val_best:  76.77%, tr:  88.19%, tr_best:  88.19%, epoch time: 127.61 seconds, 2.13 minutes\n",
      "layer   1  Sparsity: 79.4576%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.1882%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.2318%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48384 real_backward_count 11472  23.710%\n",
      "layer   1  Sparsity: 87.1582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 218.0\n",
      "fc layer 3 self.abs_max_out: 237.0\n",
      "fc layer 3 self.abs_max_out: 249.0\n",
      "fc layer 3 self.abs_max_out: 251.0\n",
      "fc layer 3 self.abs_max_out: 252.0\n",
      "fc layer 3 self.abs_max_out: 269.0\n",
      "fc layer 3 self.abs_max_out: 299.0\n",
      "fc layer 2 self.abs_max_out: 2636.0\n",
      "fc layer 3 self.abs_max_out: 312.0\n",
      "train - Value 0: 1944 occurrences\n",
      "train - Value 1: 2088 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 603.00 at epoch 3, iter 4031\n",
      "max_activation_accul updated: 874.00 at epoch 3, iter 4031\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 110 occurrences\n",
      "test - Value 1: 342 occurrences\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss: 80.365082/ 75.435898, val:  72.12%, val_best:  76.77%, tr:  90.38%, tr_best:  90.38%, epoch time: 133.64 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 79.4539%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6178%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.7905%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 64512 real_backward_count 14692  22.774%\n",
      "layer   1  Sparsity: 71.2891%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 2669.0\n",
      "fc layer 2 self.abs_max_out: 2718.0\n",
      "fc layer 3 self.abs_max_out: 343.0\n",
      "fc layer 3 self.abs_max_out: 346.0\n",
      "train - Value 0: 1914 occurrences\n",
      "train - Value 1: 2118 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 92 occurrences\n",
      "test - Value 1: 360 occurrences\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss: 87.403130/ 99.969574, val:  70.35%, val_best:  76.77%, tr:  91.62%, tr_best:  91.62%, epoch time: 135.61 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4574%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6693%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.4190%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 80640 real_backward_count 17663  21.904%\n",
      "layer   1  Sparsity: 81.0059%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 2781.0\n",
      "fc layer 2 self.abs_max_out: 2801.0\n",
      "fc layer 2 self.abs_max_out: 2871.0\n",
      "lif layer 2 self.abs_max_v: 2886.0\n",
      "lif layer 1 self.abs_max_v: 5224.5\n",
      "fc layer 1 self.abs_max_out: 4154.0\n",
      "fc layer 2 self.abs_max_out: 2897.0\n",
      "lif layer 2 self.abs_max_v: 2897.0\n",
      "train - Value 0: 1945 occurrences\n",
      "train - Value 1: 2087 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "lif layer 1 self.abs_max_v: 5320.5\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 120 occurrences\n",
      "test - Value 1: 332 occurrences\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss: 93.945831/ 71.753006, val:  74.34%, val_best:  76.77%, tr:  92.68%, tr_best:  92.68%, epoch time: 136.42 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4552%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3502%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.8016%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 96768 real_backward_count 20631  21.320%\n",
      "layer   1  Sparsity: 83.0566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 4162.0\n",
      "fc layer 1 self.abs_max_out: 4182.0\n",
      "fc layer 2 self.abs_max_out: 2926.0\n",
      "lif layer 2 self.abs_max_v: 2926.0\n",
      "fc layer 1 self.abs_max_out: 4232.0\n",
      "fc layer 2 self.abs_max_out: 2972.0\n",
      "lif layer 2 self.abs_max_v: 2972.0\n",
      "fc layer 2 self.abs_max_out: 2989.0\n",
      "lif layer 2 self.abs_max_v: 2989.0\n",
      "train - Value 0: 1978 occurrences\n",
      "train - Value 1: 2054 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 1 self.abs_max_out: 4401.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 166 occurrences\n",
      "test - Value 1: 286 occurrences\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:102.668564/113.496140, val:  81.86%, val_best:  81.86%, tr:  93.85%, tr_best:  93.85%, epoch time: 135.80 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4548%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.8614%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6643%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 112896 real_backward_count 23394  20.722%\n",
      "layer   1  Sparsity: 87.0117%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3053.0\n",
      "lif layer 2 self.abs_max_v: 3053.0\n",
      "fc layer 1 self.abs_max_out: 4431.0\n",
      "fc layer 1 self.abs_max_out: 4535.0\n",
      "fc layer 2 self.abs_max_out: 3109.0\n",
      "lif layer 2 self.abs_max_v: 3109.0\n",
      "lif layer 1 self.abs_max_v: 5323.0\n",
      "fc layer 1 self.abs_max_out: 4722.0\n",
      "train - Value 0: 1988 occurrences\n",
      "train - Value 1: 2044 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "lif layer 1 self.abs_max_v: 5598.5\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 419 occurrences\n",
      "test - Value 1: 33 occurrences\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:100.455307/113.713753, val:  57.30%, val_best:  81.86%, tr:  94.05%, tr_best:  94.05%, epoch time: 133.46 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 79.4539%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.6054%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.1624%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 129024 real_backward_count 26142  20.261%\n",
      "layer   1  Sparsity: 83.6426%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 4746.0\n",
      "fc layer 1 self.abs_max_out: 4937.0\n",
      "fc layer 1 self.abs_max_out: 5209.0\n",
      "fc layer 2 self.abs_max_out: 3168.0\n",
      "lif layer 2 self.abs_max_v: 3168.0\n",
      "train - Value 0: 1993 occurrences\n",
      "train - Value 1: 2039 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "lif layer 1 self.abs_max_v: 5643.5\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 229 occurrences\n",
      "test - Value 1: 223 occurrences\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:107.041512/116.978775, val:  88.72%, val_best:  88.72%, tr:  95.76%, tr_best:  95.76%, epoch time: 132.46 seconds, 2.21 minutes\n",
      "layer   1  Sparsity: 79.4547%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.2079%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.3354%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 145152 real_backward_count 28651  19.739%\n",
      "layer   1  Sparsity: 87.2070%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 5262.0\n",
      "fc layer 1 self.abs_max_out: 5360.0\n",
      "fc layer 1 self.abs_max_out: 5454.0\n",
      "fc layer 2 self.abs_max_out: 3182.0\n",
      "lif layer 2 self.abs_max_v: 3188.0\n",
      "fc layer 2 self.abs_max_out: 3248.0\n",
      "lif layer 2 self.abs_max_v: 3248.0\n",
      "fc layer 2 self.abs_max_out: 3267.0\n",
      "lif layer 2 self.abs_max_v: 3295.0\n",
      "train - Value 0: 1990 occurrences\n",
      "train - Value 1: 2042 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "lif layer 1 self.abs_max_v: 5848.0\n",
      "max_activation_accul updated: 928.00 at epoch 9, iter 4031\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 110 occurrences\n",
      "test - Value 1: 342 occurrences\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:108.490082/ 85.922478, val:  73.89%, val_best:  88.72%, tr:  96.28%, tr_best:  96.28%, epoch time: 134.13 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 79.4539%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.2871%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.6696%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 161280 real_backward_count 30978  19.208%\n",
      "layer   1  Sparsity: 76.2207%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3287.0\n",
      "fc layer 2 self.abs_max_out: 3345.0\n",
      "lif layer 2 self.abs_max_v: 3345.0\n",
      "fc layer 2 self.abs_max_out: 3362.0\n",
      "lif layer 2 self.abs_max_v: 3362.0\n",
      "fc layer 2 self.abs_max_out: 3408.0\n",
      "lif layer 2 self.abs_max_v: 3408.0\n",
      "fc layer 2 self.abs_max_out: 3429.0\n",
      "lif layer 2 self.abs_max_v: 3429.0\n",
      "fc layer 2 self.abs_max_out: 3467.0\n",
      "lif layer 2 self.abs_max_v: 3467.0\n",
      "fc layer 2 self.abs_max_out: 3510.0\n",
      "lif layer 2 self.abs_max_v: 3510.0\n",
      "fc layer 2 self.abs_max_out: 3557.0\n",
      "lif layer 2 self.abs_max_v: 3557.0\n",
      "train - Value 0: 1970 occurrences\n",
      "train - Value 1: 2062 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "lif layer 1 self.abs_max_v: 6085.5\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 322 occurrences\n",
      "test - Value 1: 130 occurrences\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:107.189606/ 47.708984, val:  77.88%, val_best:  88.72%, tr:  96.08%, tr_best:  96.28%, epoch time: 135.00 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4563%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.1728%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.7141%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 177408 real_backward_count 33386  18.819%\n",
      "layer   1  Sparsity: 76.2207%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3568.0\n",
      "lif layer 2 self.abs_max_v: 3568.0\n",
      "fc layer 2 self.abs_max_out: 3584.0\n",
      "lif layer 2 self.abs_max_v: 3584.0\n",
      "fc layer 2 self.abs_max_out: 3602.0\n",
      "lif layer 2 self.abs_max_v: 3602.0\n",
      "fc layer 2 self.abs_max_out: 3627.0\n",
      "lif layer 2 self.abs_max_v: 3627.0\n",
      "fc layer 2 self.abs_max_out: 3640.0\n",
      "lif layer 2 self.abs_max_v: 3640.0\n",
      "fc layer 2 self.abs_max_out: 3735.0\n",
      "lif layer 2 self.abs_max_v: 3735.0\n",
      "fc layer 2 self.abs_max_out: 3763.0\n",
      "lif layer 2 self.abs_max_v: 3763.0\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 3 self.abs_max_out: 350.0\n",
      "max_activation_accul updated: 1097.00 at epoch 11, iter 4031\n",
      "fc layer 3 self.abs_max_out: 353.0\n",
      "fc layer 3 self.abs_max_out: 355.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 96 occurrences\n",
      "test - Value 1: 356 occurrences\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:108.209709/101.320381, val:  70.35%, val_best:  88.72%, tr:  95.98%, tr_best:  96.28%, epoch time: 135.29 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4563%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6959%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.0645%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 193536 real_backward_count 35866  18.532%\n",
      "layer   1  Sparsity: 77.9785%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 27.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 364.0\n",
      "fc layer 3 self.abs_max_out: 368.0\n",
      "fc layer 3 self.abs_max_out: 382.0\n",
      "fc layer 2 self.abs_max_out: 3800.0\n",
      "lif layer 2 self.abs_max_v: 3800.0\n",
      "fc layer 2 self.abs_max_out: 3817.0\n",
      "lif layer 2 self.abs_max_v: 3817.0\n",
      "fc layer 2 self.abs_max_out: 3891.0\n",
      "lif layer 2 self.abs_max_v: 3891.0\n",
      "fc layer 2 self.abs_max_out: 3896.0\n",
      "lif layer 2 self.abs_max_v: 3896.0\n",
      "train - Value 0: 2000 occurrences\n",
      "train - Value 1: 2032 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 279 occurrences\n",
      "test - Value 1: 173 occurrences\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:105.274483/100.617088, val:  83.41%, val_best:  88.72%, tr:  96.53%, tr_best:  96.53%, epoch time: 136.21 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4559%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.2732%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.0839%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 209664 real_backward_count 38210  18.224%\n",
      "layer   1  Sparsity: 78.3691%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1978 occurrences\n",
      "train - Value 1: 2054 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 66 occurrences\n",
      "test - Value 1: 386 occurrences\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:105.986626/176.578156, val:  63.72%, val_best:  88.72%, tr:  96.63%, tr_best:  96.63%, epoch time: 134.96 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4558%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.2098%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.4070%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225792 real_backward_count 40430  17.906%\n",
      "layer   1  Sparsity: 66.1621%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 48.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2003 occurrences\n",
      "train - Value 1: 2029 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 274 occurrences\n",
      "test - Value 1: 178 occurrences\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:107.642677/104.511398, val:  83.19%, val_best:  88.72%, tr:  96.35%, tr_best:  96.63%, epoch time: 135.37 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4586%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.4796%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.6042%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 241920 real_backward_count 42637  17.624%\n",
      "layer   1  Sparsity: 72.5098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 203 occurrences\n",
      "test - Value 1: 249 occurrences\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:111.659904/ 73.795746, val:  87.83%, val_best:  88.72%, tr:  97.35%, tr_best:  97.35%, epoch time: 132.91 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 79.4571%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.6553%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.3500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 258048 real_backward_count 44852  17.381%\n",
      "layer   1  Sparsity: 80.1270%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 6114.5\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 376 occurrences\n",
      "test - Value 1: 76 occurrences\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:108.645393/ 68.090324, val:  66.81%, val_best:  88.72%, tr:  97.32%, tr_best:  97.35%, epoch time: 135.99 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4554%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.8916%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.9806%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274176 real_backward_count 46972  17.132%\n",
      "layer   1  Sparsity: 72.5098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 3938.0\n",
      "fc layer 2 self.abs_max_out: 3952.0\n",
      "lif layer 2 self.abs_max_v: 3952.0\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 128 occurrences\n",
      "test - Value 1: 324 occurrences\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:109.619720/137.047623, val:  76.99%, val_best:  88.72%, tr:  98.19%, tr_best:  98.19%, epoch time: 136.07 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4571%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.9571%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.9232%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 290304 real_backward_count 48981  16.872%\n",
      "layer   1  Sparsity: 89.9414%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 112 occurrences\n",
      "test - Value 1: 340 occurrences\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:123.633499/160.931290, val:  74.34%, val_best:  88.72%, tr:  97.87%, tr_best:  98.19%, epoch time: 135.89 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4532%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.9102%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.4576%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 306432 real_backward_count 50988  16.639%\n",
      "layer   1  Sparsity: 81.4453%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 5497.0\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 193 occurrences\n",
      "test - Value 1: 259 occurrences\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:112.800713/116.233047, val:  86.50%, val_best:  88.72%, tr:  97.62%, tr_best:  98.19%, epoch time: 135.17 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4551%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.4789%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.3623%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 322560 real_backward_count 52996  16.430%\n",
      "layer   1  Sparsity: 86.8652%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 5503.0\n",
      "fc layer 2 self.abs_max_out: 3957.0\n",
      "lif layer 2 self.abs_max_v: 3957.0\n",
      "fc layer 1 self.abs_max_out: 5648.0\n",
      "fc layer 2 self.abs_max_out: 3983.0\n",
      "lif layer 2 self.abs_max_v: 3983.0\n",
      "fc layer 2 self.abs_max_out: 4005.0\n",
      "lif layer 2 self.abs_max_v: 4005.0\n",
      "fc layer 3 self.abs_max_out: 389.0\n",
      "fc layer 2 self.abs_max_out: 4008.0\n",
      "lif layer 2 self.abs_max_v: 4008.0\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 244 occurrences\n",
      "test - Value 1: 208 occurrences\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:128.946426/122.588547, val:  86.73%, val_best:  88.72%, tr:  97.92%, tr_best:  98.19%, epoch time: 133.09 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 79.4539%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.0579%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.1251%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 338688 real_backward_count 54988  16.236%\n",
      "layer   1  Sparsity: 73.6328%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 21.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4064.0\n",
      "lif layer 2 self.abs_max_v: 4064.0\n",
      "lif layer 1 self.abs_max_v: 6119.5\n",
      "train - Value 0: 2000 occurrences\n",
      "train - Value 1: 2032 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 1181.00 at epoch 21, iter 4031\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 63 occurrences\n",
      "test - Value 1: 389 occurrences\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:124.267319/150.876617, val:  63.94%, val_best:  88.72%, tr:  98.66%, tr_best:  98.66%, epoch time: 133.31 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 79.4569%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.3067%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.9620%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 354816 real_backward_count 56856  16.024%\n",
      "layer   1  Sparsity: 87.3047%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 392.0\n",
      "train - Value 0: 1990 occurrences\n",
      "train - Value 1: 2042 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 128 occurrences\n",
      "test - Value 1: 324 occurrences\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:132.528244/157.029144, val:  77.88%, val_best:  88.72%, tr:  97.92%, tr_best:  98.66%, epoch time: 136.34 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4538%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.9055%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.0140%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 370944 real_backward_count 58802  15.852%\n",
      "layer   1  Sparsity: 79.6387%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 28.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 6201.5\n",
      "train - Value 0: 1980 occurrences\n",
      "train - Value 1: 2052 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 62 occurrences\n",
      "test - Value 1: 390 occurrences\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:124.447693/117.586823, val:  63.72%, val_best:  88.72%, tr:  97.72%, tr_best:  98.66%, epoch time: 134.79 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4555%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.1688%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.4216%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 387072 real_backward_count 60768  15.699%\n",
      "layer   1  Sparsity: 71.2891%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1985 occurrences\n",
      "train - Value 1: 2047 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 132 occurrences\n",
      "test - Value 1: 320 occurrences\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:120.718643/144.494492, val:  76.99%, val_best:  88.72%, tr:  97.79%, tr_best:  98.66%, epoch time: 135.85 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4574%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.2630%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.7627%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 403200 real_backward_count 62771  15.568%\n",
      "layer   1  Sparsity: 85.2051%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 1183.00 at epoch 25, iter 4031\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 127 occurrences\n",
      "test - Value 1: 325 occurrences\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:130.215286/163.133636, val:  77.65%, val_best:  88.72%, tr:  98.44%, tr_best:  98.66%, epoch time: 135.87 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4543%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.6723%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.8906%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 419328 real_backward_count 64628  15.412%\n",
      "layer   1  Sparsity: 79.1016%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 31.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2002 occurrences\n",
      "train - Value 1: 2030 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 224 occurrences\n",
      "test - Value 1: 228 occurrences\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:139.647202/137.677811, val:  86.73%, val_best:  88.72%, tr:  98.66%, tr_best:  98.66%, epoch time: 133.98 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 79.4557%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.7139%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.9447%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 435456 real_backward_count 66376  15.243%\n",
      "layer   1  Sparsity: 72.6562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2023 occurrences\n",
      "train - Value 1: 2009 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 174 occurrences\n",
      "test - Value 1: 278 occurrences\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:130.448441/138.803787, val:  84.96%, val_best:  88.72%, tr:  98.39%, tr_best:  98.66%, epoch time: 134.57 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 79.4571%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.7949%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.9887%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 451584 real_backward_count 68120  15.085%\n",
      "layer   1  Sparsity: 81.5430%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 4093.0\n",
      "lif layer 1 self.abs_max_v: 6249.5\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 71 occurrences\n",
      "test - Value 1: 381 occurrences\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:133.768921/175.724701, val:  65.71%, val_best:  88.72%, tr:  99.08%, tr_best:  99.08%, epoch time: 135.34 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4551%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.0913%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.7282%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 467712 real_backward_count 69734  14.910%\n",
      "layer   1  Sparsity: 83.2520%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 32.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 4145.5\n",
      "fc layer 2 self.abs_max_out: 4108.0\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 141 occurrences\n",
      "test - Value 1: 311 occurrences\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:126.909637/143.868607, val:  79.87%, val_best:  88.72%, tr:  98.61%, tr_best:  99.08%, epoch time: 135.20 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4547%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.9804%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.8486%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 483840 real_backward_count 71403  14.758%\n",
      "layer   1  Sparsity: 76.4160%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1997 occurrences\n",
      "train - Value 1: 2035 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 112 occurrences\n",
      "test - Value 1: 340 occurrences\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:133.725647/134.764084, val:  74.78%, val_best:  88.72%, tr:  98.98%, tr_best:  99.08%, epoch time: 136.32 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4563%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.7528%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.0883%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499968 real_backward_count 73032  14.607%\n",
      "layer   1  Sparsity: 75.1465%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 5667.0\n",
      "lif layer 1 self.abs_max_v: 6329.5\n",
      "fc layer 1 self.abs_max_out: 5685.0\n",
      "fc layer 3 self.abs_max_out: 405.0\n",
      "fc layer 3 self.abs_max_out: 416.0\n",
      "fc layer 3 self.abs_max_out: 438.0\n",
      "train - Value 0: 2005 occurrences\n",
      "train - Value 1: 2027 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 1184.00 at epoch 31, iter 4031\n",
      "max_activation_accul updated: 1203.00 at epoch 31, iter 4031\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 112 occurrences\n",
      "test - Value 1: 340 occurrences\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:139.405075/180.766052, val:  73.89%, val_best:  88.72%, tr:  99.28%, tr_best:  99.28%, epoch time: 135.66 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.0505%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.2323%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 516096 real_backward_count 74552  14.445%\n",
      "layer   1  Sparsity: 72.4609%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 5719.0\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 1272.00 at epoch 32, iter 4031\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 102 occurrences\n",
      "test - Value 1: 350 occurrences\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:151.372375/161.771057, val:  72.57%, val_best:  88.72%, tr:  99.21%, tr_best:  99.28%, epoch time: 134.89 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4571%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.5159%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.7288%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 532224 real_backward_count 76088  14.296%\n",
      "layer   1  Sparsity: 76.9043%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 5742.0\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 1 self.abs_max_out: 5766.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 70 occurrences\n",
      "test - Value 1: 382 occurrences\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:146.751450/149.090118, val:  65.49%, val_best:  88.72%, tr:  99.08%, tr_best:  99.28%, epoch time: 133.93 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 79.4562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.2057%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.9670%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548352 real_backward_count 77651  14.161%\n",
      "layer   1  Sparsity: 83.2520%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 5893.0\n",
      "lif layer 2 self.abs_max_v: 4407.0\n",
      "lif layer 1 self.abs_max_v: 6484.5\n",
      "lif layer 1 self.abs_max_v: 6591.0\n",
      "train - Value 0: 2002 occurrences\n",
      "train - Value 1: 2030 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 43 occurrences\n",
      "test - Value 1: 409 occurrences\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:141.369812/222.629684, val:  59.51%, val_best:  88.72%, tr:  99.06%, tr_best:  99.28%, epoch time: 135.08 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4547%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.6708%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.3219%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 564480 real_backward_count 79204  14.031%\n",
      "layer   1  Sparsity: 73.2910%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 94 occurrences\n",
      "test - Value 1: 358 occurrences\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:147.746826/201.950989, val:  70.35%, val_best:  88.72%, tr:  99.33%, tr_best:  99.33%, epoch time: 135.27 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4570%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.6918%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.3629%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 580608 real_backward_count 80689  13.897%\n",
      "layer   1  Sparsity: 77.1973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 6635.0\n",
      "lif layer 1 self.abs_max_v: 6661.0\n",
      "lif layer 2 self.abs_max_v: 4943.0\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 114 occurrences\n",
      "test - Value 1: 338 occurrences\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:143.056808/185.089813, val:  73.45%, val_best:  88.72%, tr:  99.23%, tr_best:  99.33%, epoch time: 136.47 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4561%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.3240%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.8148%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 596736 real_backward_count 82165  13.769%\n",
      "layer   1  Sparsity: 81.8359%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 6694.0\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 134 occurrences\n",
      "test - Value 1: 318 occurrences\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:136.561066/147.516876, val:  78.76%, val_best:  88.72%, tr:  99.26%, tr_best:  99.33%, epoch time: 135.67 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4551%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.2110%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.0747%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 612864 real_backward_count 83681  13.654%\n",
      "layer   1  Sparsity: 78.0273%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 6753.0\n",
      "lif layer 1 self.abs_max_v: 6763.0\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 121 occurrences\n",
      "test - Value 1: 331 occurrences\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:136.203659/185.388565, val:  75.44%, val_best:  88.72%, tr:  99.33%, tr_best:  99.33%, epoch time: 134.33 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 79.4559%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.0240%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.1819%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 628992 real_backward_count 85238  13.552%\n",
      "layer   1  Sparsity: 88.7695%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 6797.0\n",
      "lif layer 2 self.abs_max_v: 5260.0\n",
      "fc layer 2 self.abs_max_out: 4156.0\n",
      "train - Value 0: 1994 occurrences\n",
      "train - Value 1: 2038 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 115 occurrences\n",
      "test - Value 1: 337 occurrences\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:141.202576/151.913361, val:  75.00%, val_best:  88.72%, tr:  98.96%, tr_best:  99.33%, epoch time: 134.89 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4535%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.2785%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.4997%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 645120 real_backward_count 86795  13.454%\n",
      "layer   1  Sparsity: 85.0098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 6997.0\n",
      "fc layer 1 self.abs_max_out: 5913.0\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 90 occurrences\n",
      "test - Value 1: 362 occurrences\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:159.615265/205.050156, val:  69.91%, val_best:  88.72%, tr:  99.31%, tr_best:  99.33%, epoch time: 135.12 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4543%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.5365%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.5622%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 661248 real_backward_count 88205  13.339%\n",
      "layer   1  Sparsity: 79.7852%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 7023.5\n",
      "lif layer 1 self.abs_max_v: 7083.5\n",
      "fc layer 1 self.abs_max_out: 5926.0\n",
      "lif layer 1 self.abs_max_v: 7165.5\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 4165.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 162 occurrences\n",
      "test - Value 1: 290 occurrences\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:146.954376/146.097214, val:  83.63%, val_best:  88.72%, tr:  99.31%, tr_best:  99.33%, epoch time: 136.41 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4555%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.2637%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.3929%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 677376 real_backward_count 89618  13.230%\n",
      "layer   1  Sparsity: 85.7422%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 5929.0\n",
      "lif layer 1 self.abs_max_v: 7225.0\n",
      "fc layer 2 self.abs_max_out: 4168.0\n",
      "fc layer 2 self.abs_max_out: 4195.0\n",
      "fc layer 2 self.abs_max_out: 4206.0\n",
      "train - Value 0: 2004 occurrences\n",
      "train - Value 1: 2028 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 233 occurrences\n",
      "test - Value 1: 219 occurrences\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:145.626007/156.362473, val:  89.16%, val_best:  89.16%, tr:  99.26%, tr_best:  99.33%, epoch time: 135.68 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4542%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.2246%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.7596%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 693504 real_backward_count 91047  13.129%\n",
      "layer   1  Sparsity: 85.2051%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 31.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 446.0\n",
      "fc layer 2 self.abs_max_out: 4325.0\n",
      "fc layer 2 self.abs_max_out: 4356.0\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 1274.00 at epoch 43, iter 4031\n",
      "max_activation_accul updated: 1320.00 at epoch 43, iter 4031\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 52 occurrences\n",
      "test - Value 1: 400 occurrences\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:157.577301/213.176682, val:  61.50%, val_best:  89.16%, tr:  98.98%, tr_best:  99.33%, epoch time: 135.17 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4543%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.1527%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.3568%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 709632 real_backward_count 92459  13.029%\n",
      "layer   1  Sparsity: 83.9355%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 31.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 204 occurrences\n",
      "test - Value 1: 248 occurrences\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:153.465302/110.265671, val:  88.50%, val_best:  89.16%, tr:  99.50%, tr_best:  99.50%, epoch time: 133.07 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 79.4546%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.0076%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.1689%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 725760 real_backward_count 93830  12.929%\n",
      "layer   1  Sparsity: 87.1582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 91 occurrences\n",
      "test - Value 1: 361 occurrences\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:155.749481/167.682724, val:  70.13%, val_best:  89.16%, tr:  99.38%, tr_best:  99.50%, epoch time: 135.09 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4539%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.1658%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.1044%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 741888 real_backward_count 95206  12.833%\n",
      "layer   1  Sparsity: 83.2520%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 54 occurrences\n",
      "test - Value 1: 398 occurrences\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:149.682114/171.373978, val:  61.95%, val_best:  89.16%, tr:  99.40%, tr_best:  99.50%, epoch time: 136.02 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4547%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.2178%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.8294%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 758016 real_backward_count 96740  12.762%\n",
      "layer   1  Sparsity: 73.1445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2002 occurrences\n",
      "train - Value 1: 2030 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 199 occurrences\n",
      "test - Value 1: 253 occurrences\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:153.832916/175.802780, val:  87.39%, val_best:  89.16%, tr:  99.60%, tr_best:  99.60%, epoch time: 135.97 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4570%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.1021%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.6593%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 774144 real_backward_count 98111  12.673%\n",
      "layer   1  Sparsity: 68.9941%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 49.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 130 occurrences\n",
      "test - Value 1: 322 occurrences\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:158.932495/176.484436, val:  77.88%, val_best:  89.16%, tr:  99.75%, tr_best:  99.75%, epoch time: 134.56 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 79.4579%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.0126%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.9005%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 790272 real_backward_count 99409  12.579%\n",
      "layer   1  Sparsity: 81.3477%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 32.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 5763.0\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 162 occurrences\n",
      "test - Value 1: 290 occurrences\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:164.279282/149.192245, val:  84.07%, val_best:  89.16%, tr:  99.53%, tr_best:  99.75%, epoch time: 135.68 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4552%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.8654%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.5044%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 806400 real_backward_count 100750  12.494%\n",
      "layer   1  Sparsity: 88.7207%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 140 occurrences\n",
      "test - Value 1: 312 occurrences\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:163.841110/183.182220, val:  80.09%, val_best:  89.16%, tr:  99.53%, tr_best:  99.75%, epoch time: 134.36 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 79.4535%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.7301%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.7934%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822528 real_backward_count 102038  12.405%\n",
      "layer   1  Sparsity: 77.6367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 49.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 124 occurrences\n",
      "test - Value 1: 328 occurrences\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:161.416870/170.426086, val:  76.55%, val_best:  89.16%, tr:  99.78%, tr_best:  99.78%, epoch time: 134.10 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 79.4560%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.0296%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.9832%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 838656 real_backward_count 103336  12.322%\n",
      "layer   1  Sparsity: 60.1074%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 43.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 19.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4360.0\n",
      "fc layer 2 self.abs_max_out: 4386.0\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 150 occurrences\n",
      "test - Value 1: 302 occurrences\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:153.572281/135.797546, val:  80.97%, val_best:  89.16%, tr:  99.50%, tr_best:  99.78%, epoch time: 136.09 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4599%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.1733%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.0195%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 854784 real_backward_count 104625  12.240%\n",
      "layer   1  Sparsity: 83.9355%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 32.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 5787.0\n",
      "fc layer 2 self.abs_max_out: 4404.0\n",
      "fc layer 2 self.abs_max_out: 4458.0\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 4470.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 141 occurrences\n",
      "test - Value 1: 311 occurrences\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:154.285156/161.527252, val:  79.42%, val_best:  89.16%, tr:  99.38%, tr_best:  99.78%, epoch time: 135.68 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4546%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.9191%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.4620%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 870912 real_backward_count 105943  12.165%\n",
      "layer   1  Sparsity: 67.8711%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 27.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4503.0\n",
      "train - Value 0: 2004 occurrences\n",
      "train - Value 1: 2028 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 188 occurrences\n",
      "test - Value 1: 264 occurrences\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:157.141876/132.169495, val:  86.73%, val_best:  89.16%, tr:  99.65%, tr_best:  99.78%, epoch time: 136.16 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.7598%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.9148%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 887040 real_backward_count 107271  12.093%\n",
      "layer   1  Sparsity: 79.7852%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4536.0\n",
      "lif layer 2 self.abs_max_v: 5848.0\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 168 occurrences\n",
      "test - Value 1: 284 occurrences\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:167.065567/138.360611, val:  84.07%, val_best:  89.16%, tr:  99.75%, tr_best:  99.78%, epoch time: 136.46 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4555%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.8220%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.0459%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 903168 real_backward_count 108555  12.019%\n",
      "layer   1  Sparsity: 85.9375%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 447.0\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 131 occurrences\n",
      "test - Value 1: 321 occurrences\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:169.204193/176.780441, val:  77.65%, val_best:  89.16%, tr:  99.38%, tr_best:  99.78%, epoch time: 133.45 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 79.4541%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.0160%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 30.8041%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 919296 real_backward_count 110018  11.968%\n",
      "layer   1  Sparsity: 82.9590%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 26.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 454.0\n",
      "fc layer 2 self.abs_max_out: 4597.0\n",
      "fc layer 3 self.abs_max_out: 455.0\n",
      "fc layer 3 self.abs_max_out: 471.0\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 1327.00 at epoch 57, iter 4031\n",
      "max_activation_accul updated: 1328.00 at epoch 57, iter 4031\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 93 occurrences\n",
      "test - Value 1: 359 occurrences\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:168.657303/186.864655, val:  70.13%, val_best:  89.16%, tr:  99.65%, tr_best:  99.78%, epoch time: 134.57 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 79.4548%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.6236%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 30.0074%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 935424 real_backward_count 111422  11.911%\n",
      "layer   1  Sparsity: 82.1777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 154 occurrences\n",
      "test - Value 1: 298 occurrences\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:161.648880/126.044006, val:  81.86%, val_best:  89.16%, tr:  99.83%, tr_best:  99.83%, epoch time: 136.43 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4550%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.9230%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 32.0574%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 951552 real_backward_count 112823  11.857%\n",
      "layer   1  Sparsity: 87.2559%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 31.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 479.0\n",
      "fc layer 3 self.abs_max_out: 487.0\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 1375.00 at epoch 59, iter 4031\n",
      "max_activation_accul updated: 1400.00 at epoch 59, iter 4031\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 127 occurrences\n",
      "test - Value 1: 325 occurrences\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:168.491638/154.952805, val:  76.77%, val_best:  89.16%, tr:  99.48%, tr_best:  99.83%, epoch time: 136.54 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 79.4538%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.3442%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.6180%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 967680 real_backward_count 114020  11.783%\n",
      "layer   1  Sparsity: 74.8047%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 31.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2005 occurrences\n",
      "train - Value 1: 2027 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 1573.00 at epoch 60, iter 4031\n",
      "max_activation_accul updated: 1611.00 at epoch 60, iter 4031\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 47 occurrences\n",
      "test - Value 1: 405 occurrences\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:173.164215/293.093994, val:  60.40%, val_best:  89.16%, tr:  99.63%, tr_best:  99.83%, epoch time: 136.41 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.4572%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.2632%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 983808 real_backward_count 115208  11.710%\n",
      "layer   1  Sparsity: 74.6094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 492.0\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 145 occurrences\n",
      "test - Value 1: 307 occurrences\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:174.425491/171.384537, val:  79.42%, val_best:  89.16%, tr:  99.75%, tr_best:  99.83%, epoch time: 136.45 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4567%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.4830%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.2088%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 999936 real_backward_count 116457  11.646%\n",
      "layer   1  Sparsity: 92.3340%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 175 occurrences\n",
      "test - Value 1: 277 occurrences\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:179.839859/202.061218, val:  86.06%, val_best:  89.16%, tr:  99.63%, tr_best:  99.83%, epoch time: 133.00 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 79.4527%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.4371%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.2791%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1016064 real_backward_count 117719  11.586%\n",
      "layer   1  Sparsity: 89.1602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 173 occurrences\n",
      "test - Value 1: 279 occurrences\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:185.776001/167.667969, val:  86.06%, val_best:  89.16%, tr:  99.65%, tr_best:  99.83%, epoch time: 136.13 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.3937%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.9063%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1032192 real_backward_count 118957  11.525%\n",
      "layer   1  Sparsity: 75.8301%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 88 occurrences\n",
      "test - Value 1: 364 occurrences\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:164.647537/230.943680, val:  69.03%, val_best:  89.16%, tr:  99.75%, tr_best:  99.83%, epoch time: 135.49 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4564%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.8618%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.1406%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1048320 real_backward_count 120312  11.477%\n",
      "layer   1  Sparsity: 74.0234%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 24.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 127 occurrences\n",
      "test - Value 1: 325 occurrences\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:167.322449/156.895370, val:  77.21%, val_best:  89.16%, tr:  99.58%, tr_best:  99.83%, epoch time: 136.20 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4568%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.2055%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.8477%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1064448 real_backward_count 121588  11.423%\n",
      "layer   1  Sparsity: 62.8906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 45.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 16.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 5933.0\n",
      "train - Value 0: 2003 occurrences\n",
      "train - Value 1: 2029 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 189 occurrences\n",
      "test - Value 1: 263 occurrences\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:182.537109/202.452576, val:  87.39%, val_best:  89.16%, tr:  99.53%, tr_best:  99.83%, epoch time: 135.49 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.0480%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.1943%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1080576 real_backward_count 122874  11.371%\n",
      "layer   1  Sparsity: 58.3984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 45.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 30.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 5967.0\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 80 occurrences\n",
      "test - Value 1: 372 occurrences\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:173.561569/174.008118, val:  67.70%, val_best:  89.16%, tr:  99.75%, tr_best:  99.83%, epoch time: 136.43 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4603%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.8589%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.4869%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096704 real_backward_count 124121  11.318%\n",
      "layer   1  Sparsity: 61.2793%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 42.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 108 occurrences\n",
      "test - Value 1: 344 occurrences\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:155.658905/156.050644, val:  73.45%, val_best:  89.16%, tr:  99.73%, tr_best:  99.83%, epoch time: 134.91 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4596%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.9380%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.5534%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1112832 real_backward_count 125322  11.262%\n",
      "layer   1  Sparsity: 87.5488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 6215.0\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 76 occurrences\n",
      "test - Value 1: 376 occurrences\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:162.602234/204.632339, val:  66.81%, val_best:  89.16%, tr:  99.65%, tr_best:  99.83%, epoch time: 135.73 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4538%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.8965%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.4606%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1128960 real_backward_count 126612  11.215%\n",
      "layer   1  Sparsity: 78.9062%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 150 occurrences\n",
      "test - Value 1: 302 occurrences\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:156.964493/139.045090, val:  81.42%, val_best:  89.16%, tr:  99.68%, tr_best:  99.83%, epoch time: 135.29 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4557%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.4831%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.5767%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145088 real_backward_count 127842  11.164%\n",
      "layer   1  Sparsity: 78.1250%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 6245.0\n",
      "fc layer 1 self.abs_max_out: 5968.0\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 89 occurrences\n",
      "test - Value 1: 363 occurrences\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:157.994354/196.765228, val:  69.69%, val_best:  89.16%, tr:  99.78%, tr_best:  99.83%, epoch time: 135.89 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4559%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.4859%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.2329%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1161216 real_backward_count 129096  11.117%\n",
      "layer   1  Sparsity: 78.3691%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 6348.0\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 190 occurrences\n",
      "test - Value 1: 262 occurrences\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:163.185837/135.415741, val:  87.61%, val_best:  89.16%, tr:  99.80%, tr_best:  99.83%, epoch time: 136.41 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4558%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.4185%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.4671%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1177344 real_backward_count 130386  11.075%\n",
      "layer   1  Sparsity: 75.2441%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 103 occurrences\n",
      "test - Value 1: 349 occurrences\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:171.528748/183.515884, val:  72.79%, val_best:  89.16%, tr:  99.70%, tr_best:  99.83%, epoch time: 137.00 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 79.4565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.6070%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.9897%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1193472 real_backward_count 131649  11.031%\n",
      "layer   1  Sparsity: 75.9766%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 150 occurrences\n",
      "test - Value 1: 302 occurrences\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:178.649460/193.797211, val:  80.53%, val_best:  89.16%, tr:  99.73%, tr_best:  99.83%, epoch time: 132.88 seconds, 2.21 minutes\n",
      "layer   1  Sparsity: 79.4564%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.3484%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.3486%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1209600 real_backward_count 132919  10.989%\n",
      "layer   1  Sparsity: 90.2344%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 84 occurrences\n",
      "test - Value 1: 368 occurrences\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:173.495071/200.289276, val:  68.58%, val_best:  89.16%, tr:  99.55%, tr_best:  99.83%, epoch time: 134.33 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 79.4532%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.5055%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.6262%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1225728 real_backward_count 134165  10.946%\n",
      "layer   1  Sparsity: 80.2734%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 5992.0\n",
      "fc layer 1 self.abs_max_out: 6065.0\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 214 occurrences\n",
      "test - Value 1: 238 occurrences\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:158.586716/116.935677, val:  88.05%, val_best:  89.16%, tr:  99.63%, tr_best:  99.83%, epoch time: 136.28 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4554%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.4240%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.0508%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1241856 real_backward_count 135451  10.907%\n",
      "layer   1  Sparsity: 71.8750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 158 occurrences\n",
      "test - Value 1: 294 occurrences\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:157.816666/130.180695, val:  83.63%, val_best:  89.16%, tr:  99.78%, tr_best:  99.83%, epoch time: 137.57 seconds, 2.29 minutes\n",
      "layer   1  Sparsity: 79.4573%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.6222%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.6699%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1257984 real_backward_count 136680  10.865%\n",
      "layer   1  Sparsity: 79.8340%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 155 occurrences\n",
      "test - Value 1: 297 occurrences\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:157.951996/161.937378, val:  82.08%, val_best:  89.16%, tr:  99.73%, tr_best:  99.83%, epoch time: 136.09 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4555%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.8533%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.3572%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1274112 real_backward_count 137947  10.827%\n",
      "layer   1  Sparsity: 84.8145%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 131 occurrences\n",
      "test - Value 1: 321 occurrences\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:159.941681/178.108810, val:  78.10%, val_best:  89.16%, tr:  99.70%, tr_best:  99.83%, epoch time: 133.44 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 79.4544%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.7362%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.9213%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1290240 real_backward_count 139136  10.784%\n",
      "layer   1  Sparsity: 87.2070%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 148 occurrences\n",
      "test - Value 1: 304 occurrences\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:167.103531/169.331528, val:  80.97%, val_best:  89.16%, tr:  99.83%, tr_best:  99.83%, epoch time: 135.06 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4539%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.2011%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.3396%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1306368 real_backward_count 140308  10.740%\n",
      "layer   1  Sparsity: 82.0801%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 112 occurrences\n",
      "test - Value 1: 340 occurrences\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:164.414368/164.024277, val:  74.34%, val_best:  89.16%, tr:  99.88%, tr_best:  99.88%, epoch time: 135.01 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4550%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.1049%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.5600%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1322496 real_backward_count 141504  10.700%\n",
      "layer   1  Sparsity: 80.0781%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 72 occurrences\n",
      "test - Value 1: 380 occurrences\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:168.768280/232.654541, val:  65.93%, val_best:  89.16%, tr:  99.70%, tr_best:  99.88%, epoch time: 135.36 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4554%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.0515%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.7039%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1338624 real_backward_count 142705  10.661%\n",
      "layer   1  Sparsity: 88.2812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 145 occurrences\n",
      "test - Value 1: 307 occurrences\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:178.115845/150.810196, val:  80.31%, val_best:  89.16%, tr:  99.63%, tr_best:  99.88%, epoch time: 135.20 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4536%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.1684%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.0373%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1354752 real_backward_count 143951  10.626%\n",
      "layer   1  Sparsity: 72.8516%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 30.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 163 occurrences\n",
      "test - Value 1: 289 occurrences\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:164.657272/199.668701, val:  83.85%, val_best:  89.16%, tr:  99.78%, tr_best:  99.88%, epoch time: 135.51 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4571%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.0882%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.2040%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370880 real_backward_count 145128  10.586%\n",
      "layer   1  Sparsity: 77.7832%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 134 occurrences\n",
      "test - Value 1: 318 occurrences\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:171.892212/205.386337, val:  78.32%, val_best:  89.16%, tr:  99.83%, tr_best:  99.88%, epoch time: 135.10 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4560%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.2494%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.8705%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1387008 real_backward_count 146281  10.547%\n",
      "layer   1  Sparsity: 66.6016%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 46.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 31.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6099.0\n",
      "fc layer 1 self.abs_max_out: 6123.0\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 134 occurrences\n",
      "test - Value 1: 318 occurrences\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:165.698288/163.925934, val:  77.88%, val_best:  89.16%, tr:  99.85%, tr_best:  99.88%, epoch time: 134.82 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4585%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.8832%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.1716%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1403136 real_backward_count 147367  10.503%\n",
      "layer   1  Sparsity: 79.2480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6150.0\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 72 occurrences\n",
      "test - Value 1: 380 occurrences\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:166.055908/200.557495, val:  65.93%, val_best:  89.16%, tr:  99.95%, tr_best:  99.95%, epoch time: 135.95 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4556%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.1357%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.7718%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419264 real_backward_count 148566  10.468%\n",
      "layer   1  Sparsity: 73.3887%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 28.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6226.0\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 119 occurrences\n",
      "test - Value 1: 333 occurrences\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:164.310944/180.085052, val:  75.88%, val_best:  89.16%, tr:  99.70%, tr_best:  99.95%, epoch time: 136.34 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4569%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.1391%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.6210%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1435392 real_backward_count 149729  10.431%\n",
      "layer   1  Sparsity: 76.4160%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 156 occurrences\n",
      "test - Value 1: 296 occurrences\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:162.244324/144.116516, val:  82.74%, val_best:  89.16%, tr:  99.73%, tr_best:  99.95%, epoch time: 136.19 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4563%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.3068%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.7477%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1451520 real_backward_count 150929  10.398%\n",
      "layer   1  Sparsity: 82.7148%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 108 occurrences\n",
      "test - Value 1: 344 occurrences\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:171.085602/162.475159, val:  73.01%, val_best:  89.16%, tr:  99.78%, tr_best:  99.95%, epoch time: 135.05 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4549%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.1558%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.3298%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1467648 real_backward_count 152038  10.359%\n",
      "layer   1  Sparsity: 75.0977%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 126 occurrences\n",
      "test - Value 1: 326 occurrences\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:180.815552/155.177322, val:  76.99%, val_best:  89.16%, tr:  99.73%, tr_best:  99.95%, epoch time: 135.58 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.3160%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.2657%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1483776 real_backward_count 153177  10.323%\n",
      "layer   1  Sparsity: 92.0410%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 175 occurrences\n",
      "test - Value 1: 277 occurrences\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:169.956818/176.885925, val:  86.95%, val_best:  89.16%, tr:  99.83%, tr_best:  99.95%, epoch time: 133.62 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 79.4528%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.6171%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.8362%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1499904 real_backward_count 154317  10.288%\n",
      "layer   1  Sparsity: 86.1816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6245.0\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 163 occurrences\n",
      "test - Value 1: 289 occurrences\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:173.702362/167.187103, val:  83.41%, val_best:  89.16%, tr:  99.83%, tr_best:  99.95%, epoch time: 134.75 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4541%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.3483%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.2970%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1516032 real_backward_count 155522  10.258%\n",
      "layer   1  Sparsity: 77.3926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 49.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6362.0\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 63 occurrences\n",
      "test - Value 1: 389 occurrences\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:182.050278/213.970383, val:  63.94%, val_best:  89.16%, tr:  99.80%, tr_best:  99.95%, epoch time: 136.47 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4560%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.9042%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.4978%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1532160 real_backward_count 156667  10.225%\n",
      "layer   1  Sparsity: 77.7832%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2005 occurrences\n",
      "train - Value 1: 2027 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 83 occurrences\n",
      "test - Value 1: 369 occurrences\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:183.296265/195.713974, val:  68.36%, val_best:  89.16%, tr:  99.68%, tr_best:  99.95%, epoch time: 135.62 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4560%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.9628%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.6529%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1548288 real_backward_count 157824  10.193%\n",
      "layer   1  Sparsity: 80.5664%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 93 occurrences\n",
      "test - Value 1: 359 occurrences\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:175.502747/198.897507, val:  70.58%, val_best:  89.16%, tr:  99.80%, tr_best:  99.95%, epoch time: 135.08 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.2258%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.4009%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1564416 real_backward_count 159013  10.164%\n",
      "layer   1  Sparsity: 63.2324%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 47.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 215 occurrences\n",
      "test - Value 1: 237 occurrences\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:184.780090/131.128586, val:  88.27%, val_best:  89.16%, tr:  99.75%, tr_best:  99.95%, epoch time: 134.07 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 79.4592%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.2435%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.1597%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1580544 real_backward_count 160131  10.131%\n",
      "layer   1  Sparsity: 78.7109%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 147 occurrences\n",
      "test - Value 1: 305 occurrences\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:179.436050/145.480896, val:  79.87%, val_best:  89.16%, tr:  99.78%, tr_best:  99.95%, epoch time: 134.67 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 79.4558%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.1953%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.0927%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1596672 real_backward_count 161260  10.100%\n",
      "layer   1  Sparsity: 73.6328%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 28.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 149 occurrences\n",
      "test - Value 1: 303 occurrences\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:180.263611/170.577286, val:  82.08%, val_best:  89.16%, tr:  99.68%, tr_best:  99.95%, epoch time: 136.61 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 79.4569%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.0139%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.6071%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1612800 real_backward_count 162403  10.070%\n",
      "layer   1  Sparsity: 83.2520%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 126 occurrences\n",
      "test - Value 1: 326 occurrences\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:177.746536/200.698471, val:  77.43%, val_best:  89.16%, tr:  99.65%, tr_best:  99.95%, epoch time: 136.42 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4547%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.1467%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.9709%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1628928 real_backward_count 163598  10.043%\n",
      "layer   1  Sparsity: 83.2520%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 31.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 126 occurrences\n",
      "test - Value 1: 326 occurrences\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:187.814651/169.081192, val:  77.88%, val_best:  89.16%, tr:  99.68%, tr_best:  99.95%, epoch time: 135.35 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4547%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.1638%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.9754%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1645056 real_backward_count 164766  10.016%\n",
      "layer   1  Sparsity: 89.7461%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 156 occurrences\n",
      "test - Value 1: 296 occurrences\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:181.607605/173.438416, val:  82.74%, val_best:  89.16%, tr:  99.83%, tr_best:  99.95%, epoch time: 135.59 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4533%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.1275%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.3612%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1661184 real_backward_count 165960   9.990%\n",
      "layer   1  Sparsity: 81.9824%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 25.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 134 occurrences\n",
      "test - Value 1: 318 occurrences\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:183.865799/172.553070, val:  78.76%, val_best:  89.16%, tr:  99.80%, tr_best:  99.95%, epoch time: 135.01 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4550%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.1545%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.1871%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1677312 real_backward_count 167105   9.963%\n",
      "layer   1  Sparsity: 60.6934%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 45.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 30.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 76 occurrences\n",
      "test - Value 1: 376 occurrences\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:179.686569/251.692123, val:  66.81%, val_best:  89.16%, tr:  99.80%, tr_best:  99.95%, epoch time: 134.72 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4598%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.3746%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.2262%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693440 real_backward_count 168187   9.932%\n",
      "layer   1  Sparsity: 84.1309%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 130 occurrences\n",
      "test - Value 1: 322 occurrences\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:180.649078/181.023880, val:  78.76%, val_best:  89.16%, tr:  99.83%, tr_best:  99.95%, epoch time: 136.05 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4545%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.4397%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 32.9736%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1709568 real_backward_count 169301   9.903%\n",
      "layer   1  Sparsity: 86.7188%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 158 occurrences\n",
      "test - Value 1: 294 occurrences\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:185.812439/181.725143, val:  82.30%, val_best:  89.16%, tr:  99.88%, tr_best:  99.95%, epoch time: 135.66 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.4167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.4898%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1725696 real_backward_count 170377   9.873%\n",
      "layer   1  Sparsity: 83.6914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 163 occurrences\n",
      "test - Value 1: 289 occurrences\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:182.742889/162.272491, val:  82.96%, val_best:  89.16%, tr:  99.75%, tr_best:  99.95%, epoch time: 135.75 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4546%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.4456%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.6073%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1741824 real_backward_count 171498   9.846%\n",
      "layer   1  Sparsity: 71.6797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 139 occurrences\n",
      "test - Value 1: 313 occurrences\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:178.430893/169.967789, val:  79.87%, val_best:  89.16%, tr:  99.83%, tr_best:  99.95%, epoch time: 137.27 seconds, 2.29 minutes\n",
      "layer   1  Sparsity: 79.4573%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.2307%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.9163%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1757952 real_backward_count 172563   9.816%\n",
      "layer   1  Sparsity: 72.7051%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 25.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 119 occurrences\n",
      "test - Value 1: 333 occurrences\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:169.715164/166.079361, val:  75.00%, val_best:  89.16%, tr:  99.90%, tr_best:  99.95%, epoch time: 135.90 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4571%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.3174%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.8595%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1774080 real_backward_count 173631   9.787%\n",
      "layer   1  Sparsity: 83.7402%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 31.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 94 occurrences\n",
      "test - Value 1: 358 occurrences\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:171.464615/217.053497, val:  70.80%, val_best:  89.16%, tr:  99.88%, tr_best:  99.95%, epoch time: 134.26 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 79.4546%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.0596%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.5722%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1790208 real_backward_count 174709   9.759%\n",
      "layer   1  Sparsity: 66.1621%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 45.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 123 occurrences\n",
      "test - Value 1: 329 occurrences\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:174.178207/216.868225, val:  77.21%, val_best:  89.16%, tr:  99.65%, tr_best:  99.95%, epoch time: 136.43 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4586%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.8338%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.5416%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1806336 real_backward_count 175809   9.733%\n",
      "layer   1  Sparsity: 76.2207%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 87 occurrences\n",
      "test - Value 1: 365 occurrences\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:169.023727/196.473801, val:  69.25%, val_best:  89.16%, tr:  99.70%, tr_best:  99.95%, epoch time: 136.98 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 79.4563%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.6566%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 32.9894%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1822464 real_backward_count 176894   9.706%\n",
      "layer   1  Sparsity: 80.2734%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 79 occurrences\n",
      "test - Value 1: 373 occurrences\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:178.006104/267.554169, val:  67.48%, val_best:  89.16%, tr:  99.83%, tr_best:  99.95%, epoch time: 135.92 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4554%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.9829%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.7862%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1838592 real_backward_count 178056   9.684%\n",
      "layer   1  Sparsity: 78.1738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 142 occurrences\n",
      "test - Value 1: 310 occurrences\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:181.540619/182.726257, val:  79.20%, val_best:  89.16%, tr:  99.88%, tr_best:  99.95%, epoch time: 135.79 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4559%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.2065%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.2822%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1854720 real_backward_count 179123   9.658%\n",
      "layer   1  Sparsity: 81.0059%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 4610.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 208 occurrences\n",
      "test - Value 1: 244 occurrences\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:179.379929/150.640594, val:  88.50%, val_best:  89.16%, tr:  99.83%, tr_best:  99.95%, epoch time: 133.71 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 79.4552%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.2046%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.2664%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1870848 real_backward_count 180168   9.630%\n",
      "layer   1  Sparsity: 88.2324%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 28.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4613.0\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 109 occurrences\n",
      "test - Value 1: 343 occurrences\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:172.006958/219.856323, val:  73.67%, val_best:  89.16%, tr:  99.88%, tr_best:  99.95%, epoch time: 134.44 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 79.4536%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.5410%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.9056%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1886976 real_backward_count 181221   9.604%\n",
      "layer   1  Sparsity: 85.1562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 157 occurrences\n",
      "test - Value 1: 295 occurrences\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:178.179428/184.510559, val:  82.08%, val_best:  89.16%, tr:  99.88%, tr_best:  99.95%, epoch time: 136.48 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4543%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.5694%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.4310%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1903104 real_backward_count 182278   9.578%\n",
      "layer   1  Sparsity: 69.1406%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 46.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 123 occurrences\n",
      "test - Value 1: 329 occurrences\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:187.629379/196.603516, val:  76.77%, val_best:  89.16%, tr:  99.80%, tr_best:  99.95%, epoch time: 137.83 seconds, 2.30 minutes\n",
      "layer   1  Sparsity: 79.4579%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.3502%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.1522%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1919232 real_backward_count 183313   9.551%\n",
      "layer   1  Sparsity: 70.7031%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 48.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 85 occurrences\n",
      "test - Value 1: 367 occurrences\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:190.302536/212.616577, val:  68.81%, val_best:  89.16%, tr:  99.83%, tr_best:  99.95%, epoch time: 136.38 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4575%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.4826%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.0035%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1935360 real_backward_count 184357   9.526%\n",
      "layer   1  Sparsity: 76.4160%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 101 occurrences\n",
      "test - Value 1: 351 occurrences\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:191.379013/218.606644, val:  72.35%, val_best:  89.16%, tr:  99.78%, tr_best:  99.95%, epoch time: 136.88 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 79.4563%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.8164%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.2632%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1951488 real_backward_count 185365   9.499%\n",
      "layer   1  Sparsity: 77.6367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 98 occurrences\n",
      "test - Value 1: 354 occurrences\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:179.914764/201.229370, val:  71.24%, val_best:  89.16%, tr:  99.93%, tr_best:  99.95%, epoch time: 135.26 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4560%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.4747%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.0521%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1967616 real_backward_count 186350   9.471%\n",
      "layer   1  Sparsity: 81.0547%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6404.0\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 156 occurrences\n",
      "test - Value 1: 296 occurrences\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:186.764847/153.302246, val:  83.63%, val_best:  89.16%, tr:  99.83%, tr_best:  99.95%, epoch time: 136.02 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4552%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.2610%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.2071%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1983744 real_backward_count 187355   9.445%\n",
      "layer   1  Sparsity: 82.2754%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6433.0\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 134 occurrences\n",
      "test - Value 1: 318 occurrences\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:183.472504/185.643524, val:  78.32%, val_best:  89.16%, tr:  99.80%, tr_best:  99.95%, epoch time: 136.61 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 79.4550%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.4140%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.6870%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1999872 real_backward_count 188390   9.420%\n",
      "layer   1  Sparsity: 79.1016%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 26.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6446.0\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 140 occurrences\n",
      "test - Value 1: 312 occurrences\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:180.689545/159.850494, val:  80.09%, val_best:  89.16%, tr:  99.85%, tr_best:  99.95%, epoch time: 137.73 seconds, 2.30 minutes\n",
      "layer   1  Sparsity: 79.4557%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.6457%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.3092%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2016000 real_backward_count 189412   9.395%\n",
      "layer   1  Sparsity: 77.3926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 152 occurrences\n",
      "test - Value 1: 300 occurrences\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:178.484406/177.324615, val:  82.74%, val_best:  89.16%, tr:  99.90%, tr_best:  99.95%, epoch time: 136.15 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4560%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.8283%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.2051%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2032128 real_backward_count 190408   9.370%\n",
      "layer   1  Sparsity: 88.2324%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 109 occurrences\n",
      "test - Value 1: 343 occurrences\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:177.072250/206.908463, val:  74.12%, val_best:  89.16%, tr:  99.83%, tr_best:  99.95%, epoch time: 135.93 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4536%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.5255%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.4955%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2048256 real_backward_count 191439   9.346%\n",
      "layer   1  Sparsity: 78.7109%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 174 occurrences\n",
      "test - Value 1: 278 occurrences\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:171.968170/135.284302, val:  85.40%, val_best:  89.16%, tr:  99.93%, tr_best:  99.95%, epoch time: 135.67 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4558%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.4220%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.2371%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2064384 real_backward_count 192505   9.325%\n",
      "layer   1  Sparsity: 88.1836%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 183 occurrences\n",
      "test - Value 1: 269 occurrences\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:175.841461/156.325394, val:  86.06%, val_best:  89.16%, tr:  99.88%, tr_best:  99.95%, epoch time: 133.34 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 79.4536%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.4175%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.0807%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2080512 real_backward_count 193517   9.301%\n",
      "layer   1  Sparsity: 84.2285%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 152 occurrences\n",
      "test - Value 1: 300 occurrences\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:167.632828/217.740891, val:  82.30%, val_best:  89.16%, tr:  99.83%, tr_best:  99.95%, epoch time: 135.41 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4545%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.5989%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.2194%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2096640 real_backward_count 194548   9.279%\n",
      "layer   1  Sparsity: 76.0254%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 126 occurrences\n",
      "test - Value 1: 326 occurrences\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:172.090149/242.933441, val:  77.88%, val_best:  89.16%, tr:  99.88%, tr_best:  99.95%, epoch time: 137.13 seconds, 2.29 minutes\n",
      "layer   1  Sparsity: 79.4564%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.5567%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.5191%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2112768 real_backward_count 195625   9.259%\n",
      "layer   1  Sparsity: 86.6699%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 27.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 67 occurrences\n",
      "test - Value 1: 385 occurrences\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:183.972153/214.616104, val:  64.82%, val_best:  89.16%, tr:  99.83%, tr_best:  99.95%, epoch time: 138.03 seconds, 2.30 minutes\n",
      "layer   1  Sparsity: 79.4540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.4422%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.5003%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2128896 real_backward_count 196607   9.235%\n",
      "layer   1  Sparsity: 78.0273%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 111 occurrences\n",
      "test - Value 1: 341 occurrences\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:187.555069/258.953461, val:  73.67%, val_best:  89.16%, tr:  99.78%, tr_best:  99.95%, epoch time: 135.55 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4559%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.3416%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.2303%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2145024 real_backward_count 197614   9.213%\n",
      "layer   1  Sparsity: 68.6523%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 46.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 19.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 106 occurrences\n",
      "test - Value 1: 346 occurrences\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:208.496323/217.547577, val:  73.01%, val_best:  89.16%, tr:  99.85%, tr_best:  99.95%, epoch time: 133.85 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 79.4580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.5330%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.3950%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2161152 real_backward_count 198672   9.193%\n",
      "layer   1  Sparsity: 80.8105%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 126 occurrences\n",
      "test - Value 1: 326 occurrences\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:190.042557/187.581055, val:  77.43%, val_best:  89.16%, tr:  99.95%, tr_best:  99.95%, epoch time: 134.68 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 79.4553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.4758%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.3799%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2177280 real_backward_count 199653   9.170%\n",
      "layer   1  Sparsity: 92.0898%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4620.0\n",
      "fc layer 2 self.abs_max_out: 4925.0\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 183 occurrences\n",
      "test - Value 1: 269 occurrences\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:191.725006/177.954132, val:  86.50%, val_best:  89.16%, tr:  99.90%, tr_best:  99.95%, epoch time: 136.62 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 79.4528%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.4430%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.5936%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2193408 real_backward_count 200653   9.148%\n",
      "layer   1  Sparsity: 76.4160%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 5088.0\n",
      "fc layer 2 self.abs_max_out: 5127.0\n",
      "lif layer 2 self.abs_max_v: 6535.5\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 199 occurrences\n",
      "test - Value 1: 253 occurrences\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:190.319580/194.540787, val:  88.72%, val_best:  89.16%, tr:  99.75%, tr_best:  99.95%, epoch time: 136.62 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 79.4563%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.4462%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.4748%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2209536 real_backward_count 201633   9.126%\n",
      "layer   1  Sparsity: 74.2188%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 6596.0\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 195 occurrences\n",
      "test - Value 1: 257 occurrences\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:186.032181/197.531189, val:  88.72%, val_best:  89.16%, tr:  99.90%, tr_best:  99.95%, epoch time: 137.03 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 79.4568%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.6603%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.2375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2225664 real_backward_count 202677   9.106%\n",
      "layer   1  Sparsity: 86.3770%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 149 occurrences\n",
      "test - Value 1: 303 occurrences\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:192.633835/209.325974, val:  82.96%, val_best:  89.16%, tr:  99.90%, tr_best:  99.95%, epoch time: 136.92 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 79.4540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.8862%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.5626%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2241792 real_backward_count 203692   9.086%\n",
      "layer   1  Sparsity: 73.2422%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 75 occurrences\n",
      "test - Value 1: 377 occurrences\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:194.826477/252.127167, val:  66.59%, val_best:  89.16%, tr:  99.95%, tr_best:  99.95%, epoch time: 134.24 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 79.4570%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.9712%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.1674%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2257920 real_backward_count 204630   9.063%\n",
      "layer   1  Sparsity: 66.3574%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 46.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 121 occurrences\n",
      "test - Value 1: 331 occurrences\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:193.718002/211.400162, val:  76.77%, val_best:  89.16%, tr:  99.98%, tr_best:  99.98%, epoch time: 137.18 seconds, 2.29 minutes\n",
      "layer   1  Sparsity: 79.4585%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.8148%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7168%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2274048 real_backward_count 205622   9.042%\n",
      "layer   1  Sparsity: 54.1504%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 43.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 30.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 152 occurrences\n",
      "test - Value 1: 300 occurrences\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:194.745087/184.352448, val:  83.19%, val_best:  89.16%, tr:  99.73%, tr_best:  99.98%, epoch time: 136.71 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 79.4612%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.0030%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.1152%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2290176 real_backward_count 206596   9.021%\n",
      "layer   1  Sparsity: 77.5879%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 32.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 191 occurrences\n",
      "test - Value 1: 261 occurrences\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:197.414017/134.440811, val:  88.72%, val_best:  89.16%, tr:  99.98%, tr_best:  99.98%, epoch time: 136.23 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4560%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.2342%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7540%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2306304 real_backward_count 207540   8.999%\n",
      "layer   1  Sparsity: 86.8652%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 5171.0\n",
      "lif layer 2 self.abs_max_v: 7044.0\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 102 occurrences\n",
      "test - Value 1: 350 occurrences\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:191.685898/229.370148, val:  72.57%, val_best:  89.16%, tr:  99.85%, tr_best:  99.98%, epoch time: 136.98 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 79.4539%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.0318%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.6715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2322432 real_backward_count 208528   8.979%\n",
      "layer   1  Sparsity: 74.7559%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 75 occurrences\n",
      "test - Value 1: 377 occurrences\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:187.689102/279.697357, val:  66.59%, val_best:  89.16%, tr:  99.90%, tr_best:  99.98%, epoch time: 136.91 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 79.4566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.0246%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.1351%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2338560 real_backward_count 209499   8.958%\n",
      "layer   1  Sparsity: 90.6250%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 170 occurrences\n",
      "test - Value 1: 282 occurrences\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:205.206375/157.055771, val:  86.28%, val_best:  89.16%, tr:  99.80%, tr_best:  99.98%, epoch time: 136.33 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4531%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.9428%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.3751%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2354688 real_backward_count 210505   8.940%\n",
      "layer   1  Sparsity: 61.8164%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 46.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 501.0\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 108 occurrences\n",
      "test - Value 1: 344 occurrences\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:192.840759/181.188904, val:  73.89%, val_best:  89.16%, tr:  99.98%, tr_best:  99.98%, epoch time: 134.15 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 79.4595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.9838%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.1834%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2370816 real_backward_count 211443   8.919%\n",
      "layer   1  Sparsity: 70.3125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 206 occurrences\n",
      "test - Value 1: 246 occurrences\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:187.727036/182.220795, val:  88.05%, val_best:  89.16%, tr:  99.90%, tr_best:  99.98%, epoch time: 136.22 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4576%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.1218%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.1687%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2386944 real_backward_count 212405   8.899%\n",
      "layer   1  Sparsity: 79.3945%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 167 occurrences\n",
      "test - Value 1: 285 occurrences\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:187.419968/181.503418, val:  84.29%, val_best:  89.16%, tr:  99.95%, tr_best:  99.98%, epoch time: 135.22 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4556%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.1311%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.3096%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2403072 real_backward_count 213375   8.879%\n",
      "layer   1  Sparsity: 83.6426%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 503.0\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 105 occurrences\n",
      "test - Value 1: 347 occurrences\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:187.830002/198.924011, val:  73.23%, val_best:  89.16%, tr:  99.98%, tr_best:  99.98%, epoch time: 135.36 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4547%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.2542%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5071%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2419200 real_backward_count 214322   8.859%\n",
      "layer   1  Sparsity: 88.0859%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7089.0\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "lif layer 1 self.abs_max_v: 7487.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 199 occurrences\n",
      "test - Value 1: 253 occurrences\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:184.382553/203.032608, val:  88.72%, val_best:  89.16%, tr:  99.90%, tr_best:  99.98%, epoch time: 134.06 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 79.4537%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.2068%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.9332%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2435328 real_backward_count 215304   8.841%\n",
      "layer   1  Sparsity: 79.2969%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7338.0\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "lif layer 1 self.abs_max_v: 7620.5\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 132 occurrences\n",
      "test - Value 1: 320 occurrences\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:194.129852/188.731018, val:  79.20%, val_best:  89.16%, tr:  99.90%, tr_best:  99.98%, epoch time: 134.43 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 79.4556%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.3449%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.3072%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2451456 real_backward_count 216282   8.823%\n",
      "layer   1  Sparsity: 72.9980%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 530.0\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 125 occurrences\n",
      "test - Value 1: 327 occurrences\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:208.239426/195.778397, val:  77.21%, val_best:  89.16%, tr:  99.93%, tr_best:  99.98%, epoch time: 136.42 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4570%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.3589%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.5985%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2467584 real_backward_count 217172   8.801%\n",
      "layer   1  Sparsity: 89.0625%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 133 occurrences\n",
      "test - Value 1: 319 occurrences\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:202.615845/201.282074, val:  78.10%, val_best:  89.16%, tr:  99.90%, tr_best:  99.98%, epoch time: 136.79 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 79.4534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.4341%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.6437%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2483712 real_backward_count 218093   8.781%\n",
      "layer   1  Sparsity: 80.9570%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 5237.0\n",
      "fc layer 2 self.abs_max_out: 5309.0\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 94 occurrences\n",
      "test - Value 1: 358 occurrences\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:203.221725/245.087616, val:  70.80%, val_best:  89.16%, tr:  99.93%, tr_best:  99.98%, epoch time: 137.01 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 79.4553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.4292%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.8586%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2499840 real_backward_count 218975   8.760%\n",
      "layer   1  Sparsity: 72.9492%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7418.5\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 145 occurrences\n",
      "test - Value 1: 307 occurrences\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:198.810272/202.132782, val:  80.75%, val_best:  89.16%, tr:  99.90%, tr_best:  99.98%, epoch time: 137.63 seconds, 2.29 minutes\n",
      "layer   1  Sparsity: 79.4570%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.3455%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.9382%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2515968 real_backward_count 219907   8.740%\n",
      "layer   1  Sparsity: 76.2695%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7466.0\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 86 occurrences\n",
      "test - Value 1: 366 occurrences\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:201.863831/234.120605, val:  69.03%, val_best:  89.16%, tr:  99.88%, tr_best:  99.98%, epoch time: 136.01 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4563%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.4914%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.1145%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2532096 real_backward_count 220856   8.722%\n",
      "layer   1  Sparsity: 74.5605%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 105 occurrences\n",
      "test - Value 1: 347 occurrences\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:209.635284/213.688538, val:  73.23%, val_best:  89.16%, tr: 100.00%, tr_best: 100.00%, epoch time: 134.80 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4567%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.4999%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.3655%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2548224 real_backward_count 221832   8.705%\n",
      "layer   1  Sparsity: 75.4395%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 106 occurrences\n",
      "test - Value 1: 346 occurrences\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:213.802505/234.311005, val:  73.01%, val_best:  89.16%, tr:  99.98%, tr_best: 100.00%, epoch time: 135.59 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.6536%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.3197%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2564352 real_backward_count 222756   8.687%\n",
      "layer   1  Sparsity: 92.0410%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 99 occurrences\n",
      "test - Value 1: 353 occurrences\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:211.444885/260.986877, val:  71.46%, val_best:  89.16%, tr:  99.98%, tr_best: 100.00%, epoch time: 136.90 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 79.4528%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.7445%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.0100%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2580480 real_backward_count 223674   8.668%\n",
      "layer   1  Sparsity: 84.6680%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 136 occurrences\n",
      "test - Value 1: 316 occurrences\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:207.149719/212.962982, val:  79.65%, val_best:  89.16%, tr:  99.93%, tr_best: 100.00%, epoch time: 135.76 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4544%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.7999%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.3833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2596608 real_backward_count 224582   8.649%\n",
      "layer   1  Sparsity: 92.7246%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 1 self.abs_max_out: 6542.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 121 occurrences\n",
      "test - Value 1: 331 occurrences\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:194.676422/221.328613, val:  76.33%, val_best:  89.16%, tr:  99.88%, tr_best: 100.00%, epoch time: 136.06 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4526%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.7104%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.4550%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2612736 real_backward_count 225525   8.632%\n",
      "layer   1  Sparsity: 75.7812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 164 occurrences\n",
      "test - Value 1: 288 occurrences\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:196.425735/154.586533, val:  84.07%, val_best:  89.16%, tr:  99.95%, tr_best: 100.00%, epoch time: 136.57 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 79.4564%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.6911%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.8715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2628864 real_backward_count 226454   8.614%\n",
      "layer   1  Sparsity: 73.4863%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 162 occurrences\n",
      "test - Value 1: 290 occurrences\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:208.710236/207.533798, val:  84.96%, val_best:  89.16%, tr:  99.88%, tr_best: 100.00%, epoch time: 135.09 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4569%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.6094%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.8507%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2644992 real_backward_count 227422   8.598%\n",
      "layer   1  Sparsity: 74.0723%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 173 occurrences\n",
      "test - Value 1: 279 occurrences\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:189.451523/178.692062, val:  86.06%, val_best:  89.16%, tr:  99.93%, tr_best: 100.00%, epoch time: 133.13 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 79.4568%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.5286%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5071%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2661120 real_backward_count 228355   8.581%\n",
      "layer   1  Sparsity: 83.3496%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 174 occurrences\n",
      "test - Value 1: 278 occurrences\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:188.157883/159.933594, val:  84.96%, val_best:  89.16%, tr:  99.93%, tr_best: 100.00%, epoch time: 136.02 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4547%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.5628%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.7558%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2677248 real_backward_count 229328   8.566%\n",
      "layer   1  Sparsity: 74.5605%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 5317.0\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 120 occurrences\n",
      "test - Value 1: 332 occurrences\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:192.874237/189.909271, val:  76.55%, val_best:  89.16%, tr:  99.98%, tr_best: 100.00%, epoch time: 136.24 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4567%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.5982%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.5775%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2693376 real_backward_count 230282   8.550%\n",
      "layer   1  Sparsity: 74.8047%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7656.5\n",
      "fc layer 2 self.abs_max_out: 5338.0\n",
      "fc layer 2 self.abs_max_out: 5378.0\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 142 occurrences\n",
      "test - Value 1: 310 occurrences\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:194.798965/227.043228, val:  80.09%, val_best:  89.16%, tr:  99.90%, tr_best: 100.00%, epoch time: 135.84 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.5287%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.4033%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2709504 real_backward_count 231263   8.535%\n",
      "layer   1  Sparsity: 71.8262%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 5475.0\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 122 occurrences\n",
      "test - Value 1: 330 occurrences\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:195.635208/217.746918, val:  76.55%, val_best:  89.16%, tr:  99.85%, tr_best: 100.00%, epoch time: 132.18 seconds, 2.20 minutes\n",
      "layer   1  Sparsity: 79.4573%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.4773%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.3641%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2725632 real_backward_count 232231   8.520%\n",
      "layer   1  Sparsity: 78.3691%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 112 occurrences\n",
      "test - Value 1: 340 occurrences\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:197.266373/230.419525, val:  74.34%, val_best:  89.16%, tr:  99.95%, tr_best: 100.00%, epoch time: 133.75 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 79.4558%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.4694%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.0967%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2741760 real_backward_count 233237   8.507%\n",
      "layer   1  Sparsity: 82.0801%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 152 occurrences\n",
      "test - Value 1: 300 occurrences\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:201.648010/222.990921, val:  82.74%, val_best:  89.16%, tr:  99.98%, tr_best: 100.00%, epoch time: 135.67 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4550%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.3001%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.3632%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2757888 real_backward_count 234156   8.490%\n",
      "layer   1  Sparsity: 79.3945%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 120 occurrences\n",
      "test - Value 1: 332 occurrences\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:217.331726/223.999237, val:  76.11%, val_best:  89.16%, tr: 100.00%, tr_best: 100.00%, epoch time: 134.58 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 79.4556%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.3019%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5427%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2774016 real_backward_count 235077   8.474%\n",
      "layer   1  Sparsity: 84.9121%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 125 occurrences\n",
      "test - Value 1: 327 occurrences\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:217.012268/253.272842, val:  76.77%, val_best:  89.16%, tr: 100.00%, tr_best: 100.00%, epoch time: 136.04 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4544%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.1713%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.2540%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2790144 real_backward_count 235949   8.457%\n",
      "layer   1  Sparsity: 83.9355%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 180 occurrences\n",
      "test - Value 1: 272 occurrences\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:204.989273/179.116272, val:  87.61%, val_best:  89.16%, tr: 100.00%, tr_best: 100.00%, epoch time: 135.14 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4546%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.0558%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.8907%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2806272 real_backward_count 236891   8.441%\n",
      "layer   1  Sparsity: 89.5508%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 92 occurrences\n",
      "test - Value 1: 360 occurrences\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:205.917526/217.348709, val:  70.35%, val_best:  89.16%, tr: 100.00%, tr_best: 100.00%, epoch time: 133.67 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 79.4533%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.2344%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.1132%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2822400 real_backward_count 237777   8.425%\n",
      "layer   1  Sparsity: 75.5371%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 186 occurrences\n",
      "test - Value 1: 266 occurrences\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:202.591644/172.099518, val:  88.50%, val_best:  89.16%, tr:  99.98%, tr_best: 100.00%, epoch time: 134.92 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.3643%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.7992%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2838528 real_backward_count 238692   8.409%\n",
      "layer   1  Sparsity: 79.8828%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 175 occurrences\n",
      "test - Value 1: 277 occurrences\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:201.823456/173.417709, val:  86.50%, val_best:  89.16%, tr:  99.98%, tr_best: 100.00%, epoch time: 136.24 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4555%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.4560%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.0689%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2854656 real_backward_count 239618   8.394%\n",
      "layer   1  Sparsity: 70.4102%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 27.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7700.5\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 89 occurrences\n",
      "test - Value 1: 363 occurrences\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:215.433960/229.606903, val:  69.25%, val_best:  89.16%, tr:  99.98%, tr_best: 100.00%, epoch time: 136.66 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 79.4576%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.4038%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.3764%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2870784 real_backward_count 240547   8.379%\n",
      "layer   1  Sparsity: 75.6836%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 30.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 97 occurrences\n",
      "test - Value 1: 355 occurrences\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:198.035400/201.567688, val:  71.46%, val_best:  89.16%, tr:  99.93%, tr_best: 100.00%, epoch time: 136.88 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 79.4564%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.1435%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.5492%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2886912 real_backward_count 241528   8.366%\n",
      "layer   1  Sparsity: 76.7090%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 105 occurrences\n",
      "test - Value 1: 347 occurrences\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:210.171829/220.030624, val:  73.23%, val_best:  89.16%, tr:  99.98%, tr_best: 100.00%, epoch time: 134.96 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.0541%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.7543%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2903040 real_backward_count 242447   8.351%\n",
      "layer   1  Sparsity: 81.5430%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 89 occurrences\n",
      "test - Value 1: 363 occurrences\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:217.427536/229.777161, val:  69.69%, val_best:  89.16%, tr:  99.98%, tr_best: 100.00%, epoch time: 136.34 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4551%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.1601%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.2820%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2919168 real_backward_count 243325   8.335%\n",
      "layer   1  Sparsity: 81.2012%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 147 occurrences\n",
      "test - Value 1: 305 occurrences\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:206.147247/203.120239, val:  81.64%, val_best:  89.16%, tr:  99.98%, tr_best: 100.00%, epoch time: 134.33 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 79.4552%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.3332%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.5672%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2935296 real_backward_count 244286   8.322%\n",
      "layer   1  Sparsity: 88.3789%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 138 occurrences\n",
      "test - Value 1: 314 occurrences\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:202.231339/190.671646, val:  79.65%, val_best:  89.16%, tr:  99.98%, tr_best: 100.00%, epoch time: 133.35 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 79.4536%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.4612%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.2926%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2951424 real_backward_count 245232   8.309%\n",
      "layer   1  Sparsity: 89.4043%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 131 occurrences\n",
      "test - Value 1: 321 occurrences\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:196.460373/172.936859, val:  78.10%, val_best:  89.16%, tr:  99.95%, tr_best: 100.00%, epoch time: 135.16 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.5854%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.8961%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2967552 real_backward_count 246169   8.295%\n",
      "layer   1  Sparsity: 85.1562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 140 occurrences\n",
      "test - Value 1: 312 occurrences\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:195.035294/222.796539, val:  79.65%, val_best:  89.16%, tr:  99.95%, tr_best: 100.00%, epoch time: 136.51 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 79.4543%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.5013%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.4061%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2983680 real_backward_count 247148   8.283%\n",
      "layer   1  Sparsity: 74.4141%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 123 occurrences\n",
      "test - Value 1: 329 occurrences\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:191.822098/179.033569, val:  77.21%, val_best:  89.16%, tr:  99.95%, tr_best: 100.00%, epoch time: 136.28 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4567%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.1968%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.3957%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2999808 real_backward_count 248065   8.269%\n",
      "layer   1  Sparsity: 69.6777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 45.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 143 occurrences\n",
      "test - Value 1: 309 occurrences\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:187.836243/211.772980, val:  80.31%, val_best:  89.16%, tr:  99.90%, tr_best: 100.00%, epoch time: 133.81 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 79.4578%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.2573%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.5051%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3015936 real_backward_count 249032   8.257%\n",
      "layer   1  Sparsity: 84.1797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 127 occurrences\n",
      "test - Value 1: 325 occurrences\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:193.259262/195.186493, val:  78.10%, val_best:  89.16%, tr:  99.98%, tr_best: 100.00%, epoch time: 134.93 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4545%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.0396%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.6464%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3032064 real_backward_count 250003   8.245%\n",
      "layer   1  Sparsity: 84.8633%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 30.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 1 self.abs_max_out: 6649.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 142 occurrences\n",
      "test - Value 1: 310 occurrences\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:209.717560/200.981781, val:  80.53%, val_best:  89.16%, tr:  99.85%, tr_best: 100.00%, epoch time: 135.27 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4544%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.8715%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.5454%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3048192 real_backward_count 250977   8.234%\n",
      "layer   1  Sparsity: 85.7422%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 134 occurrences\n",
      "test - Value 1: 318 occurrences\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:218.969360/251.188156, val:  78.76%, val_best:  89.16%, tr:  99.85%, tr_best: 100.00%, epoch time: 136.07 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.4542%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.9128%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.9942%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3064320 real_backward_count 251918   8.221%\n",
      "layer   1  Sparsity: 85.9863%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 1 self.abs_max_out: 6744.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 138 occurrences\n",
      "test - Value 1: 314 occurrences\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:210.004272/160.364136, val:  79.20%, val_best:  89.16%, tr:  99.93%, tr_best: 100.00%, epoch time: 136.56 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 79.4541%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.8810%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.6714%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3080448 real_backward_count 252853   8.208%\n",
      "layer   1  Sparsity: 72.4609%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 138 occurrences\n",
      "test - Value 1: 314 occurrences\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:190.997192/162.373901, val:  80.09%, val_best:  89.16%, tr:  99.85%, tr_best: 100.00%, epoch time: 134.93 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4571%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.9892%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.6086%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3096576 real_backward_count 253814   8.197%\n",
      "layer   1  Sparsity: 84.1797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 149 occurrences\n",
      "test - Value 1: 303 occurrences\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:202.512375/175.011673, val:  81.19%, val_best:  89.16%, tr:  99.98%, tr_best: 100.00%, epoch time: 133.75 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 79.4545%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.2963%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3112704 real_backward_count 254800   8.186%\n",
      "layer   1  Sparsity: 76.0254%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 93 occurrences\n",
      "test - Value 1: 359 occurrences\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:195.656769/286.091797, val:  70.58%, val_best:  89.16%, tr:  99.95%, tr_best: 100.00%, epoch time: 133.70 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 79.4564%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.0717%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.0289%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3128832 real_backward_count 255726   8.173%\n",
      "layer   1  Sparsity: 88.6719%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 84 occurrences\n",
      "test - Value 1: 368 occurrences\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:205.112442/214.612381, val:  68.58%, val_best:  89.16%, tr:  99.98%, tr_best: 100.00%, epoch time: 134.36 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 79.4535%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.9943%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.5852%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3144960 real_backward_count 256642   8.160%\n",
      "layer   1  Sparsity: 83.3008%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 159 occurrences\n",
      "test - Value 1: 293 occurrences\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:223.165436/182.726608, val:  83.41%, val_best:  89.16%, tr:  99.95%, tr_best: 100.00%, epoch time: 135.51 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.4547%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.9569%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.3778%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3161088 real_backward_count 257588   8.149%\n",
      "layer   1  Sparsity: 90.4297%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 31.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 182 occurrences\n",
      "test - Value 1: 270 occurrences\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:221.688736/177.850632, val:  86.28%, val_best:  89.16%, tr:  99.80%, tr_best: 100.00%, epoch time: 132.44 seconds, 2.21 minutes\n",
      "layer   1  Sparsity: 79.4531%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.7494%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.6005%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3177216 real_backward_count 258528   8.137%\n",
      "layer   1  Sparsity: 87.5977%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 159 occurrences\n",
      "test - Value 1: 293 occurrences\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:218.578644/162.696198, val:  83.85%, val_best:  89.16%, tr:  99.95%, tr_best: 100.00%, epoch time: 134.00 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 79.4538%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.7912%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.6560%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3193344 real_backward_count 259451   8.125%\n",
      "layer   1  Sparsity: 83.2520%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 106 occurrences\n",
      "test - Value 1: 346 occurrences\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:216.425140/249.041306, val:  73.45%, val_best:  89.16%, tr:  99.90%, tr_best: 100.00%, epoch time: 134.96 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.4547%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.8765%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.7538%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3209472 real_backward_count 260357   8.112%\n",
      "layer   1  Sparsity: 88.8672%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 31.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 147 occurrences\n",
      "test - Value 1: 305 occurrences\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:202.785873/184.689270, val:  81.64%, val_best:  89.16%, tr:  99.90%, tr_best: 100.00%, epoch time: 132.57 seconds, 2.21 minutes\n",
      "layer   1  Sparsity: 79.4535%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.9157%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.4987%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c2bd15a50c47c2a10b9b15656e625c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÇ‚ñÑ‚ñÉ‚ñà‚ñÅ‚ñÖ‚ñÑ‚ñÉ‚ñá‚ñÉ‚ñÖ‚ñá‚ñÖ‚ñÇ‚ñÉ‚ñá‚ñÑ‚ñÇ‚ñÖ‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÉ‚ñá‚ñÑ‚ñà‚ñÉ‚ñà‚ñÖ‚ñÑ‚ñá‚ñÜ‚ñà‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÇ‚ñÑ‚ñÉ‚ñà‚ñÅ‚ñÖ‚ñÑ‚ñÉ‚ñá‚ñÉ‚ñÖ‚ñá‚ñÖ‚ñÇ‚ñÉ‚ñá‚ñÑ‚ñÇ‚ñÖ‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÉ‚ñá‚ñÑ‚ñà‚ñÉ‚ñà‚ñÖ‚ñÑ‚ñá‚ñÜ‚ñà‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñà‚ñÖ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99901</td></tr><tr><td>tr_epoch_loss</td><td>202.78587</td></tr><tr><td>val_acc_best</td><td>0.89159</td></tr><tr><td>val_acc_now</td><td>0.81637</td></tr><tr><td>val_loss</td><td>184.68927</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tough-sweep-1</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/6l9rncdr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/6l9rncdr</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251222_124412-6l9rncdr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: p575rt6z with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloser_encourage_mode: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 27145\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttimestep_sums_threshold: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: n_tidigits_tonic\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251222_201551-p575rt6z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/p575rt6z' target=\"_blank\">woven-sweep-21</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/qxn8dtpu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/qxn8dtpu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/qxn8dtpu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/qxn8dtpu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/p575rt6z' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/p575rt6z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'timestep_sums_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'loser_encourage_mode' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251222_201600_979', 'my_seed': 27145, 'TIME': 4, 'BATCH': 1, 'IMAGE_SIZE': 8, 'which_data': 'n_tidigits_tonic', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 64, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 1, 'dvs_duration': 0, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': False, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 8, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'timestep_sums_threshold': 0, 'lif_layer_sg_width2': 2, 'lif_layer_v_threshold2': 64, 'init_scaling': [0.5, 0.25, 0.0625], 'learning_rate': 1, 'learning_rate2': 1, 'loser_encourage_mode': True} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target word: 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target word: 0\n",
      "\n",
      "\n",
      "\n",
      "train_dataset length = 4032, test_dataset length = 452\n",
      "\n",
      "len(train_loader): 4032 BATCH: 1 train_data_count: 4032\n",
      "len(test_loader): 452 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 4, self.v_threshold 64\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 2, self.v_threshold 64\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=512, out_features=200, TIME=4, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.25, 0.0625])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=64, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=4, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=4, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.25, 0.0625])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=64, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=4, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=4, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.25, 0.0625])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 144,400\n",
      "========================================================\n",
      "\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 560.0\n",
      "lif layer 1 self.abs_max_v: 560.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 449.0\n",
      "lif layer 2 self.abs_max_v: 449.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 69.0\n",
      "fc layer 1 self.abs_max_out: 1270.0\n",
      "lif layer 1 self.abs_max_v: 1330.5\n",
      "fc layer 3 self.abs_max_out: 70.0\n",
      "lif layer 2 self.abs_max_v: 532.5\n",
      "layer   1  Sparsity: 90.8691%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 482.0\n",
      "fc layer 1 self.abs_max_out: 1712.0\n",
      "lif layer 1 self.abs_max_v: 1740.5\n",
      "lif layer 2 self.abs_max_v: 593.0\n",
      "fc layer 1 self.abs_max_out: 1856.0\n",
      "lif layer 1 self.abs_max_v: 2666.5\n",
      "fc layer 2 self.abs_max_out: 568.0\n",
      "lif layer 2 self.abs_max_v: 860.5\n",
      "fc layer 3 self.abs_max_out: 80.0\n",
      "fc layer 2 self.abs_max_out: 593.0\n",
      "fc layer 2 self.abs_max_out: 663.0\n",
      "fc layer 2 self.abs_max_out: 674.0\n",
      "lif layer 2 self.abs_max_v: 884.5\n",
      "lif layer 2 self.abs_max_v: 910.5\n",
      "fc layer 1 self.abs_max_out: 1964.0\n",
      "fc layer 3 self.abs_max_out: 100.0\n",
      "fc layer 1 self.abs_max_out: 2163.0\n",
      "fc layer 3 self.abs_max_out: 107.0\n",
      "fc layer 3 self.abs_max_out: 121.0\n",
      "lif layer 1 self.abs_max_v: 3139.0\n",
      "lif layer 2 self.abs_max_v: 1027.5\n",
      "fc layer 2 self.abs_max_out: 692.0\n",
      "fc layer 2 self.abs_max_out: 707.0\n",
      "lif layer 2 self.abs_max_v: 1121.5\n",
      "fc layer 2 self.abs_max_out: 773.0\n",
      "lif layer 2 self.abs_max_v: 1146.0\n",
      "lif layer 2 self.abs_max_v: 1159.5\n",
      "fc layer 2 self.abs_max_out: 787.0\n",
      "lif layer 2 self.abs_max_v: 1184.0\n",
      "fc layer 2 self.abs_max_out: 840.0\n",
      "fc layer 2 self.abs_max_out: 842.0\n",
      "lif layer 2 self.abs_max_v: 1207.5\n",
      "lif layer 2 self.abs_max_v: 1247.5\n",
      "fc layer 2 self.abs_max_out: 885.0\n",
      "lif layer 2 self.abs_max_v: 1497.0\n",
      "fc layer 2 self.abs_max_out: 919.0\n",
      "fc layer 1 self.abs_max_out: 2185.0\n",
      "fc layer 2 self.abs_max_out: 956.0\n",
      "lif layer 2 self.abs_max_v: 1527.0\n",
      "lif layer 2 self.abs_max_v: 1546.5\n",
      "fc layer 2 self.abs_max_out: 1004.0\n",
      "lif layer 2 self.abs_max_v: 1680.0\n",
      "fc layer 1 self.abs_max_out: 2294.0\n",
      "fc layer 1 self.abs_max_out: 2305.0\n",
      "fc layer 2 self.abs_max_out: 1017.0\n",
      "lif layer 2 self.abs_max_v: 1730.0\n",
      "fc layer 2 self.abs_max_out: 1062.0\n",
      "lif layer 2 self.abs_max_v: 1738.5\n",
      "fc layer 2 self.abs_max_out: 1097.0\n",
      "lif layer 2 self.abs_max_v: 1740.5\n",
      "fc layer 1 self.abs_max_out: 2446.0\n",
      "lif layer 1 self.abs_max_v: 3274.0\n",
      "fc layer 2 self.abs_max_out: 1137.0\n",
      "lif layer 2 self.abs_max_v: 1755.5\n",
      "fc layer 2 self.abs_max_out: 1189.0\n",
      "lif layer 2 self.abs_max_v: 1778.0\n",
      "lif layer 2 self.abs_max_v: 1836.5\n",
      "lif layer 2 self.abs_max_v: 1852.5\n",
      "lif layer 2 self.abs_max_v: 1854.5\n",
      "lif layer 1 self.abs_max_v: 3366.0\n",
      "fc layer 1 self.abs_max_out: 2549.0\n",
      "lif layer 1 self.abs_max_v: 3404.5\n",
      "lif layer 1 self.abs_max_v: 3748.0\n",
      "fc layer 1 self.abs_max_out: 2707.0\n",
      "lif layer 2 self.abs_max_v: 1859.0\n",
      "fc layer 3 self.abs_max_out: 124.0\n",
      "fc layer 3 self.abs_max_out: 126.0\n",
      "lif layer 1 self.abs_max_v: 3828.0\n",
      "lif layer 1 self.abs_max_v: 3927.0\n",
      "fc layer 2 self.abs_max_out: 1202.0\n",
      "lif layer 2 self.abs_max_v: 1962.5\n",
      "lif layer 2 self.abs_max_v: 2034.5\n",
      "fc layer 1 self.abs_max_out: 2732.0\n",
      "fc layer 1 self.abs_max_out: 2862.0\n",
      "fc layer 2 self.abs_max_out: 1243.0\n",
      "lif layer 1 self.abs_max_v: 4194.5\n",
      "fc layer 2 self.abs_max_out: 1262.0\n",
      "fc layer 2 self.abs_max_out: 1273.0\n",
      "fc layer 2 self.abs_max_out: 1276.0\n",
      "fc layer 2 self.abs_max_out: 1295.0\n",
      "fc layer 1 self.abs_max_out: 2925.0\n",
      "lif layer 2 self.abs_max_v: 2062.5\n",
      "fc layer 1 self.abs_max_out: 3028.0\n",
      "lif layer 1 self.abs_max_v: 4250.5\n",
      "lif layer 2 self.abs_max_v: 2071.5\n",
      "lif layer 2 self.abs_max_v: 2101.0\n",
      "fc layer 2 self.abs_max_out: 1312.0\n",
      "fc layer 2 self.abs_max_out: 1352.0\n",
      "fc layer 1 self.abs_max_out: 3034.0\n",
      "fc layer 2 self.abs_max_out: 1358.0\n",
      "fc layer 2 self.abs_max_out: 1454.0\n",
      "fc layer 1 self.abs_max_out: 3036.0\n",
      "fc layer 2 self.abs_max_out: 1512.0\n",
      "fc layer 3 self.abs_max_out: 131.0\n",
      "fc layer 3 self.abs_max_out: 139.0\n",
      "lif layer 2 self.abs_max_v: 2128.5\n",
      "lif layer 2 self.abs_max_v: 2136.0\n",
      "lif layer 2 self.abs_max_v: 2208.0\n",
      "fc layer 1 self.abs_max_out: 3176.0\n",
      "lif layer 2 self.abs_max_v: 2246.5\n",
      "lif layer 2 self.abs_max_v: 2324.5\n",
      "fc layer 3 self.abs_max_out: 140.0\n",
      "fc layer 1 self.abs_max_out: 3262.0\n",
      "lif layer 2 self.abs_max_v: 2342.0\n",
      "lif layer 2 self.abs_max_v: 2352.0\n",
      "lif layer 2 self.abs_max_v: 2404.5\n",
      "lif layer 2 self.abs_max_v: 2407.0\n",
      "lif layer 2 self.abs_max_v: 2407.5\n",
      "lif layer 2 self.abs_max_v: 2417.0\n",
      "fc layer 3 self.abs_max_out: 151.0\n",
      "fc layer 3 self.abs_max_out: 173.0\n",
      "fc layer 2 self.abs_max_out: 1514.0\n",
      "fc layer 2 self.abs_max_out: 1527.0\n",
      "fc layer 2 self.abs_max_out: 1564.0\n",
      "lif layer 2 self.abs_max_v: 2425.0\n",
      "lif layer 2 self.abs_max_v: 2442.0\n",
      "fc layer 2 self.abs_max_out: 1623.0\n",
      "fc layer 2 self.abs_max_out: 1643.0\n",
      "lif layer 2 self.abs_max_v: 2478.0\n",
      "fc layer 2 self.abs_max_out: 1692.0\n",
      "fc layer 1 self.abs_max_out: 3276.0\n",
      "fc layer 2 self.abs_max_out: 1724.0\n",
      "lif layer 2 self.abs_max_v: 2489.5\n",
      "train - Value 0: 1998 occurrences\n",
      "train - Value 1: 2034 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 156.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 197.00 at epoch 0, iter 4031\n",
      "lif layer 2 self.abs_max_v: 2541.0\n",
      "max_activation_accul updated: 218.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 227.00 at epoch 0, iter 4031\n",
      "fc layer 2 self.abs_max_out: 1739.0\n",
      "fc layer 2 self.abs_max_out: 1812.0\n",
      "max_activation_accul updated: 229.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 238.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 240.00 at epoch 0, iter 4031\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 41 occurrences\n",
      "test - Value 1: 411 occurrences\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 48.595284/ 32.296696, val:  58.63%, val_best:  58.63%, tr:  78.62%, tr_best:  78.62%, epoch time: 135.07 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 79.3429%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.2816%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.9958%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 16128 real_backward_count 4383  27.176%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "layer   1  Sparsity: 75.9766%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 4296.0\n",
      "lif layer 2 self.abs_max_v: 2545.0\n",
      "lif layer 2 self.abs_max_v: 2608.0\n",
      "lif layer 2 self.abs_max_v: 2664.0\n",
      "lif layer 2 self.abs_max_v: 2664.5\n",
      "lif layer 2 self.abs_max_v: 2684.5\n",
      "lif layer 2 self.abs_max_v: 2697.5\n",
      "lif layer 2 self.abs_max_v: 2731.0\n",
      "lif layer 2 self.abs_max_v: 2771.5\n",
      "lif layer 2 self.abs_max_v: 2793.5\n",
      "fc layer 3 self.abs_max_out: 227.0\n",
      "lif layer 2 self.abs_max_v: 2874.5\n",
      "fc layer 2 self.abs_max_out: 1829.0\n",
      "lif layer 2 self.abs_max_v: 2974.0\n",
      "fc layer 1 self.abs_max_out: 3322.0\n",
      "fc layer 2 self.abs_max_out: 1832.0\n",
      "fc layer 2 self.abs_max_out: 1845.0\n",
      "fc layer 2 self.abs_max_out: 1853.0\n",
      "fc layer 2 self.abs_max_out: 1900.0\n",
      "fc layer 2 self.abs_max_out: 1943.0\n",
      "fc layer 2 self.abs_max_out: 2029.0\n",
      "lif layer 1 self.abs_max_v: 4375.5\n",
      "lif layer 1 self.abs_max_v: 5012.0\n",
      "fc layer 1 self.abs_max_out: 3372.0\n",
      "fc layer 1 self.abs_max_out: 3429.0\n",
      "train - Value 0: 1949 occurrences\n",
      "train - Value 1: 2083 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 242.00 at epoch 1, iter 4031\n",
      "max_activation_accul updated: 244.00 at epoch 1, iter 4031\n",
      "max_activation_accul updated: 329.00 at epoch 1, iter 4031\n",
      "max_activation_accul updated: 338.00 at epoch 1, iter 4031\n",
      "max_activation_accul updated: 420.00 at epoch 1, iter 4031\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 131 occurrences\n",
      "test - Value 1: 321 occurrences\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 65.184578/ 66.288277, val:  73.23%, val_best:  73.23%, tr:  85.44%, tr_best:  85.44%, epoch time: 137.08 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 79.3462%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.4597%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.1854%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 32256 real_backward_count 7903  24.501%\n",
      "layer   1  Sparsity: 91.5039%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 3014.0\n",
      "lif layer 2 self.abs_max_v: 3060.0\n",
      "lif layer 2 self.abs_max_v: 3098.0\n",
      "lif layer 1 self.abs_max_v: 5040.0\n",
      "fc layer 1 self.abs_max_out: 3449.0\n",
      "fc layer 2 self.abs_max_out: 2030.0\n",
      "fc layer 1 self.abs_max_out: 3580.0\n",
      "fc layer 1 self.abs_max_out: 3643.0\n",
      "lif layer 1 self.abs_max_v: 5502.5\n",
      "fc layer 2 self.abs_max_out: 2067.0\n",
      "train - Value 0: 1915 occurrences\n",
      "train - Value 1: 2117 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 443.00 at epoch 2, iter 4031\n",
      "max_activation_accul updated: 470.00 at epoch 2, iter 4031\n",
      "lif layer 2 self.abs_max_v: 3184.5\n",
      "max_activation_accul updated: 473.00 at epoch 2, iter 4031\n",
      "fc layer 2 self.abs_max_out: 2077.0\n",
      "max_activation_accul updated: 494.00 at epoch 2, iter 4031\n",
      "max_activation_accul updated: 531.00 at epoch 2, iter 4031\n",
      "max_activation_accul updated: 555.00 at epoch 2, iter 4031\n",
      "max_activation_accul updated: 652.00 at epoch 2, iter 4031\n",
      "max_activation_accul updated: 752.00 at epoch 2, iter 4031\n",
      "fc layer 3 self.abs_max_out: 231.0\n",
      "lif layer 2 self.abs_max_v: 3308.5\n",
      "fc layer 2 self.abs_max_out: 2089.0\n",
      "fc layer 3 self.abs_max_out: 238.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 71.364403/147.495041, val:  50.00%, val_best:  73.23%, tr:  89.41%, tr_best:  89.41%, epoch time: 136.93 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 79.3427%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.8197%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.8656%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48384 real_backward_count 11344  23.446%\n",
      "layer   1  Sparsity: 80.8105%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 5609.0\n",
      "fc layer 2 self.abs_max_out: 2127.0\n",
      "fc layer 1 self.abs_max_out: 3729.0\n",
      "fc layer 2 self.abs_max_out: 2212.0\n",
      "fc layer 2 self.abs_max_out: 2228.0\n",
      "train - Value 0: 1937 occurrences\n",
      "train - Value 1: 2095 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 99 occurrences\n",
      "test - Value 1: 353 occurrences\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss: 71.826675/ 89.790009, val:  71.02%, val_best:  73.23%, tr:  90.85%, tr_best:  90.85%, epoch time: 135.74 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 79.3451%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.5302%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.9480%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 64512 real_backward_count 14594  22.622%\n",
      "layer   1  Sparsity: 92.4805%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 3826.0\n",
      "fc layer 3 self.abs_max_out: 243.0\n",
      "fc layer 3 self.abs_max_out: 282.0\n",
      "fc layer 1 self.abs_max_out: 3967.0\n",
      "fc layer 2 self.abs_max_out: 2237.0\n",
      "fc layer 2 self.abs_max_out: 2250.0\n",
      "fc layer 2 self.abs_max_out: 2297.0\n",
      "fc layer 1 self.abs_max_out: 4252.0\n",
      "fc layer 2 self.abs_max_out: 2322.0\n",
      "fc layer 1 self.abs_max_out: 4278.0\n",
      "fc layer 2 self.abs_max_out: 2336.0\n",
      "fc layer 2 self.abs_max_out: 2343.0\n",
      "fc layer 2 self.abs_max_out: 2349.0\n",
      "fc layer 2 self.abs_max_out: 2403.0\n",
      "fc layer 1 self.abs_max_out: 4286.0\n",
      "fc layer 1 self.abs_max_out: 4336.0\n",
      "fc layer 1 self.abs_max_out: 4921.0\n",
      "train - Value 0: 1954 occurrences\n",
      "train - Value 1: 2078 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 101 occurrences\n",
      "test - Value 1: 351 occurrences\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss: 74.893547/ 98.942337, val:  71.90%, val_best:  73.23%, tr:  91.91%, tr_best:  91.91%, epoch time: 133.77 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 79.3425%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.6679%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.2215%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 80640 real_backward_count 17640  21.875%\n",
      "layer   1  Sparsity: 77.2949%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 3402.5\n",
      "lif layer 2 self.abs_max_v: 3508.5\n",
      "fc layer 1 self.abs_max_out: 4938.0\n",
      "fc layer 3 self.abs_max_out: 301.0\n",
      "train - Value 0: 1964 occurrences\n",
      "train - Value 1: 2068 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 325 occurrences\n",
      "test - Value 1: 127 occurrences\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss: 83.610306/ 83.701286, val:  74.12%, val_best:  74.12%, tr:  91.96%, tr_best:  91.96%, epoch time: 136.60 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 79.3459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6728%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.4267%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 96768 real_backward_count 20671  21.361%\n",
      "layer   1  Sparsity: 88.3789%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 310.0\n",
      "lif layer 1 self.abs_max_v: 5619.0\n",
      "fc layer 1 self.abs_max_out: 5028.0\n",
      "train - Value 0: 1963 occurrences\n",
      "train - Value 1: 2069 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 312 occurrences\n",
      "test - Value 1: 140 occurrences\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss: 86.132965/ 70.084900, val:  77.88%, val_best:  77.88%, tr:  92.98%, tr_best:  92.98%, epoch time: 136.98 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 79.3434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.5959%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.0213%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 112896 real_backward_count 23722  21.012%\n",
      "layer   1  Sparsity: 69.7754%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 2410.0\n",
      "lif layer 2 self.abs_max_v: 3541.5\n",
      "fc layer 2 self.abs_max_out: 2443.0\n",
      "lif layer 2 self.abs_max_v: 3546.5\n",
      "lif layer 2 self.abs_max_v: 3556.0\n",
      "fc layer 2 self.abs_max_out: 2636.0\n",
      "fc layer 1 self.abs_max_out: 5222.0\n",
      "lif layer 2 self.abs_max_v: 3600.0\n",
      "lif layer 2 self.abs_max_v: 3639.0\n",
      "train - Value 0: 1961 occurrences\n",
      "train - Value 1: 2071 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 45 occurrences\n",
      "test - Value 1: 407 occurrences\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss: 87.765602/117.872162, val:  59.96%, val_best:  77.88%, tr:  93.38%, tr_best:  93.38%, epoch time: 136.89 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 79.3476%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.2618%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.3646%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 129024 real_backward_count 26582  20.602%\n",
      "layer   1  Sparsity: 80.1270%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 5957.0\n",
      "lif layer 2 self.abs_max_v: 3681.0\n",
      "lif layer 2 self.abs_max_v: 3779.0\n",
      "lif layer 2 self.abs_max_v: 3885.0\n",
      "lif layer 2 self.abs_max_v: 3897.5\n",
      "train - Value 0: 1949 occurrences\n",
      "train - Value 1: 2083 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "lif layer 2 self.abs_max_v: 3913.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 171 occurrences\n",
      "test - Value 1: 281 occurrences\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss: 86.629890/ 91.919991, val:  80.31%, val_best:  80.31%, tr:  93.33%, tr_best:  93.38%, epoch time: 136.96 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 79.3453%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.1642%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.9523%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 145152 real_backward_count 29455  20.293%\n",
      "layer   1  Sparsity: 61.0352%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 47.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 6158.5\n",
      "fc layer 2 self.abs_max_out: 2650.0\n",
      "train - Value 0: 1979 occurrences\n",
      "train - Value 1: 2053 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 141 occurrences\n",
      "test - Value 1: 311 occurrences\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss: 94.696159/ 83.395279, val:  78.10%, val_best:  80.31%, tr:  94.82%, tr_best:  94.82%, epoch time: 137.07 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 79.3495%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6389%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.9056%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 161280 real_backward_count 32054  19.875%\n",
      "layer   1  Sparsity: 80.9570%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 6265.0\n",
      "train - Value 0: 1973 occurrences\n",
      "train - Value 1: 2059 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 1 self.abs_max_out: 5368.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 53 occurrences\n",
      "test - Value 1: 399 occurrences\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss: 93.577187/100.393990, val:  60.84%, val_best:  80.31%, tr:  95.16%, tr_best:  95.16%, epoch time: 136.20 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.3451%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.5073%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.6733%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 177408 real_backward_count 34570  19.486%\n",
      "layer   1  Sparsity: 86.8652%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 6660.0\n",
      "lif layer 2 self.abs_max_v: 3923.0\n",
      "lif layer 2 self.abs_max_v: 4052.0\n",
      "lif layer 2 self.abs_max_v: 4070.0\n",
      "train - Value 0: 1972 occurrences\n",
      "train - Value 1: 2060 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 179 occurrences\n",
      "test - Value 1: 273 occurrences\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss: 91.732422/ 75.043236, val:  82.52%, val_best:  82.52%, tr:  95.68%, tr_best:  95.68%, epoch time: 136.76 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 79.3438%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7927%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.4300%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 193536 real_backward_count 37036  19.136%\n",
      "layer   1  Sparsity: 83.3496%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 5401.0\n",
      "train - Value 0: 1953 occurrences\n",
      "train - Value 1: 2079 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 879.00 at epoch 12, iter 4031\n",
      "max_activation_accul updated: 1021.00 at epoch 12, iter 4031\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss: 89.846222/201.049744, val:  50.00%, val_best:  82.52%, tr:  95.21%, tr_best:  95.68%, epoch time: 136.05 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.3445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.3016%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.7526%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 209664 real_backward_count 39494  18.837%\n",
      "layer   1  Sparsity: 78.1250%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 4151.0\n",
      "lif layer 2 self.abs_max_v: 4152.0\n",
      "fc layer 1 self.abs_max_out: 5413.0\n",
      "fc layer 1 self.abs_max_out: 5453.0\n",
      "fc layer 1 self.abs_max_out: 5496.0\n",
      "train - Value 0: 1978 occurrences\n",
      "train - Value 1: 2054 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 1 self.abs_max_out: 5678.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 225 occurrences\n",
      "test - Value 1: 227 occurrences\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss: 97.051422/136.493256, val:  86.95%, val_best:  86.95%, tr:  95.78%, tr_best:  95.78%, epoch time: 136.16 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 79.3457%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.4402%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.8080%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225792 real_backward_count 41818  18.521%\n",
      "layer   1  Sparsity: 67.7734%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 49.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 311.0\n",
      "fc layer 3 self.abs_max_out: 329.0\n",
      "lif layer 2 self.abs_max_v: 4222.5\n",
      "lif layer 2 self.abs_max_v: 4297.0\n",
      "lif layer 2 self.abs_max_v: 4356.5\n",
      "train - Value 0: 1992 occurrences\n",
      "train - Value 1: 2040 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "target_word=0\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'random', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}_targetword{target_word}_new251129',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        # \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [4]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [8]},\n",
    "        \"which_data\": {\"values\": ['n_tidigits_tonic']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [64.0]},\n",
    "        \"lif_layer_v_threshold2\": {\"values\": [64.0]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [4]},\n",
    "        \"lif_layer_sg_width2\": {\"values\": [2]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [4.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "        \"learning_rate\": {\"values\": [1.0]}, \n",
    "        \"learning_rate2\": {\"values\": [1.0]},\n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [1]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [target_word]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [False]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [9]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [False]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [8]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [0]},\n",
    "        # # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "\n",
    "        \"scale_exp_2w\": {\"values\": [0]},\n",
    "        # # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "\n",
    "        \"scale_exp_3w\": {\"values\": [0]},\n",
    "        # # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "\n",
    "        \"timestep_sums_threshold\": {\"values\": [0]},\n",
    "\n",
    "        \"loser_encourage_mode\": {\"values\": [True]},\n",
    "        \n",
    "        \"init_scaling_0\": {\"values\": [1/2]},\n",
    "        \"init_scaling_1\": {\"values\": [1/4]},\n",
    "        \"init_scaling_2\": {\"values\": [1/16]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"5\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_2w,wandb.config.scale_exp_2w],[wandb.config.scale_exp_3w,wandb.config.scale_exp_3w]],\n",
    "        timestep_sums_threshold  =  wandb.config.timestep_sums_threshold,\n",
    "        loser_encourage_mode  =  wandb.config.loser_encourage_mode,\n",
    "        lif_layer_sg_width2  =  wandb.config.lif_layer_sg_width2,\n",
    "        lif_layer_v_threshold2  =  wandb.config.lif_layer_v_threshold2,\n",
    "        learning_rate2  =  wandb.config.learning_rate2,\n",
    "        init_scaling = [wandb.config.init_scaling_0,wandb.config.init_scaling_1,wandb.config.init_scaling_2],\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'qxn8dtpu'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn NTIDIGITS SWEEP LOSER ONOFF new251129')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn NTIDIGITS SWEEP LOSER ONOFF new251129')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
