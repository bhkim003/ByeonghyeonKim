{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2455/4213678604.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os \n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "\n",
    "                    timestep_sums_threshold = 15,\n",
    "\n",
    "                    loser_encourage_mode = False, # True # False\n",
    "                    \n",
    "                    lif_layer_sg_width2 = None,\n",
    "                    lif_layer_v_threshold2 = None,\n",
    "                    learning_rate2 = None,\n",
    "                    init_scaling = None,\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    if which_data == 'n_tidigits_tonic':\n",
    "        assert merge_polarities == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "    synapse_fc_out_features = 10\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp,\n",
    "                    ANPI_MODE=False,\n",
    "                    lif_layer_sg_width2=lif_layer_sg_width2,\n",
    "                    lif_layer_v_threshold2=lif_layer_v_threshold2,\n",
    "                    init_scaling=init_scaling).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    # # wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    # ###########################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    # class CustomLossFunction(torch.autograd.Function):\n",
    "    #     @staticmethod\n",
    "    #     def forward(ctx, input, target):\n",
    "    #         ctx.save_for_backward(input, target)\n",
    "    #         return F.cross_entropy(input, target)\n",
    "\n",
    "    #     @staticmethod\n",
    "    #     def backward(ctx, grad_output):\n",
    "    #         # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "    #         input, target = ctx.saved_tensors\n",
    "    #         input_argmax = input.argmax(dim=1)\n",
    "    #         input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "    #         target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "    #         # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "    #         return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "    \n",
    "\n",
    "    print(\"ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\")\n",
    "    print(\"ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\")\n",
    "    print(\"ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\")\n",
    "    print(\"ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\")\n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            input, target = ctx.saved_tensors\n",
    "            assert input.shape[0] == 1 and target.shape[0] == 1, \"Batch size must be 1 for this custom loss function.\"\n",
    "            batch_size, num_classes = input.shape\n",
    "\n",
    "            target_0 = [0,1,2,3,4]\n",
    "            target_1 = [5,6,7,8,9]\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "\n",
    "            if (target.item() == 0) and (input_argmax.item() in target_0) or \\\n",
    "                (target.item() == 1) and (input_argmax.item() in target_1):\n",
    "                return input_one_hot - input_one_hot, None  \n",
    "            else:\n",
    "                if target.item() == 0:\n",
    "                    input_slice = input[:, 0:5]\n",
    "                    if loser_encourage_mode:\n",
    "                        input_argmin = input_slice.argmin(dim=1)\n",
    "                    else:\n",
    "                        input_argmin = input_slice.argmax(dim=1)\n",
    "                elif target.item() == 1:\n",
    "                    input_slice = input[:, 5:10] \n",
    "                    if loser_encourage_mode:\n",
    "                        input_argmin = input_slice.argmin(dim=1) + 5\n",
    "                    else:\n",
    "                        input_argmin = input_slice.argmax(dim=1) + 5\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected target: {target.item()}\")\n",
    "\n",
    "                # gradient Î∞©Ìñ•ÏùÑ argmin Ï™ΩÏúºÎ°ú\n",
    "                modified_target_one_hot = torch.zeros_like(input).scatter_(1, input_argmin.unsqueeze(1), 1.0)\n",
    "\n",
    "                return input_one_hot - modified_target_one_hot, None\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "            self.additional_dw_weight = 1.0\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                # lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        lr = learning_rate\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        lr = learning_rate2\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        lr = 1.0\n",
    "\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                    \n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                            \n",
    "                    \n",
    "                    dw = dw * self.additional_dw_weight\n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    max_activation_accul = 0\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "        # optimizer.additional_dw_weight = 1.0 if epoch % 2 ==0 else 0.0\n",
    "        optimizer.additional_dw_weight = 1.0\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        # if epoch %2 == 0:\n",
    "        #     iterator = enumerate(train_loader, 0)\n",
    "        # else:\n",
    "        #     iterator = enumerate(test_loader, 0)\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        train_spike_distribution = []\n",
    "        train_predicted_distribution = []\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "                \n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                # inputs: [Batch, Time, Channel, Height, Width]\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif (which_data == 'n_tidigits_tonic'):\n",
    "                inputs = inputs.unsqueeze(-1)\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "                # labels = torch.tensor(labels) \n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "\n",
    "                            \n",
    "            if i == 1:\n",
    "                # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                        \n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            \n",
    "            \n",
    "                        \n",
    "            ## Îç∞Ïù¥ÌÑ∞ÎßàÎã§ TIMESTEPSÎã§Î•¥Îã§ ########################################################\n",
    "            hetero_timesteps = True\n",
    "            if hetero_timesteps == True:\n",
    "                assert real_batch == 1\n",
    "                this_data_timesteps = inputs.shape[0]\n",
    "                TIME = this_data_timesteps//temporal_filter\n",
    "                net.module.change_timesteps(TIME) # netÏóê TIME ÏÑ§Ï†ï\n",
    "            ## Îç∞Ïù¥ÌÑ∞ÎßàÎã§ TIMESTEPSÎã§Î•¥Îã§ ########################################################\n",
    "            \n",
    "\n",
    "            \n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    # inputs # [Time, Batch, Channel, Height, Width]\n",
    "                    # inputs # [Batch, Channel, Height,Time, Width]\n",
    "                    # inputs # [Batch, Channel, Height,Time * Width]\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "            \n",
    "            # if hetero_timesteps == True:\n",
    "            #     assert real_batch == 1\n",
    "            #     # inputs # [Time, Batch, Channel, Height, Width]\n",
    "            #     # inputs timestpeÎ≥ÑÎ°ú sumÍ∞íÏù¥ 10ÎØ∏ÎßåÏùº Ïãú Ï†úÏô∏\n",
    "            #     # time stepÎ≥Ñ Ìï© Í≥ÑÏÇ∞: shape = [T]\n",
    "            #     timestep_sums = inputs.sum(dim=(1,2,3,4))  # sum over (B, C, H, W)\n",
    "\n",
    "            #     # 10 Ïù¥ÏÉÅÏù∏ ÌÉÄÏûÑÏä§ÌÖùÎßå ÏÑ†ÌÉù\n",
    "            #     valid_timesteps = timestep_sums >= timestep_sums_threshold\n",
    "            #     assert valid_timesteps.sum().item() != 0, \"No valid timesteps found. Check your data preprocessing.\"\n",
    "\n",
    "            #     # Ìï¥Îãπ ÌÉÄÏûÑÏä§ÌÖùÎßå Ï∂îÏ∂ú\n",
    "            #     inputs = inputs[valid_timesteps]\n",
    "            #     TIME = inputs.shape[0] # validÌïú time stepÏùò Í∞úÏàò\n",
    "            #     net.module.change_timesteps(TIME) # netÏóê TIME ÏÑ§Ï†ï\n",
    "            train_spike_distribution.append(TIME)\n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device).to(torch.float)\n",
    "            labels = labels.to(device).to(torch.long)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            bp_timestep = random.randint(0, TIME - 1)  # 0 ~ TIME-1 Ï§ë ÌïòÎÇò ÏÑ†ÌÉù\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = ((outputs_one_time.detach()).argmax(dim=1) >= 5).long()\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "                    # optimizer.additional_dw_weight = 1.0 if t == bp_timestep else 0.0\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            \n",
    "            # target_0 = [0,1,2,3,4]\n",
    "            # target_1 = [5,6,7,8,9]\n",
    "            predicted = (predicted >= 5).long()\n",
    "            train_predicted_distribution.append(predicted.cpu().numpy())\n",
    "\n",
    "\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            # if True :\n",
    "            if i == len(train_loader)-1 :\n",
    "                \n",
    "                \n",
    "                train_predicted_distribution = np.array(train_predicted_distribution)\n",
    "                unique_vals, counts = np.unique(train_predicted_distribution, return_counts=True)\n",
    "                for val, count in zip(unique_vals, counts):\n",
    "                    print(f\"train - Value {val}: {count} occurrences\")\n",
    "\n",
    "                print(f'train_spike_distribution.mean {np.mean(train_spike_distribution):.6f}, min {np.min(train_spike_distribution)}, max {np.max(train_spike_distribution)}')\n",
    "\n",
    "\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "                \n",
    "                test_spike_distribution = []\n",
    "                test_predicted_distribution = []\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    # for data_val in train_loader:\n",
    "                    for data_val in test_loader:\n",
    "                    # for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "                            \n",
    "                        ## batch ÌÅ¨Í∏∞ ######################################\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        ###########################################################\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif (which_data == 'n_tidigits_tonic'):\n",
    "                            inputs_val = inputs_val.unsqueeze(-1)\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                            # labels_val = torch.tensor(labels_val)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "                        \n",
    "                        ## Îç∞Ïù¥ÌÑ∞ÎßàÎã§ TIMESTEPSÎã§Î•¥Îã§ ########################################################\n",
    "                        hetero_timesteps = True\n",
    "                        if hetero_timesteps == True:\n",
    "                            assert real_batch == 1\n",
    "                            this_data_timesteps = inputs_val.shape[0]\n",
    "                            TIME = this_data_timesteps//temporal_filter\n",
    "                            net.module.change_timesteps(TIME) # netÏóê TIME ÏÑ§Ï†ï\n",
    "                        ## Îç∞Ïù¥ÌÑ∞ÎßàÎã§ TIMESTEPSÎã§Î•¥Îã§ ########################################################\n",
    "                        \n",
    "\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs_val = (inputs_val != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        \n",
    "                                    \n",
    "                        # if hetero_timesteps == True:\n",
    "                        #     assert real_batch == 1\n",
    "                        #     # inputs_val # [Time, Batch, Channel, Height, Width]\n",
    "                        #     # inputs_val timestpeÎ≥ÑÎ°ú sumÍ∞íÏù¥ 10ÎØ∏ÎßåÏùº Ïãú Ï†úÏô∏\n",
    "                        #     # time stepÎ≥Ñ Ìï© Í≥ÑÏÇ∞: shape = [T]\n",
    "                        #     timestep_sums = inputs_val.sum(dim=(1,2,3,4))  # sum over (B, C, H, W)\n",
    "\n",
    "                        #     # 10 Ïù¥ÏÉÅÏù∏ ÌÉÄÏûÑÏä§ÌÖùÎßå ÏÑ†ÌÉù\n",
    "                        #     valid_timesteps = timestep_sums >= timestep_sums_threshold\n",
    "                        #     assert valid_timesteps.sum().item() != 0, \"No valid timesteps found. Check your data preprocessing.\"\n",
    "\n",
    "                        #     # Ìï¥Îãπ ÌÉÄÏûÑÏä§ÌÖùÎßå Ï∂îÏ∂ú\n",
    "                        #     inputs_val = inputs_val[valid_timesteps]\n",
    "                        #     TIME = inputs_val.shape[0] # validÌïú time stepÏùò Í∞úÏàò\n",
    "                        #     net.module.change_timesteps(TIME) # netÏóê TIME ÏÑ§Ï†ï\n",
    "                        test_spike_distribution.append(TIME)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "                        # ##############################################################################################\n",
    "                        # dvs_visualization(inputs_val, labels_val, TIME, BATCH, my_seed)\n",
    "                        # #####################################################################################################\n",
    "\n",
    "                        inputs_val = inputs_val.to(torch.float).to(device)\n",
    "                        labels_val = labels_val.to(torch.long).to(device)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                            \n",
    "                            if max_activation_accul < outputs.abs().max().item() * TIME * (2**(-scale_exp[2][0])):\n",
    "                                max_activation_accul = outputs.abs().max().item() * TIME * (2**(-scale_exp[2][0]))\n",
    "                                print(f\"max_activation_accul updated: {max_activation_accul:.2f} at epoch {epoch}, iter {i}\")\n",
    "                       \n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                                    \n",
    "                        predicted = (predicted >= 5).long()\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "                        test_predicted_distribution.append(predicted.cpu().numpy())\n",
    "\n",
    "                    print(f'test_spike_distribution.mean {np.mean(test_spike_distribution):.6f}, min {np.min(test_spike_distribution)}, max {np.max(test_spike_distribution)}')\n",
    "\n",
    "                    test_predicted_distribution = np.array(test_predicted_distribution)\n",
    "                    unique_vals, counts = np.unique(test_predicted_distribution, return_counts=True)\n",
    "                    for val, count in zip(unique_vals, counts):\n",
    "                        print(f\"test - Value {val}: {count} occurrences\")\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "\n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    assert val_acc_best > 0.2\n",
    "                elif epoch > 10:\n",
    "                    assert val_acc_best > 0.4\n",
    "                elif epoch > 30:\n",
    "                    assert val_acc_best > 0.5\n",
    "                elif epoch > 100:\n",
    "                    assert val_acc_best > 0.6\n",
    "                    \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "# wandb.init(project=f'my_snn NTIDIGITS SWEEP LOSER ONOFF new251129',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "\n",
    "# my_snn_system(  devices = \"5\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 42,\n",
    "#                 TIME = 4, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 8, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "#                 # n_tidigits_tonic 8\n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'n_tidigits_tonic',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','n_tidigits_tonic', 'DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 256,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 # pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_20250704_185524_987.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 2, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 200,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 1, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 2, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = 9, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = False, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 8, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[0,0],[0,0],[0,0]], \n",
    "#                 # quantize_bit_list=[],\n",
    "#                 # scale_exp=[], \n",
    "#                 timestep_sums_threshold = 0,\n",
    "\n",
    "#                 loser_encourage_mode = False,# True # False\n",
    "                \n",
    "#                 lif_layer_sg_width2 = 16.0,\n",
    "#                 lif_layer_v_threshold2 = 512.0,\n",
    "#                 learning_rate2 = 1,\n",
    "#                 # init_scaling = [10000+ 12,10000+ 12,10000+ 11],\n",
    "#                 init_scaling = [1/2,1/4,1/16],\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ocd0ibmx with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 2048\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloser_encourage_mode: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttimestep_sums_threshold: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: n_tidigits_tonic\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251224_202144-ocd0ibmx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/ocd0ibmx' target=\"_blank\">confused-sweep-22</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9lv70ttl' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9lv70ttl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9lv70ttl' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9lv70ttl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/ocd0ibmx' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/ocd0ibmx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'timestep_sums_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'loser_encourage_mode' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251224_202152_199', 'my_seed': 42, 'TIME': 8, 'BATCH': 1, 'IMAGE_SIZE': 8, 'which_data': 'n_tidigits_tonic', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 2048, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 1, 'dvs_duration': 2, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': False, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 8, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'timestep_sums_threshold': 0, 'lif_layer_sg_width2': 64, 'lif_layer_v_threshold2': 128, 'init_scaling': [0.5, 0.25, 0.0625], 'learning_rate': 4, 'learning_rate2': 8, 'loser_encourage_mode': False} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target word: 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target word: 2\n",
      "\n",
      "\n",
      "\n",
      "train_dataset length = 4030, test_dataset length = 452\n",
      "\n",
      "len(train_loader): 4030 BATCH: 1 train_data_count: 4030\n",
      "len(test_loader): 452 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 2048\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHFCAYAAAAUpjivAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAB3eklEQVR4nO3deVxUVf8H8M/MMAyLgALJgKHimgpumHuiKZC7WVpqpmZKbrk+mllJ7pqppbmVCWmo/UptMRc0cQlNxcytxzbFJYhUAmWb7fz+4Jmb4wyrjLPweb9e9+Wde8+999xzjvd+OXeTCSEEiIiIiJyU3NYZICIiIrImBjtERETk1BjsEBERkVNjsENEREROjcEOEREROTUGO0REROTUGOwQERGRU2OwQ0RERE6NwQ4RERE5NQY75NTi4uIgk8ksDtOmTTNJW1BQgFWrVqFjx46oVq0aXF1dUaNGDQwcOBCHDh2S0qWkpGDcuHEICwuDl5cXAgIC0K1bN3z33Xcl5ufzzz+HTCbDtm3bzOY1a9YMMpkMe/fuNZtXt25dtGzZskz7Pnz4cNSuXbtMyxjFxsZCJpPh5s2bJaZdsGABdu7cWep131sHCoUC1apVQ7NmzRATE4Pjx4+bpb9y5QpkMhni4uLKsAdAQkICVqxYUaZlLG2rLGVRWhcvXkRsbCyuXLliNu9B6q0i/P7771CpVDh27Jg0rXPnzggNDS3V8jKZDLGxsdLv4va1vIQQ+PDDDxEeHg5vb2/4+fkhIiICu3btMkn3yy+/wNXVFadPn66wbZODEkRObOPGjQKA2Lhxozh27JjJkJqaKqX7+++/RXh4uFAqlSImJkbs3LlTHD58WGzZskU8//zzQqFQiDNnzgghhJg6dapo1aqVWLZsmThw4ID46quvRI8ePQQAER8fX2x+/v77byGTyURMTIzJ9Fu3bgmZTCY8PT3FjBkzTOZdu3ZNABBTpkwp077/9ttv4vTp02Vaxmj27NkCgPj7779LTOvp6SmGDRtW6nUDEM8++6w4duyYSE5OFnv27BFLly4VTZs2FQDEq6++apI+Pz9fHDt2TGRkZJRpH3r27Clq1apVpmUsbassZVFa//d//ycAiIMHD5rNe5B6qwj9+vUTPXv2NJkWEREhmjRpUqrljx07Jq5duyb9Lm5fy+vNN98UAMQrr7wi9u3bJ7766isRGRkpAIgvvvjCJO3w4cNFp06dKmzb5JgY7JBTMwY7J0+eLDZd9+7dhYuLizhw4IDF+SdOnJCCo7/++stsvk6nE02bNhV169YtMU9hYWGiYcOGJtO2b98ulEqlePXVV0Xr1q1N5n3yyScCgPj6669LXHdFsXawM27cOLPpOp1OvPTSSwKAWL16dVmya1FZgh2dTify8/MtznvYwY4tXbx4UQAQe/bsMZlelmDnftbY1xo1aoiOHTuaTMvLyxM+Pj6iT58+JtNPnTolAIjvv/++wrZPjoeXsajSS0lJwe7duzFy5Eg8+eSTFtM8/vjjqFmzJgCgevXqZvMVCgXCw8Nx7dq1ErfXpUsXXLp0CWlpadK0pKQkPP744+jRowdSUlJw584dk3kKhQJPPPEEgMIu/NWrV6N58+Zwd3dHtWrV8Oyzz+KPP/4w2Y6lyyH//PMPRo4cCV9fX1SpUgU9e/bEH3/8YXbpweivv/7CoEGD4OPjg4CAALz00kvIysqS5stkMuTk5CA+Pl66NNW5c+cSy8AShUKBVatWwd/fH++884403dKlpb///hujR49GcHAwVCoVHnnkEXTo0AH79+8HUHjZZdeuXUhNTTW5bHbv+pYsWYJ58+YhJCQEKpUKBw8eLPaS2bVr19C/f394e3vDx8cHL7zwAv7++2+TNEWVY+3atTF8+HAAhZdWBwwYAKCwLRjzZtympXrLz8/HzJkzERISIl1eHTduHP755x+z7fTq1Qt79uxBy5Yt4e7ujsceewwff/xxCaVfaM2aNVCr1YiMjLQ4/8iRI2jbti3c3d1Ro0YNvPnmm9Dr9UWWQUn7Wl5KpRI+Pj4m09zc3KThXuHh4WjUqBHWrl37QNskx8ZghyoFvV4PnU5nMhjt27cPANCvX79yr1+n0+HIkSNo0qRJiWm7dOkCoDCIMTp48CAiIiLQoUMHyGQyHDlyxGRey5YtpYN7TEwMJk2ahG7dumHnzp1YvXo1Lly4gPbt2+Ovv/4qcrsGgwG9e/dGQkICZsyYgR07dqBNmzZ46qmnilzmmWeeQYMGDfDFF1/gtddeQ0JCAiZPnizNP3bsGNzd3dGjRw8cO3YMx44dw+rVq0ssg6K4u7ujW7duuHz5Mq5fv15kuqFDh2Lnzp146623sG/fPnz00Ufo1q0bbt26BQBYvXo1OnToALVaLeXr3ntQAOD999/Hd999h6VLl2L37t147LHHis3b008/jXr16uHzzz9HbGwsdu7ciejoaGi12jLtY8+ePbFgwQIAwAcffCDlrWfPnhbTCyHQr18/LF26FEOHDsWuXbswZcoUxMfH48knn0RBQYFJ+p9++glTp07F5MmT8eWXX6Jp06YYOXIkDh8+XGLedu3ahU6dOkEuNz81pKen4/nnn8eQIUPw5Zdf4tlnn8W8efMwceLEcu+rwWAw+39pabg/oJo4cSL27NmDDRs2IDMzE2lpaZgyZQqysrLw6quvmuWjc+fO2L17N4QQJZYBOSkb9ywRWZXxMpalQavVCiGEeOWVVwQA8d///rfc25k1a5YAIHbu3Fli2tu3bwu5XC5Gjx4thBDi5s2bQiaTSZcOWrduLaZNmyaEEOLq1asCgJg+fboQovB+CADi3XffNVnntWvXhLu7u5ROCCGGDRtmchln165dAoBYs2aNybILFy4UAMTs2bOlacZLN0uWLDFJO3bsWOHm5iYMBoM0raIuYxnNmDFDABA//PCDEEKIy5cvS/ddGVWpUkVMmjSp2O0UdRnLuL66desKjUZjcd692zKWxeTJk03SfvrppwKA2Lx5s8m+3VuORrVq1TIpo+Iu7dxfb3v27LFYF9u2bRMAxPr160224+bmZnI/Wl5envD19TW7T+x+f/31lwAgFi1aZDYvIiJCABBffvmlyfRRo0YJuVxusr37y6C4fTWWbUmDpXpcu3atUKlUUhpfX1+RmJhocd8+/PBDAUD8/PPPxZYBOS/27FCl8Mknn+DkyZMmg4uLS4Ws+6OPPsL8+fMxdepU9O3bt8T0xqePjD07hw4dgkKhQIcOHQAAEREROHjwIABI/xp7g7755hvIZDK88MILJn/5qtVqk3VaYnyibODAgSbTBw0aVOQyffr0MfndtGlT5OfnIyMjo8T9LC9Rir++W7dujbi4OMybNw/Hjx8vc+8KULhvSqWy1OmHDBli8nvgwIFwcXGR6shajE/5GS+DGQ0YMACenp44cOCAyfTmzZtLl1yBwss7DRo0QGpqarHb+fPPPwFYvkwLAF5eXmbtYfDgwTAYDKXqNbJk9OjRZv8vLQ1ff/21yXIbN27ExIkTMX78eOzfvx/ffvstoqKi0LdvX4tPMxr36caNG+XKJzm+ijnaE9m5Ro0aoVWrVhbnGU8Mly9fRsOGDcu03o0bNyImJgajR482uc+kJF26dMGyZcvw559/4uDBgwgPD0eVKlUAFAY77777LrKysnDw4EG4uLigY8eOAArvoRFCICAgwOJ669SpU+Q2b926BRcXF/j6+ppML2pdAODn52fyW6VSAQDy8vJK3slyMp6Ug4KCikyzbds2zJs3Dx999BHefPNNVKlSBU8//TSWLFkCtVpdqu0EBgaWKV/3r9fFxQV+fn7SpTNrMdbbI488YjJdJpNBrVabbf/+OgMK662kOjPOv/+eFyNL7cRYJuUtA7VaXWRwdS/j/VYAkJmZiXHjxuHll1/G0qVLpendu3dH586d8corr+Dy5csmyxv3yZrtluwbe3ao0ouOjgaAMr0rBigMdF5++WUMGzYMa9euNTkgl+Te+3aSkpIQEREhzTMGNocPH5ZuXDYGQv7+/pDJZDh69KjFv4CL2wc/Pz/odDrcvn3bZHp6enqp821teXl52L9/P+rWrYtHH320yHT+/v5YsWIFrly5gtTUVCxcuBDbt2836/0oTlnqCzAvJ51Oh1u3bpkEFyqVyuweGqD8wQDwb73dfzO0EALp6enw9/cv97rvZVzP/e3DyNL9YMYysRRglcacOXOgVCpLHOrWrSstc+nSJeTl5eHxxx83W1+rVq1w5coV3L1712S6cZ8qqqzI8TDYoUqvZcuW6N69OzZs2FDkiwFPnTqFq1evSr/j4uLw8ssv44UXXsBHH31U5hNnp06doFAo8Pnnn+PChQsmTzD5+PigefPmiI+Px5UrV6TACAB69eoFIQRu3LiBVq1amQ1hYWFFbtMYUN3/QsOtW7eWKe/3K02vQWno9XqMHz8et27dwowZM0q9XM2aNTF+/HhERkaavDyuovJl9Omnn5r8/uyzz6DT6Uzqrnbt2jh79qxJuu+++87s5FuWHrKuXbsCADZv3mwy/YsvvkBOTo40/0HVqlUL7u7u+P333y3Ov3PnDr766iuTaQkJCZDL5ejUqVOR6y1uX8tzGcvY43f/CyiFEDh+/DiqVasGT09Pk3l//PEH5HJ5mXtuyXnwMhYRCu/peeqpp9C9e3e89NJL6N69O6pVq4a0tDR8/fXX2LJlC1JSUlCzZk383//9H0aOHInmzZsjJiYGJ06cMFlXixYtpAN8Uby9vdGyZUvs3LkTcrlcul/HKCIiQnr7773BTocOHTB69GiMGDECp06dQqdOneDp6Ym0tDQcPXoUYWFhGDNmjMVtPvXUU+jQoQOmTp2K7OxshIeH49ixY/jkk08AwOITOKURFhaGpKQkfP311wgMDISXl1eJJ5W//voLx48fhxACd+7cwfnz5/HJJ5/gp59+wuTJkzFq1Kgil83KykKXLl0wePBgPPbYY/Dy8sLJkyexZ88e9O/f3yRf27dvx5o1axAeHg65XF7kpczS2L59O1xcXBAZGYkLFy7gzTffRLNmzUzugRo6dCjefPNNvPXWW4iIiMDFixexatUqs8ekjW8jXr9+Pby8vODm5oaQkBCLPSSRkZGIjo7GjBkzkJ2djQ4dOuDs2bOYPXs2WrRogaFDh5Z7n+7l6uqKdu3aWXyLNVDYezNmzBhcvXoVDRo0wLfffosPP/wQY8aMMblH6H7F7WtQUFCxlystqVmzJvr374/169dDpVKhR48eKCgoQHx8PL7//nvMnTvX7I+P48ePo3nz5qhWrVqZtkVOxJZ3RxNZW2lfKihE4VMr77//vmjXrp3w9vYWLi4uIigoSPTv31/s2rVLSjds2LBinxy5fPlyqfI2ffp0AUC0atXKbN7OnTsFAOHq6ipycnLM5n/88ceiTZs2wtPTU7i7u4u6deuKF198UZw6dcokn/c/xXL79m0xYsQIUbVqVeHh4SEiIyPF8ePHBQDx3nvvSemKepGesTzv3cczZ86IDh06CA8PDwFAREREFLvf95aVXC4X3t7eIiwsTIwePVocO3bMLP39T0jl5+eLV155RTRt2lR4e3sLd3d30bBhQzF79myTsrp9+7Z49tlnRdWqVYVMJhPGw51xfe+8806J27q3LFJSUkTv3r1FlSpVhJeXlxg0aJDZCyYLCgrE9OnTRXBwsHB3dxcRERHizJkzZk9jCSHEihUrREhIiFAoFCbbtFRveXl5YsaMGaJWrVpCqVSKwMBAMWbMGJGZmWmSrlatWmZvPxai8GmqkupFCCE2bNggFAqF+PPPP82Wb9KkiUhKShKtWrUSKpVKBAYGitdff116qtEIFp5IK2pfyysvL0+88847omnTpsLLy0v4+vqKtm3bis2bN5s8KSiEEHfu3BEeHh5mTzBS5SITgi8eIKrMEhISMGTIEHz//fdo3769rbNDNpSfn4+aNWti6tSpZbqUaM82bNiAiRMn4tq1a+zZqcQY7BBVIlu2bMGNGzcQFhYGuVyO48eP45133kGLFi1MPnZKldeaNWsQGxuLP/74w+zeF0ej0+nQuHFjDBs2DLNmzbJ1dsiGeM8OUSXi5eWFrVu3Yt68ecjJyUFgYCCGDx+OefPm2TprZCdGjx6Nf/75B3/88UexN7w7gmvXruGFF17A1KlTbZ0VsjH27BAREZFT46PnRERE5NQY7BAREZFTs2mwU7t2bchkMrNh3LhxAApfEhUbG4ugoCC4u7ujc+fOuHDhgsk6CgoKMGHCBPj7+8PT0xN9+vQp9mvJREREVLnY9J6dv//+G3q9Xvp9/vx5REZG4uDBg+jcuTMWL16M+fPnIy4uDg0aNMC8efNw+PBhXLp0CV5eXgCAMWPG4Ouvv0ZcXBz8/PwwdepU3L59GykpKVAoFKXKh8FgwJ9//gkvL68yvwmXiIiIbEP878WkQUFBxb8Y1VYv+LFk4sSJom7dusJgMAiDwSDUarVYtGiRND8/P1/4+PiItWvXCiGE+Oeff4RSqRRbt26V0ty4cUPI5XKxZ8+eUm/32rVrxb4kjgMHDhw4cOBgv8O1a9eKPc/bzaPnGo0GmzdvxpQpUyCTyfDHH38gPT0dUVFRUhqVSoWIiAgkJycjJiYGKSkp0Gq1JmmCgoIQGhqK5ORk6QOPJTH2El27dg3e3t4Vsj+5Gh1azz8AADgxqys8XO2mqMtMq9Vi3759iIqKglKptHV2nA7L1/pYxtbF8rU+Ry1ja58Ls7OzERwcLJ3Hi2I3Z+CdO3fin3/+kb5abPyabkBAgEm6gIAApKamSmlcXV3N3ooZEBBQ7JecCwoKTL5MfOfOHQCAu7s73N3dH3hfAEAodJCrPP5drwMHOy4uLvDw8IC7u7tD/SdzFCxf62MZWxfL1/octYytfS7UarUAUOItKHZzBt6wYQO6d+9u9lG4+3dACFHiTpWUZuHChXj77bfNpu/btw8eHh5lyHXRCvSAsXj37t0HVeluH7JriYmJts6CU7NF+WoNwOZfC69zv1DfAKWTP5/pjG3YnurQGcvX3jhaGVv7XJibm1uqdHYR7KSmpmL//v3Yvn27NE2tVgMo7L0JDAyUpmdkZEi9PWq1GhqNBpmZmSa9OxkZGcV+42fmzJmYMmWK9NvYDRYVFVWhl7Gmn/gOABAdHeXwl7ESExMRGRnpUH9ROApblm+uRodpPxS20/iobg7dTovjzG3YHurQmcvXXjhqGVv7XJidnV2qdHZxZNu4cSOqV6+Onj17StNCQkKgVquRmJiIFi1aACi8r+fQoUNYvHgxACA8PBxKpRKJiYkYOHAgACAtLQ3nz5/HkiVLityeSqWCSqUym65UKiusEbnJ5Him5aOF4ypXKF0cv2unIsuHzNmifJXi3x7Qwu3bxSHBapyxDdtTHTpj+dobRytja58LS1sWNj+yGQwGbNy4EcOGDYOLy7/ZkclkmDRpEhYsWID69eujfv36WLBgATw8PDB48GAAgI+PD0aOHImpU6fCz88Pvr6+mDZtGsLCwtCtWzdb7RIAQOWiwLsDm9k0D0RE1qDX66V7JYDCXgcXFxfk5+ebvE6EKo4jl/H8Pg0BAEKnRb5OW0JqU0qlstSvkSmOzYOd/fv34+rVq3jppZfM5k2fPh15eXkYO3YsMjMz0aZNG+zbt8/kruvly5fDxcUFAwcORF5eHrp27Yq4uLgKKRwiIvqXEALp6en4559/zKar1Wpcu3aN7yqzkspcxlWrVoVarX6g/bZ5sBMVFQVRxHsNZTIZYmNjERsbW+Tybm5uWLlyJVauXGmlHJaPEAJ52sLo212pqHSNk4icjzHQqV69Ojw8PKTjmsFgwN27d1GlSpXiX+xG5eaoZSyEgOF/p3i5rOSnpu5fNjc3FxkZGQBgcv9uWdk82HFWeVo9Gr+1FwBwcU600974SUSVg16vlwIdPz8/k3kGgwEajQZubm4OdSJ2JI5axnqDwIU/swAATYJ8oJCX7Q9/4+tgMjIyUL169XJftXGcEiMiIpsx3qNTUa/nICotY5u79z6xsmKwQ0REpcZL8vSwVUSbY7BDRERETo3BDhERUSV169YtVK9eHVeuXHno2542bRpeffXVh7ItBjtEROS0hg8fjn79+pn8lslkWLRokUm6nTt3SpdLjGmKGwBAp9PhjTfeQEhICNzd3VGnTh3MmTMHBoPhoe3fg1q4cCF69+6N2rVrS9MmTpyI8PBwqFQqNG/e3GyZpKQk9O3bF4GBgfD09ETz5s3x6aefmqQxlqGLQo5mwdXQLLgaXBRyNGnSREozffp0bNy4EZcvX7bW7kkY7BARUaXi5uaGxYsXIzMz0+L89957D2lpadIAFL7p//5pixcvxtq1a7Fq1Sr8/PPPWLJkCd555x27exVKUfLy8rBhwwa8/PLLJtOFEHjppZfw3HPPWVwuOTkZTZs2xRdffIGzZ8/ipZdewosvvoivv/5aSmMsw+s3/sSBlP9i34nz8PX1xYABA6Q01atXR1RUFNauXWudHbwHgx0rkctk6BGmRo8wNeS8oY/sFNup42Mdll23bt2gVquxcOFCi/N9fHygVqulAfj3xXb3Tjt27Bj69u2Lnj17onbt2nj22WcRFRWFU6dOFbnt2NhYNG/eHB9//DFq1qyJKlWqYMyYMdDr9ViyZAnUajWqV6+O+fPnmyy3fPlytG/fHl5eXggODsbYsWNx9+5daf5LL72Epk2boqCgAEDhk0vh4eEYMmRIkXnZvXs3XFxc0K5dO5Pp77//PsaNG4c6depYXO7111/H3Llz0b59e9StWxevvvoqnnrqKezYscOsDAPVatSt9Sgu//ccMjMzMWLECJN19enTB1u2bCkyjxWFwY6VuCkVWD0kHKuHhMNNybc5k31iO3V8tq7DXI0OuRod8jR6adw45Gv1FtNaGkqbtiIoFAosWLAAK1euxPXr18u9no4dO+LAgQP45ZdfAAA//fQTjh49ih49ehS73O+//47du3djz5492LJlCz7++GP07NkT169fl77/+MYbb+D48ePSMnK5HIsXL8bZs2cRHx+P7777DtOnT5fmv//++8jJycFrr70GAHjzzTdx8+ZNrF69ush8HD58GK1atSr3/t8rKysLvr6+ZtPlchlq+Xni688+Rbdu3VCrVi2T+a1bt8a1a9eQmppaIfkoCt90R0RE5WZ8eaolXRo+go0jWku/w+ful94sf782Ib7YFvNvD0PHxQdxO0djlu7Kop5m08rj6aefRvPmzTF79mxs2LChXOuYMWMGsrKy8Nhjj0GhUECv12P+/PkYNGhQscsZDAZ8/PHH8PLyQuPGjdGlSxdcunQJ3377LeRyORo2bIjFixcjKSkJbdu2BVB4H012dja8vb1Rt25dzJ07F2PGjJGCmSpVqmDz5s2IiIiAl5cX3n33XRw4cAA+Pj5F5uPKlSsICgoq177f6/PPP8fJkyexbt06i/PT0tKwe/duJCQkmM2rUaOGlJf7A6GKxJ4dIqJSqv3aLltngSrQ4sWLER8fj4sXL5Zr+W3btmHz5s1ISEjA6dOnER8fj6VLlyI+Pr7Y5WrXrm3yjceAgAA0btzY5M3IAQEB0mcSAODgwYN4+umnERwcDC8vL7z44ou4desWcnJypDTt2rXDtGnTMHfuXEydOhWdOnUqNh95eXlwc3Mr626bSEpKwvDhw/Hhhx+a3Hx8r7i4OFStWtXkRnEj4xuSc3NzHygfJWHPjpXkanT8XATZPbZTx2frOrw4JxoGgwF3su/Ay9vL5IR9/z1EKW92K3I996c9OqNLxWbUgk6dOiE6Ohqvv/46hg8fXubl//Of/+C1117D888/DwAICwtDamoqFi5ciGHDhhW5nFKpNPktk8ksTjM+1ZWamopevXphxIgRmD9/Pvz9/XH06FGMHDnS5K3CBoMB33//PRQKBX799dcS8+/v71/kTdqlcejQIfTu3RvLli3Diy++aDGNTm/A2vUfoXu/gVC4KM3m3759GwDwyCOPlDsfpcEjGxERlZuHqwsMBgN0rgp4uLoU+92msgRiDytoW7RoEZo3b44GDRqUednc3Fyz/VUoFBX+6PmpU6eg0+kwb948VK1aFXK5HJ999plZunfeeQc///wzDh06hOjoaGzcuNHshuB7tWjRAps3by5XnpKSktCrVy8sXrwYo0ePLjLdoUOHcPXKH+j3/AsW558/fx5KpbLIXqGKwmCHqBJzVyqQ8kY3aZwcD+vwwYSFhWHIkCHlely8d+/emD9/PmrWrIkmTZrgxx9/xLJly/DSSy9VaB7r1q0LnU6H9evX49lnn8WxY8fMHtc+c+YM3nrrLXz++efo0KED3nvvPUycOBERERFFPlUVHR2NmTNnIjMzE9WqVZOm//bbb7h79y7S09ORl5eHM2fOAAAaN24MV1dXJCUloWfPnpg4cSKeeeYZpKenAwBcXV3NblLe+PHHCGvRCvUfa2wxD0eOHMETTzwhXc6yFt6zQ1SJyWQy+FVRwa+Kyu6+ecT7Y0rHnuvQUcydOxdCiDIvt3LlSjz77LMYO3YsGjVqhGnTpiEmJgZz586t0Pw1b94c7777Lt577z00bdoUn376qclj8/n5+RgyZAiGDx+O3r17AwBGjhyJbt26YejQodDrLd8UHhYWhlatWpn1Er388sto0aIF1q1bh19++QUtWrRAixYt8OeffwIovAcnNzcXCxcuRGBgoDT079/fZD1ZWVnYvv0LPF1Erw4AbNmyBaNGjSpXuZSFTJSnhp1MdnY2fHx8kJWVBW9v7wpZp62vo1ckrVaLb7/9Fj169DC7rkwPjuVrWe3XdlXYkzcVVcYVmSdHk5+fj8uXLyMkJMTsplaDwSA9KVTcZSwqP2uV8bfffotp06bh/PnzVqk7vUHgwp9ZAIAmQT5QyP8NyHft2oX//Oc/OHv2LFxcij5HFtf2Snv+ZqskqsQKdHq8ufM83tx5HgU6y3/9sYfFvpWmDomK0qNHD8TExODGjRsPfds5OTnYuHFjsYFORWGwQ1SJ6Q0Cm46nYtPxVOgN5e/kZUBkOxVVh1R5TZw4EcHBwQ99uwMHDkSbNm0eyrYc99qKnZPLZOjS8BFpnIiIqLKRAfByU0rjtsKeHStxUyqwcURrbBzRmq/hJ6pglb0nqbLvPzkOuVyGEH9PhPh7Qi63XbjDYIeIyo0n3aKxbIjsB4MdIiIicmoMdqwkV6NDozf3oNGbeyrsS71ERESORG8QOH8jC+dvZNn0BnreoGxFRX3dl4iIqLIw2MHr/NizQ0SVFu+rIaocGOwQEVUgBlBkDQqFArt2PXjb+u677/DYY49V+MdKy6OgoAA1a9ZESkqK1bfFYIeIiJzW8OHD0a9fP5PfMpkMixYtMkm3c+dO6dtixjTFDQCg0+nwxhtvICQkBO7u7qhTpw7mzJljlUDixo0b6Nat2wOvZ/r06Zg1a1axn4a4cOECnnnmGdSuXRsymQwrVqwwS7Nw4UI8/vjj8PLyQvXq1dGvXz9cunTJJM3du3fx6oTxiHy8CVrXC0Rok8ZYs2aNNF+lUmHatGmYMWPGA+9XSRjsEBFRpeLm5obFixcjMzPT4vz33nsPaWlp0gAAGzduNJu2ePFirF27FqtWrcLPP/+MJUuW4J133inXF9RLolaroVKpHmgdycnJ+PXXXzFgwIBi0+Xm5qJOnTpYtGgR1Gq1xTSHDh3CuHHjcPz4cSQmJkKn0yEqKgo5OTlSmsmTJ2Pv3r1Y8P467Dj4AyZOnIQJEybgyy+/lNIMGTIER44cwc8///xA+1YSBjtERFSpdOvWDWq12uTL4ffy8fGBWq2WBgCoWrWq2bRjx46hb9++6NmzJ2rXro1nn30WUVFROHXqVJHbjo2NRfPmzfHxxx+jZs2aqFKlCsaMGQO9Xo8lS5ZArVajevXqmD9/vsly917GunLlCmQyGbZv344uXbrAw8MDzZo1w7Fjx4rd761btyIqKsrsY5r3e/zxx/HOO+/g+eefLzLA2rNnD4YPH44mTZqgWbNm2LhxI65evWpySerYsWMY+uKLeLxdR9QIrolRo0ejWbNmJuXj5+eH9u3bY8uWLcXm6UEx2LESuUyGNiG+aBPiy89FkN1iO3V8tq7DXI0OuRod8jR6adw45N/3ROr988uTtiIoFAosWLAAK1euxPXr18u9no4dO+LAgQP45ZdfAAA//fQTjh49ih49ehS73O+//47du3djz5492LJlCz7++GP07NkT169fx6FDh7B48WK88cYbOH78eLHrmTVrFqZNm4YzZ86gQYMGGDRoEHS6osvo8OHDaNWqVdl3tBSysgq/bO7r6ytN69ixI775+mvcuZ0BD1cFkg4exC+//ILo6GiTZVu3bo0jR45YJV9GfPTcStyUCmyLaWfrbBAVyxHaae3XduHKop62zsZDU9b9tXUdNn5rb5HzujR8BBtHtJZ+h8/dX+QrOdqE+JrsR8fFB3E7R2OWrqLawtNPP43mzZtj9uzZ2LBhQ7nWMWPGDGRlZeGxxx6DQqGAXq/H/PnzMWjQoGKXMxgM+Pjjj+Hl5YXGjRujS5cuuHTpEr799lvI5XI0bNgQixcvRlJSEtq2bVvkeqZNm4aePQvL4+2330aTJk3w22+/4bHHHrOY/sqVKwgKCirXvhZHCIEpU6agY8eOCA0Nlaa///77GDVqFDo2awgXFxfI5XJ89NFH6Nixo8nyNWrUwJUrVyo8X/dizw4R0T34NFXlsXjxYsTHx+PixYvlWn7btm3YvHkzEhIScPr0acTHx2Pp0qWIj48vdrnatWvDy8tL+h0QEIDGjRub3DQcEBCAjIyMYtfTtGlTaTwwMBAAil0mLy/P5BLW1atXUaVKFWlYsGBBsdsryvjx43H27FmzS1Hvv/8+jh8/jq+++gopKSl49913MXbsWOzfv98knbu7O3Jzc8u17dJizw4REZXbxTnRMBgMuJN9B17eXiYn7Psvq6W8WfTTRPenPTqjS8Vm1IJOnTohOjoar7/+OoYPH17m5f/zn//gtddew/PPPw8ACAsLQ2pqKhYuXIhhw4YVuZxSqTT5LZPJLE4r6amue5cxPiFW3DL+/v4mN2UHBQXhzJkz0u97L0GV1oQJE/DVV1/h8OHDePTRR6XpeXl5eP3117Fjxw6p96lp06Y4c+YMli5davJk2e3bt/HII4+UedtlwWDHSnI1OnRcfBBA4X9aD1cWNdmfh9FOK9tlqIfN1scaD1cXGAwG6FwV8HB1KfaR5rLk7WHtx6JFi9C8eXM0aNCgzMvm5uaa7a9CobCLd9hY0qJFC5NeLBcXF9SrV69c6xJCYMKECdixYweSkpIQEhJiMl+r1UKr1UJAhot/ZgMAGqq9LJbP+fPn0aJFi3Llo7R4GcuKbudoLF5zJrInztpOK9PlKGetw4chLCwMQ4YMKdfj4r1798b8+fOxa9cuXLlyBTt27MCyZcvw9NNPWyGnDy46OhpHjx4tMZ1Go8GZM2dw5swZaDQa3LhxA2fOnMFvv/0mpRk3bpx0Cc/Lywvp6elIT09HXl4eAMDb2xsRERF4bcZ0HPv+MK5cuYz4uDh88sknZuVz5MgRREVFVezO3ofBDlEl5uaiwL7JnbBvcie4uSgqdN2VKdiwJWvWYWUxd+5ciHJ8v2nlypV49tlnMXbsWDRq1AjTpk1DTEwM5s6da4VcPrgXXngBFy9eNHv53/3+/PNPtGjRAi1atEBaWhqWLl2KFi1a4OWXX5bSrFmzBllZWejcuTMCAwOlYdu2bVKarVu3olWrxzFzwmj0f7ItlixZjPnz5+OVV16R0hw7dgxZWVl49tlnK36H78FrK0SVmFwuQ4MAr5ITkt1iHRYvLi6u2N8AUKtWLeTn5xe5jqICIS8vL6xYscLiG4aLEhsbi9jY2BLzlJSUZPJbr9cjO7vwclDt2rXN8lS1atUSA7Zq1aph/PjxWLZsGdatW1dkOkvrv19pgkO1Wo0NH3+MC38WPpbeJMgHCrnpvVnLli3Df/7zH7i7u5e4vgfBnh0icljsPSIqm1mzZqFWrVrQ6y2/AuBhKigoQLNmzTB58mSrb4vBDlElptEZsDzxFyxP/AUanX3eVOmMKjJIYx1SWfj4+OD111+HQmH7S54qlQpvvPGG1Xt1AF7GIqrUdAYD3jvwKwAgJqIOXPn3j8NhHRKVjP8rrEQuk6Hpoz5o+qgPX8NPdB9bX36y9fbLy1HzTZWXDIC7qwLurgrY8kzInh0rcVMq8NX4jiUnJCKHw3cHEZWOXC5D/eq2v4He5j07N27cwAsvvAA/Pz94eHigefPmJl9NFUIgNjYWQUFBcHd3R+fOnXHhwgWTdRQUFGDChAnw9/eHp6cn+vTp80AfdyMiIiLnYdNgJzMzEx06dIBSqcTu3btx8eJFvPvuu6hataqUZsmSJVi2bBlWrVqFkydPQq1WIzIyEnfu3JHSTJo0CTt27MDWrVtx9OhR3L17F7169bKLu82JiIx4GYrINmx6GWvx4sUIDg7Gxo0bpWm1a9eWxoUQWLFiBWbNmoX+/fsDAOLj4xEQEICEhATExMQgKysLGzZswKZNm6RvbWzevBnBwcHYv3+/2afkH5Y8jR7dlh0CAOyfEgF3V9vf+U5kb3g5iMi5GQwCv/xV2DnRIMALcrlt7tyxabDz1VdfITo6GgMGDMChQ4dQo0YNjB07FqNGjQIAXL58Genp6SavkVapVIiIiEBycjJiYmKQkpICrVZrkiYoKAihoaFITk62GOwUFBSgoKBA+m18UZPxWx4VQaPV4cY/ef8b18BF5ri3RxnLpKLKhkzZsny1Wp1JPrQy8xeFqRSiyLwZ51lKU955Fb28cd/u/bc06w6N3YvzsdEW5z2sfJdm3aWpw4qg1WohhIDBYDD7tpHxBXPG+VTxHLWMDQLQ6A3/GxeFE8q6DoMBQhS2+fsfmS/tcVMmyvOO7Api/NT8lClTMGDAAJw4cQKTJk3CunXr8OKLLyI5ORkdOnTAjRs3EBQUJC03evRopKamYu/evUhISMCIESNMghcAiIqKQkhIiMW3RMbGxuLtt982m56QkAAPD48K2bcCPTD9RGGAs6S1Dip27JAdYjt1fA+rDl1cXKBWqxEcHAxXV1frbIScjkEA13MKxx/1BMrTsaPRaHDt2jWkp6dDp9OZzMvNzcXgwYORlZUFb2/volcibEipVIp27dqZTJswYYJo27atEEKI77//XgAQf/75p0mal19+WURHRwshhPj000+Fq6ur2bq7desmYmJiLG43Pz9fZGVlScO1a9cEAHHz5k2h0WgqZPjnbq6oNeMbUWvGN+Kfu7kVtl5bDDk5OWLnzp0iJyfH5nlxxsGW5Vuadtrg9a+LXN44z1Ka8s6r6OUtlbGj5Ls0yz+sY012dra4cOGCyMnJEXq93mTQ6XQiMzNT6HQ6s3kcKmbQ6XRixowZomHDhsLDw0NUrVpVdO3aVSQnJ0tp/v77bzFu3DjRoEED4e7uLoKDg8X48ePF7du3S1z/qlWrRO3atYVKpRItW7YUSUlJZtt/6623RGBgoHBzcxMRERHi7NmzJa5Xq9OLn65lip+uZQqtrnz7npOTIy5cuCCys7PN2uXNmzcFAJGVlVVsvGHTayuBgYFo3LixybRGjRrhiy++AFD4XQ0ASE9PR2BgoJQmIyMDAQEBUhqNRoPMzExUq1bNJE379u0tblelUkGlUplNVyqVUCqVD7ZTxnWJf8PXwvU67mUso4osHzJni/ItTTst0MuKzJdxnqU05Z1X0cub7O//ytge8228f6ms635Yxxq9Xg+ZTAa5XA653PTZFuNlFeN8qngGgwF169bF+++/j3r16iEvLw/Lly/HU089hd9++w2PPPII0tPTpQ93Nm7cGKmpqXjllVeQlpaGzz//vMh1b9u2DZMnT8bq1avRoUMHrFu3Dj179sTFixdRs2ZNAIX32C5fvhxxcXFo0KAB5s2bh+joaFy6dAleXkU/Wi7uuWxV2D7K3rUjl8shk8ksHiNLe8y0aavs0KGD2ddXf/nlF9SqVQsAEBISArVajcTERGm+RqPBoUOHpEAmPDwcSqXSJE1aWhrOnz9fZLBDRESVQ+fOnTFhwgRMmjQJ1apVQ0BAANavX4+cnByMGDECXl5eqFu3Lnbv3i0to9frMXLkSISEhMDd3R0NGzbEe++9J83Pz89HkyZNMHr0aGna5cuX4ePjgw8//NBq+zJgwAB069YNderUQZMmTbBs2TJkZ2fj7NmzAIDQ0FB88cUX6N27N+rWrYsnn3wS8+fPx9dff212+edey5Ytw8iRI/Hyyy+jUaNGWLFiBYKDg7FmzRoA5g8LhYaGIj4+Hrm5uUhISLDa/lYkmwY7kydPxvHjx7FgwQL89ttvSEhIwPr16zFu3DgAhVHgpEmTsGDBAuzYsQPnz5/H8OHD4eHhgcGDBwMo/M7HyJEjMXXqVBw4cAA//vgjXnjhBYSFhUlPZxERkXXkanTI1eiQp9FL4yUNOv2/N9jq9AbkanTI1+otrvf+oTzi4+Ph7++PEydOYMKECRgzZgwGDBiA9u3b4/Tp04iOjsbQoUORm5sLoLAX5dFHH8Vnn32Gixcv4q233sLrr7+Ozz77DEDh/aaffvop4uPjsXPnTuj1egwdOhRdunSRHrCxpHv37qhSpUqxQ2lpNBqsX78ePj4+aNasWZHpjPeyuLhY7vHTaDRISUkxecgHKLzvNTk5GUDJDws5ApteW3n88cexY8cOzJw5E3PmzEFISAhWrFiBIUOGSGmmT5+OvLw8jB07FpmZmWjTpg327dtn0m22fPlyuLi4YODAgcjLy0PXrl0RFxdn0w+dySBD/epVpHEie8R26vhsXYeN39pb5mU+GNwSPZsW3pqw98JfGJdwGm1CfLEtpp2UpuPig7idozFbtjyvKmjWrBneeOMNAMDMmTOxaNEi+Pv7S4HJW2+9hTVr1uDs2bNo27YtlEqlyUMsISEhSE5OxmeffYaBAwcCAJo3b4558+Zh1KhRGDRoEH7//Xfs3Lmz2Hx89NFHyMvLK3P+7/XNN99g8ODByM3NRWBgIBITE+Hv728x7a1btzB37lzExMQUub6bN29Cr9dLt4YYBQQEID09HQCkfy2lSU1NLTa/MgBuLgpp3FZsfiNJr1690KtXryLny2QyxMbGIjY2tsg0bm5uWLlyJVauXGmFHJaPu6sCiVMibJ0NomKxnTo+1mHJmjZtKo0rFAr4+fkhLCxMmmY8iWdkZEjT1q5di48++gipqanIy8uDRqNB8+bNTdY7depUfPnll1i5ciV2795dZNBhVKNGjQfely5duuDMmTO4efMmPvzwQwwcOBA//PADqlevbpIuOzsbPXv2ROPGjTF79uwS1yu77xuOQgizaaVJcz+5XIYGatt/LsLmwQ4RETmui3OiYTAYcCf7Dry8vUp1g7Kr4t800U0CcHFOtNkHk4/O6FJhebz/Jlbjza73/gb+vdH6s88+w+TJk/Huu++iXbt28PLywjvvvIMffvjBZD0ZGRm4dOkSFAoFfv31Vzz11FPF5qN79+44cuRIsWnu3r1b7HxPT0/Uq1cP9erVQ9u2bVG/fn1s2LABM2fOlNLcuXMHTz31FKpUqYIdO3YUexOvv78/FAqF1Htz777d+yAQUPzDQvaOwQ4REZWbh6sLDAYDdK4KeLi6lPlpLBeFHC4K82U8XG13ejpy5Ajat2+PsWPHStN+//13s3QvvfQSQkNDMWrUKIwcORJdu3Y1e8L4XhVxGet+Qgizl+RGR0dDpVLhq6++kt5nVxRXV1eEh4cjMTERTz/9tDQ9MTERffv2BWD6sFCLFi0A/Puw0OLFiyt0f6yFwY6V5Gn06LPqKADgq/Ed+bkIskvO2E4r2yconLEOba1evXr45JNPsHfvXoSEhGDTpk04efIkQkJCpDQffPABjh07hrNnzyI4OBi7d+/GkCFD8MMPPxT50sUHuYyVk5ODOXPm4Nlnn0WNGjVw69YtrF69GtevX8eAAQMAFPboREVFITc3F5s3b0Z2drb0hYBHHnlEuo+1a9euePrppzF+/HgAhS/2HTp0KFq1aoV27dph/fr1uHr1Kl555RUApg8L1a9fH/Xr18eCBQtMHhYqisEg8FtGYW9VvepVKufnIpyZgMCv/6tgAZu9pJqoWGynjo91WPFeeeUVnDlzBs899xxkMhkGDRqEsWPHSo+n//e//8V//vMfbNiwAcHBwQAKg59mzZrhzTfftEpvh/FS2YABA3Dz5k34+fnh8ccfx5EjR9CkSRMAQEpKinSprV69eibLX758Wfr25O+//46bN29K85577jncunULc+bMQVpaGkJDQ/Htt99Kr4EBSvewkCUCQL5OL43bCoMdokpM5aLAllFtpXFyPKzD4iUlJZlNu3Llitk0cc+Xk1QqFTZu3GjykWoAWLhwIQDgsccekx5TN/L29sbly5cfPMNFcHNzw6ZNm+Dt7V3kpcLOnTub7EdRLO3/2LFjTS7b3a80DwvZMwY7RJWYQi5Du7p+ts4GPQDWIVHJ+F5vIiIicmrs2SGqxLR6A7acuAoAGNS6JpQWnooh+8Y6JCoZgx2iSkyrN+CtLy8AAJ4Nf5QnSgfEOiQqGYMdK5FBhhpV3aVxIiKiykaGf18iWak/F+Gs3F0V+P61J22dDSIiIpuRy2V4LNDb1tngDcpERETk3BjsEBERkVPjZSwrydfqMXDdMQDAZzHt4Kbky76IiKhyMRgEfr9Z+Ibvuv62+1wEe3asxCAEzl7PwtnrWTCU4o2WRERkH5KSkiCTyfDPP//YOisOT6Dw+215Gr1NPxfBYIeIiOge7du3R1paGnx8fGydFRO3b99G9+7dERQUBJVKheDgYIwfP1762CdQGKj17dsXgYGB8PT0RPPmzfHpp5+WuO7MzEwMHToUPj4+8PHxwdChQ82CvatXr6J3797w9PSEv78/Xn31VWg0moreTatgsENERHQPV1dXqNVqyGT29doQuVyOPn364KuvvsIvv/yCuLg47N+/X/o6OQAkJyejadOm+OKLL3D27Fm89NJLePHFF/H1118Xu+7BgwfjzJkz2LNnD/bs2YMzZ85g6NCh0ny9Xo+ePXsiJycHR48exdatW/HFF19g6tSpVtvfisRgh4iInFbnzp0xYcIETJo0CdWqVUNAQADWr1+PnJwcjBgxAl5eXqhbt670RXPA/DJWXFwcqlatir1796JRo0aoUqUKnnrqKaSlpT3UfalatSrGjBmDVq1aoVatWujatSvGjh2LI0eOSGlef/11zJ07F+3bt0fdunXx6quv4qmnnsKOHTuKXO/PP/+MPXv24KOPPkK7du3Qrl07fPjhh/jmm29w6dIlAMC+fftw8eJFbN68GS1atEC3bt3w7rvv4sMPPzTpWbJXDHaIiKjccjU65Gp0yNPopfGSBp3eIC2v0xuQq9EhX6u3uN77h/KIj4+Hv78/Tpw4gQkTJmDMmDEYMGAA2rdvj9OnTyM6OhpDhw41+5K5SX5yc7F06VJs2rQJhw8fxtWrVzFt2rRit1ulSpVih+7du5drf4z+/PNPbN++HREREcWmy8rKgq+vb5Hzjx07Bh8fH7Rp00aa1rZtW/j4+CA5OVlKExoaiqCgIClNdHQ0CgoKkJKS8kD78TDwaSwiIiq3xm/tLfMyHwxuiZ5NAwEAey/8hXEJp9EmxBfbYtpJaTouPojbOeb3g1xZ1LPM22vWrBneeOMNAMDMmTOxaNEi+Pv7Y9SoUQCAt956C2vWrMHZs2fRtm1bi+vQarVYu3Yt6tatCwAYP3485syZU+x2z5w5U+x8d3f3Mu5JoUGDBuHLL79EXl4eevfujY8++qjItJ9//jlOnjyJdevWFZkmPT0d1atXN5tevXp1pKenS2kCAgJM5lerVg2urq5SGnvGYMeKfD1dbZ0FohKxnTo+1mHxmjZtKo0rFAr4+fkhLCxMmmY8iWdkZBS5Dg8PDynQAYDAwMBi0wNAvXr1yptldO/eXbo8VatWLZw7d06at3z5csyePRuXLl3C66+/jilTpmD16tVm60hKSsLw4cPx4YcfokmTJsVuz9L9SUIIk+mlSWOJi9z2F5EY7FiJh6sLTr8ZaetsEBWL7dTx2boOL86JhsFgwJ3sO/Dy9oK8FCc213s+VhrdJAAX50RDft8J8+iMLhWWR6VSafJbJpOZTDOerA0GA4piaR2ihNeKVKlSpdj5TzzxhMm9Qvf66KOPkJeXZ3HbarUaarUajz32GPz8/PDEE0/gzTffRGBgoJTm0KFD6N27N5YtW4YXX3yx2Hyo1Wr89ddfZtP//vtvKRBUq9X44YcfTOZnZmZCq9Wa9fjcSyGXoXGQ7T8XwWCHiIjKzcPVBQaDATpXBTxcXUoV7NzLRSGHi4UvtXu4Ov7p6UEuY9WoUcPkd1GBmDHgKigokKYlJSWhV69eWLx4MUaPHl1iPtu1a4esrCycOHECrVu3BgD88MMPyMrKQvv27aU08+fPR1pamhRU7du3DyqVCuHh4SVuw9YcvzURERHZoQe5jGXJvn37cOfOHbRp0wZVqlTBxYsXMX36dHTo0AG1a9cGUBjo9OzZExMnTsQzzzwj3U/j6uoq3aR84sQJvPjiizhw4ABq1KiBRo0a4amnnsKoUaOke3tGjx6NXr16oWHDhgCAqKgoNG7cGEOHDsU777yD27dvY9q0aRg1ahS8vW3fc1MS219Ic1L5Wj2eW3cMz607ZvaUAZG9YDt1fKzDysPd3R0bNmxAx44d0ahRI0yaNAm9evXCN998I6WJi4tDbm4uFi5ciMDAQGno37+/lCY3NxeXLl2CVquVpn366acICwtDVFQUoqKi0LRpU2zatEmar1AosGvXLri5uaFDhw4YOHAg+vXrh6VLlxabZ4NB4Pe/7+L3v+/CYLDdO5TZs2MlBiHww+Xb0jiRPWI7dXysw+IlJSWZTbty5YrZtHvvv+ncubPJ7+HDh2P48OEm6fv161fiPTsV7YknnkDPnj2LvVQYFxeHuLi4Ytdz//4BgK+vLzZv3lzscjVr1jQJrEpDAMgp0EnjtsJgh6gSc1XI8cHgltI4OR7WIVHJGOwQVWIuCrn0vhNyTKxDopLxzwAiIiJyauzZIarEdHoD9l4ofL9GdJMAi48Ak31jHRKVjMEOUSWm0RswLuE0gMKXw/FE6Xgedh0+7JtyiSqizfHIZkXuSgXclQpbZ4OI6IEZ3+Jb3McyiSyRy2Rmb8guC2Obu/9N0mXBnh0r8XB1wc9zn7J1NoiIKoRCoUDVqlWl70F5eHiYfGZBo9EgPz+/zG9QptJx5DKu56cCAGg1BdCWkPZeQgjk5uYiIyMDVatWhUJR/s4DBjtERFQqarUagPkHM4UQyMvLg7u7e4kfhaTyqcxlXLVqVantlReDHSIiKhWZTIbAwEBUr17d5O27Wq0Whw8fRqdOnR7oUgMVrbKWsVKpfKAeHSMGO1aSr9VjzOYUAMCaF8Lhxnt3iMhJKBQKkxOQQqGATqeDm5tbpToRP0yOWsb2ci5ksGMlBiFw8NLf0jgREVFlYy/nQse6y4mIiIiojBjsEBERkVNjsENEREROjcEOEREROTWbBjuxsbGQyWQmw73P0gshEBsbi6CgILi7u6Nz5864cOGCyToKCgowYcIE+Pv7w9PTE3369MH169cf9q4QERGRnbJ5z06TJk2QlpYmDefOnZPmLVmyBMuWLcOqVatw8uRJqNVqREZG4s6dO1KaSZMmYceOHdi6dSuOHj2Ku3fvolevXtDr9bbYHSIiIrIzNn/03MXFxeKbEYUQWLFiBWbNmoX+/fsDAOLj4xEQEICEhATExMQgKysLGzZswKZNm9CtWzcAwObNmxEcHIz9+/cjOjr6oe7LvTxcXXBlUU+bbZ+oNNhOHR/rkOyZvbRPmwc7v/76K4KCgqBSqdCmTRssWLAAderUweXLl5Geno6oqCgprUqlQkREBJKTkxETE4OUlBRotVqTNEFBQQgNDUVycnKRwU5BQQEKCgqk39nZ2QAK31B571tBqZCxTFg21mHv5atSiCLzZpxnKU1551lj3ff/6yj5Lsu6bcne27AzYBlbVtrykImK+HZ6Oe3evRu5ublo0KAB/vrrL8ybNw///e9/ceHCBVy6dAkdOnTAjRs3EBQUJC0zevRopKamYu/evUhISMCIESNMAhcAiIqKQkhICNatW2dxu7GxsXj77bfNpickJMDDw6Nid5KIiIisIjc3F4MHD0ZWVha8vb2LTGfTnp3u3btL42FhYWjXrh3q1q2L+Ph4tG3bFgDMPngmhCjxI2glpZk5cyamTJki/c7OzkZwcDCioqKKLayyKNDqMe2L8wCApc+EQuXAn4vQarVITExEZGSkQ72m3FHYsnxL005DY/fifKzlXlLjPEtpyjvPGuu+v4wdJd+lWbc9HGt4jLA+Ry1ja7dP45WZktj8Mta9PD09ERYWhl9//RX9+vUDAKSnpyMwMFBKk5GRgYCAAACFX+DVaDTIzMxEtWrVTNK0b9++yO2oVCqoVCqz6UqlssIakVbIsOfCXwCAZc81h1JpV0VdLhVZPmTOFuVbmnZaoJcVmS/jPEtpyjvPmus2lrGj5bu4NPZ0rOExwvocrYyt3T5LWxY2fxrrXgUFBfj5558RGBiIkJAQqNVqJCYmSvM1Gg0OHTokBTLh4eFQKpUmadLS0nD+/Pligx0iKqRUyDGnbxPM6dsESoVdHQ6olFiHRCWzaXfDtGnT0Lt3b9SsWRMZGRmYN28esrOzMWzYMMhkMkyaNAkLFixA/fr1Ub9+fSxYsAAeHh4YPHgwAMDHxwcjR47E1KlT4efnB19fX0ybNg1hYWHS01lEVDSlQo4X29W2dTboAbAOiUpm02Dn+vXrGDRoEG7evIlHHnkEbdu2xfHjx1GrVi0AwPTp05GXl4exY8ciMzMTbdq0wb59++Dl5SWtY/ny5XBxccHAgQORl5eHrl27Ii4uDgqF494jQ0RERBXHpsHO1q1bi50vk8kQGxuL2NjYItO4ublh5cqVWLlyZQXnjsj56Q0CJy7fBgC0DvGFQl78zf9kf1iHRCVz/LtmiajcCnR6DPrwOADg4pxoeLjykOBoWIdEJePdbEREROTU+CeAlbgrFbg4J1oaJyIiqmzs5VzIYMdKZDIZu5OJiKhSs5dzIS9jERERkVNjsGMlBTo9pn72E6Z+9hMKdHpbZ4eIiOihs5dzIYMdK9EbBL44fR1fnL4OvcFm31olIiKyGXs5FzLYISIiIqfGYIeIiIicGoMdIiIicmoMdoiIiMipMdghIiIip8Zgh4iIiJya7V9r6KTclQqkvNFNGieyR2ynjo91SPbMXtongx0rkclk8KuisnU2iIrFdur4WIdkz+ylffIyFhERETk19uxYSYFOj3nf/AwAeKNXI6hc2L1M9oft1PGxDsme2Uv7ZM+OlegNApuOp2LT8VR+LoLsFtup42Mdkj2zl/bJnh2iSsxFLsfErvWlcXI8rEOikjHYIarEXF3kmBzZwNbZoAfAOiQqGf8MICIiIqfGnh2iSsxgEPjt77sAgHqPVIFcLrNxjqisWIdEJWOwQ1SJ5ev0iFp+GABwcU40PFx5SHA0rEOikvEyFhERETk1/glgJW4uChyZ3kUaJyIiqmzs5VzIYMdK5HIZgn09bJ0NIiIim7GXcyEvYxEREZFTY8+OlWh0BizddwkAMC2qIVxdGFcSEVHlYi/nQp6BrURnMGD94T+w/vAf0BkMts4OERHRQ2cv50IGO0REROTUGOwQERGRU2OwQ0RERE6NwQ4RERE5NQY7RERE5NQY7BAREZFT43t2rMTNRYF9kztJ40T2iO3U8bEOyZ7ZS/tksGMlcrkMDQK8bJ0NomKxnTo+1iHZM3tpn7yMRURERE6NPTtWotEZ8MHB3wAA47rU4+ciyC6xnTo+1iHZM3tpnwx2rERnMOC9A78CAGIi6sCVnWhkh9hOHR/rkOyZvbRPBjtElZhCLsPQtrWkcXI8rEOikjHYIarEVC4KzO0Xauts0ANgHRKVzG76OxcuXAiZTIZJkyZJ04QQiI2NRVBQENzd3dG5c2dcuHDBZLmCggJMmDAB/v7+8PT0RJ8+fXD9+vWHnHsiIiKyV3YR7Jw8eRLr169H06ZNTaYvWbIEy5Ytw6pVq3Dy5Emo1WpERkbizp07UppJkyZhx44d2Lp1K44ePYq7d++iV69e0Ov1D3s3iByOEAK37hbg1t0CCCFsnR0qB9YhUclsHuzcvXsXQ4YMwYcffohq1apJ04UQWLFiBWbNmoX+/fsjNDQU8fHxyM3NRUJCAgAgKysLGzZswLvvvotu3bqhRYsW2Lx5M86dO4f9+/fbapeIHEaeVo/wefsRPm8/8rT8A8ERsQ6JSmbze3bGjRuHnj17olu3bpg3b540/fLly0hPT0dUVJQ0TaVSISIiAsnJyYiJiUFKSgq0Wq1JmqCgIISGhiI5ORnR0dEWt1lQUICCggLpd3Z2NgBAq9VCq9VWyH5ptbp7xrXQyhz3Ly5jmVRU2ZApW5ZvadqpSiGKzJtxnqU05Z1njXXf/6+j5Ls067aHYw2PEdbnqGVs7fZZ2vKQCRv2e27duhXz58/HyZMn4ebmhs6dO6N58+ZYsWIFkpOT0aFDB9y4cQNBQUHSMqNHj0Zqair27t2LhIQEjBgxwiRwAYCoqCiEhIRg3bp1FrcbGxuLt99+22x6QkICPDw8KmTfDAK4llM4HuwJ8CEJskcFemD6icK/eZa01kHFrw04HNYh2TNrnwtzc3MxePBgZGVlwdvbu8h0NuvZuXbtGiZOnIh9+/bBzc2tyHQymWnJCCHMpt2vpDQzZ87ElClTpN/Z2dkIDg5GVFRUsYVVWWm1WiQmJiIyMhJKpdLW2XE6tizfXI0O0098BwCIjo6Ch6v5ISE0di/Ox1ruJTXOs5SmvPOsse77y9hR8l2adZemDq2NxwjrYxlbZrwyUxKbBTspKSnIyMhAeHi4NE2v1+Pw4cNYtWoVLl26BABIT09HYGCglCYjIwMBAQEAALVaDY1Gg8zMTJP7fTIyMtC+ffsit61SqaBSqcymK5VKNqJisHysyxblqxT//lFQuH3zQ0KBXlZkvozzLKUp7zxrrttYxo6W7+LSlKYOHxYeI6yPZWyqtGVhsxuUu3btinPnzuHMmTPS0KpVKwwZMgRnzpxBnTp1oFarkZiYKC2j0Whw6NAhKZAJDw+HUqk0SZOWlobz588XG+w8DBqdAesO/Y51h36HRmewaV6IiIhswV7OhTb7E8DLywuhoaYvwvL09ISfn580fdKkSViwYAHq16+P+vXrY8GCBfDw8MDgwYMBAD4+Phg5ciSmTp0KPz8/+Pr6Ytq0aQgLC0O3bt0e+j7dS2cwYOHu/wIAhrarxVe4ExFRpWMv50KbP41VnOnTpyMvLw9jx45FZmYm2rRpg3379sHL69/PxS9fvhwuLi4YOHAg8vLy0LVrV8TFxUGh4F16REREZGfBTlJSkslvmUyG2NhYxMbGFrmMm5sbVq5ciZUrV1o3c0REROSQytWfVKdOHdy6dcts+j///IM6deo8cKaIiIiIKkq5gp0rV65Y/BxDQUEBbty48cCZIiIiIqooZbqM9dVXX0nje/fuhY+Pj/Rbr9fjwIEDqF27doVljoiIiOhBlSnY6devH4DCe2mGDRtmMk+pVKJ27dp49913KyxzRERERA+qTMGOwVD4jHxISAhOnjwJf39/q2TKGahcFNgyqq00TmSP2E4dH+uQ7Jm9tM9yPY11+fLlis6H01HIZWhX18/W2SAqFtup42Mdkj2zl/ZZ7kfPDxw4gAMHDiAjI0Pq8TH6+OOPHzhjRERERBWhXMHO22+/jTlz5qBVq1YIDAws8cOclZFWb8CWE1cBAINa14RSwTcok/1hO3V8rEOyZ/bSPssV7KxduxZxcXEYOnRoRefHaWj1Brz15QUAwLPhj/IARHaJ7dTxsQ7JntlL+yxXsKPRaGz+oU0ienBymQw9wtTSODke1iFRycoV7Lz88stISEjAm2++WdH5IaKHyE2pwOoh4bbOBj0A1iFRycoV7OTn52P9+vXYv38/mjZtCqVSaTJ/2bJlFZI5IiIiogdVrmDn7NmzaN68OQDg/PnzJvN4szIRERHZk3IFOwcPHqzofBCRDeRqdGj81l4AwMU50fBwLffbKMhGWIdEJeNt+0REROTUyvUnQJcuXYq9XPXdd9+VO0POwlUhx8fDW0njRERElY29nAvLFewY79cx0mq1OHPmDM6fP2/2gdDKykUhx5OPBdg6G0RERDZjL+fCcgU7y5cvtzg9NjYWd+/efaAMEREREVWkCu1TeuGFF/hdrP/R6g34v1PX8H+nrkGrN5S8ABERkZOxl3Nhhd62f+zYMbi5uVXkKh2WVm/Afz4/CwDo2TSQr3AnIqJKx17OheUKdvr372/yWwiBtLQ0nDp1im9VJiIiIrtSrmDHx8fH5LdcLkfDhg0xZ84cREVFVUjGiIiIiCpCuYKdjRs3VnQ+iIiIiKzige7ZSUlJwc8//wyZTIbGjRujRYsWFZUvIiIiogpRrmAnIyMDzz//PJKSklC1alUIIZCVlYUuXbpg69ateOSRRyo6n0RERETlUq7boidMmIDs7GxcuHABt2/fRmZmJs6fP4/s7Gy8+uqrFZ1HIiIionIrV8/Onj17sH//fjRq1Eia1rhxY3zwwQe8Qfl/XBVyfDC4pTROZI/YTh0f65Dsmb20z3IFOwaDAUql0my6UqmEwcAX6AGFr8ju2TTQ1tkgKhbbqeNjHZI9s5f2Wa4w68knn8TEiRPx559/StNu3LiByZMno2vXrhWWOSIiIqIHVa5gZ9WqVbhz5w5q166NunXrol69eggJCcGdO3ewcuXKis6jQ9LpDdh1Ng27zqZBx89FkJ1iO3V8rEOyZ/bSPst1GSs4OBinT59GYmIi/vvf/0IIgcaNG6Nbt24VnT+HpdEbMC7hNADg4pxouPBaOtkhtlPHxzoke2Yv7bNMwc53332H8ePH4/jx4/D29kZkZCQiIyMBAFlZWWjSpAnWrl2LJ554wiqZJaKKJZfJ0CbEVxonx8M6JCpZmYKdFStWYNSoUfD29jab5+Pjg5iYGCxbtozBDpGDcFMqsC2mna2zQQ+AdUhUsjL1J/3000946qmnipwfFRWFlJSUB84UERERUUUpU7Dz119/WXzk3MjFxQV///33A2eKiIiIqKKUKdipUaMGzp07V+T8s2fPIjDQ9s/TE1Hp5Gp0aDk3ES3nJiJXo7N1dqgcWIdEJStTsNOjRw+89dZbyM/PN5uXl5eH2bNno1evXhWWOSKyvts5GtzO0dg6G/QAWIdExSvTDcpvvPEGtm/fjgYNGmD8+PFo2LAhZDIZfv75Z3zwwQfQ6/WYNWuWtfLqUJQKOd55tqk0TkREVNnYy7mwTMFOQEAAkpOTMWbMGMycORNCCACATCZDdHQ0Vq9ejYCAAKtk1NEoFXIMaBVs62wQERHZjL2cC8v8UsFatWrh22+/RWZmJn777TcIIVC/fn1Uq1bNGvkjIiIieiDleoMyAFSrVg2PP/54RebFqej0Bhz+tfDJtE71H+FbTYmIqNKxl3OhTc/Aa9asQdOmTeHt7Q1vb2+0a9cOu3fvluYLIRAbG4ugoCC4u7ujc+fOuHDhgsk6CgoKMGHCBPj7+8PT0xN9+vTB9evXH/aumNHoDXgp7hReijsFDb9XQ0RElZC9nAttGuw8+uijWLRoEU6dOoVTp07hySefRN++faWAZsmSJVi2bBlWrVqFkydPQq1WIzIyEnfu3JHWMWnSJOzYsQNbt27F0aNHcffuXfTq1Qt6vd5Wu0VERER2xKbBTu/evdGjRw80aNAADRo0wPz581GlShUcP34cQgisWLECs2bNQv/+/REaGor4+Hjk5uYiISEBQOH3uDZs2IB3330X3bp1Q4sWLbB582acO3cO+/fvt+WuERERkZ0o9z07FU2v1+P//u//kJOTg3bt2uHy5ctIT09HVFSUlEalUiEiIgLJycmIiYlBSkoKtFqtSZqgoCCEhoYiOTkZ0dHRFrdVUFCAgoIC6Xd2djYAQKvVQqvVVsj+aLW6e8a10MpEhazXFoxlUlFlQ6ZsWb6laacqhSgyb8Z5ltKUd5411n3/v46S79Ks2x6ONTxGWJ+jlrG122dpy0MmjM+P28i5c+fQrl075Ofno0qVKkhISECPHj2QnJyMDh064MaNGwgKCpLSjx49Gqmpqdi7dy8SEhIwYsQIk8AFKPxGV0hICNatW2dxm7GxsXj77bfNpickJMDDw6NC9qtAD0w/URhLLmmtg0pRIaslqlBsp46PdUj2zNrtMzc3F4MHD0ZWVpbFj5Qb2bxnp2HDhjhz5gz++ecffPHFFxg2bBgOHTokzZfJZCbphRBm0+5XUpqZM2diypQp0u/s7GwEBwcjKiqq2MIqi1yNDtNPfAcAiI6OgoerzYu63LRaLRITExEZGVnst9GofGxZvqVpp6Gxe3E+1nIvqXGepTTlnWeNdd9fxo6S79Ks2x6ONTxGWJ+jlrG126fxykxJbH4GdnV1Rb169QAArVq1wsmTJ/Hee+9hxowZAID09HST721lZGRILy5Uq9XQaDTIzMw0ec9PRkYG2rdvX+Q2VSoVVCqV2XSlUllhjUgp/g22Ctdr86J+YBVZPmTOFuVbmnZaoJcVmS/jPEtpyjvPmus2lrGj5bu4NPZ0rOExwvocrYyt3T5LWxZ29/IXIQQKCgoQEhICtVqNxMREaZ5Go8GhQ4ekQCY8PBxKpdIkTVpaGs6fP19ssPMwKBVyzOnbBHP6NuHnIshuGdupcZwcD481ZM/spX3atLvh9ddfR/fu3REcHIw7d+5g69atSEpKwp49eyCTyTBp0iQsWLAA9evXR/369bFgwQJ4eHhg8ODBAAAfHx+MHDkSU6dOhZ+fH3x9fTFt2jSEhYWhW7duttw1KBVyvNiutk3zQFQSYzt968sLPFE6KB5ryJ7ZS/u0abDz119/YejQoUhLS4OPjw+aNm2KPXv2IDIyEgAwffp05OXlYezYscjMzESbNm2wb98+eHl5SetYvnw5XFxcMHDgQOTl5aFr166Ii4uDQsG79IiIiMjGwc6GDRuKnS+TyRAbG4vY2Ngi07i5uWHlypVYuXJlBefuwegNAicu3wYAtA7xhUJe/E3VRLZwbzvVGwTbqQPisYbsmb20T/ZbW0mBTo9BHx7HoA+Po0DHtzmTfTK2U+M4OR4ea8ie2Uv7ZLBDVInJIEP96lWkcXI8xjqsX70K65CoCI7/PDQRlZu7qwKJUyJQ+7VdcHflfW6OyFiHRFQ09uwQERGRU2OwQ0RERE6NwQ5RJZan0SNy2SFpnByPsQ4jlx1iHRIVgffsEFViAgK/ZtyVxsnxsA6JSsZgx0pc5HLM7P6YNE5ERFTZ2Mu5kMGOlbi6yBETUdfW2SAiIrIZezkXssuBiIiInBp7dqxEbxA4fyMLABBaw4evcCciokrHXs6F7NmxkgKdHn0/+B59P/ier3AnIqJKyV7OhQx2iIiIyKkx2CEiIiKnxmCHiIiInBqDHSIiInJqDHaIiIjIqTHYISIiIqfG9+xYiYtcjold60vjRPbI2E7fO/Ar26mD4rGG7Jm9tE8GO1bi6iLH5MgGts4GUbGM7fS9A7/C1YUnSkfEYw3ZM3tpnzy6ERERkVNjz46VGAwCv/19FwBQ75EqkPNzEWSH7m2nBoNgO3VAPNaQPbOX9smeHSvJ1+kRtfwwopYfRj4/F0F2ythOjePkeHisIXtmL+2TwQ5RJefr6WrrLNAD8vV0ZT0SFYOXsYgqMQ9XF5x+MxK1X9sFD1ceDhyRsQ6JqGjs2SEiIiKnxmCHiIiInBqDHaJKLF+rx3Prjknj5HiMdfjcumOsQ6Ii8CI9USVmEAI/XL4tjZPjYR0SlYzBjpW4yOUY3amONE5ERFTZ2Mu5kMGOlbi6yPF6j0a2zgYREZHN2Mu5kF0ORERE5NTYs2MlBoPAjX/yAAA1qrrzFe5ERFTp2Mu5kD07VpKv0+OJJQfxxJKDfIU7ERFVSvZyLmSwQ0RERE6NwQ4RERE5NQY7RERE5NQY7BAREZFTY7BDRERETo3BDhERETk1vmfHShRyGYa2rSWNE9kjYzvddDyV7dRB8VhD9sxe2ieDHStRuSgwt1+orbNBVCxjO910PBUqF4Wts0PlwGMN2TN7aZ82vYy1cOFCPP744/Dy8kL16tXRr18/XLp0ySSNEAKxsbEICgqCu7s7OnfujAsXLpikKSgowIQJE+Dv7w9PT0/06dMH169ff5i7QkRERHbKpsHOoUOHMG7cOBw/fhyJiYnQ6XSIiopCTk6OlGbJkiVYtmwZVq1ahZMnT0KtViMyMhJ37tyR0kyaNAk7duzA1q1bcfToUdy9exe9evWCXm+7tzUKIXDrbgFu3S2AEMJm+SAqjrGdGsfJ8fBYQ/bMXtqnTS9j7dmzx+T3xo0bUb16daSkpKBTp04QQmDFihWYNWsW+vfvDwCIj49HQEAAEhISEBMTg6ysLGzYsAGbNm1Ct27dAACbN29GcHAw9u/fj+jo6Ie+XwCQp9UjfN5+AMDFOdHwcOUVQ7I/97bTPK2e7dQB8VhD9sxe2qdd/a/IysoCAPj6+gIALl++jPT0dERFRUlpVCoVIiIikJycjJiYGKSkpECr1ZqkCQoKQmhoKJKTky0GOwUFBSgoKJB+Z2dnAwC0Wi20Wm2F7ItWq7tnXAutzHH/4jKWSUWVDZmyZfmWpp2qFKLIvBnnWUpT3nnWWPf9/zpKvkuzbns41vAYYX2OWsbWbp+lLQ+ZsJN+TyEE+vbti8zMTBw5cgQAkJycjA4dOuDGjRsICgqS0o4ePRqpqanYu3cvEhISMGLECJPgBQCioqIQEhKCdevWmW0rNjYWb7/9ttn0hIQEeHh4VMj+FOiB6ScKY8klrXVQ8d5PIiKqZKx9LszNzcXgwYORlZUFb2/vItPZTc/O+PHjcfbsWRw9etRsnkxm+riaEMJs2v2KSzNz5kxMmTJF+p2dnY3g4GBERUUVW1hlkavRYfqJ7wAA0dFRDt21rNVqkZiYiMjISCiVSltnx+nYQ/mGxu7F+VjLl3xLM89SmvLOs8a67y9jR8l3WdZtS/bQhp2do5axtc+FxiszJbGLM/CECRPw1Vdf4fDhw3j00Uel6Wq1GgCQnp6OwMBAaXpGRgYCAgKkNBqNBpmZmahWrZpJmvbt21vcnkqlgkqlMpuuVCorrBEpxb+BVuF67aKoH0hFlg+Zs2X5FuhlRW67NPMspSnvPGuu21jGjpbvsqSxJR4jrM/Rytja58LSloVNn8YSQmD8+PHYvn07vvvuO4SEhJjMDwkJgVqtRmJiojRNo9Hg0KFDUiATHh4OpVJpkiYtLQ3nz58vMtghokL5Wj3GfpoijZPjMdbh2E9TWIdERbBpd8O4ceOQkJCAL7/8El5eXkhPTwcA+Pj4wN3dHTKZDJMmTcKCBQtQv3591K9fHwsWLICHhwcGDx4spR05ciSmTp0KPz8/+Pr6Ytq0aQgLC5OeziIiywxC4Ntz6dI4OZ5763DpANYhkSU2DXbWrFkDAOjcubPJ9I0bN2L48OEAgOnTpyMvLw9jx45FZmYm2rRpg3379sHLy0tKv3z5cri4uGDgwIHIy8tD165dERcXB4XCdncFK+QyPNPyUWmciIiosrGXc6FNg53SPAgmk8kQGxuL2NjYItO4ublh5cqVWLlyZQXm7sGoXBR4d2AzW2eDiIjIZuzlXMivnhMREZFTc/xHhOyUEAJ5/7tZ0F2pKPFReSIiImdjL+dC9uxYSZ5Wj8Zv7UXjt/ZKFU1ERFSZ2Mu5kMEOEREROTUGO0REROTUGOwQERGRU2OwQ0RERE6NwQ4RERE5NQY7RERE5NT4nh0rkctk6BGmlsaJ7JGxnX57Lp3t1EHxWEP2zF7aJ4MdK3FTKrB6SLits0FULGM7rf3aLrgpbfctOSo/HmvIntlL++RlLCIiInJqDHaIiIjIqTHYsZJcjQ61X9uF2q/tQq5GZ+vsEFlkbKfGcXI8PNaQPbOX9slgh4iIiJwagx2iSsxdqUDKG92kcXI8xjpMeaMb65CoCHwai6gSk8lk8KuiksbJ8dxbh0RkGXt2iIiIyKmxZ4eoEivQ6THvm5+lcZULL4M4mnvr8I1ejViHRBawZ4eoEtMbBDYdT5XGyfEY63DT8VTWIVER2LNjJXKZDF0aPiKNExERVTb2ci5ksGMlbkoFNo5obetsEBER2Yy9nAt5GYuIiIicGoMdIiIicmoMdqwkV6NDozf3oNGbe/gKdyIiqpTs5VzIe3asKE+rt3UWiIiIbMoezoXs2SEiIiKnxmCHiIiInBqDHSIiInJqDHaIiIjIqTHYISIiIqfGp7GsRC6ToU2IrzROZI+M7fSHy7fZTh0UjzVkz+ylfTLYsRI3pQLbYtrZOhtExTK209qv7YKbkl/LdkQ81pA9s5f2yctYRERE5NQY7BAREZFTY7BjJbkaHVrOTUTLuYn8XATZLWM7NY6T4+GxhuyZvbRP3rNjRbdzNLbOAlGJ2E4dH+uQ7Jk9tE/27BBVYm4uCuyb3EkaJ8djrMN9kzuxDomKwJ4dokpMLpehQYCXNE6O5946JCLL2LNDRERETo09O0SVmEZnwAcHf5PGXV3494+jubcOx3WpxzokssCm/ysOHz6M3r17IygoCDKZDDt37jSZL4RAbGwsgoKC4O7ujs6dO+PChQsmaQoKCjBhwgT4+/vD09MTffr0wfXr1x/iXhA5Lp3BgPcO/CqNk+Mx1uF7B35lHRIVwabBTk5ODpo1a4ZVq1ZZnL9kyRIsW7YMq1atwsmTJ6FWqxEZGYk7d+5IaSZNmoQdO3Zg69atOHr0KO7evYtevXpBr9c/rN2wSC6ToemjPmj6qA9f4U5ERJWSvZwLbXoZq3v37ujevbvFeUIIrFixArNmzUL//v0BAPHx8QgICEBCQgJiYmKQlZWFDRs2YNOmTejWrRsAYPPmzQgODsb+/fsRHR390Pblfm5KBb4a39Fm2yciIrI1ezkX2u09O5cvX0Z6ejqioqKkaSqVChEREUhOTkZMTAxSUlKg1WpN0gQFBSE0NBTJyclFBjsFBQUoKCiQfmdnZwMAtFottFqtlfbIcRnLhGVjHbYsX61Wd8+4FlqZMEujUogi82acZylNeedZY933/+so+S7NuktTh9bGY4T1sYwtK215yIQQD/9/hgUymQw7duxAv379AADJycno0KEDbty4gaCgICnd6NGjkZqair179yIhIQEjRowwCVwAICoqCiEhIVi3bp3FbcXGxuLtt982m56QkAAPD4+K2ykiO1egB6afKPybZ0lrHVR8TYvDYR1SZZabm4vBgwcjKysL3t7eRaaz254dI9l91/iEEGbT7ldSmpkzZ2LKlCnS7+zsbAQHByMqKqrYwiqLPI0e3Vd+DwDYPaED3F0d9wik1WqRmJiIyMhIKJVKW2fH6diyfHM1Okw/8R0AIDo6Ch6u5oeE0Ni9OB9ruZfUOM9SmvLOs8a67y9jR8l3adZdmjq0Nh4jrM9Ry9ja50LjlZmS2G2wo1arAQDp6ekIDAyUpmdkZCAgIEBKo9FokJmZiWrVqpmkad++fZHrVqlUUKlUZtOVSmWFNSKtkOHGP/kAABelC5RKuy3qUqvI8iFztihfpfj3j4LC7Zu30wK9rMh8GedZSlPeedZct7GMHS3fxaUpTR0+LDxGWJ+jlbG1z4WlLQu7fSFDSEgI1Go1EhMTpWkajQaHDh2SApnw8HAolUqTNGlpaTh//nyxwQ4RERFVHjbtbrh79y5+++036ffly5dx5swZ+Pr6ombNmpg0aRIWLFiA+vXro379+liwYAE8PDwwePBgAICPjw9GjhyJqVOnws/PD76+vpg2bRrCwsKkp7OIiIiocrNpsHPq1Cl06dJF+m28j2bYsGGIi4vD9OnTkZeXh7FjxyIzMxNt2rTBvn374OX173dgli9fDhcXFwwcOBB5eXno2rUr4uLioFA47j0yREREVHFsGux07twZxT0MJpPJEBsbi9jY2CLTuLm5YeXKlVi5cqUVckhERESOzm7v2SEiIiKqCI7/iJCdkkGG+tWrSONE9sjYTn/NuMt26qB4rCF7Zi/tk8GOlbi7KpA4JcLW2SAqlrGd1n5tl0O/C6oy47GG7Jm9tE9exiIiIiKnxmCHiIiInBqDHSvJ0+gRuewQIpcdQp5Gb+vsEFlkbKfGcXI8PNaQPbOX9sl7dqxEQODXjLvSOJE9Yjt1fKxDsmf20j7Zs0NUialcFNgyqq00To7HWIdbRrVlHRIVgT07RJWYQi5Du7p+0jg5nnvrkIgsY88OEREROTX27BBVYlq9AVtOXJXGlQr+/eNo7q3DQa1rsg6JLGCwQ1SJafUGvPXlBWmcJ0rHc28dPhv+KOuQyAIGO1Yigww1qrpL40RERJWNvZwLGexYiburAt+/9qSts0FERGQz9nIuZH8nEREROTUGO0REROTUGOxYSb5Wjz6rjqLPqqPI1/IV7kREVPnYy7mQ9+xYiUEInL2eJY0TERFVNvZyLmTPDhERETk1BjtERETk1BjsEBERkVNjsENEREROjcEOEREROTU+jWVFvp6uts4CUYl8PV1xO0dj62zQA+CxhuyZPbRPBjtW4uHqgtNvRto6G0TFMrbT2q/tgocrDweOiMcasmf20j55GYuIiIicGoMdIiIicmoMdqwkX6vHc+uO4bl1x/i5CLJbxnZqHCfHw2MN2TN7aZ+8SG8lBiHww+Xb0jiRPWI7dXysQ7Jn9tI+2bNDVIm5KuT4YHBLaZwcj7EOPxjcknVIVAT+zyCqxFwUcvRsGiiNk+Mx1mHPpoGsQ6Ii8H8GEREROTXes0NUien0Buy98Jc0zp4Bx3NvHUY3CWAdElnAYIeoEtPoDRiXcFoa54nS8dxbhxfnRLMOiSxgsGNF7kqFrbNARERkU/ZwLmSwYyUeri74ee5Tts4GERGRzdjLuZD9nUREROTUGOwQERGRU2OwYyX5Wj1GbDyBERtP8BXuRERUKdnLuZD37FiJQQgcvPS3NE5ERFTZ2Mu5kD07RERE5NScJthZvXo1QkJC4ObmhvDwcBw5csTWWSIiIiI74BTBzrZt2zBp0iTMmjULP/74I5544gl0794dV69etXXWiIiIyMacIthZtmwZRo4ciZdffhmNGjXCihUrEBwcjDVr1tg6a0RERGRjDh/saDQapKSkICoqymR6VFQUkpOTbZQrIiIishcO/zTWzZs3odfrERAQYDI9ICAA6enpFpcpKChAQUGB9DsrKwsAcPv2bWi12grJV65GB0NBLgDg1q1byHN13KLWarXIzc3FrVu3oFQqbZ0dp2PL8i1NO3XR5eDWrVsWlzfOs5SmvPOsse77y9hR8l2addvDsYbHCOtz1DK2dvu8c+cOAECU9KSXcHA3btwQAERycrLJ9Hnz5omGDRtaXGb27NkCAAcOHDhw4MDBCYZr164VGys4bnfD//j7+0OhUJj14mRkZJj19hjNnDkTU6ZMkX4bDAbcvn0bfn5+kMlkVs2vI8rOzkZwcDCuXbsGb29vW2fH6bB8rY9lbF0sX+tjGVsmhMCdO3cQFBRUbDqHD3ZcXV0RHh6OxMREPP3009L0xMRE9O3b1+IyKpUKKpXKZFrVqlWtmU2n4O3tzf9kVsTytT6WsXWxfK2PZWzOx8enxDQOH+wAwJQpUzB06FC0atUK7dq1w/r163H16lW88sorts4aERER2ZhTBDvPPfccbt26hTlz5iAtLQ2hoaH49ttvUatWLVtnjYiIiGzMKYIdABg7dizGjh1r62w4JZVKhdmzZ5td+qOKwfK1PpaxdbF8rY9l/GBkQvArlUREROS8HP6lgkRERETFYbBDRERETo3BDhERETk1BjtERETk1BjskGT+/Plo3749PDw8inzJ4tWrV9G7d294enrC398fr776KjQajUmac+fOISIiAu7u7qhRowbmzJlT8ndLKqnatWtDJpOZDK+99ppJmtKUORVt9erVCAkJgZubG8LDw3HkyBFbZ8khxcbGmrVVtVotzRdCIDY2FkFBQXB3d0fnzp1x4cIFG+bY/h0+fBi9e/dGUFAQZDIZdu7caTK/NGVaUFCACRMmwN/fH56enujTpw+uX7/+EPfCMTDYIYlGo8GAAQMwZswYi/P1ej169uyJnJwcHD16FFu3bsUXX3yBqVOnSmmys7MRGRmJoKAgnDx5EitXrsTSpUuxbNmyh7UbDsf4fijj8MYbb0jzSlPmVLRt27Zh0qRJmDVrFn788Uc88cQT6N69O65evWrrrDmkJk2amLTVc+fOSfOWLFmCZcuWYdWqVTh58iTUajUiIyOlDzWSuZycHDRr1gyrVq2yOL80ZTpp0iTs2LEDW7duxdGjR3H37l306tULer3+Ye2GY6iAb3GSk9m4caPw8fExm/7tt98KuVwubty4IU3bsmWLUKlUIisrSwghxOrVq4WPj4/Iz8+X0ixcuFAEBQUJg8Fg9bw7mlq1aonly5cXOb80ZU5Fa926tXjllVdMpj322GPitddes1GOHNfs2bNFs2bNLM4zGAxCrVaLRYsWSdPy8/OFj4+PWLt27UPKoWMDIHbs2CH9Lk2Z/vPPP0KpVIqtW7dKaW7cuCHkcrnYs2fPQ8u7I2DPDpXasWPHEBoaavLBtejoaBQUFCAlJUVKExERYfLiq+joaPz555+4cuXKw86yQ1i8eDH8/PzQvHlzzJ8/3+QSVWnKnCzTaDRISUlBVFSUyfSoqCgkJyfbKFeO7ddff0VQUBBCQkLw/PPP448//gAAXL58Genp6SZlrVKpEBERwbIup9KUaUpKCrRarUmaoKAghIaGstzv4zRvUCbrS09PN/uSfLVq1eDq6ip9dT49PR21a9c2SWNcJj09HSEhIQ8lr45i4sSJaNmyJapVq4YTJ05g5syZuHz5Mj766CMApStzsuzmzZvQ6/Vm5RcQEMCyK4c2bdrgk08+QYMGDfDXX39h3rx5aN++PS5cuCCVp6WyTk1NtUV2HV5pyjQ9PR2urq6oVq2aWRq2cVPs2XFylm4qvH84depUqdcnk8nMpgkhTKbfn0b87+ZkS8s6o7KU+eTJkxEREYGmTZvi5Zdfxtq1a7FhwwbcunVLWl9pypyKZqk9suzKrnv37njmmWcQFhaGbt26YdeuXQCA+Ph4KQ3LuuKVp0xZ7ubYs+Pkxo8fj+eff77YNPf3xBRFrVbjhx9+MJmWmZkJrVYr/fWhVqvN/qLIyMgAYP4XirN6kDJv27YtAOC3336Dn59fqcqcLPP394dCobDYHll2D87T0xNhYWH49ddf0a9fPwCFPQ2BgYFSGpZ1+RmfdCuuTNVqNTQaDTIzM016dzIyMtC+ffuHm2E7x54dJ+fv74/HHnus2MHNza1U62rXrh3Onz+PtLQ0adq+ffugUqkQHh4upTl8+LDJfSf79u1DUFBQqYMqR/cgZf7jjz8CgHRwK02Zk2Wurq4IDw9HYmKiyfTExESeCCpAQUEBfv75ZwQGBiIkJARqtdqkrDUaDQ4dOsSyLqfSlGl4eDiUSqVJmrS0NJw/f57lfj8b3hxNdiY1NVX8+OOP4u233xZVqlQRP/74o/jxxx/FnTt3hBBC6HQ6ERoaKrp27SpOnz4t9u/fLx599FExfvx4aR3//POPCAgIEIMGDRLnzp0T27dvF97e3mLp0qW22i27lZycLJYtWyZ+/PFH8ccff4ht27aJoKAg0adPHylNacqcirZ161ahVCrFhg0bxMWLF8WkSZOEp6enuHLliq2z5nCmTp0qkpKSxB9//CGOHz8uevXqJby8vKSyXLRokfDx8RHbt28X586dE4MGDRKBgYEiOzvbxjm3X3fu3JGOswCk40FqaqoQonRl+sorr4hHH31U7N+/X5w+fVo8+eSTolmzZkKn09lqt+wSgx2SDBs2TAAwGw4ePCilSU1NFT179hTu7u7C19dXjB8/3uQxcyGEOHv2rHjiiSeESqUSarVaxMbG8rFzC1JSUkSbNm2Ej4+PcHNzEw0bNhSzZ88WOTk5JulKU+ZUtA8++EDUqlVLuLq6ipYtW4pDhw7ZOksO6bnnnhOBgYFCqVSKoKAg0b9/f3HhwgVpvsFgELNnzxZqtVqoVCrRqVMnce7cORvm2P4dPHjQ4jF32LBhQojSlWleXp4YP3688PX1Fe7u7qJXr17i6tWrNtgb+yYTgq+2JSIiIufFe3aIiIjIqTHYISIiIqfGYIeIiIicGoMdIiIicmoMdoiIiMipMdghIiIip8Zgh4iIiJwagx0isgtxcXGoWrVqmZYZPny49F0mW7ty5QpkMhnOnDlj66wQ0X0Y7BBRmaxduxZeXl7Q6XTStLt370KpVOKJJ54wSXvkyBHIZDL88ssvJa73ueeeK1W6sqpduzZWrFhR4eslIsfBYIeIyqRLly64e/cuTp06JU07cuQI1Go1Tp48idzcXGl6UlISgoKC0KBBgxLX6+7ujurVq1slz0RUuTHYIaIyadiwIYKCgpCUlCRNS0pKQt++fVG3bl0kJyebTO/SpQuAwi82T58+HTVq1ICnpyfatGljsg5Ll7HmzZuH6tWrw8vLCy+//DJee+01NG/e3CxPS5cuRWBgIPz8/DBu3DhotVoAQOfOnZGamorJkydDJpNBJpNZ3KdBgwbh+eefN5mm1Wrh7++PjRs3AgD27NmDjh07omrVqvDz80OvXr3w+++/F1lOlvZn586dZnn4+uuvER4eDjc3N9SpUwdvv/22Sa8ZET04BjtEVGadO3fGwYMHpd8HDx5E586dERERIU3XaDQ4duyYFOyMGDEC33//PbZu3YqzZ89iwIABeOqpp/Drr79a3Mann36K+fPnY/HixUhJSUHNmjWxZs0as3QHDx7E77//joMHDyI+Ph5xcXGIi4sDAGzfvh2PPvoo5syZg7S0NKSlpVnc1pAhQ/DVV1/h7t270rS9e/ciJycHzzzzDAAgJycHU6ZMwcmTJ3HgwAHI5XI8/fTTMBgMZS/Ae7bxwgsv4NVXX8XFixexbt06xMXFYf78+eVeJxFZYOsvkRKR41m/fr3w9PQUWq1WZGdnCxcXF/HXX3+JrVu3ivbt2wshhDh06JAAIH7//Xfx22+/CZlMJm7cuGGynq5du4qZM2cKIYTYuHGj8PHxkea1adNGjBs3ziR9hw4dRLNmzaTfw4YNE7Vq1RI6nU6aNmDAAPHcc89Jv2vVqiWWL19e7P5oNBrh7+8vPvnkE2naoEGDxIABA4pcJiMjQwCQvkJ9+fJlAUD8+OOPFvdHCCF27Ngh7j3sPvHEE2LBggUmaTZt2iQCAwOLzS8RlQ17doiozLp06YKcnBycPHkSR44cQYMGDVC9enVERETg5MmTyMnJQVJSEmrWrIk6derg9OnTEEKgQYMGqFKlijQcOnSoyEtBly5dQuvWrU2m3f8bAJo0aQKFQiH9DgwMREZGRpn2R6lUYsCAAfj0008BFPbifPnllxgyZIiU5vfff8fgwYNRp04deHt7IyQkBABw9erVMm3rXikpKZgzZ45JmYwaNQppaWkm9z4R0YNxsXUGiMjx1KtXD48++igOHjyIzMxMREREAADUajVCQkLw/fff4+DBg3jyyScBAAaDAQqFAikpKSaBCQBUqVKlyO3cf3+LEMIsjVKpNFumPJeWhgwZgoiICGRkZCAxMRFubm7o3r27NL93794IDg7Ghx9+iKCgIBgMBoSGhkKj0Vhcn1wuN8uv8V4iI4PBgLfffhv9+/c3W97Nza3M+0BEljHYIaJy6dKlC5KSkpCZmYn//Oc/0vSIiAjs3bsXx48fx4gRIwAALVq0gF6vR0ZGhtnj6UVp2LAhTpw4gaFDh0rT7n0CrLRcXV2h1+tLTNe+fXsEBwdj27Zt2L17NwYMGABXV1cAwK1bt/Dzzz9j3bp1Uv6PHj1a7PoeeeQR3LlzBzk5OfD09AQAs3fwtGzZEpcuXUK9evXKvF9EVHoMdoioXLp06SI9+WTs2QEKg50xY8YgPz9fujm5QYMGGDJkCF588UW8++67aNGiBW7evInvvvsOYWFh6NGjh9n6J0yYgFGjRqFVq1Zo3749tm3bhrNnz6JOnTplymft2rVx+PBhPP/881CpVPD397eYTiaTYfDgwVi7di1++eUXkxuwq1WrBj8/P6xfvx6BgYG4evUqXnvttWK326ZNG3h4eOD111/HhAkTcOLECenGaaO33noLvXr1QnBwMAYMGAC5XI6zZ8/i3LlzmDdvXpn2k4iKxnt2iKhcunTpgry8PNSrVw8BAQHS9IiICNy5cwd169ZFcHCwNH3jxo148cUXMXXqVDRs2BB9+vTBDz/8YJLmXkOGDMHMmTMxbdo0tGzZEpcvX8bw4cPLfHlnzpw5uHLlCurWrYtHHnmk2LRDhgzBxYsXUaNGDXTo0EGaLpfLsXXrVqSkpCA0NBSTJ0/GO++8U+y6fH19sXnzZnz77bcICwvDli1bEBsba5ImOjoa33zzDRITE/H444+jbdu2WLZsGWrVqlWmfSSi4smEpYvgRER2KDIyEmq1Gps2bbJ1VojIgfAyFhHZpdzcXKxduxbR0dFQKBTYsmUL9u/fj8TERFtnjYgcDHt2iMgu5eXloXfv3jh9+jQKCgrQsGFDvPHGGxafXCIiKg6DHSIiInJqvEGZiIiInBqDHSIiInJqDHaIiIjIqTHYISIiIqfGYIeIiIicGoMdIiIicmoMdoiIiMipMdghIiIip8Zgh4iIiJza/wNbSPyhnD903QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 64, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHFCAYAAAAUpjivAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAByBElEQVR4nO3deVxU1f8/8NfMMKwCCiQDioDmLm6Y5pJoCuRWZmoumZapueOSS25o7ppZmprlVubSr3Lp44q5J6ai5PqxMlwykVJih9nO7w++3A/jDMg2zMLr+XjMw5lzz73n3OMc7nvOuYtMCCFAREREZKfklq4AERERkTkx2CEiIiK7xmCHiIiI7BqDHSIiIrJrDHaIiIjIrjHYISIiIrvGYIeIiIjsGoMdIiIismsMdoiIiMiuMdghu7Z582bIZDKTr8mTJxvkzcnJwerVq9GuXTtUqVIFjo6OqFatGvr27YsTJ05I+e7du4dXX30VNWvWhJubGzw9PdGsWTOsXr0aWq220Pp8++23kMlk2Llzp9GyJk2aQCaT4dChQ0bLatWqhebNmxdr34cMGYKgoKBirZMnOjoaMpkM//zzz1PzLly4ELt37y7ytvP/HygUClSpUgVNmjTBiBEjcPbsWaP8t2/fhkwmw+bNm4uxB8C2bduwcuXKYq1jqqzitEVRXb9+HdHR0bh9+7bRstL8v5WFW7duwcnJCbGxsVJahw4d0KhRoyKtL5PJEB0dLX0ubF9LSgiBzz//HKGhofDw8IC3tzfCwsKwb98+g3y//vorHB0dcfHixTIrm2yUILJjmzZtEgDEpk2bRGxsrMHrzp07Ur6///5bhIaGCqVSKUaMGCF2794tTp48KbZv3y769esnFAqFiI+PF0IIcePGDfHmm2+KjRs3iiNHjoj9+/eLMWPGCABi6NChhdbn77//FjKZTIwYMcIg/dGjR0Imkwk3NzcxdepUg2X37t0TAMTEiROLte+///67uHjxYrHWyTNnzhwBQPz9999Pzevm5iYGDx5c5G0DEL179xaxsbHizJkz4uDBg2L58uWicePGAoAYN26cQf7s7GwRGxsrkpKSirUP3bp1E4GBgcVax1RZxWmLovp//+//CQDi2LFjRstK8/9WFnr27Cm6detmkBYWFiYaNmxYpPVjY2PFvXv3pM+F7WtJzZo1SwAQ7777rjh8+LDYu3evCA8PFwDEd999Z5B3yJAhon379mVWNtkmBjtk1/KCnfPnzxear0uXLsLBwUH8+OOPJpefO3fOIDgypW/fvsLBwUFkZ2cXmi8kJETUrVvXIO37778XSqVSjBs3TrRs2dJg2ZdffikAiB9++KHQ7ZYlcwc7o0ePNkrXarXi7bffFgDEmjVrilNdk4oT7Gi12gL/38o72LGk69evCwDi4MGDBunFCXaeZI59rVatmmjXrp1BWlZWlvD09BQvv/yyQfqFCxcEAPHTTz+VWflkeziNRRVeXFwcDhw4gKFDh+LFF180mee5555DjRo1Ct3OM888A7lcDoVCUWi+jh074ubNm3jw4IGUdvz4cTz33HPo2rUr4uLikJaWZrBMoVDghRdeAJA7hL9mzRo0bdoULi4uqFKlCnr37o0//vjDoBxT0yH//vsvhg4dCi8vL1SqVAndunXDH3/8YTT1kOfhw4fo378/PD094evri7fffhspKSnScplMhoyMDGzZskWamurQoUOh+18QhUKB1atXw8fHB8uWLZPSTU0t/f333xg+fDgCAgLg5OSEZ555Bm3btsWRI0cA5E677Nu3D3fu3DGYNsu/vaVLl2L+/PkIDg6Gk5MTjh07VuiU2b1799CrVy94eHjA09MTb7zxBv7++2+DPAW1Y1BQEIYMGQIgd2q1T58+AHK/C3l1yyvT1P9bdnY2pk+fjuDgYGl6dfTo0fj333+NyunevTsOHjyI5s2bw8XFBfXq1cPGjRuf0vq51q5dC5VKhfDwcJPLT506heeffx4uLi6oVq0aZs2aBZ1OV2AbPG1fS0qpVMLT09MgzdnZWXrlFxoaivr162PdunWlKpNsG4MdqhB0Oh20Wq3BK8/hw4cBAD179izWNoUQ0Gq1SE5Oxs6dO7F582ZMmjQJDg4Oha7XsWNHALlBTJ5jx44hLCwMbdu2hUwmw6lTpwyWNW/eXPrjPmLECERFRaFz587YvXs31qxZg2vXrqFNmzZ4+PBhgeXq9Xr06NED27Ztw9SpU7Fr1y60atUKL730UoHrvPbaa6hTpw6+++47TJs2Ddu2bcOECROk5bGxsXBxcUHXrl0RGxuL2NhYrFmzptD9L4yLiws6d+6MhIQE/PnnnwXmGzRoEHbv3o3Zs2fj8OHD+OKLL9C5c2c8evQIALBmzRq0bdsWKpVKqlf+c1AA4JNPPsHRo0exfPlyHDhwAPXq1Su0bq+++iqeffZZfPvtt4iOjsbu3bsRGRkJjUZTrH3s1q0bFi5cCAD49NNPpbp169bNZH4hBHr27Inly5dj0KBB2LdvHyZOnIgtW7bgxRdfRE5OjkH+X375BZMmTcKECROwZ88eNG7cGEOHDsXJkyefWrd9+/ahffv2kMuNDw2JiYno168fBg4ciD179qB3796YP38+xo8fX+J91ev1Rv3S1OvJgGr8+PE4ePAgNmzYgOTkZDx48AATJ05ESkoKxo0bZ1SPDh064MCBAxBCPLUNyE5ZdmCJyLzyprFMvTQajRBCiHfffVcAEP/973+Lte1FixZJ25LJZGLGjBlFWu/x48dCLpeL4cOHCyGE+Oeff4RMJpOmDlq2bCkmT54shBDi7t27AoCYMmWKECL3fAgA4sMPPzTY5r1794SLi4uUTwghBg8ebDCNs2/fPgFArF271uR+zJkzR0rLm7pZunSpQd5Ro0YJZ2dnodfrpbSymsbKM3XqVAFA/Pzzz0IIIRISEqTzrvJUqlRJREVFFVpOQdNYedurVauWUKvVJpflLyuvLSZMmGCQ9+uvvxYAxNatWw32LX875gkMDDRoo8Kmdp78fzt48KDJ/4udO3cKAGL9+vUG5Tg7OxtMuWZlZQkvLy+j88Se9PDhQwFALF682GhZWFiYACD27NljkD5s2DAhl8sNynuyDQrb17y2fdrL1P/junXrhJOTk5THy8tLxMTEmNy3zz//XAAQN27cKLQNyH5xZIcqhC+//BLnz583eD1tBOZphgwZgvPnz+PQoUOYMmUKli1bhrFjxz51vbyrj/JGdk6cOAGFQoG2bdsCAMLCwnDs2DEAkP7NGw36z3/+A5lMhjfeeMPgl69KpTLYpil5V5T17dvXIL1///4FrvPyyy8bfG7cuDGys7ORlJT01P0sKVGEX98tW7bE5s2bMX/+fJw9e7bYoytA7r4plcoi5x84cKDB5759+8LBwUH6PzKXo0ePAoA0DZanT58+cHNzw48//miQ3rRpU4MpV2dnZ9SpUwd37twptJy//voLAFC1alWTy93d3Y2+DwMGDIBery/SqJEpw4cPN+qXpl4//PCDwXqbNm3C+PHjMWbMGBw5cgT79+9HREQEXnnlFZNXM+bt0/3790tUT7J9pftrT2Qj6tevjxYtWphclndgSEhIQN26dYu8TZVKBZVKBQCIiIhAlSpVMG3aNLz99tto1qxZoet27NgRK1aswF9//YVjx44hNDQUlSpVApAb7Hz44YdISUnBsWPH4ODggHbt2gHIPYdGCAFfX1+T261Zs2aBZT569AgODg7w8vIySC9oWwDg7e1t8NnJyQkAkJWVVej+lUbeQdnf37/APDt37sT8+fPxxRdfYNasWahUqRJeffVVLF26VPo/eRo/P79i1evJ7To4OMDb21uaOjOXvP+3Z555xiBdJpNBpVIZlf/k/xmQ+//2tP+zvOVPnvOSx9T3JK9NStoGKpWqwOAqv7zzrQAgOTkZo0ePxjvvvIPly5dL6V26dEGHDh3w7rvvIiEhwWD9vH0y5/eWrBtHdqjCi4yMBIBi3SvGlJYtWwLIvbfH0+Q/b+f48eMICwuTluUFNidPnpROXM4LhHx8fCCTyXD69GmTv4AL2wdvb29otVo8fvzYID0xMbFY+2lOWVlZOHLkCGrVqoXq1asXmM/HxwcrV67E7du3cefOHSxatAjff/+90ehHYfIfQIviyXbSarV49OiRQXDh5ORkdA4NUPJgAPjf/9uTJ0MLIZCYmAgfH58Sbzu/vO08+f3IY+p8sLw2MRVgFcW8efOgVCqf+qpVq5a0zs2bN5GVlYXnnnvOaHstWrTA7du3kZ6ebpCet09l1VZkexjsUIXXvHlzdOnSBRs2bJCmDJ504cIF3L17t9Dt5E1nPPvss08ts3379lAoFPj2229x7do1gyuYPD090bRpU2zZsgW3b9+WAiMA6N69O4QQuH//Plq0aGH0CgkJKbDMvIDqyRsa7tix46n1LUxRRg2KQqfTYcyYMXj06BGmTp1a5PVq1KiBMWPGIDw83ODmcWVVrzxff/21wedvvvkGWq3W4P8uKCgIly9fNsh39OhRo4NvcUbIOnXqBADYunWrQfp3332HjIwMaXlpBQYGwsXFBbdu3TK5PC0tDXv37jVI27ZtG+RyOdq3b1/gdgvb15JMY+WN+D15A0ohBM6ePYsqVarAzc3NYNkff/wBuVxerJFbsi+cxiJC7jk9L730Erp06YK3334bXbp0QZUqVfDgwQP88MMP2L59O+Li4lCjRg3MmTMHDx8+RPv27VGtWjX8+++/OHjwID7//HP06dMHoaGhTy3Pw8MDzZs3x+7duyGXy6XzdfKEhYVJd//NH+y0bdsWw4cPx1tvvYULFy6gffv2cHNzw4MHD3D69GmEhIRg5MiRJst86aWX0LZtW0yaNAmpqakIDQ1FbGwsvvzySwAweQVOUYSEhOD48eP44Ycf4OfnB3d396ceVB4+fIizZ89CCIG0tDRcvXoVX375JX755RdMmDABw4YNK3DdlJQUdOzYEQMGDEC9evXg7u6O8+fP4+DBg+jVq5dBvb7//nusXbsWoaGhkMvlBU5lFsX3338PBwcHhIeH49q1a5g1axaaNGlicA7UoEGDMGvWLMyePRthYWG4fv06Vq9ebXSZdN7diNevXw93d3c4OzsjODjY5AhJeHg4IiMjMXXqVKSmpqJt27a4fPky5syZg2bNmmHQoEEl3qf8HB0d0bp1a5N3sQZyR29GjhyJu3fvok6dOti/fz8+//xzjBw5stDbMhS2r/7+/oVOV5pSo0YN9OrVC+vXr4eTkxO6du2KnJwcbNmyBT/99BM++OADo1G7s2fPomnTpqhSpUqxyiI7Ysmzo4nMrag3FRQi96qVTz75RLRu3Vp4eHgIBwcH4e/vL3r16iX27dsn5du7d6/o3Lmz8PX1FQ4ODqJSpUqiZcuW4pNPPpGu8CqKKVOmCACiRYsWRst2794tAAhHR0eRkZFhtHzjxo2iVatWws3NTbi4uIhatWqJN998U1y4cEHK8+RVPULkXgn21ltvicqVKwtXV1cRHh4uzp49KwCIjz/+WMpX0I308tozISFBSouPjxdt27YVrq6uAoAICwsrdL+R7yobuVwuPDw8REhIiBg+fLiIjY01yv/kFVLZ2dni3XffFY0bNxYeHh7CxcVF1K1bV8yZM8egrR4/fix69+4tKleuLGQymcj7c5e3vWXLlj21rPxtERcXJ3r06CEqVaok3N3dRf/+/cXDhw8N1s/JyRFTpkwRAQEBwsXFRYSFhYn4+Hijq7GEEGLlypUiODhYKBQKgzJN/b9lZWWJqVOnisDAQKFUKoWfn58YOXKkSE5ONsgXGBhodPdjIXKvpnra/4sQQmzYsEEoFArx119/Ga3fsGFDcfz4cdGiRQvh5OQk/Pz8xPvvv2/0nYeJK9IK2teSysrKEsuWLRONGzcW7u7uwsvLSzz//PNi69atBlcKCiFEWlqacHV1NbqCkSoWmRC88QBRRbZt2zYMHDgQP/30E9q0aWPp6pAFZWdno0aNGpg0aVKxphKt2YYNGzB+/Hjcu3ePIzsVGIMdogpk+/btuH//PkJCQiCXy3H27FksW7YMzZo1M3jYKVVca9euRXR0NP744w+jc19sjVarRYMGDTB48GDMmDHD0tUhC+I5O0QViLu7O3bs2IH58+cjIyMDfn5+GDJkCObPn2/pqpGVGD58OP7991/88ccfhZ7wbgvu3buHN954A5MmTbJ0VcjCOLJDREREdo2XnhMREZFdY7BDREREdo3BDhEREdk1nqAMQK/X46+//oK7u3uxbyFPREREliH+78ak/v7+hd4YlcEOcp/2GxAQYOlqEBERUQncu3ev0OfpMdhB7uW4QG5jeXh4lMk2M9VatFzwIwDg3IxOcHW03abWaDQ4fPgwIiIioFQqLV0du8P2NT+2sXmxfc3PVtvY3MfC1NRUBAQESMfxgtjuEbgM5U1deXh4lFmw46DWQu7kKm3X1oMdV1dXeHh42FQnsxVsX/NjG5sX29f8bLWNy+tY+LRTUHiCMhHZhGyNDqO+jsOor+OQrdHZXXlEZD4MdojIJuiFwP4ridh/JRH6crgXanmXR0TmY7tzK1ZOIZfhtebVpfdEREQVjbUcCxnsmImTgwIf9m1i6WoQEZU5nU4HjUYjfdZoNHBwcEB2djZ0Ok75mYMtt/GCl+sCAIRWg2yt5im5DSmVSigUilLXgcEOEREViRACiYmJ+Pfff43SVSoV7t27x3uVmUlFbuPKlStDpVKVar8Z7JiJEAJZ/3dSo4tSUeG+nERkf/ICnapVq8LV1VX6u6bX65Geno5KlSoVemM3KjlbbWMhBPT/d8qbXPb0q6aeXDczMxNJSUkAAD8/vxLXg8GOmWRpdGgw+xAA4Pq8SJu+9JyISKfTSYGOt7e3wTK9Xg+1Wg1nZ2ebOhDbElttY51e4NpfKQCAhv6exT5vx8XFBQCQlJSEqlWrlnhKy3ZajIiILCbvHB1XV1cL14QqmrzvXP7zxIqLwQ4RERUZp+SpvJXFd47BDhEREdk1BjtEREQV1KNHj1C1alXcvn273MuePHkyxo0bVy5lMdghIiK7NWTIEPTs2dPgs0wmw+LFiw3y7d69W5ouyctT2AsAtFotZs6cieDgYLi4uKBmzZqYN28e9Hp9ue1faS1atAg9evRAUFCQlDZ+/HiEhobCyckJTZs2NVrn+PHjeOWVV+Dn5wc3Nzc0bdoUX3/9tUGevDZ0UMjRJKAKmgRUgYNCjoYNG0p5pkyZgk2bNiEhIcFcuydhsENERBWKs7MzlixZguTkZJPLP/74Yzx48EB6AcCmTZuM0pYsWYJ169Zh9erVuHHjBpYuXYply5Zh1apV5bYvpZGVlYUNGzbgnXfeMUgXQuDtt9/G66+/bnK9M2fOoHHjxvjuu+9w+fJlvP3223jzzTfxww8/SHny2vDP+3/hx7j/4vC5q/Dy8kKfPn2kPFWrVkVERATWrVtnnh3Mh8GOmchlMnQNUaFriApyntBHVGrl3afYh+1X586doVKpsGjRIpPLPT09oVKppBfwvxvb5U+LjY3FK6+8gm7duiEoKAi9e/dGREQELly4UGDZ0dHRaNq0KTZu3IgaNWqgUqVKGDlyJHQ6HZYuXQqVSoWqVatiwYIFBut99NFHaNOmDdzd3REQEIBRo0YhPT1dWv7222+jcePGyMnJAZB75VJoaCgGDhxYYF0OHDgABwcHtG7d2iD9k08+wejRo1GzZk2T673//vv44IMP0KZNG9SqVQvjxo3DSy+9hF27dhm1oZ9KhVqB1ZHw3ytITk7GW2+9ZbCtl19+Gdu3by+wjmWFwY6ZOCsVWDMwFGsGhsJZWfpbXRNVdOXdp9iHiyZTrUWmWosstU56n/d68mnxTy4vSd6yoFAosHDhQqxatQp//vlnibfTrl07/Pjjj/j1118BAL/88gtOnz6Nrl27FrrerVu3cODAARw8eBDbt2/Hxo0b0a1bN/z55584ceIElixZgpkzZ+Ls2bPSOnK5HEuWLMHly5exZcsWHD16FFOmTJGWf/LJJ8jIyMC0adMAALNmzcI///yDNWvWFFiPkydPokWLFiXe//xSUlLg5eVllC6XyxDo7YYfvvkanTt3RmBgoMHyli1b4t69e7hz506Z1KMgvNMdERGVWN7NU03pWPcZbHqrpfQ59IMj0p3ln9Qq2As7R/xvhKHdkmN4nKE2ynd7cbdS1PZ/Xn31VTRt2hRz5szBhg0bSrSNqVOnIiUlBfXq1YNCoYBOp8OCBQvQv3//QtfT6/XYuHEj3N3d0aBBA3Ts2BE3b97E/v37IZfLUbduXSxZsgTHjx/H888/DyD3PJrU1FR4eHigVq1a+OCDDzBy5EgpmKlUqRK2bt2KsLAwuLu748MPP8SPP/4IT0/PAutx+/Zt+Pv7l2jf8/v2229x/vx5fPbZZyaXP3jwAAcOHMC2bduMllWrVk2qy5OBUFlisENERBXSkiVL8OKLL2LSpEklWn/nzp3YunUrtm3bhoYNGyI+Ph5RUVHw9/fH4MGDC1wvKCgI7u7u0mdfX18oFAqDOyP7+vpKj0kAgGPHjmH+/Pn49ddfkZqaCq1Wi+zsbGRkZMDNzQ0A0Lp1a0yePBkffPABpk6divbt2xda/6ysLDg7O5do3/McP34cQ4YMweeff25w8nF+mzdvRuXKlQ1OFM+Td4fkzMzMUtXjaRjsmEmmWsvHRRCVofLuU+zDRXN9XiT0ej3SUtPg7uFucMB+8lynuFmdC9zOk3lPT+1YthU1oX379oiMjMT777+PIUOGFHv99957D9OmTUO/fv0AACEhIbhz5w4WLVpUaLCjVCoNPstkMpNpeVd13blzB927d8dbb72FBQsWwMfHB6dPn8bQoUMN7iqs1+vx008/QaFQ4Lfffntq/X18fAo8SbsoTpw4gR49emDFihV48803TebR6vRYt/4LdOnZFwoHpdHyx48fAwCeeeaZEtejKNh7iYioxFwdHaDX66F1VMDV0aHQ5zYVJ2Asr+By8eLFaNq0KerUqVPsdTMzM432V6FQlPml5xcuXIBWq8X8+fNRuXJlyOVyfPPNN0b5li1bhhs3buDEiROIjIzEpk2bjE4Izq9Zs2bYunVriep0/PhxdO/eHUuWLMHw4cMLzHfixAncvf0HevZ7w+Tyq1evQqlUFjgqVFYY7BCRTXBRKhA3s7P03t7KI8sICQnBwIEDS3S5eI8ePbBgwQLUqFEDDRs2xKVLl7BixQq8/fbbZVrHWrVqQavVYv369ejduzdiY2ONLteOj4/H7Nmz8e2336Jt27b4+OOPMX78eISFhRV4VVVkZCSmT5+O5ORkVKlSRUr//fffkZ6ejsTERGRlZSE+Ph4A0KBBAzg6OuL48ePo1q0bxo8fj9deew2JiYkAAEdHR6OTlDdt3IiQZi1Qu14Dk3U4deoUXnjhBWk6y1x4NRYR2QSZTAbvSk7wruRULs9nKu/yyHI++OADCCGKvd6qVavQu3dvjBo1CvXr18fkyZMxYsQIfPDBB2Vav6ZNm+LDDz/Exx9/jMaNG+Prr782uGw+OzsbAwcOxJAhQ9CjRw8AwNChQ9G5c2cMGjQIOp3pk8JDQkLQokULo1Gid955B82aNcNnn32GX3/9Fc2aNUOzZs3w119/Acg9ByczMxOLFi2Cn5+f9OrVq5fBdlJSUvD999/h1QJGdQBg+/btGDZsWInapThkoiT/w3YmNTUVnp6eSElJgYeHR5ls057m+zUaDfbv34+uXbsazStT6bF9zY9tXHrZ2dlISEhAcHCw0Umter1eulKosGksKjlztfH+/fsxefJkXL161Sz/dzq9wLW/UgAADf09oZD/74fDvn378N577+Hy5ctwcCj4GFnYd6+ox2/bPQITUYWSo9Vh/n9uAABmdq8PJwfzTi2Vd3lEltC1a1f89ttvuH//PgICAsq17IyMDGzatKnQQKesMNghIpug0wt8dTb3xmPTu9azu/KILGX8+PEWKbdv377lVhaDHTORy2ToWPcZ6T0REVFFIwPg7qyU3lsKgx0zcVYqDO4cSkREVNHI5TIE+7hZuhq8GouIiIjsG4MdIiIismsMdswkU61F/VkHUX/WwTJ7Ui8REZEt0ekFrt5PwdX7KdDpLXenG56zY0YFPd2XiIiootBbwe38OLJDREREdo3BDhERkZVTKBTYt29fqbdz9OhR1KtXr8wfVloSOTk5qFGjBuLi4sxeFoMdIiKyW0OGDEHPnj0NPstkMixevNgg3+7du6VnoOXlKewFAFqtFjNnzkRwcDBcXFxQs2ZNzJs3zyyBxP3799G5c+dSb2fKlCmYMWNGoY+GuHbtGl577TUEBQVBJpNh5cqVRnkWLVqE5557Du7u7qhatSp69uyJmzdvGuRJT0/HuLFjEP5cQ7R81g+NGjbA2rVrpeVOTk6YPHkypk6dWur9ehoGO0REVKE4OztjyZIlSE5ONrn8448/xoMHD6QXAGzatMkobcmSJVi3bh1Wr16NGzduYOnSpVi2bFmJnqD+NCqVCk5OTqXaxpkzZ/Dbb7+hT58+hebLzMxEzZo1sXjxYqhUKpN5Tpw4gdGjR+Ps2bOIiYmBVqtFREQEMjIypDwTJkzAoUOHsPCTz7Dr2M8YPz4KY8eOxZ49e6Q8AwcOxKlTp3Djxo1S7dvTMNghIqIKpXPnzlCpVAZPDs/P09MTKpVKegFA5cqVjdJiY2PxyiuvoFu3bggKCkLv3r0RERGBCxcuFFh2dHQ0mjZtio0bN6JGjRqoVKkSRo4cCZ1Oh6VLl0KlUqFq1apYsGCBwXr5p7Fu374NmUyG77//Hh07doSrqyuaNGmC2NjYQvd7x44diIiIMHqY5pOee+45LFu2DP369SswwDp48CCGDBmChg0bokmTJti0aRPu3r1rMCUVGxuLQW++iedat0O1gBoYNnw4mjRpYtA+3t7eaNOmDbZv315onUqLwY6ZyGUytAr2QqtgLz4ugqgMlHefYh8umky1FplqLbLUOul93iv7iStSn1xekrxlQaFQYOHChVi1ahX+/PPPEm+nXbt2+PHHH/Hrr78CAH755RecPn0aXbt2LXS9W7du4cCBAzh48CC2b9+OjRs3olu3bvjzzz9x4sQJLFmyBDNnzsTZs2cL3c6MGTMwefJkxMfHo06dOujfvz+02oLb6OTJk2jRokXxd7QIUlJyn2zu5eUlpbVr1w7/+eEHpD1OgqujAsePHcOvv/6KyMhIg3VbtmyJU6dOmaVeeSx66fnJkyexbNkyxMXF4cGDB9i1a5fB3Gp+I0aMwPr16/HRRx8hKipKSs/JycHkyZOxfft2ZGVloVOnTlizZg2qV69ePjtRAGelAjtHtLZoHYjsSXn3Kfbhomkw+1CByzrWfcbgsTmhHxwp8JYcrYK9DNq73ZJjeJyhNsp3e3G3UtT2f1599VU0bdoUc+bMwYYNG0q0jalTpyIlJQX16tWDQqGATqfDggUL0L9//0LX0+v12LhxI9zd3dGgQQN07NgRN2/exP79+yGXy1G3bl0sWbIEx48fx/PPP1/gdiZPnoxu3XLbY+7cuWjYsCF+//131Ktn+sG1t2/fhr+/f4n2tTBCCEycOBHt2rVDo0aNpPRPPvkEw4YNQ7smdeHg4AC5XI4vvvgC7dq1M1i/WrVquH37dpnXKz+LjuxkZGSgSZMmWL16daH5du/ejZ9//tnkf1JUVBR27dqFHTt24PTp00hPT0f37t2h0/EeN0REVLAlS5Zgy5YtuH79eonW37lzJ7Zu3Ypt27bh4sWL2LJlC5YvX44tW7YUul5QUBDc3d2lz76+vmjQoIHBScO+vr5ISkoqdDuNGzeW3vv5+QFAoetkZWUZTGHdvXsXlSpVkl4LFy4stLyCjBkzBpcvXzaaivrkk09w9uxZ7N27F3Fxcfjwww8xatQoHDlyxCCfi4sLMjMzS1R2UVl0ZKdLly7o0qVLoXnu37+PMWPG4NChQ1IEmyclJQUbNmzAV199JZ2lvnXrVgQEBODIkSNGQ2VERFS2rs+LhF6vR1pqGtw93A0O2E9O/8XNKvhqoifznp7asWwrakL79u0RGRmJ999/H0OGDCn2+u+99x6mTZuGfv36AQBCQkJw584dLFq0CIMHDy5wPaVSafBZJpOZTHvaVV3518m7QqywdXx8fAxOyvb390d8fLz0Of8UVFGNHTsWe/fuxcmTJw1mVLKysvD+++9j165d0rG7cePGiI+Px/Llyw2uLHv8+DGeeeaZYpddHFZ9B2W9Xo9BgwbhvffeQ8OGDY2Wx8XFQaPRICIiQkrz9/dHo0aNcObMmQKDnZycHOTk5EifU1NTAQAajQYajaZM6p6p1qLDh7lzkMcnvQBXR6tu6kLltUlZtQ0ZYvsWTWn6VEna2J76cFnQaDQQQkCv1xscUJ0d5BBCBq2jAi5KhXTQzfNk3sIUJW9xL+sWQkj1NvV54cKFaN68OWrXrl3o9p/cbwDSaET+dLlcbjJv/vo8uc6TdcqfXlha/nJMpT2padOmuHbtmrRcLpejZs2aRvtpqs6m6jFu3Djs3r0bR48eRWBgoEGenJyc//vOANf/yj2fp45vbjCs0+kM8l65cgVNmzYttO2FENBoNFAoFAbLitqnrbr3LlmyBA4ODhg3bpzJ5YmJiXB0dESVKlUM0n19fZGYmFjgdhctWoS5c+capR8+fBiurq6lq/T/ydEByZm5zXvo0GE4KZ6ygg2IiYmxdBXsGtu3cGXRp4rTxvbYh0vDwcEBKpUK6enpUKuNz6UBgLS0tHKu1dNpNBpotVqDH7X5PwcGBqJPnz7S6RR56U/KysoyWhYZGYmFCxfCx8cH9evXx+XLl7FixQoMHDiwwO3k5ORAp9MZLH+yTkDuPXzUarXRdtLS0pCeng4g91SQvOV5bZ+ZmVlg2WFhYdi+fXuBy/Oo1Wrpnjk5OTn4448/8NNPP8HNzU0KjiZNmoRvv/0W27ZtAwD89ttvAAAPDw+4uLgAANq2bYv33puMyfOWwa9aAI7u+glfffUV5s+fb1CHkydP4v333y+wXmq1GllZWTh58qTRCdhFnf6y2mAnLi4OH3/8MS5evGj0S+FphBCFrjN9+nRMnDhR+pyamoqAgABERETAw8OjxHXOL1OtxZRzRwEAkZERNv2rUKPRICYmBuHh4UZDrVR6bN+i0esFGrXMvYdHrWfcIJcX/e9CSdq4NOXZo+zsbNy7dw+VKlUyunRZCIG0tDS4u7sX+++1uSmVSjg4OEh/25/8DOT+AN69ezcAFHgMcHFxMVq2du1azJ49G1OmTEFSUhL8/f0xYsQIzJo1C46Ojia34+TkBIVCYbAtU3VycHCAo6OjUZnu7u6oVKkSAMDNzU1anjcq4urqWuA+DB06FNHR0Xjw4AHq1q1rMg+QeyJz+/btpc+rV6/G6tWrERYWhqNHc49rGzduBAB0797dYN0NGzZIU4LffPMNpr//PqaPHY7Uf5MRFBSI+fPnIyoqSvqexMbGIi0tDYMGDZKCpCdlZ2fDxcUF7du3N/ruPS1wkwgrAUDs2rVL+vzRRx8JmUwmFAqF9AIg5HK5CAwMFEII8eOPPwoA4vHjxwbbaty4sZg9e3aRy05JSREAREpKSlnsihBCiIwcjQic+h8ROPU/IiNHU2bbtQS1Wi12794t1Gq1patil9i+5sc2Lr2srCxx/fp1kZWVZbRMp9OJ5ORkodPpLFCziqGs2vi9994Tw4cPL6NaPZ1Wpxe/3EsWv9xLFlqd3mh57969xYIFCwrdRmHfvaIev632PjuDBg3C5cuXER8fL738/f3x3nvv4dCh3EsdQ0NDoVQqDYamHzx4gKtXr6JNmzaWqjoREZFVmjFjBgIDA63iiuWcnBw0adIEEyZMMHtZFp1bSU9Px++//y59TkhIQHx8PLy8vFCjRg14e3sb5FcqlVCpVNLwm6enJ4YOHYpJkybB29sbXl5emDx5MkJCQsrkGSJEZD3UWj0+PZb792J0x2fh+JSTXW2tPKLy4Onpiffff9/S1QCQO6U3c+bMcinLosHOhQsX0LHj/y4vzDuPZvDgwdi8eXORtvHRRx/BwcEBffv2lW4quHnzZqMztonItmn1enz8Y+5JkCPCasLRzLcJK+/yiMh8LBrsdOjQQboMryhM3WHR2dkZq1atMsuD10pDLpOhcXVP6T0REVFFIwPg4qiQ3luK7V4iZOWclQrsHdPu6RmJiIjslFwuQ+2q7k/PaO56WLoCRERERObEYIeIiIjsGqexzCRLrUPnFScAAEcmhklzlkRERBWFXi/w68PcuzvnPi7CMmfuMNgxEwGB+/9mSe+JiIgqGgFArdNL7y2F01hERERk1xjsEBER2YD09HSMHTsW1atXh4uLC+rXr4+1a9c+db3vvvsODRo0gJOTExo0aIBdu3YZ5VmzZg2Cg4Ph7OyM0NBQnDp1yhy7YDEMdoiIiGzAjBkzcOjQIWzduhU3btzAhAkTMHbsWOzZs6fAdWJjY/H6669j0KBB+OWXXzBo0CD07dsXP//8s5Rn586diIqKwowZM3Dp0iW88MIL6NKlC+7evVseu1UuGOwQEZHd6tChA8aOHYuoqChUqVIFvr6+WL9+PTIyMvDWW2/B3d0dtWrVwoEDB6R1dDodhg4diuDgYLi4uKBu3br4+OOPpeXZ2dlo2LAhhg8fLqUlJCTA09MTn3/+udn25dy5c3jzzTfRoUMHBAUFYfjw4WjSpAkuXLhQ4DorV65EeHg4pk+fjnr16mH69Ono1KkTVq5cKeVZsWIFhg4dinfeeQf169fHypUrERAQUKRRI1vBYIeIiEosU61FplqLLLVOev+0l/b/TlgFAK1Oj0y1FtkancntPvkqiS1btsDHxwfnzp3D2LFjMXLkSPTp0wdt2rTBxYsXERkZiUGDBiEzMxMAoNfrUb16dXzzzTe4fv06Zs+ejffffx/ffPMNgNw793/99dfYsmULdu/eDZ1Oh0GDBqFjx44YNmxYgfXo0qULKlWqVOirMM8//zx++OEH3L9/H0IIHDt2DL/++isiIyMLXCc2NhYREREGaZGRkThz5gwAQK1WIy4uzihPRESElMce8GosM5FBhtpVK0nviah0yrtPsQ8XTYPZh4q9zqcDmqNbYz8AwKFrDzF620W0CvbCzhGtpTztlhzD4wy10bq3F3crdnlNmjSRHjg5ffp0LF68GD4+PlJgMnv2bKxduxaXL1/G888/D6VSiblz50rrBwcH48yZM/jmm2/Qt29fAEDTpk0xf/58DBs2DP3798etW7ewe/fuQuvxxRdfICsrq9j1z7NkyRJMnjwZ1atXh4ODA+RyOb744gu0a1fw3foTExPh6+trkObr64vExEQAwD///AOdTldontKQAXB24OMi7JaLowIxE8MsXQ0iu1HefYp92H40btxYeq9QKODt7Y2QkBApLe9An5SUJKWtW7cOX3zxBe7cuYOsrCyo1Wo0bdrUYLuTJk3Cnj17sGrVKhw4cAA+Pj6F1qNatWql2o/PPvsMP//8M/bu3YvAwECcPHkSo0aNgp+fHzp37lzgerInns8ohDBKK0qekpDLZaijsvzjIhjsEBFRiV2fFwm9Xo+01DS4e7hDLn/62RGOiv/liWzoi+vzIo0emHx6ascyq6NSqTT4LJPJDNLyDup6fe702jfffIMJEybgww8/ROvWreHu7o5ly5YZnNQL5AZHN2/ehEKhwG+//YaXXnqp0Hp06dLlqVc5paenm0zPysrCBx98gO+++w49evQAkBvExcfHY/ny5QUGOyqVymiEJikpSQrwfHx8oFAoCs1jDxjsEBFRibk6OkCv10PrqICro0ORgp38HBRyOCiM13F1tNzh6dSpU2jTpg1GjRolpd26dcso39tvv41GjRph2LBhGDp0KDp16oQGDRoUuN3STGNpNBpoNBqj9lUoFFKQZkrr1q0RExODCRMmSGmHDx9GmzZtAACOjo4IDQ1FTEwMXn31VSlPTEwMXnnllRLV1Rox2DGTLLUOL68+DQDYO6YdHxdBVEr5+9RvSeklOnejpOWxD1cszz77LL788kscOnQIwcHB+Oqrr3D+/HkEBwdLeT799FPExsbi8uXLCAgIwIEDBzBw4ED8/PPPcHR0NLnd0kxjeXh4oG3btpg6dSrc3NwQGBiIEydO4Msvv8SKFSukfG+++SaqVauGRYsWAQDGjx+P9u3bY8mSJXjllVewZ88eHDlyBKdPn5bWmThxIgYNGoQWLVqgdevWWL9+Pe7evYt33323xPXNo9cL/J6UO1r1bNVKfFyEvREQ+O3//oP5uAii0svfp8q7PPbhiuXdd99FfHw8Xn/9dchkMvTv3x+jRo2SLk//73//i/feew8bNmxAQEAAgNzgp0mTJpg1axaWLFlilnpt2LABixYtwsCBA/H48WMEBgZiwYIFBkHJ3bt3DUZ/2rRpgx07dmDmzJmYNWsWatWqhZ07d6JVq1ZSntdffx2PHj3CvHnz8ODBAzRq1Aj79+9HYGBgqessAGRrddJ7S5EJISp8L05NTYWnpydSUlLg4eFRJtvMVGulqxSuz4u06JBsaWk0Guzfvx9du3Y1mvum0mP7Fo1OL3Au4TEAoP/nZ4s1slOSNs5fXstgLygs9IvUWmRnZyMhIUG6y25+er0eqamp8PDwKPY0FhWNrbaxTi9w7a8UAEBDf88S9aPCvntFPX7b7hGYiCoUhVyG1rW87bY8IjIf2wkPiYiIiEqAIztEZBM0Oj22nyu/Z/XkL69/yxpQmrhiiIhsA4MdIrIJGp0es/dcs0h5vUOrM9ghsmEMdsxEBhmqVXaR3hMREVU0MvzvJpJ8XIQdcnFU4KdpL1q6GkRERBYjl8tQz69srnIuVT0sXQEiIiIic2KwQ0R2JWjaPktXgYisDKexzCRbo0Pfz2IBAN+MaA1nJW81T0REFYteL3Drn9w7kdfysdzjIjiyYyZ6IXD5zxRc/jMFet6kmojIZhw/fhwymQz//vuvpati8wRynzOXpdZZ9HERDHaIiIjyadOmDR48eABPT09LV8XI+fPn0alTJ1SuXBlVqlRBREQE4uPjC10nJycHY8eOhY+PD9zc3PDyyy/jzz//NMiTnJyMQYMGwdPTE56enhg0aJBdBXsMdoiI8uE5P+To6AiVSgWZzLpuG5KWloYuXbqgRo0a+Pnnn3H69Gl4eHggMjISGo2mwPWioqKwa9cu7NixA6dPn0Z6ejq6d+8OnU4n5RkwYADi4+Nx8OBBHDx4EPHx8Rg0aFB57Fa5YLBDRER2q0OHDhg7diyioqJQpUoV+Pr6Yv369cjIyMBbb70Fd3d31KpVS3qiOWA8jbV582ZUrlwZhw4dQv369VGpUiW89NJLePDgQbnuy++//47k5GTMmzcPdevWRcOGDTFnzhwkJSXh7l3TdxdPSUnBhg0b8OGHH6Jz585o1qwZtm7diitXruDIkSMAgBs3buDgwYP44osv0Lp1a7Ru3Rqff/45/vOf/+DmzZvluYtmw2CHiIhKLFOtRaZaiyy1Tnr/tJdWp5fW1+r0yFRrka3Rmdzuk6+S2LJlC3x8fHDu3DmMHTsWI0eORJ8+fdCmTRtcvHgRkZGRGDRoEDIzMwvez8xMLF++HF999RVOnjyJu3fvYvLkyYWWW6lSpUJfXbp0KdZ+PPvss/Dx8cGGDRugVquRlZWFDRs2oGHDhggMDDS5TlxcHDQaDSIiIqQ0f39/NGrUCGfOnAEAxMbGwtPTE61atZLyPP/88/D09JTy2DpejUVERCXWYPahYq/z6YDm6NbYDwBw6NpDjN52Ea2CvbBzRGspT7slx/A4Q2207u3F3YpdXpMmTTBz5kwAwPTp07F48WL4+Phg2LBhAIDZs2dj7dq1uHz5Mp5//nmT29BoNFi3bh1q1aoFABgzZgzmzZtXaLlPO5fGxcWlWPvh7u6Oo0eP4tVXX8UHH3wAAKhTpw4OHToEBwfTh/PExEQ4OjqiSpUqBum+vr5ITEyU8lStWtVo3apVq0p5bB2DHTPycnO0dBWI7EpenzJ1EDRneWTbGjduLL1XKBTw9vZGSEiIlObr6wsASEpKKnAbrq6uUqADAH5+foXmB3JHYkqqS5cuOHXqFAAgMDAQV65cQVZWFt555x20bdsW27dvh06nw/Lly9G1a1ecP3++WMGTEMLgnCRT5yc9maekHOSWn0RisGMmro4OuDgr3NLVILIb+ftUeZxEzD5cNNfnRUKv1yMtNQ3uHu6QF+HA5pjvoaqRDX1xfV4k5E8cVE9P7VhmdVQqlQafZTKZQVreAV2v16MgprYhnnJbkUqVKhW6/IUXXjA4Vyi/L774AllZWQZlf/vtt7h9+zZiY2Oldt62bRuqVKmCPXv2oF+/fkbbUalUUKvVSE5ONhjdSUpKQps2baQ8Dx8+NFr377//lgLBklLIZWjgb/nHRTDYISKiEnN1dIBer4fWUQFXR4ciBTv5OSjkcDDxRHlXR9s/PJVmGqtatWoGn/V6PbKysiCXyw1GW/I+FxSohYaGQqlUIiYmBn379gUAPHjwAFevXsXSpUsBAK1bt0ZKSgrOnTuHli1bAgB+/vlnpKSkSAGRrbP9bxMREZEVKs00likdOnTA7NmzMXr0aIwdOxZ6vR6LFy+Gg4MDOnbMHQm7f/8+OnXqhC+//BItW7aEp6cnhg4dikmTJsHb2xteXl6YPHkyQkJC0LlzZwBA/fr18dJLL2HYsGH47LPPAADDhw9H9+7dUbdu3TLdB0ux6ETayZMn0aNHD/j7+0Mmk2H37t3SMo1Gg6lTpyIkJARubm7w9/fHm2++ib/++stgG0W5WZIlZGt0eP2zWLz+WazRVQZEVHz5+1R5l8c+TNagTp062LNnDy5fvozWrVvjhRdewF9//YWDBw/Czy/3hG+NRoObN28aXFn20UcfoWfPnujbty/atm0LV1dX/PDDD1Ao/vcYo6+//hohISGIiIhAREQEGjdujK+++qrUddbrBW79nY5bf6dDr7fcPZQtOrKTkZGBJk2a4K233sJrr71msCwzMxMXL17ErFmz0KRJEyQnJyMqKgovv/wyLly4IOWLiorCDz/8gB07dsDb2xuTJk1C9+7dERcXZ/AfWd70QuDnhMfSeyIqnfx9qrzLYx+2XcePHzdKu337tlFa/vNvOnToYPB5yJAhGDJkiEH+nj17PvWcHXMIDw9HZGRkgcuDgoKM6uXs7IxVq1Zh1apVBa7n5eWFrVu3llk98wgAGTla6b2lWDTY6dKlS4H3GfD09ERMTIxB2qpVq9CyZUvcvXsXNWrUkG6W9NVXX0nDcVu3bkVAQACOHDlS6BeCiGyLo0KOTwc0BwCM3naxXMtzNHFOCRHZDps6ZyclJQUymQyVK1cG8PSbJRUU7OTk5CAnJ0f6nJqaCiB3+K+wW24Xh0ajzfdeA43Mdn8Z5rVJWbUNGWL7Fl1EfR8AgJNCFNheppYVp43zr59XntDroNFX7KksjUYDIQT0er3RybB5Iwl5y6ns2Wob5x9kyq178Y+Fer0eQuT2yydnbIr6d9Nmgp3s7GxMmzYNAwYMgIdH7mVsRblZkimLFi3C3LlzjdIPHz4MV1fXMqlvjg7Ia95Dhw7DyXIzamXmyZE2Klts36Jb2hLYv39/sZcVpY0LW78ic3BwgEqlQnp6OtRq0/c5SktLK+daVTy21sb5Y5vU1FTIS3Dbnry7RZ88eRJareFdtAu763V+NhHsaDQa9OvXD3q9HmvWrHlq/qfdCGn69OmYOHGi9Dk1NRUBAQGIiIiQAqnSylRrMeXcUQBAZGSETV9GqdFoEBMTg/DwcKN7TVDpsX2LRqvTI+ZG7k3cJv+/X3BtrumR20bRh3A12nBZcdo4b/385YXXr2ry8uiKJDs7G/fu3UOlSpXg7OxssEwIgbS0NLi7u1vdwzPtha22sV4AyMidPfHw8ChRsJOdnQ0XFxe0b9/e6LuXNzPzNFZ/BNZoNOjbty8SEhJw9OhRg2CkKDdLMsXJyQlOTk5G6UqlsswONkrxv//R3O1afVM/VVm2Dxlj+xZOI7QYt/Py/32SFdhWObqClxWljfPWz1/e9XmRdtGHS0On00Emk0EmkxndSydvWsXUMiobttrGIt/QTm7dix/t5H3vTPXfov7NtOoWywt0fvvtNxw5cgTe3t4Gy/PfLClP3s2SrOFGSC5KBVyUdjB/RUQVXt5BpajTBkR55DKZ0R2yiyPvO1eaH4MW/amSnp6O33//XfqckJCA+Ph4eHl5wd/fH71798bFixfxn//8BzqdTjoPx8vLC46OjkW6WZKluDo64MYHL1m0DkREZUWhUKBy5crS86BcXV0NHrOgVquRnZ1tU6MOtsSW2/hZ79yZFI06B8W5DEMIgczMTCQlJaFy5cqlup2MRYOdCxcuSHd9BCCdRzN48GBER0dj7969AICmTZsarHfs2DF06NABQO7NkhwcHNC3b19kZWWhU6dO2Lx5s0XvsUNEZI9UKhUA4wdmCiGQlZUFFxcXmzqfxJZU5DauXLmy9N0rKYsGO0/euOlJRblhU1FulkRERKUnk8ng5+eHqlWrGlzyq9FocPLkSbRv357nnZlJRW1jpVJZJoMXFfuMOzPK1ugwcmscAGDtG6Fw5rk7RGQnFAqFwQFIoVBAq9XC2dm5Qh2Iy5OttrG1HAsZ7JiJXggcu/m39J6IiKiisZZjoW2d5URERERUTAx2iIiIyK4x2CEiIiK7xmCHiIiI7BqDHSIiIrJrDHaIiIjIrvHSczNxdXTA7cXdLF0NIruRv08FTdtXruURUclYSz/iyA4RERHZNQY7REREZNc4jWUm2RodJn4TDwBY0bcpHxdBVEr5+1R5l8c+TFQy1tKPOLJjJnohsP9KIvZfSeTjIojKQP4+Vd7lsQ8TlYy19COO7BCRTVAq5Jj3SkMAwOw918q1PKWCvwuJbBmDHSKyCUqFHG+2DgJQfsFOXnlEZNv4c4WIiIjsGkd2iMgm6PQC5xIeW6S8lsFeUMhl5VY2EZUtBjtEZBNytDr0//ysRcq7Pi8Sro78c0lkqziNRURERHaNP1XMxEWpwPV5kdJ7IiKiisZajoUMdsxEJpNx2JuIiCo0azkWchqLiIiI7BqDHTPJ0eow6ZtfMOmbX5Cj1Vm6OkREROXOWo6FDHbMRKcX+O7in/ju4p/Q6XmreSIiqnis5VjIYIeIiIjsGoMdIiIismsMdoiIiMiuMdghIiIiu8Zgh4iIiOwagx0iIiKya5a/raGdclEqEDezs/SeiEonf58KnX+kXMtjHyYqGWvpRwx2zEQmk8G7kpOlq0FkN8q7T7EPE5WetfQjTmMRERGRXePIjpnkaHWY/58bAICZ3evDyYHD4ESlkb9PlXd57MNEJWMt/YjBjpno9AJfnb0DAJjetZ6Fa0Nk+/L3qfIuj32YqGSspR9ZdBrr5MmT6NGjB/z9/SGTybB7926D5UIIREdHw9/fHy4uLujQoQOuXbtmkCcnJwdjx46Fj48P3Nzc8PLLL+PPP/8sx70govLgIJdjfKfaGN+pdrmX5yDnjD+RLbNoD87IyECTJk2wevVqk8uXLl2KFStWYPXq1Th//jxUKhXCw8ORlpYm5YmKisKuXbuwY8cOnD59Gunp6ejevTt0Oj5pnMieODrIMSG8DiaE1yn38hwdGOwQ2TKLTmN16dIFXbp0MblMCIGVK1dixowZ6NWrFwBgy5Yt8PX1xbZt2zBixAikpKRgw4YN+Oqrr9C5c+6lbVu3bkVAQACOHDmCyMjIctsXIiIisk5W+3MlISEBiYmJiIiIkNKcnJwQFhaGM2fOAADi4uKg0WgM8vj7+6NRo0ZSHiKyD3q9wK8P0/Drw7SnZy7j8vR6US5lEpF5WO0JyomJiQAAX19fg3RfX1/cuXNHyuPo6IgqVaoY5clb35ScnBzk5ORIn1NTUwEAGo0GGo2mTOqv0WjzvddAI7PdP5Z5bVJWbUOG2L5Fk6nWIuKjkwAAR7kosL2cFMbLitPGeevnL++XWS/C1dFq/1xaHL/D5merbWzuY2FR28Pqe69MJjP4LIQwSnvS0/IsWrQIc+fONUo/fPgwXF1dS1bRJ+TogLzmPXToMJzs4KrVmJgYS1fBrrF9C5e/T81vocP+/ftN5lvaEgUuK0ob561vj33Y3PgdNj9ba2Nz96PMzMwi5bPaYEelUgHIHb3x8/OT0pOSkqTRHpVKBbVajeTkZIPRnaSkJLRp06bAbU+fPh0TJ06UPqempiIgIAARERHw8PAok/rr9QLPtcsGAPh7OkMuLzxAs2YajQYxMTEIDw+HUqm0dHXsDtu3aDLVWkw5dxQAMPOCAtfmmj4nr1H0IVyNNlxWnDbOWz9/eZGRERzZKQS/w+Znq21s7mNh3szM01ht7w0ODoZKpUJMTAyaNWsGAFCr1Thx4gSWLFkCAAgNDYVSqURMTAz69u0LAHjw4AGuXr2KpUuXFrhtJycnODkZ375aqVSW6ZcouKpjmW3LGpR1+5Ahtm/hlOJ/fyTVelmBbZWjK3hZUdo4b/385eWuZ7V/Lq0Gv8PmZ4ttbM5jYVHbwqK9Nz09Hb///rv0OSEhAfHx8fDy8kKNGjUQFRWFhQsXonbt2qhduzYWLlwIV1dXDBgwAADg6emJoUOHYtKkSfD29oaXlxcmT56MkJAQ6eosIiIiqtgsGuxcuHABHTt2lD7nTS0NHjwYmzdvxpQpU5CVlYVRo0YhOTkZrVq1wuHDh+Hu7i6t89FHH8HBwQF9+/ZFVlYWOnXqhM2bN0OhsOwEu1qrx/LDNwEAkyPq8j4dRERU4VjLsdCiwU6HDh0gRMFnZstkMkRHRyM6OrrAPM7Ozli1ahVWrVplhhqWnFavx/qTfwAAojrXhqP1XuVPRERkFtZyLOQRmIiIiOwagx0iIiKyawx2iIiIyK4x2CEiIiK7xmCHiIiI7BqDHSIiIrJrvCWomTg7KHB4QnvpPRGVTv4+lfeAzvIqj32YqGSspR8x2DETuVyGOr7uT89IREVS3n2KfZio9KylH3Eai4iIiOwaR3bMRK3V49Njuc/9Gt3xWT4ugqiU8vep8i6PfZioZKylHzHYMROtXo+Pf/wNADAirCYfF0FUSvn7VHmXxz5MVDLW0o8Y7BCRTVDIZRj0fCAA4Kuzd8q1PIVcZvbyiMh8GOwQkU1wclDgg56NAJRPsJO/PCKybRyXJSIiIrvGkR0isglCCDzOUFukPC83R8hknMoislUMdojIJmRpdAidf8Qi5V2fFwlXR/65JLJVnMYiIiIiu8afKmbi5KDAntFtpfdEREQVjbUcCxnsmIlCLkOTgMqWrgYREZHFWMuxkNNYREREZNc4smMmaq0em35KAAC81TaYt5onIqIKx1qOhQx2zESr12PRgf8CAAa1DuSt5omIqMKxlmMhj8BERERk1xjsEBERkV0rUbBTs2ZNPHr0yCj933//Rc2aNUtdKSIiIqKyUqJg5/bt29DpdEbpOTk5uH//fqkrRURERFRWinWC8t69e6X3hw4dgqenp/RZp9Phxx9/RFBQUJlVjoiIiKi0ihXs9OzZEwAgk8kwePBgg2VKpRJBQUH48MMPy6xyRERERKVVrGBHr9cDAIKDg3H+/Hn4+PiYpVL2wMlBge3DnpfeE1Hp5O9T/T8/W67lsQ8TlYy19KMS3WcnISGhrOthdxRyGVrX8rZ0NYjsRnn3KfZhotKzln5U4psK/vjjj/jxxx+RlJQkjfjk2bhxY6krRkRERFQWShTszJ07F/PmzUOLFi3g5+cHmUxW1vWyeRqdHtvP3QUA9G9ZA0oFb2lEVBr5+1R5l8c+TFQy1tKPShTsrFu3Dps3b8agQYPKuj52Q6PTY/aeawCA3qHV+YeSqJTy96nyLo99mKhkrKUflSjYUavVaNOmTVnXhYioQHKZDF1DVACA/VcSy7U8OUeviWxaiUKsd955B9u2bSvruhARFchZqcCagaFYMzC03MtzVvJqLCJbVqKRnezsbKxfvx5HjhxB48aNoVQqDZavWLGiTCpHREREVFolGtm5fPkymjZtCrlcjqtXr+LSpUvSKz4+vswqp9VqMXPmTAQHB8PFxQU1a9bEvHnzDK7+EkIgOjoa/v7+cHFxQYcOHXDtWvnN6xMREZF1K9HIzrFjx8q6HiYtWbIE69atw5YtW9CwYUNcuHABb731Fjw9PTF+/HgAwNKlS7FixQps3rwZderUwfz58xEeHo6bN2/C3d29XOpJROaXqdaiwexDFinv+rxIuDqW+E4dRGRhVn15QWxsLF555RV069YNQUFB6N27NyIiInDhwgUAuaM6K1euxIwZM9CrVy80atQIW7ZsQWZmJs8pIiIiIgAlHNnp2LFjoffWOXr0aIkrlF+7du2wbt06/Prrr6hTpw5++eUXnD59GitXrgSQeyfnxMRERERESOs4OTkhLCwMZ86cwYgRI0xuNycnBzk5OdLn1NRUAIBGo4FGoymTusv0eqx/o9n/vddBoxFlsl1LyGuTsmobMsT2LRqNRiu9d5SLAtvLSWG8rDhtnLd+/vI0Gg00Mtvtw+bG77D52Wobm/tYWNT2KFGw07RpU6PC4uPjcfXqVaMHhJbG1KlTkZKSgnr16kGhUECn02HBggXo378/ACAxMffyU19fX4P1fH19cefOnQK3u2jRIsydO9co/fDhw3B1dS2z+kvbvVXmm7SImJgYS1fBrrF9C5ejA/L+ZM1vocP+/ftN5lvaEgUuK0ob562fv7xDhw7DiRdkPRW/w+Zny21sjmNhZmZmkfKVKNj56KOPTKZHR0cjPT29JJs0aefOndi6dSu2bduGhg0bIj4+HlFRUfD39zcIqp4cZRJCFDryNH36dEycOFH6nJqaioCAAERERMDDw6PM6m8vNBoNYmJiEB4ebnTlHZUe27doMtVaTDmXO2o884IC1+ZGmszXKPoQrkYbLitOG+etn7+8yMgInrNTCH6HzY9tbFrezMzTlGnvfeONN9CyZUssX768TLb33nvvYdq0aejXrx8AICQkBHfu3MGiRYswePBgqFS5N/xKTEyEn5+ftF5SUpLRaE9+Tk5OcHJyMkpXKpVl9iXS6PTYfek+AKBns2p2cffVsmwfMsb2LZxS/O8HjFovK7CtcnQFLytKG+etn7+83PUY7DwNv8PmZ2ttbO5jYVHbokxLjY2NhbOzc5ltLzMzE3K5YRUVCoV06XlwcDBUKpXBsJ5arcaJEycsfodnjU6P9769jPe+vQyNTv/0FYiIiOyMtRwLS/RTpVevXgafhRB48OABLly4gFmzZpVJxQCgR48eWLBgAWrUqIGGDRvi0qVLWLFiBd5++20AudNXUVFRWLhwIWrXro3atWtj4cKFcHV1xYABA8qsHkRERGS7ShTseHp6GnyWy+WoW7cu5s2bZ3BlVGmtWrUKs2bNwqhRo5CUlAR/f3+MGDECs2fPlvJMmTIFWVlZGDVqFJKTk9GqVSscPnyY99ghIiIiACUMdjZt2lTW9TDJ3d0dK1eulC41N0UmkyE6OhrR0dHlUiciIiKyLaU64y4uLg43btyATCZDgwYN0KxZs7KqFxEREVGZKFGwk5SUhH79+uH48eOoXLkyhBBISUlBx44dsWPHDjzzzDNlXU8iIiKiEinR1Vhjx45Famoqrl27hsePHyM5ORlXr15Famoqxo0bV9Z1JCIiIiqxEo3sHDx4EEeOHEH9+vWltAYNGuDTTz8t0xOUbZmjQo5PBzSX3hNR6eTvU6O3XSzX8tiHiUrGWvpRiYIdvV5v8kY+SqVSugdOReegkKNbY7+nZySiIsnfp0aXw3N+2YeJSs9a+lGJwqwXX3wR48ePx19//SWl3b9/HxMmTECnTp3KrHJEREREpVWikZ3Vq1fjlVdeQVBQEAICAiCTyXD37l2EhIRg69atZV1Hm6TV6XHo2kMAQGRDXzhwGJyoVPL3qfIuj32YqGSspR+VKNgJCAjAxYsXERMTg//+978QQqBBgwbo3LlzWdfPZql1eum8guvzIvmHkqiU8vep8i6PfZioZKylHxWr1KNHj6JBgwbSU0bDw8MxduxYjBs3Ds899xwaNmyIU6dOmaWiRFSxyWUytAr2Qqtgr3IvTy6TPX0FIrJaxRrZWblyJYYNGwYPDw+jZZ6enhgxYgRWrFiBF154ocwqSEQEAM5KBXaOaA0ACJq2r1zLIyLbVqyRnV9++QUvvfRSgcsjIiIQFxdX6koRERERlZViBTsPHz40ecl5HgcHB/z999+lrhQRERFRWSnWNFa1atVw5coVPPvssyaXX758GX5+lr+enojsT6Zai3ZLjlmkvNNTO8LVsVSPEiQiCyrWyE7Xrl0xe/ZsZGdnGy3LysrCnDlz0L179zKrHBFRfo8z1Hicobbb8ojIPIr1U2XmzJn4/vvvUadOHYwZMwZ169aFTCbDjRs38Omnn0Kn02HGjBnmqqtNUSrkWNa7sfSeiIioorGWY2Gxgh1fX1+cOXMGI0eOxPTp0yGEAADIZDJERkZizZo18PX1NUtFbY1SIUefFgGWrgYREZHFWMuxsNiT0IGBgdi/fz+Sk5Px+++/QwiB2rVro0qVKuaoHxEREVGplPiMuypVquC5554ry7rYFa1Oj5O/5V6Z1r72M7z7KhERVTjWcizk5QVmotbp8fbmCwB4q3kiIqqYrOVYyCMwERER2TUGO0RERGTXGOwQERGRXWOwQ0RERHaNwQ4RERHZNQY7REREZNd46bmZKBVyzHulofSeiEonf5+avedauZbHPkxUMtbSjxjsmIlSIcebrYMsXQ0iu5G/T5VXsMM+TFQ61tKP+HOFiIiI7BpHdsxEpxc4l/AYANAy2AsKuczCNSKybfn7VHmXxz5MVDLW0o8Y7JhJjlaH/p+fBZB7i2xXRzY1UWnk71PlXR77MFHJWEs/Yu8lIpsggwy1q1YCAPyWlF6u5cnAUR0iW8Zgh4hsgoujAjETwwAAQdP2lWt5RGTbeIIyERER2TUGO0RERGTXOI1FRDYhS63Dy6tPW6S8vWPawcVRUW5lE1HZYrBDRDZBQJTLicmmyhMQ5VYuEZU9q5/Gun//Pt544w14e3vD1dUVTZs2RVxcnLRcCIHo6Gj4+/vDxcUFHTp0wLVr5r+76tM4yOWY3qUepnepBwe51TczERFRmbOWY6FVj+wkJyejbdu26NixIw4cOICqVavi1q1bqFy5spRn6dKlWLFiBTZv3ow6depg/vz5CA8Px82bN+Hu7m6xujs6yDEirJbFyiciIrI0azkWWnWws2TJEgQEBGDTpk1SWlBQkPReCIGVK1dixowZ6NWrFwBgy5Yt8PX1xbZt2zBixIjyrjIRERFZGasOdvbu3YvIyEj06dMHJ06cQLVq1TBq1CgMGzYMAJCQkIDExERERERI6zg5OSEsLAxnzpwpMNjJyclBTk6O9Dk1NRUAoNFooNFoyqTuOr3Atb9yt9vQ38OmbzWf1yZl1TZkiO1bNBqNVnrvKBcFtpeTwnhZcdo4b/385Wk0GmhkPG+nIPwOm5+ttrG5j4VFbQ+ZEMJqe7CzszMAYOLEiejTpw/OnTuHqKgofPbZZ3jzzTdx5swZtG3bFvfv34e/v7+03vDhw3Hnzh0cOnTI5Hajo6Mxd+5co/Rt27bB1dW1TOqeowOmnMuNJZe21MKJF3IQlUp59yn2YaLSM3c/yszMxIABA5CSkgIPD48C81l1sOPo6IgWLVrgzJkzUtq4ceNw/vx5xMbGSsHOX3/9BT8/PynPsGHDcO/ePRw8eNDkdk2N7AQEBOCff/4ptLGKI1OtRZMPjgIAfpn1ok0/V0ej0SAmJgbh4eFQKpWWro7dYfsWTf4+5SgXuDY30mS+RtGHcDXacFlx2jhvfXvqw+bG77D52Wobm7sfpaamwsfH56nBjlX3Xj8/PzRo0MAgrX79+vjuu+8AACqVCgCQmJhoEOwkJSXB19e3wO06OTnBycnJKF2pVJbZl0gp/jdUl7tdq27qIinL9iFjbN/C5e9Tar2swLbK0RW8rChtnLe+PfZhc+N32PxsrY3N3Y+K2hZWfU1027ZtcfPmTYO0X3/9FYGBgQCA4OBgqFQqxMTESMvVajVOnDiBNm3alGtdiYiIyDpZ9U+VCRMmoE2bNli4cCH69u2Lc+fOYf369Vi/fj0AQCaTISoqCgsXLkTt2rVRu3ZtLFy4EK6urhgwYICFa09ERETWwKqDneeeew67du3C9OnTMW/ePAQHB2PlypUYOHCglGfKlCnIysrCqFGjkJycjFatWuHw4cMWvccOERERWQ+rDnYAoHv37ujevXuBy2UyGaKjoxEdHV1+lSIiIiKbYfXBjq1ykMsxvlNt6T0RlU7+PvXxj7+Va3nsw0QlYy39iMGOmTg6yDEhvI6lq0FkN/L3qfIIdtiHiUrPWvoRf64QERGRXePIjpno9QK//50OAHj2mUqQ2/DjIoisQf4+Vd7lsQ8TlYy19CMGO2aSrdUh4qOTAIDr8yJ591WiUsrfp8q7PPZhopKxln7E3ktENsPLzREA8DhDXa7lEZFtY7BDRDbB1dEBF2eFAwCCpu0r1/KIyLbxBGUiIiKyawx2iIiIyK5xGouIbEK2RofBG89ZpLwtb7eEs1JRbmUTUdlisENENkEvBH5OeGyR8vRClFu5RFT2GOyYiYNcjuHta0rviYiIKhprORYy2DETRwc53u9a39LVICIishhrORZyyIGIiIjsGkd2zESvF7j/bxYAoFplF95qnoiIKhxrORZyZMdMsrU6vLD0GF5YegzZWp2lq0NERFTurOVYyGCHiIiI7BqDHSIiIrJrDHaIiIjIrjHYISIiIrvGYIeIiIjsGoMdIiIismu8z46ZKOQyDHo+UHpPRKWTv099dfZOuZbHPkxUMtbSjxjsmImTgwIf9Gxk6WoQ2Y38fao8gh32YaLSs5Z+xGksIiIismsc2TETIQQeZ6gBAF5ujpDJOAxOVBr5+1R5l8c+TFQy1tKPGOyYSZZGh9D5RwAA1+dFwtWRTU1UGvn7VHmXxz5MVDLW0o84jUVERER2jT9ViMgmuDo64PbibgCAoGn7yrU8IrJtHNkhIiIiu8Zgh4iIiOwap7GIyCZka3SY+E28Rcpb0bcpnJWKciubiMoWgx0isgl6IbD/SqJFylveR5RbuURU9hjsmIlCLsNrzatL74mIiCoaazkWMtgxEycHBT7s28TS1SAiIrIYazkW2tQJyosWLYJMJkNUVJSUJoRAdHQ0/P394eLigg4dOuDatWuWqyQRERFZFZsJds6fP4/169ejcePGBulLly7FihUrsHr1apw/fx4qlQrh4eFIS0uzUE1zCSGQqdYiU62FEJzvJyKiisdajoU2Eeykp6dj4MCB+Pzzz1GlShUpXQiBlStXYsaMGejVqxcaNWqELVu2IDMzE9u2bbNgjXNvkd1g9iE0mH0IWRqdRetCRERkCdZyLLSJc3ZGjx6Nbt26oXPnzpg/f76UnpCQgMTEREREREhpTk5OCAsLw5kzZzBixAiT28vJyUFOTo70OTU1FQCg0Wig0WjKpM4ajTbfew00Mtsd3clrk7JqGzLE9i2a/H3KUS4KbC8nhfGy4rRx3vr21IfNjd9h87PVNjZ3Pypqe1h9sLNjxw5cvHgR58+fN1qWmJh7Waivr69Buq+vL+7cuVPgNhctWoS5c+capR8+fBiurq6lrHGuHB2Q17yHDh2Gkx3coiMmJsbSVbBrbN/C5e9T81vosH//fpP5lrZEgcuK0sZ569tjHzY3fofNz9ba2Nz9KDMzs0j5rDrYuXfvHsaPH4/Dhw/D2dm5wHxPPjJeCFHoY+SnT5+OiRMnSp9TU1MREBCAiIgIeHh4lL7iADLVWkw5dxQAEBkZYdNPTNZoNIiJiUF4eDiUSqWlq2N32L5Fk79PzbygwLW5kSbzNYo+hKvRhsuK08Z569tTHzY3fofNz1bb2Nz9KG9m5mmsuvfGxcUhKSkJoaGhUppOp8PJkyexevVq3Lx5E0DuCI+fn5+UJykpyWi0Jz8nJyc4OTkZpSuVyjL7EinF/4Kt3O1adVMXSVm2Dxlj+xYuf59S62UFtlWOruBlRWnjvPXtsQ+bG7/D5mdrbWzuflTUtrDqE5Q7deqEK1euID4+Xnq1aNECAwcORHx8PGrWrAmVSmUwrKdWq3HixAm0adPGgjUnIiIia2HVP1Xc3d3RqFEjgzQ3Nzd4e3tL6VFRUVi4cCFq166N2rVrY+HChXB1dcWAAQMsUWUiIiKyMlYd7BTFlClTkJWVhVGjRiE5ORmtWrXC4cOH4e7ubtF6yWUydA1RSe+JqHTy96nyeEYW+zBR6VlLP7K5YOf48eMGn2UyGaKjoxEdHW2R+hTEWanAmoGhT89IREWSv08FTdtXruURUclYSz+y6nN2iIiIiEqLwQ4RERHZNZubxrIVmWotGsw+BAC4Pi+S9+ggKqX8faq8y2MfJioZa+lHHNkhIiIiu8afKkRkE1yUCsTN7AwACJ1/pFzLc1HyWRFEtozBDhHZBJlMBu9Kxnc+t5fyiMh8OI1FREREdo0jO0RkE3K0Osz/zw2LlDeze304OXAqi8hWMdghIpug0wt8dfaORcqb3rVeuZVLRGWPwY6ZyGUydKz7jPSeiIioorGWYyGDHTNxViqw6a2Wlq4GERGRxVjLsZAnKBMREZFdY7BDREREdo3BjplkqrWoP+sg6s86iEy11tLVISIiKnfWcizkOTtmlKXRWboKREREFmUNx0KO7BAREZFdY7BDREREdo3BDhEREdk1BjtERERk1xjsEBERkV3j1VhmIpfJ0CrYS3pPRKWTv0/9nPC4XMtjHyYqGWvpRwx2zMRZqcDOEa0tXQ0iu5G/TwVN21eu5RFRyVhLP+I0FhEREdk1BjtERERk1ziNZSaZai3aLTkGADg9tSNcHdnURKWRv0+Vd3nsw0QlYy39iL3XjB5nqC1dBSK7Ut59in2YqPSsoR8x2CEim+DsoMDhCe0BABEfnSzX8pwdFGYvj4jMh8EOEdkEuVyGOr7udlseEZkPT1AmIiIiu8aRHSKyCWqtHp8e+90i5Y3u+CwcHfjbkMhWMdghIpug1evx8Y+/WaS8EWE14ciBcCKbxWDHTOQyGRpX95TeExERVTTWcixksGMmzkoF9o5pZ+lqEBERWYy1HAs5LktERER2jcEOERER2TUGO2aSpdah7eKjaLv4KLLUOktXh4iIqNxZy7HQqoOdRYsW4bnnnoO7uzuqVq2Knj174ubNmwZ5hBCIjo6Gv78/XFxc0KFDB1y7ds1CNc5XLwjc/zcL9//NgoCwdHWIiIjKnbUcC6062Dlx4gRGjx6Ns2fPIiYmBlqtFhEREcjIyJDyLF26FCtWrMDq1atx/vx5qFQqhIeHIy0tzYI1JyIiImth1VdjHTx40ODzpk2bULVqVcTFxaF9+/YQQmDlypWYMWMGevXqBQDYsmULfH19sW3bNowYMcIS1SYiIiIrYtXBzpNSUlIAAF5eXgCAhIQEJCYmIiIiQsrj5OSEsLAwnDlzpsBgJycnBzk5OdLn1NRUAIBGo4FGoymTumo02nzvNdDIbHcqK69NyqptyBDbt2jy9ylHuSiwvZwUxsuK08Z569tTHzY3fofNz1bb2Nz9qKjtIRNC2EQPFkLglVdeQXJyMk6dOgUAOHPmDNq2bYv79+/D399fyjt8+HDcuXMHhw4dMrmt6OhozJ071yh927ZtcHV1LZP65uiAKedyY8mlLbVw4kOTiUqlvPsU+zBR6Zm7H2VmZmLAgAFISUmBh4dHgflsZmRnzJgxuHz5Mk6fPm20TPbEXRmFEEZp+U2fPh0TJ06UPqempiIgIAARERGFNlZxZKq1mHLuKAAgMjICro4209RGNBoNYmJiEB4eDqVSaenq2B22b9Hk71MzLyhwbW6kyXyNog/harThsuK0cd769tSHzY3fYfOz1TY2dz/Km5l5GpvovWPHjsXevXtx8uRJVK9eXUpXqVQAgMTERPj5+UnpSUlJ8PX1LXB7Tk5OcHJyMkpXKpVl9iVyFHLUrlop973SEUql7f8sLMv2IWNs38Ll71O/JaUX2FY5OlmBy4rSxnnr22MfNjd+h83P1trY3P2oqG1h1cGOEAJjx47Frl27cPz4cQQHBxssDw4OhkqlQkxMDJo1awYAUKvVOHHiBJYsWWKJKktcHBWImRhm0ToQ2ZP8fSpo2r5yLY+ISsZa+pFVBzujR4/Gtm3bsGfPHri7uyMxMREA4OnpCRcXF8hkMkRFRWHhwoWoXbs2ateujYULF8LV1RUDBgywcO2JiIjIGlh1sLN27VoAQIcOHQzSN23ahCFDhgAApkyZgqysLIwaNQrJyclo1aoVDh8+DHd393KuLREREVkjqw52inKhmEwmQ3R0NKKjo81foWLIUuvw8urck6n3jmkHF0fO9xOVRv4+Vd7lsQ8TlYy19COrDnZsmYDAb0np0nsiKp38faq8y2MfJioZa+lHDHaIyCY4OSiwfdjzAID+n58t1/KcHDiqQ2TLGOwQkU1QyGVoXcvbbssjIvOx6geBEhEREZUWR3aIyCZodHpsP3fXIuX1b1kDSgV/GxLZKgY7RGQTNDo9Zu+5ZpHyeodWZ7BDZMMY7JiJDDJUq+wivSciIqporOVYyGDHTFwcFfhp2ouWrgYREZHFWMuxkOOyREREZNcY7BAREZFdY7BjJtma3Ftkv7z6NLI1OktXh4iIqNxZy7GQ5+yYiV4IXP4zRXpPRERU0VjLsZAjO0RERGTXGOwQERGRXWOwQ0RERHaNwQ4RERHZNQY7REREZNd4NZYZebk5WroKRHYlr089zlCXa3lEVHLW0I8Y7JiJq6MDLs4Kt3Q1iOxG/j4VNG1fuZZHRCVjLf2I01hERERk1xjsEBERkV3jNJaZZGt0GLzxHABgy9st4axUWLhGRLYtf58q7/LYh4lKxlr6EYMdM9ELgZ8THkvviah08vep8i6PfZioZKylHzHYISKb4KiQ49MBzQEAo7ddLNfyHBWc8SeyZQx2iMgmOCjk6NbYDwAwelv5lkdEto0/V4iIiMiucWSHiGyCVqfHoWsPLVJeZENfOHAqi8hmMdghIpug1unL5VwdU+VdnxfJYIfIhjHYMSMXXqpKREQVnDUcCxnsmImrowNufPCSpatBRERkMdZyLOS4LBEREdk1BjtERERk1xjsmEm2Roe3Np3DW5vOIVujs3R1iIiIyp21HAt5zo6Z6IXAsZt/S++JiIgqGms5FnJkh4iIiOya3QQ7a9asQXBwMJydnREaGopTp05ZukpERERkBewi2Nm5cyeioqIwY8YMXLp0CS+88AK6dOmCu3fvWrpqREREZGF2EeysWLECQ4cOxTvvvIP69etj5cqVCAgIwNq1ay1dNSIiIrIwmw921Go14uLiEBERYZAeERGBM2fOWKhWREREZC1s/mqsf/75BzqdDr6+vgbpvr6+SExMNLlOTk4OcnJypM8pKSkAgMePH0Oj0ZRJvTLVWuhzMgEAjx49Qpaj7Ta1RqNBZmYmHj16BKVSaenq2B22b9Hk71NKucCjR49M5nPQZhgtK04b561vT33Y3PgdNj9bbWNz96O0tDQAgHjKlV5203tlMpnBZyGEUVqeRYsWYe7cuUbpwcHBZqlbjZVm2SxRheazopBlH5Zy20+szz5MVHrm7EdpaWnw9PQscLnNBzs+Pj5QKBRGozhJSUlGoz15pk+fjokTJ0qf9Xo9Hj9+DG9v7wIDpIosNTUVAQEBuHfvHjw8PCxdHbvD9jU/trF5sX3Nj21smhACaWlp8Pf3LzSfzQc7jo6OCA0NRUxMDF599VUpPSYmBq+88orJdZycnODk5GSQVrlyZXNW0y54eHiwk5kR29f82MbmxfY1P7axscJGdPLYfLADABMnTsSgQYPQokULtG7dGuvXr8fdu3fx7rvvWrpqREREZGF2Eey8/vrrePToEebNm4cHDx6gUaNG2L9/PwIDAy1dNSIiIrIwuwh2AGDUqFEYNWqUpathl5ycnDBnzhyjqT8qG2xf82Mbmxfb1/zYxqUjE0+7XouIiIjIhtn8TQWJiIiICsNgh4iIiOwagx0iIiKyawx2iIiIyK4x2CHJggUL0KZNG7i6uhZ4k8W7d++iR48ecHNzg4+PD8aNGwe1Wm2Q58qVKwgLC4OLiwuqVauGefPmPfW5JRVVUFAQZDKZwWvatGkGeYrS5lSwNWvWIDg4GM7OzggNDcWpU6csXSWbFB0dbfRdValU0nIhBKKjo+Hv7w8XFxd06NAB165ds2CNrd/JkyfRo0cP+Pv7QyaTYffu3QbLi9KmOTk5GDt2LHx8fODm5oaXX34Zf/75ZznuhW1gsEMStVqNPn36YOTIkSaX63Q6dOvWDRkZGTh9+jR27NiB7777DpMmTZLypKamIjw8HP7+/jh//jxWrVqF5cuXY8WKQh5kVMHl3R8q7zVz5kxpWVHanAq2c+dOREVFYcaMGbh06RJeeOEFdOnSBXfv3rV01WxSw4YNDb6rV65ckZYtXboUK1aswOrVq3H+/HmoVCqEh4dLD2okYxkZGWjSpAlWr15tcnlR2jQqKgq7du3Cjh07cPr0aaSnp6N79+7Q6XTltRu2QRA9YdOmTcLT09Moff/+/UIul4v79+9Ladu3bxdOTk4iJSVFCCHEmjVrhKenp8jOzpbyLFq0SPj7+wu9Xm/2utuawMBA8dFHHxW4vChtTgVr2bKlePfddw3S6tWrJ6ZNm2ahGtmuOXPmiCZNmphcptfrhUqlEosXL5bSsrOzhaenp1i3bl051dC2ARC7du2SPhelTf/991+hVCrFjh07pDz3798XcrlcHDx4sNzqbgs4skNFFhsbi0aNGhk8cC0yMhI5OTmIi4uT8oSFhRnc+CoyMhJ//fUXbt++Xd5VtglLliyBt7c3mjZtigULFhhMURWlzck0tVqNuLg4REREGKRHRETgzJkzFqqVbfvtt9/g7++P4OBg9OvXD3/88QcAICEhAYmJiQZt7eTkhLCwMLZ1CRWlTePi4qDRaAzy+Pv7o1GjRmz3J9jNHZTJ/BITE42eJF+lShU4OjpKT51PTExEUFCQQZ68dRITExEcHFwudbUV48ePR/PmzVGlShWcO3cO06dPR0JCAr744gsARWtzMu2ff/6BTqczaj9fX1+2XQm0atUKX375JerUqYOHDx9i/vz5aNOmDa5duya1p6m2vnPnjiWqa/OK0qaJiYlwdHRElSpVjPLwO26IIzt2ztRJhU++Lly4UOTtyWQyozQhhEH6k3nE/52cbGpde1ScNp8wYQLCwsLQuHFjvPPOO1i3bh02bNiAR48eSdsrSptTwUx9H9l2xdelSxe89tprCAkJQefOnbFv3z4AwJYtW6Q8bOuyV5I2Zbsb48iOnRszZgz69etXaJ4nR2IKolKp8PPPPxukJScnQ6PRSL8+VCqV0S+KpKQkAMa/UOxVadr8+eefBwD8/vvv8Pb2LlKbk2k+Pj5QKBQmv49su9Jzc3NDSEgIfvvtN/Ts2RNA7kiDn5+flIdtXXJ5V7oV1qYqlQpqtRrJyckGoztJSUlo06ZN+VbYynFkx875+PigXr16hb6cnZ2LtK3WrVvj6tWrePDggZR2+PBhODk5ITQ0VMpz8uRJg/NODh8+DH9//yIHVbauNG1+6dIlAJD+uBWlzck0R0dHhIaGIiYmxiA9JiaGB4IykJOTgxs3bsDPzw/BwcFQqVQGba1Wq3HixAm2dQkVpU1DQ0OhVCoN8jx48ABXr15luz/JgidHk5W5c+eOuHTpkpg7d66oVKmSuHTpkrh06ZJIS0sTQgih1WpFo0aNRKdOncTFixfFkSNHRPXq1cWYMWOkbfz777/C19dX9O/fX1y5ckV8//33wsPDQyxfvtxSu2W1zpw5I1asWCEuXbok/vjjD7Fz507h7+8vXn75ZSlPUdqcCrZjxw6hVCrFhg0bxPXr10VUVJRwc3MTt2/ftnTVbM6kSZPE8ePHxR9//CHOnj0runfvLtzd3aW2XLx4sfD09BTff/+9uHLliujfv7/w8/MTqampFq659UpLS5P+zgKQ/h7cuXNHCFG0Nn333XdF9erVxZEjR8TFixfFiy++KJo0aSK0Wq2ldssqMdghyeDBgwUAo9exY8ekPHfu3BHdunUTLi4uwsvLS4wZM8bgMnMhhLh8+bJ44YUXhJOTk1CpVCI6OpqXnZsQFxcnWrVqJTw9PYWzs7OoW7eumDNnjsjIyDDIV5Q2p4J9+umnIjAwUDg6OormzZuLEydOWLpKNun1118Xfn5+QqlUCn9/f9GrVy9x7do1ablerxdz5swRKpVKODk5ifbt24srV65YsMbW79ixYyb/5g4ePFgIUbQ2zcrKEmPGjBFeXl7CxcVFdO/eXdy9e9cCe2PdZELw1rZERERkv3jODhEREdk1BjtERERk1xjsEBERkV1jsENERER2jcEOERER2TUGO0RERGTXGOwQERGRXWOwQ0RWYfPmzahcuXKx1hkyZIj0XCZLu337NmQyGeLj4y1dFSJ6AoMdIiqWdevWwd3dHVqtVkpLT0+HUqnECy+8YJD31KlTkMlk+PXXX5+63ddff71I+YorKCgIK1euLPPtEpHtYLBDRMXSsWNHpKen48KFC1LaqVOnoFKpcP78eWRmZkrpx48fh7+/P+rUqfPU7bq4uKBq1apmqTMRVWwMdoioWOrWrQt/f38cP35cSjt+/DheeeUV1KpVC2fOnDFI79ixI4DcJzZPmTIF1apVg5ubG1q1amWwDVPTWPPnz0fVqlXh7u6Od955B9OmTUPTpk2N6rR8+XL4+fnB29sbo0ePhkajAQB06NABd+7cwYQJEyCTySCTyUzuU//+/dGvXz+DNI1GAx8fH2zatAkAcPDgQbRr1w6VK1eGt7c3unfvjlu3bhXYTqb2Z/fu3UZ1+OGHHxAaGgpnZ2fUrFkTc+fONRg1I6LSY7BDRMXWoUMHHDt2TPp87NgxdOjQAWFhYVK6Wq1GbGysFOy89dZb+Omnn7Bjxw5cvnwZffr0wUsvvYTffvvNZBlff/01FixYgCVLliAuLg41atTA2rVrjfIdO3YMt27dwrFjx7BlyxZs3rwZmzdvBgB8//33qF69OubNm4cHDx7gwYMHJssaOHAg9u7di/T0dCnt0KFDyMjIwGuvvQYAyMjIwMSJE3H+/Hn8+OOPkMvlePXVV6HX64vfgPnKeOONNzBu3Dhcv34dn332GTZv3owFCxaUeJtEZIKln0RKRLZn/fr1ws3NTWg0GpGamiocHBzEw4cPxY4dO0SbNm2EEEKcOHFCABC3bt0Sv//+u5DJZOL+/fsG2+nUqZOYPn26EEKITZs2CU9PT2lZq1atxOjRow3yt23bVjRp0kT6PHjwYBEYGCi0Wq2U1qdPH/H6669LnwMDA8VHH31U6P6o1Wrh4+MjvvzySymtf//+ok+fPgWuk5SUJABIT6FOSEgQAMSlS5dM7o8QQuzatUvk/7P7wgsviIULFxrk+eqrr4Sfn1+h9SWi4uHIDhEVW8eOHZGRkYHz58/j1KlTqFOnDqpWrYqwsDCcP38eGRkZOH78OGrUqIGaNWvi4sWLEEKgTp06qFSpkvQ6ceJEgVNBN2/eRMuWLQ3SnvwMAA0bNoRCoZA++/n5ISkpqVj7o1Qq0adPH3z99dcAckdx9uzZg4EDB0p5bt26hQEDBqBmzZrw8PBAcHAwAODu3bvFKiu/uLg4zJs3z6BNhg0bhgcPHhic+0REpeNg6QoQke159tlnUb16dRw7dgzJyckICwsDAKhUKgQHB+Onn37CsWPH8OKLLwIA9Ho9FAoF4uLiDAITAKhUqVKB5Tx5fosQwiiPUqk0WqckU0sDBw5EWFgYkpKSEBMTA2dnZ3Tp0kVa3qNHDwQEBODzzz+Hv78/9Ho9GjVqBLVabXJ7crncqL555xLl0ev1mDt3Lnr16mW0vrOzc7H3gYhMY7BDRCXSsWNHHD9+HMnJyXjvvfek9LCwMBw6dAhnz57FW2+9BQBo1qwZdDodkpKSjC5PL0jdunVx7tw5DBo0SErLfwVYUTk6OkKn0z01X5s2bRAQEICdO3fiwIED6NOnDxwdHQEAjx49wo0bN/DZZ59J9T99+nSh23vmmWeQlpaGjIwMuLm5AYDRPXiaN2+Omzdv4tlnny32fhFR0THYIaIS6dixo3TlU97IDpAb7IwcORLZ2dnSycl16tTBwIED8eabb+LDDz9Es2bN8M8//+Do0aMICQlB165djbY/duxYDBs2DC1atECbNm2wc+dOXL58GTVr1ixWPYOCgnDy5En069cPTk5O8PHxMZlPJpNhwIABWLduHX799VeDE7CrVKkCb29vrF+/Hn5+frh79y6mTZtWaLmtWrWCq6sr3n//fYwdOxbnzp2TTpzOM3v2bHTv3h0BAQHo06cP5HI5Ll++jCtXrmD+/PnF2k8iKhjP2SGiEunYsSOysrLw7LPPwtfXV0oPCwtDWloaatWqhYCAACl906ZNePPNNzFp0iTUrVsXL7/8Mn7++WeDPPkNHDgQ06dPx+TJk9G8eXMkJCRgyJAhxZ7emTdvHm7fvo1atWrhmWeeKTTvwIEDcf36dVSrVg1t27aV0uVyOXbs2IG4uDg0atQIEyZMwLJlywrdlpeXF7Zu3Yr9+/cjJCQE27dvR3R0tEGeyMhI/Oc//0FMTAyee+45PP/881ixYgUCAwOLtY9EVDiZMDUJTkRkhcLDw6FSqfDVV19ZuipEZEM4jUVEVikzMxPr1q1DZGQkFAoFtm/fjiNHjiAmJsbSVSMiG8ORHSKySllZWejRowcuXryInJwc1K1bFzNnzjR55RIRUWEY7BAREZFd4wnKREREZNcY7BAREZFdY7BDREREdo3BDhEREdk1BjtERERk1xjsEBERkV1jsENERER2jcEOERER2TUGO0RERGTX/j8nw77D4sPHVAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=512, out_features=200, TIME=8, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.25, 0.0625])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=2048, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=8, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=8, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.25, 0.0625])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=64, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=8, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=8, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.25, 0.0625])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 144,400\n",
      "========================================================\n",
      "\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 4\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 1594.0\n",
      "lif layer 1 self.abs_max_v: 1594.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "lif layer 1 self.abs_max_v: 2165.0\n",
      "fc layer 2 self.abs_max_out: 32.0\n",
      "lif layer 2 self.abs_max_v: 32.0\n",
      "layer   1  Sparsity: 88.8916%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 100.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 1880.0\n",
      "lif layer 1 self.abs_max_v: 2231.5\n",
      "fc layer 1 self.abs_max_out: 1887.0\n",
      "lif layer 1 self.abs_max_v: 2327.0\n",
      "fc layer 1 self.abs_max_out: 1912.0\n",
      "lif layer 1 self.abs_max_v: 2341.0\n",
      "lif layer 1 self.abs_max_v: 2423.5\n",
      "fc layer 1 self.abs_max_out: 2156.0\n",
      "lif layer 1 self.abs_max_v: 2807.5\n",
      "fc layer 2 self.abs_max_out: 57.0\n",
      "lif layer 2 self.abs_max_v: 57.0\n",
      "fc layer 1 self.abs_max_out: 2190.0\n",
      "fc layer 2 self.abs_max_out: 84.0\n",
      "lif layer 2 self.abs_max_v: 84.0\n",
      "lif layer 1 self.abs_max_v: 2946.0\n",
      "fc layer 2 self.abs_max_out: 85.0\n",
      "lif layer 2 self.abs_max_v: 112.5\n",
      "fc layer 1 self.abs_max_out: 2618.0\n",
      "fc layer 1 self.abs_max_out: 3250.0\n",
      "lif layer 1 self.abs_max_v: 3932.0\n",
      "fc layer 1 self.abs_max_out: 4310.0\n",
      "lif layer 1 self.abs_max_v: 4310.0\n",
      "fc layer 2 self.abs_max_out: 114.0\n",
      "lif layer 2 self.abs_max_v: 144.5\n",
      "fc layer 3 self.abs_max_out: 14.0\n",
      "lif layer 1 self.abs_max_v: 4845.5\n",
      "fc layer 1 self.abs_max_out: 4322.0\n",
      "lif layer 1 self.abs_max_v: 4975.5\n",
      "fc layer 1 self.abs_max_out: 4670.0\n",
      "fc layer 1 self.abs_max_out: 5471.0\n",
      "lif layer 1 self.abs_max_v: 5471.0\n",
      "fc layer 2 self.abs_max_out: 118.0\n",
      "lif layer 2 self.abs_max_v: 161.0\n",
      "fc layer 3 self.abs_max_out: 19.0\n",
      "fc layer 1 self.abs_max_out: 5980.0\n",
      "lif layer 1 self.abs_max_v: 5980.0\n",
      "lif layer 1 self.abs_max_v: 6540.0\n",
      "fc layer 2 self.abs_max_out: 128.0\n",
      "fc layer 3 self.abs_max_out: 27.0\n",
      "lif layer 2 self.abs_max_v: 179.0\n",
      "fc layer 1 self.abs_max_out: 6341.0\n",
      "lif layer 1 self.abs_max_v: 6849.0\n",
      "fc layer 2 self.abs_max_out: 134.0\n",
      "fc layer 2 self.abs_max_out: 162.0\n",
      "lif layer 2 self.abs_max_v: 192.0\n",
      "fc layer 2 self.abs_max_out: 171.0\n",
      "fc layer 1 self.abs_max_out: 6462.0\n",
      "lif layer 1 self.abs_max_v: 6860.0\n",
      "fc layer 2 self.abs_max_out: 183.0\n",
      "fc layer 2 self.abs_max_out: 197.0\n",
      "lif layer 2 self.abs_max_v: 197.0\n",
      "fc layer 1 self.abs_max_out: 6734.0\n",
      "fc layer 1 self.abs_max_out: 6757.0\n",
      "fc layer 1 self.abs_max_out: 7082.0\n",
      "lif layer 1 self.abs_max_v: 7082.0\n",
      "lif layer 2 self.abs_max_v: 210.0\n",
      "fc layer 2 self.abs_max_out: 205.0\n",
      "fc layer 2 self.abs_max_out: 207.0\n",
      "lif layer 1 self.abs_max_v: 7240.0\n",
      "fc layer 1 self.abs_max_out: 7239.0\n",
      "lif layer 2 self.abs_max_v: 221.5\n",
      "fc layer 1 self.abs_max_out: 7523.0\n",
      "lif layer 1 self.abs_max_v: 7523.0\n",
      "fc layer 1 self.abs_max_out: 7843.0\n",
      "lif layer 1 self.abs_max_v: 7843.0\n",
      "fc layer 2 self.abs_max_out: 242.0\n",
      "lif layer 2 self.abs_max_v: 242.0\n",
      "lif layer 1 self.abs_max_v: 8099.0\n",
      "fc layer 1 self.abs_max_out: 7879.0\n",
      "fc layer 1 self.abs_max_out: 7885.0\n",
      "fc layer 1 self.abs_max_out: 8216.0\n",
      "lif layer 1 self.abs_max_v: 8216.0\n",
      "lif layer 1 self.abs_max_v: 8843.0\n",
      "fc layer 1 self.abs_max_out: 9390.0\n",
      "lif layer 1 self.abs_max_v: 10199.5\n",
      "fc layer 1 self.abs_max_out: 9989.0\n",
      "fc layer 1 self.abs_max_out: 10104.0\n",
      "lif layer 2 self.abs_max_v: 249.0\n",
      "fc layer 1 self.abs_max_out: 10493.0\n",
      "lif layer 1 self.abs_max_v: 10493.0\n",
      "lif layer 2 self.abs_max_v: 257.5\n",
      "fc layer 2 self.abs_max_out: 246.0\n",
      "lif layer 2 self.abs_max_v: 260.5\n",
      "lif layer 2 self.abs_max_v: 274.5\n",
      "lif layer 2 self.abs_max_v: 309.5\n",
      "fc layer 1 self.abs_max_out: 10709.0\n",
      "lif layer 1 self.abs_max_v: 10709.0\n",
      "lif layer 1 self.abs_max_v: 10817.0\n",
      "fc layer 1 self.abs_max_out: 10813.0\n",
      "fc layer 1 self.abs_max_out: 10938.0\n",
      "lif layer 1 self.abs_max_v: 10938.0\n",
      "fc layer 1 self.abs_max_out: 12062.0\n",
      "lif layer 1 self.abs_max_v: 12062.0\n",
      "fc layer 1 self.abs_max_out: 12156.0\n",
      "lif layer 1 self.abs_max_v: 12156.0\n",
      "fc layer 1 self.abs_max_out: 12232.0\n",
      "lif layer 1 self.abs_max_v: 12232.0\n",
      "fc layer 1 self.abs_max_out: 13014.0\n",
      "lif layer 1 self.abs_max_v: 13014.0\n",
      "fc layer 1 self.abs_max_out: 13102.0\n",
      "lif layer 1 self.abs_max_v: 13217.0\n",
      "fc layer 1 self.abs_max_out: 13390.0\n",
      "lif layer 1 self.abs_max_v: 13390.0\n",
      "fc layer 1 self.abs_max_out: 13881.0\n",
      "lif layer 1 self.abs_max_v: 13881.0\n",
      "lif layer 1 self.abs_max_v: 14635.5\n",
      "fc layer 2 self.abs_max_out: 248.0\n",
      "fc layer 1 self.abs_max_out: 14033.0\n",
      "fc layer 1 self.abs_max_out: 14614.0\n",
      "fc layer 1 self.abs_max_out: 15415.0\n",
      "lif layer 1 self.abs_max_v: 15415.0\n",
      "fc layer 1 self.abs_max_out: 16041.0\n",
      "lif layer 1 self.abs_max_v: 16041.0\n",
      "fc layer 1 self.abs_max_out: 16329.0\n",
      "lif layer 1 self.abs_max_v: 16329.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 2.2 seconds.), retrying request\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc layer 1 self.abs_max_out: 17721.0\n",
      "lif layer 1 self.abs_max_v: 17721.0\n",
      "fc layer 1 self.abs_max_out: 17759.0\n",
      "lif layer 1 self.abs_max_v: 17759.0\n",
      "fc layer 1 self.abs_max_out: 17998.0\n",
      "lif layer 1 self.abs_max_v: 17998.0\n",
      "fc layer 1 self.abs_max_out: 18418.0\n",
      "lif layer 1 self.abs_max_v: 18418.0\n",
      "fc layer 1 self.abs_max_out: 18798.0\n",
      "lif layer 1 self.abs_max_v: 18798.0\n",
      "fc layer 1 self.abs_max_out: 19522.0\n",
      "lif layer 1 self.abs_max_v: 19522.0\n",
      "fc layer 1 self.abs_max_out: 19799.0\n",
      "lif layer 1 self.abs_max_v: 19799.0\n",
      "fc layer 2 self.abs_max_out: 275.0\n",
      "fc layer 2 self.abs_max_out: 279.0\n",
      "fc layer 2 self.abs_max_out: 288.0\n",
      "fc layer 2 self.abs_max_out: 294.0\n",
      "fc layer 2 self.abs_max_out: 297.0\n",
      "lif layer 2 self.abs_max_v: 360.5\n",
      "fc layer 1 self.abs_max_out: 20059.0\n",
      "lif layer 1 self.abs_max_v: 20059.0\n",
      "fc layer 2 self.abs_max_out: 307.0\n",
      "fc layer 1 self.abs_max_out: 21359.0\n",
      "lif layer 1 self.abs_max_v: 21359.0\n",
      "fc layer 2 self.abs_max_out: 329.0\n",
      "fc layer 1 self.abs_max_out: 21595.0\n",
      "lif layer 1 self.abs_max_v: 21595.0\n",
      "fc layer 1 self.abs_max_out: 21630.0\n",
      "lif layer 1 self.abs_max_v: 21630.0\n",
      "fc layer 2 self.abs_max_out: 330.0\n",
      "fc layer 1 self.abs_max_out: 21842.0\n",
      "lif layer 1 self.abs_max_v: 22819.0\n",
      "fc layer 1 self.abs_max_out: 22091.0\n",
      "lif layer 1 self.abs_max_v: 23074.5\n",
      "fc layer 1 self.abs_max_out: 22520.0\n",
      "fc layer 1 self.abs_max_out: 22959.0\n",
      "lif layer 1 self.abs_max_v: 23588.0\n",
      "fc layer 1 self.abs_max_out: 23654.0\n",
      "lif layer 1 self.abs_max_v: 23654.0\n",
      "lif layer 2 self.abs_max_v: 361.5\n",
      "fc layer 1 self.abs_max_out: 24089.0\n",
      "lif layer 1 self.abs_max_v: 24089.0\n",
      "fc layer 1 self.abs_max_out: 24559.0\n",
      "lif layer 1 self.abs_max_v: 24559.0\n",
      "fc layer 1 self.abs_max_out: 25052.0\n",
      "lif layer 1 self.abs_max_v: 25052.0\n",
      "fc layer 1 self.abs_max_out: 25189.0\n",
      "lif layer 1 self.abs_max_v: 25189.0\n",
      "fc layer 1 self.abs_max_out: 25325.0\n",
      "lif layer 1 self.abs_max_v: 25325.0\n",
      "fc layer 1 self.abs_max_out: 25402.0\n",
      "lif layer 1 self.abs_max_v: 25402.0\n",
      "fc layer 1 self.abs_max_out: 25586.0\n",
      "lif layer 1 self.abs_max_v: 25586.0\n",
      "fc layer 1 self.abs_max_out: 25706.0\n",
      "lif layer 1 self.abs_max_v: 25706.0\n",
      "fc layer 1 self.abs_max_out: 25981.0\n",
      "lif layer 1 self.abs_max_v: 25981.0\n",
      "fc layer 1 self.abs_max_out: 26110.0\n",
      "lif layer 1 self.abs_max_v: 26110.0\n",
      "fc layer 1 self.abs_max_out: 26274.0\n",
      "lif layer 1 self.abs_max_v: 26274.0\n",
      "fc layer 1 self.abs_max_out: 26367.0\n",
      "lif layer 1 self.abs_max_v: 26367.0\n",
      "fc layer 1 self.abs_max_out: 26427.0\n",
      "lif layer 1 self.abs_max_v: 26427.0\n",
      "train - Value 0: 1711 occurrences\n",
      "train - Value 1: 2319 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 38.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 90.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 91.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 106.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 108.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 124.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 128.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 142.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 144.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 148.00 at epoch 0, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-0   lr=['4.0000000'], tr/val_loss:  8.994615/  9.737357, val:  50.00%, val_best:  50.00%, tr:  82.88%, tr_best:  82.88%, epoch time: 255.68 seconds, 4.26 minutes\n",
      "layer   1  Sparsity: 84.4093%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.2814%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 99.3688%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 32240 real_backward_count 9663  29.972%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "layer   1  Sparsity: 84.8389%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 99.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 26567.0\n",
      "lif layer 1 self.abs_max_v: 26567.0\n",
      "fc layer 1 self.abs_max_out: 26681.0\n",
      "lif layer 1 self.abs_max_v: 26681.0\n",
      "fc layer 1 self.abs_max_out: 26898.0\n",
      "lif layer 1 self.abs_max_v: 26898.0\n",
      "fc layer 1 self.abs_max_out: 27077.0\n",
      "lif layer 1 self.abs_max_v: 27077.0\n",
      "fc layer 1 self.abs_max_out: 27104.0\n",
      "lif layer 1 self.abs_max_v: 27104.0\n",
      "fc layer 1 self.abs_max_out: 27365.0\n",
      "lif layer 1 self.abs_max_v: 27365.0\n",
      "fc layer 1 self.abs_max_out: 27482.0\n",
      "lif layer 1 self.abs_max_v: 27482.0\n",
      "fc layer 1 self.abs_max_out: 27604.0\n",
      "lif layer 1 self.abs_max_v: 27604.0\n",
      "fc layer 1 self.abs_max_out: 27701.0\n",
      "lif layer 1 self.abs_max_v: 27701.0\n",
      "fc layer 1 self.abs_max_out: 27776.0\n",
      "lif layer 1 self.abs_max_v: 27776.0\n",
      "fc layer 1 self.abs_max_out: 27813.0\n",
      "lif layer 1 self.abs_max_v: 27813.0\n",
      "fc layer 1 self.abs_max_out: 27980.0\n",
      "lif layer 1 self.abs_max_v: 27980.0\n",
      "fc layer 1 self.abs_max_out: 28054.0\n",
      "lif layer 1 self.abs_max_v: 28054.0\n",
      "fc layer 1 self.abs_max_out: 28174.0\n",
      "lif layer 1 self.abs_max_v: 28174.0\n",
      "fc layer 1 self.abs_max_out: 29267.0\n",
      "lif layer 1 self.abs_max_v: 29267.0\n",
      "fc layer 3 self.abs_max_out: 29.0\n",
      "fc layer 1 self.abs_max_out: 29915.0\n",
      "lif layer 1 self.abs_max_v: 29915.0\n",
      "fc layer 3 self.abs_max_out: 31.0\n",
      "fc layer 3 self.abs_max_out: 33.0\n",
      "train - Value 0: 1641 occurrences\n",
      "train - Value 1: 2389 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "lif layer 2 self.abs_max_v: 362.5\n",
      "max_activation_accul updated: 155.00 at epoch 1, iter 4029\n",
      "max_activation_accul updated: 161.00 at epoch 1, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 8 occurrences\n",
      "test - Value 1: 444 occurrences\n",
      "epoch-1   lr=['4.0000000'], tr/val_loss:  9.951480/ 10.100951, val:  50.88%, val_best:  50.88%, tr:  85.66%, tr_best:  85.66%, epoch time: 255.46 seconds, 4.26 minutes\n",
      "layer   1  Sparsity: 84.4102%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.2624%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 99.0649%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 64480 real_backward_count 18960  29.404%\n",
      "layer   1  Sparsity: 84.0088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 99.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 29927.0\n",
      "lif layer 1 self.abs_max_v: 29927.0\n",
      "fc layer 1 self.abs_max_out: 29935.0\n",
      "lif layer 1 self.abs_max_v: 29935.0\n",
      "lif layer 2 self.abs_max_v: 365.5\n",
      "fc layer 2 self.abs_max_out: 333.0\n",
      "fc layer 2 self.abs_max_out: 335.0\n",
      "fc layer 1 self.abs_max_out: 30163.0\n",
      "lif layer 1 self.abs_max_v: 30163.0\n",
      "fc layer 1 self.abs_max_out: 30294.0\n",
      "lif layer 1 self.abs_max_v: 30294.0\n",
      "fc layer 1 self.abs_max_out: 31357.0\n",
      "lif layer 1 self.abs_max_v: 31357.0\n",
      "fc layer 1 self.abs_max_out: 31468.0\n",
      "lif layer 1 self.abs_max_v: 31468.0\n",
      "fc layer 3 self.abs_max_out: 35.0\n",
      "fc layer 3 self.abs_max_out: 39.0\n",
      "fc layer 1 self.abs_max_out: 31477.0\n",
      "lif layer 1 self.abs_max_v: 31477.0\n",
      "fc layer 1 self.abs_max_out: 31551.0\n",
      "lif layer 1 self.abs_max_v: 31551.0\n",
      "fc layer 1 self.abs_max_out: 31746.0\n",
      "lif layer 1 self.abs_max_v: 31746.0\n",
      "fc layer 1 self.abs_max_out: 31800.0\n",
      "lif layer 1 self.abs_max_v: 31800.0\n",
      "train - Value 0: 1709 occurrences\n",
      "train - Value 1: 2321 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 181.00 at epoch 2, iter 4029\n",
      "max_activation_accul updated: 182.00 at epoch 2, iter 4029\n",
      "max_activation_accul updated: 193.00 at epoch 2, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-2   lr=['4.0000000'], tr/val_loss: 11.196013/ 11.588607, val:  50.00%, val_best:  50.88%, tr:  88.83%, tr_best:  88.83%, epoch time: 252.65 seconds, 4.21 minutes\n",
      "layer   1  Sparsity: 84.4104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.2231%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.9990%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 96720 real_backward_count 28290  29.249%\n",
      "layer   1  Sparsity: 78.8330%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 347.0\n",
      "lif layer 2 self.abs_max_v: 369.0\n",
      "fc layer 1 self.abs_max_out: 31939.0\n",
      "lif layer 1 self.abs_max_v: 31939.0\n",
      "fc layer 1 self.abs_max_out: 31995.0\n",
      "lif layer 1 self.abs_max_v: 31995.0\n",
      "fc layer 1 self.abs_max_out: 33082.0\n",
      "lif layer 1 self.abs_max_v: 33082.0\n",
      "lif layer 2 self.abs_max_v: 399.0\n",
      "fc layer 2 self.abs_max_out: 354.0\n",
      "lif layer 2 self.abs_max_v: 404.5\n",
      "fc layer 2 self.abs_max_out: 390.0\n",
      "lif layer 2 self.abs_max_v: 451.0\n",
      "fc layer 3 self.abs_max_out: 42.0\n",
      "fc layer 1 self.abs_max_out: 33127.0\n",
      "lif layer 1 self.abs_max_v: 33127.0\n",
      "fc layer 1 self.abs_max_out: 34044.0\n",
      "lif layer 1 self.abs_max_v: 34044.0\n",
      "train - Value 0: 1748 occurrences\n",
      "train - Value 1: 2282 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 202.00 at epoch 3, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-3   lr=['4.0000000'], tr/val_loss: 12.340517/ 11.208435, val:  50.00%, val_best:  50.88%, tr:  88.11%, tr_best:  88.83%, epoch time: 254.79 seconds, 4.25 minutes\n",
      "layer   1  Sparsity: 84.4116%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.2002%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.9532%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 128960 real_backward_count 37787  29.301%\n",
      "layer   1  Sparsity: 70.6787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 34768.0\n",
      "lif layer 1 self.abs_max_v: 34768.0\n",
      "fc layer 3 self.abs_max_out: 43.0\n",
      "fc layer 3 self.abs_max_out: 46.0\n",
      "fc layer 3 self.abs_max_out: 47.0\n",
      "fc layer 2 self.abs_max_out: 392.0\n",
      "fc layer 2 self.abs_max_out: 398.0\n",
      "fc layer 2 self.abs_max_out: 413.0\n",
      "fc layer 2 self.abs_max_out: 419.0\n",
      "fc layer 3 self.abs_max_out: 51.0\n",
      "fc layer 1 self.abs_max_out: 34966.0\n",
      "lif layer 1 self.abs_max_v: 34966.0\n",
      "train - Value 0: 1728 occurrences\n",
      "train - Value 1: 2302 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 227.00 at epoch 4, iter 4029\n",
      "max_activation_accul updated: 233.00 at epoch 4, iter 4029\n",
      "max_activation_accul updated: 238.00 at epoch 4, iter 4029\n",
      "max_activation_accul updated: 239.00 at epoch 4, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 446 occurrences\n",
      "test - Value 1: 6 occurrences\n",
      "epoch-4   lr=['4.0000000'], tr/val_loss: 10.786988/ 16.468838, val:  51.33%, val_best:  51.33%, tr:  85.83%, tr_best:  88.83%, epoch time: 255.90 seconds, 4.27 minutes\n",
      "layer   1  Sparsity: 84.4134%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.1759%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.7150%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 161200 real_backward_count 47279  29.329%\n",
      "layer   1  Sparsity: 84.2285%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 35021.0\n",
      "lif layer 1 self.abs_max_v: 35021.0\n",
      "fc layer 1 self.abs_max_out: 35480.0\n",
      "lif layer 1 self.abs_max_v: 35480.0\n",
      "fc layer 3 self.abs_max_out: 56.0\n",
      "fc layer 2 self.abs_max_out: 426.0\n",
      "fc layer 3 self.abs_max_out: 58.0\n",
      "fc layer 2 self.abs_max_out: 433.0\n",
      "train - Value 0: 1839 occurrences\n",
      "train - Value 1: 2191 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 242.00 at epoch 5, iter 4029\n",
      "max_activation_accul updated: 245.00 at epoch 5, iter 4029\n",
      "max_activation_accul updated: 246.00 at epoch 5, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-5   lr=['4.0000000'], tr/val_loss: 12.018829/  9.905818, val:  50.00%, val_best:  51.33%, tr:  89.98%, tr_best:  89.98%, epoch time: 257.27 seconds, 4.29 minutes\n",
      "layer   1  Sparsity: 84.4104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.1024%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.3040%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 193440 real_backward_count 56243  29.075%\n",
      "layer   1  Sparsity: 84.3262%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 59.0\n",
      "fc layer 3 self.abs_max_out: 60.0\n",
      "fc layer 1 self.abs_max_out: 36148.0\n",
      "lif layer 1 self.abs_max_v: 36148.0\n",
      "fc layer 2 self.abs_max_out: 438.0\n",
      "train - Value 0: 1932 occurrences\n",
      "train - Value 1: 2098 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 251.00 at epoch 6, iter 4029\n",
      "max_activation_accul updated: 252.00 at epoch 6, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 445 occurrences\n",
      "test - Value 1: 7 occurrences\n",
      "epoch-6   lr=['4.0000000'], tr/val_loss: 11.185575/ 10.648862, val:  51.55%, val_best:  51.55%, tr:  91.74%, tr_best:  91.74%, epoch time: 259.55 seconds, 4.33 minutes\n",
      "layer   1  Sparsity: 84.4104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.0788%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.2614%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225680 real_backward_count 65293  28.932%\n",
      "layer   1  Sparsity: 82.6172%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 64.0\n",
      "fc layer 1 self.abs_max_out: 36659.0\n",
      "lif layer 1 self.abs_max_v: 36659.0\n",
      "fc layer 2 self.abs_max_out: 443.0\n",
      "fc layer 2 self.abs_max_out: 444.0\n",
      "train - Value 0: 1882 occurrences\n",
      "train - Value 1: 2148 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 255.00 at epoch 7, iter 4029\n",
      "max_activation_accul updated: 273.00 at epoch 7, iter 4029\n",
      "max_activation_accul updated: 309.00 at epoch 7, iter 4029\n",
      "max_activation_accul updated: 318.00 at epoch 7, iter 4029\n",
      "max_activation_accul updated: 333.00 at epoch 7, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-7   lr=['4.0000000'], tr/val_loss: 14.297206/ 15.322121, val:  50.00%, val_best:  51.55%, tr:  90.94%, tr_best:  91.74%, epoch time: 253.13 seconds, 4.22 minutes\n",
      "layer   1  Sparsity: 84.4107%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.0413%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.0362%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 257920 real_backward_count 74181  28.761%\n",
      "layer   1  Sparsity: 71.6797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 37070.0\n",
      "lif layer 1 self.abs_max_v: 37070.0\n",
      "train - Value 0: 1913 occurrences\n",
      "train - Value 1: 2117 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-8   lr=['4.0000000'], tr/val_loss: 13.498202/ 13.910364, val:  50.00%, val_best:  51.55%, tr:  90.32%, tr_best:  91.74%, epoch time: 255.41 seconds, 4.26 minutes\n",
      "layer   1  Sparsity: 84.4132%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9993%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.0247%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 290160 real_backward_count 83183  28.668%\n",
      "layer   1  Sparsity: 78.1250%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 37446.0\n",
      "lif layer 1 self.abs_max_v: 37446.0\n",
      "train - Value 0: 1996 occurrences\n",
      "train - Value 1: 2034 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-9   lr=['4.0000000'], tr/val_loss: 13.581135/ 14.071198, val:  50.00%, val_best:  51.55%, tr:  91.79%, tr_best:  91.79%, epoch time: 256.04 seconds, 4.27 minutes\n",
      "layer   1  Sparsity: 84.4117%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9895%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.0537%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 322400 real_backward_count 92155  28.584%\n",
      "layer   1  Sparsity: 77.7588%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 69.0\n",
      "lif layer 2 self.abs_max_v: 472.0\n",
      "fc layer 2 self.abs_max_out: 447.0\n",
      "fc layer 2 self.abs_max_out: 469.0\n",
      "lif layer 2 self.abs_max_v: 510.0\n",
      "fc layer 2 self.abs_max_out: 471.0\n",
      "fc layer 2 self.abs_max_out: 472.0\n",
      "lif layer 2 self.abs_max_v: 511.5\n",
      "fc layer 2 self.abs_max_out: 491.0\n",
      "fc layer 1 self.abs_max_out: 37912.0\n",
      "lif layer 1 self.abs_max_v: 37912.0\n",
      "train - Value 0: 1966 occurrences\n",
      "train - Value 1: 2064 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 446 occurrences\n",
      "test - Value 1: 6 occurrences\n",
      "epoch-10  lr=['4.0000000'], tr/val_loss: 14.660205/ 11.796434, val:  51.33%, val_best:  51.55%, tr:  90.79%, tr_best:  91.79%, epoch time: 256.33 seconds, 4.27 minutes\n",
      "layer   1  Sparsity: 84.4118%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9680%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.9442%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 354640 real_backward_count 101414  28.596%\n",
      "layer   1  Sparsity: 80.5664%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 70.0\n",
      "fc layer 2 self.abs_max_out: 495.0\n",
      "fc layer 2 self.abs_max_out: 505.0\n",
      "fc layer 1 self.abs_max_out: 38102.0\n",
      "lif layer 1 self.abs_max_v: 38102.0\n",
      "train - Value 0: 2003 occurrences\n",
      "train - Value 1: 2027 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 240 occurrences\n",
      "test - Value 1: 212 occurrences\n",
      "epoch-11  lr=['4.0000000'], tr/val_loss: 14.348378/ 11.669551, val:  84.07%, val_best:  84.07%, tr:  90.92%, tr_best:  91.79%, epoch time: 253.24 seconds, 4.22 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9561%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.9328%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 386880 real_backward_count 110714  28.617%\n",
      "layer   1  Sparsity: 89.9658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 518.0\n",
      "lif layer 2 self.abs_max_v: 565.0\n",
      "fc layer 1 self.abs_max_out: 38372.0\n",
      "lif layer 1 self.abs_max_v: 38372.0\n",
      "train - Value 0: 1955 occurrences\n",
      "train - Value 1: 2075 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 359 occurrences\n",
      "test - Value 1: 93 occurrences\n",
      "epoch-12  lr=['4.0000000'], tr/val_loss: 14.356378/ 11.858587, val:  70.13%, val_best:  84.07%, tr:  90.92%, tr_best:  91.79%, epoch time: 256.89 seconds, 4.28 minutes\n",
      "layer   1  Sparsity: 84.4091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9559%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.9490%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 419120 real_backward_count 120011  28.634%\n",
      "layer   1  Sparsity: 84.4727%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 38485.0\n",
      "lif layer 1 self.abs_max_v: 38485.0\n",
      "train - Value 0: 1925 occurrences\n",
      "train - Value 1: 2105 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 128 occurrences\n",
      "test - Value 1: 324 occurrences\n",
      "epoch-13  lr=['4.0000000'], tr/val_loss: 14.497585/ 12.185347, val:  71.68%, val_best:  84.07%, tr:  90.07%, tr_best:  91.79%, epoch time: 256.16 seconds, 4.27 minutes\n",
      "layer   1  Sparsity: 84.4103%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9537%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.9284%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 451360 real_backward_count 129394  28.668%\n",
      "layer   1  Sparsity: 94.7998%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 99.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 73.0\n",
      "fc layer 2 self.abs_max_out: 520.0\n",
      "fc layer 1 self.abs_max_out: 38768.0\n",
      "lif layer 1 self.abs_max_v: 38768.0\n",
      "train - Value 0: 1900 occurrences\n",
      "train - Value 1: 2130 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 3 self.abs_max_out: 75.0\n",
      "max_activation_accul updated: 348.00 at epoch 14, iter 4029\n",
      "max_activation_accul updated: 382.00 at epoch 14, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-14  lr=['4.0000000'], tr/val_loss: 14.579591/ 16.629618, val:  50.00%, val_best:  84.07%, tr:  90.35%, tr_best:  91.79%, epoch time: 256.25 seconds, 4.27 minutes\n",
      "layer   1  Sparsity: 84.4080%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9549%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.9347%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 483600 real_backward_count 138739  28.689%\n",
      "layer   1  Sparsity: 87.0605%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 76.0\n",
      "fc layer 1 self.abs_max_out: 38821.0\n",
      "lif layer 1 self.abs_max_v: 38821.0\n",
      "fc layer 2 self.abs_max_out: 531.0\n",
      "fc layer 2 self.abs_max_out: 545.0\n",
      "train - Value 0: 1894 occurrences\n",
      "train - Value 1: 2136 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-15  lr=['4.0000000'], tr/val_loss: 14.985400/ 15.097343, val:  50.00%, val_best:  84.07%, tr:  90.30%, tr_best:  91.79%, epoch time: 257.12 seconds, 4.29 minutes\n",
      "layer   1  Sparsity: 84.4097%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9596%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.9487%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 515840 real_backward_count 148110  28.712%\n",
      "layer   1  Sparsity: 85.1074%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 548.0\n",
      "fc layer 1 self.abs_max_out: 39003.0\n",
      "lif layer 1 self.abs_max_v: 39003.0\n",
      "train - Value 0: 1867 occurrences\n",
      "train - Value 1: 2163 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 266 occurrences\n",
      "test - Value 1: 186 occurrences\n",
      "epoch-16  lr=['4.0000000'], tr/val_loss: 14.997128/ 12.208221, val:  83.63%, val_best:  84.07%, tr:  90.12%, tr_best:  91.79%, epoch time: 255.78 seconds, 4.26 minutes\n",
      "layer   1  Sparsity: 84.4102%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9546%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.9389%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548080 real_backward_count 157404  28.719%\n",
      "layer   1  Sparsity: 86.4014%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 39231.0\n",
      "lif layer 1 self.abs_max_v: 39231.0\n",
      "fc layer 2 self.abs_max_out: 550.0\n",
      "train - Value 0: 1848 occurrences\n",
      "train - Value 1: 2182 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 339 occurrences\n",
      "test - Value 1: 113 occurrences\n",
      "epoch-17  lr=['4.0000000'], tr/val_loss: 15.019391/ 11.767618, val:  73.67%, val_best:  84.07%, tr:  88.71%, tr_best:  91.79%, epoch time: 254.21 seconds, 4.24 minutes\n",
      "layer   1  Sparsity: 84.4099%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9542%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.9360%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 580320 real_backward_count 166766  28.737%\n",
      "layer   1  Sparsity: 79.7119%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 39331.0\n",
      "lif layer 1 self.abs_max_v: 39331.0\n",
      "fc layer 2 self.abs_max_out: 551.0\n",
      "fc layer 2 self.abs_max_out: 572.0\n",
      "lif layer 2 self.abs_max_v: 572.0\n",
      "train - Value 0: 1854 occurrences\n",
      "train - Value 1: 2176 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-18  lr=['4.0000000'], tr/val_loss: 15.015070/ 16.535395, val:  50.00%, val_best:  84.07%, tr:  89.55%, tr_best:  91.79%, epoch time: 256.64 seconds, 4.28 minutes\n",
      "layer   1  Sparsity: 84.4114%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9609%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.9554%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 612560 real_backward_count 175999  28.732%\n",
      "layer   1  Sparsity: 88.1348%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 578.5\n",
      "lif layer 2 self.abs_max_v: 595.0\n",
      "fc layer 3 self.abs_max_out: 77.0\n",
      "fc layer 2 self.abs_max_out: 575.0\n",
      "fc layer 2 self.abs_max_out: 606.0\n",
      "lif layer 2 self.abs_max_v: 606.0\n",
      "fc layer 2 self.abs_max_out: 612.0\n",
      "lif layer 2 self.abs_max_v: 612.0\n",
      "fc layer 1 self.abs_max_out: 39617.0\n",
      "lif layer 1 self.abs_max_v: 39617.0\n",
      "train - Value 0: 1894 occurrences\n",
      "train - Value 1: 2136 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-19  lr=['4.0000000'], tr/val_loss: 14.791232/ 14.856621, val:  50.00%, val_best:  84.07%, tr:  90.65%, tr_best:  91.79%, epoch time: 256.49 seconds, 4.27 minutes\n",
      "layer   1  Sparsity: 84.4095%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9672%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.9591%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 644800 real_backward_count 185239  28.728%\n",
      "layer   1  Sparsity: 79.7363%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 39741.0\n",
      "lif layer 1 self.abs_max_v: 39741.0\n",
      "train - Value 0: 1838 occurrences\n",
      "train - Value 1: 2192 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 295 occurrences\n",
      "test - Value 1: 157 occurrences\n",
      "epoch-20  lr=['4.0000000'], tr/val_loss: 14.716178/ 11.573215, val:  81.64%, val_best:  84.07%, tr:  90.50%, tr_best:  91.79%, epoch time: 253.43 seconds, 4.22 minutes\n",
      "layer   1  Sparsity: 84.4114%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9655%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.9312%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 677040 real_backward_count 194351  28.706%\n",
      "layer   1  Sparsity: 88.6963%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 39764.0\n",
      "lif layer 1 self.abs_max_v: 39764.0\n",
      "train - Value 0: 1893 occurrences\n",
      "train - Value 1: 2137 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-21  lr=['4.0000000'], tr/val_loss: 14.441512/ 15.976814, val:  50.00%, val_best:  84.07%, tr:  90.97%, tr_best:  91.79%, epoch time: 255.11 seconds, 4.25 minutes\n",
      "layer   1  Sparsity: 84.4094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9452%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.9011%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 709280 real_backward_count 203692  28.718%\n",
      "layer   1  Sparsity: 84.8389%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 39831.0\n",
      "lif layer 1 self.abs_max_v: 39831.0\n",
      "train - Value 0: 1909 occurrences\n",
      "train - Value 1: 2121 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 5 occurrences\n",
      "test - Value 1: 447 occurrences\n",
      "epoch-22  lr=['4.0000000'], tr/val_loss: 13.866303/ 12.690962, val:  51.11%, val_best:  84.07%, tr:  89.73%, tr_best:  91.79%, epoch time: 256.20 seconds, 4.27 minutes\n",
      "layer   1  Sparsity: 84.4102%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9359%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.8966%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 741520 real_backward_count 213066  28.734%\n",
      "layer   1  Sparsity: 86.8896%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 39985.0\n",
      "lif layer 1 self.abs_max_v: 39985.0\n",
      "train - Value 0: 1866 occurrences\n",
      "train - Value 1: 2164 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 330 occurrences\n",
      "test - Value 1: 122 occurrences\n",
      "epoch-23  lr=['4.0000000'], tr/val_loss: 13.908290/ 11.061737, val:  74.78%, val_best:  84.07%, tr:  90.99%, tr_best:  91.79%, epoch time: 254.87 seconds, 4.25 minutes\n",
      "layer   1  Sparsity: 84.4098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9502%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.8692%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773760 real_backward_count 222313  28.732%\n",
      "layer   1  Sparsity: 78.1250%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40008.0\n",
      "lif layer 1 self.abs_max_v: 40008.0\n",
      "lif layer 2 self.abs_max_v: 617.0\n",
      "train - Value 0: 1809 occurrences\n",
      "train - Value 1: 2221 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 261 occurrences\n",
      "test - Value 1: 191 occurrences\n",
      "epoch-24  lr=['4.0000000'], tr/val_loss: 14.141632/ 11.565924, val:  83.41%, val_best:  84.07%, tr:  90.12%, tr_best:  91.79%, epoch time: 254.80 seconds, 4.25 minutes\n",
      "layer   1  Sparsity: 84.4117%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9629%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.9149%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 806000 real_backward_count 231506  28.723%\n",
      "layer   1  Sparsity: 87.6221%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40070.0\n",
      "lif layer 1 self.abs_max_v: 40070.0\n",
      "train - Value 0: 1859 occurrences\n",
      "train - Value 1: 2171 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 326 occurrences\n",
      "test - Value 1: 126 occurrences\n",
      "epoch-25  lr=['4.0000000'], tr/val_loss: 14.366748/ 11.797606, val:  75.22%, val_best:  84.07%, tr:  90.72%, tr_best:  91.79%, epoch time: 257.13 seconds, 4.29 minutes\n",
      "layer   1  Sparsity: 84.4096%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9526%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.8834%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 838240 real_backward_count 240673  28.712%\n",
      "layer   1  Sparsity: 84.4727%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40121.0\n",
      "lif layer 1 self.abs_max_v: 40121.0\n",
      "train - Value 0: 1823 occurrences\n",
      "train - Value 1: 2207 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 384.00 at epoch 26, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-26  lr=['4.0000000'], tr/val_loss: 14.845458/ 15.676645, val:  50.00%, val_best:  84.07%, tr:  90.97%, tr_best:  91.79%, epoch time: 253.84 seconds, 4.23 minutes\n",
      "layer   1  Sparsity: 84.4103%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9233%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.7801%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 870480 real_backward_count 249970  28.716%\n",
      "layer   1  Sparsity: 83.1055%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 621.0\n",
      "fc layer 1 self.abs_max_out: 40182.0\n",
      "lif layer 1 self.abs_max_v: 40182.0\n",
      "train - Value 0: 1818 occurrences\n",
      "train - Value 1: 2212 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 112 occurrences\n",
      "test - Value 1: 340 occurrences\n",
      "epoch-27  lr=['4.0000000'], tr/val_loss: 14.755937/ 13.529222, val:  69.47%, val_best:  84.07%, tr:  91.89%, tr_best:  91.89%, epoch time: 256.53 seconds, 4.28 minutes\n",
      "layer   1  Sparsity: 84.4106%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9160%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.7497%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 902720 real_backward_count 259253  28.719%\n",
      "layer   1  Sparsity: 79.2480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 628.5\n",
      "train - Value 0: 1854 occurrences\n",
      "train - Value 1: 2176 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 429 occurrences\n",
      "test - Value 1: 23 occurrences\n",
      "epoch-28  lr=['4.0000000'], tr/val_loss: 15.473839/ 13.490479, val:  55.09%, val_best:  84.07%, tr:  90.99%, tr_best:  91.89%, epoch time: 256.34 seconds, 4.27 minutes\n",
      "layer   1  Sparsity: 84.4115%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.8909%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.6840%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 934960 real_backward_count 268748  28.744%\n",
      "layer   1  Sparsity: 88.5254%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 78.0\n",
      "train - Value 0: 1862 occurrences\n",
      "train - Value 1: 2168 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 252 occurrences\n",
      "test - Value 1: 200 occurrences\n",
      "epoch-29  lr=['4.0000000'], tr/val_loss: 16.210312/ 12.992678, val:  82.74%, val_best:  84.07%, tr:  90.94%, tr_best:  91.89%, epoch time: 254.93 seconds, 4.25 minutes\n",
      "layer   1  Sparsity: 84.4094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9008%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.7106%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 967200 real_backward_count 278122  28.755%\n",
      "layer   1  Sparsity: 85.9863%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40188.0\n",
      "lif layer 1 self.abs_max_v: 40188.0\n",
      "train - Value 0: 1889 occurrences\n",
      "train - Value 1: 2141 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 231 occurrences\n",
      "test - Value 1: 221 occurrences\n",
      "epoch-30  lr=['4.0000000'], tr/val_loss: 15.491397/ 12.386612, val:  81.64%, val_best:  84.07%, tr:  90.97%, tr_best:  91.89%, epoch time: 253.92 seconds, 4.23 minutes\n",
      "layer   1  Sparsity: 84.4100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9015%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.7238%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 999440 real_backward_count 287537  28.770%\n",
      "layer   1  Sparsity: 82.9834%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40220.0\n",
      "lif layer 1 self.abs_max_v: 40220.0\n",
      "fc layer 2 self.abs_max_out: 632.0\n",
      "lif layer 2 self.abs_max_v: 632.0\n",
      "lif layer 2 self.abs_max_v: 635.5\n",
      "train - Value 0: 1861 occurrences\n",
      "train - Value 1: 2169 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 336 occurrences\n",
      "test - Value 1: 116 occurrences\n",
      "epoch-31  lr=['4.0000000'], tr/val_loss: 13.848526/  9.683800, val:  75.22%, val_best:  84.07%, tr:  91.17%, tr_best:  91.89%, epoch time: 255.68 seconds, 4.26 minutes\n",
      "layer   1  Sparsity: 84.4107%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.8839%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.6688%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1031680 real_backward_count 296836  28.772%\n",
      "layer   1  Sparsity: 78.1006%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 650.5\n",
      "fc layer 2 self.abs_max_out: 635.0\n",
      "fc layer 3 self.abs_max_out: 82.0\n",
      "fc layer 3 self.abs_max_out: 84.0\n",
      "fc layer 1 self.abs_max_out: 40366.0\n",
      "lif layer 1 self.abs_max_v: 40366.0\n",
      "fc layer 3 self.abs_max_out: 90.0\n",
      "fc layer 2 self.abs_max_out: 637.0\n",
      "fc layer 2 self.abs_max_out: 638.0\n",
      "fc layer 3 self.abs_max_out: 91.0\n",
      "fc layer 3 self.abs_max_out: 95.0\n",
      "fc layer 2 self.abs_max_out: 645.0\n",
      "fc layer 2 self.abs_max_out: 646.0\n",
      "train - Value 0: 1906 occurrences\n",
      "train - Value 1: 2124 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 3 self.abs_max_out: 101.0\n",
      "max_activation_accul updated: 497.00 at epoch 32, iter 4029\n",
      "fc layer 3 self.abs_max_out: 103.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-32  lr=['4.0000000'], tr/val_loss: 19.918587/ 20.001402, val:  50.00%, val_best:  84.07%, tr:  87.72%, tr_best:  91.89%, epoch time: 257.05 seconds, 4.28 minutes\n",
      "layer   1  Sparsity: 84.4117%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.8259%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.1038%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1063920 real_backward_count 306684  28.826%\n",
      "layer   1  Sparsity: 82.4707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 649.0\n",
      "fc layer 1 self.abs_max_out: 40377.0\n",
      "lif layer 1 self.abs_max_v: 40377.0\n",
      "train - Value 0: 1896 occurrences\n",
      "train - Value 1: 2134 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 545.00 at epoch 33, iter 4029\n",
      "max_activation_accul updated: 552.00 at epoch 33, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 367 occurrences\n",
      "test - Value 1: 85 occurrences\n",
      "epoch-33  lr=['4.0000000'], tr/val_loss: 18.242758/ 15.852789, val:  68.81%, val_best:  84.07%, tr:  90.20%, tr_best:  91.89%, epoch time: 256.02 seconds, 4.27 minutes\n",
      "layer   1  Sparsity: 84.4108%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9295%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.7480%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096160 real_backward_count 316227  28.849%\n",
      "layer   1  Sparsity: 80.4199%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 95.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1897 occurrences\n",
      "train - Value 1: 2133 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 336 occurrences\n",
      "test - Value 1: 116 occurrences\n",
      "epoch-34  lr=['4.0000000'], tr/val_loss: 20.059505/ 15.518707, val:  75.22%, val_best:  84.07%, tr:  91.76%, tr_best:  91.89%, epoch time: 257.04 seconds, 4.28 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9439%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.7996%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1128400 real_backward_count 325600  28.855%\n",
      "layer   1  Sparsity: 80.4443%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 95.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40378.0\n",
      "lif layer 1 self.abs_max_v: 40378.0\n",
      "train - Value 0: 1896 occurrences\n",
      "train - Value 1: 2134 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 556.00 at epoch 35, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 334 occurrences\n",
      "test - Value 1: 118 occurrences\n",
      "epoch-35  lr=['4.0000000'], tr/val_loss: 20.099934/ 15.862092, val:  75.66%, val_best:  84.07%, tr:  91.44%, tr_best:  91.89%, epoch time: 253.13 seconds, 4.22 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9339%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.7445%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1160640 real_backward_count 335028  28.866%\n",
      "layer   1  Sparsity: 92.4805%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40444.0\n",
      "lif layer 1 self.abs_max_v: 40444.0\n",
      "train - Value 0: 1887 occurrences\n",
      "train - Value 1: 2143 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 563.00 at epoch 36, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 315 occurrences\n",
      "test - Value 1: 137 occurrences\n",
      "epoch-36  lr=['4.0000000'], tr/val_loss: 20.968424/ 16.945534, val:  78.10%, val_best:  84.07%, tr:  91.36%, tr_best:  91.89%, epoch time: 236.88 seconds, 3.95 minutes\n",
      "layer   1  Sparsity: 84.4085%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9220%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.6725%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1192880 real_backward_count 344616  28.889%\n",
      "layer   1  Sparsity: 81.3721%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40465.0\n",
      "lif layer 1 self.abs_max_v: 40465.0\n",
      "train - Value 0: 1959 occurrences\n",
      "train - Value 1: 2071 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 566.00 at epoch 37, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-37  lr=['4.0000000'], tr/val_loss: 18.695246/ 21.664301, val:  50.00%, val_best:  84.07%, tr:  90.67%, tr_best:  91.89%, epoch time: 236.36 seconds, 3.94 minutes\n",
      "layer   1  Sparsity: 84.4110%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9165%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.6734%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1225120 real_backward_count 354025  28.897%\n",
      "layer   1  Sparsity: 80.6641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40478.0\n",
      "lif layer 1 self.abs_max_v: 40478.0\n",
      "train - Value 0: 1911 occurrences\n",
      "train - Value 1: 2119 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 424 occurrences\n",
      "test - Value 1: 28 occurrences\n",
      "epoch-38  lr=['4.0000000'], tr/val_loss: 20.574854/ 16.210503, val:  56.19%, val_best:  84.07%, tr:  91.91%, tr_best:  91.91%, epoch time: 234.08 seconds, 3.90 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9211%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.7383%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1257360 real_backward_count 363336  28.897%\n",
      "layer   1  Sparsity: 91.2842%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1956 occurrences\n",
      "train - Value 1: 2074 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 163 occurrences\n",
      "test - Value 1: 289 occurrences\n",
      "epoch-39  lr=['4.0000000'], tr/val_loss: 19.232201/ 18.504839, val:  78.10%, val_best:  84.07%, tr:  91.84%, tr_best:  91.91%, epoch time: 232.87 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.8968%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.6344%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1289600 real_backward_count 372725  28.902%\n",
      "layer   1  Sparsity: 77.5146%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 95.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40501.0\n",
      "lif layer 1 self.abs_max_v: 40501.0\n",
      "fc layer 2 self.abs_max_out: 651.0\n",
      "lif layer 2 self.abs_max_v: 651.0\n",
      "train - Value 0: 1918 occurrences\n",
      "train - Value 1: 2112 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 582.00 at epoch 40, iter 4029\n",
      "max_activation_accul updated: 583.00 at epoch 40, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 2 occurrences\n",
      "test - Value 1: 450 occurrences\n",
      "epoch-40  lr=['4.0000000'], tr/val_loss: 21.511496/ 20.579264, val:  50.44%, val_best:  84.07%, tr:  91.84%, tr_best:  91.91%, epoch time: 234.59 seconds, 3.91 minutes\n",
      "layer   1  Sparsity: 84.4119%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.8841%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.5740%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321840 real_backward_count 382103  28.907%\n",
      "layer   1  Sparsity: 83.3984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40526.0\n",
      "lif layer 1 self.abs_max_v: 40526.0\n",
      "fc layer 2 self.abs_max_out: 718.0\n",
      "lif layer 2 self.abs_max_v: 718.0\n",
      "fc layer 2 self.abs_max_out: 728.0\n",
      "lif layer 2 self.abs_max_v: 728.0\n",
      "fc layer 2 self.abs_max_out: 745.0\n",
      "lif layer 2 self.abs_max_v: 745.0\n",
      "fc layer 2 self.abs_max_out: 751.0\n",
      "lif layer 2 self.abs_max_v: 751.0\n",
      "fc layer 2 self.abs_max_out: 762.0\n",
      "lif layer 2 self.abs_max_v: 762.0\n",
      "fc layer 2 self.abs_max_out: 770.0\n",
      "lif layer 2 self.abs_max_v: 770.0\n",
      "train - Value 0: 1932 occurrences\n",
      "train - Value 1: 2098 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 2 self.abs_max_out: 774.0\n",
      "lif layer 2 self.abs_max_v: 774.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-41  lr=['4.0000000'], tr/val_loss: 21.630148/ 20.578245, val:  50.00%, val_best:  84.07%, tr:  92.58%, tr_best:  92.58%, epoch time: 237.16 seconds, 3.95 minutes\n",
      "layer   1  Sparsity: 84.4106%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.8713%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.4303%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1354080 real_backward_count 391532  28.915%\n",
      "layer   1  Sparsity: 86.5967%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 783.0\n",
      "lif layer 2 self.abs_max_v: 783.0\n",
      "fc layer 1 self.abs_max_out: 40538.0\n",
      "lif layer 1 self.abs_max_v: 40538.0\n",
      "train - Value 0: 1890 occurrences\n",
      "train - Value 1: 2140 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 277 occurrences\n",
      "test - Value 1: 175 occurrences\n",
      "epoch-42  lr=['4.0000000'], tr/val_loss: 22.169462/ 16.234426, val:  83.85%, val_best:  84.07%, tr:  93.03%, tr_best:  93.03%, epoch time: 236.42 seconds, 3.94 minutes\n",
      "layer   1  Sparsity: 84.4099%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.8916%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.2977%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1386320 real_backward_count 400856  28.915%\n",
      "layer   1  Sparsity: 82.2510%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40557.0\n",
      "lif layer 1 self.abs_max_v: 40557.0\n",
      "fc layer 2 self.abs_max_out: 785.0\n",
      "lif layer 2 self.abs_max_v: 785.0\n",
      "fc layer 2 self.abs_max_out: 792.0\n",
      "lif layer 2 self.abs_max_v: 792.0\n",
      "fc layer 3 self.abs_max_out: 104.0\n",
      "train - Value 0: 1859 occurrences\n",
      "train - Value 1: 2171 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 319 occurrences\n",
      "test - Value 1: 133 occurrences\n",
      "epoch-43  lr=['4.0000000'], tr/val_loss: 20.873108/ 17.439562, val:  78.10%, val_best:  84.07%, tr:  92.46%, tr_best:  93.03%, epoch time: 234.87 seconds, 3.91 minutes\n",
      "layer   1  Sparsity: 84.4108%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.8984%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.2872%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1418560 real_backward_count 410198  28.917%\n",
      "layer   1  Sparsity: 91.2598%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 112.0\n",
      "fc layer 1 self.abs_max_out: 40584.0\n",
      "lif layer 1 self.abs_max_v: 40584.0\n",
      "fc layer 3 self.abs_max_out: 116.0\n",
      "train - Value 0: 1873 occurrences\n",
      "train - Value 1: 2157 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 597.00 at epoch 44, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 121 occurrences\n",
      "test - Value 1: 331 occurrences\n",
      "epoch-44  lr=['4.0000000'], tr/val_loss: 22.899343/ 18.731518, val:  72.35%, val_best:  84.07%, tr:  92.70%, tr_best:  93.03%, epoch time: 235.41 seconds, 3.92 minutes\n",
      "layer   1  Sparsity: 84.4088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.8968%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.2346%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1450800 real_backward_count 419495  28.915%\n",
      "layer   1  Sparsity: 92.9443%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40614.0\n",
      "lif layer 1 self.abs_max_v: 40614.0\n",
      "fc layer 2 self.abs_max_out: 795.0\n",
      "lif layer 2 self.abs_max_v: 795.0\n",
      "train - Value 0: 1890 occurrences\n",
      "train - Value 1: 2140 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 664.00 at epoch 45, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-45  lr=['4.0000000'], tr/val_loss: 22.449318/ 22.057434, val:  50.00%, val_best:  84.07%, tr:  92.33%, tr_best:  93.03%, epoch time: 234.73 seconds, 3.91 minutes\n",
      "layer   1  Sparsity: 84.4084%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.8800%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.2418%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1483040 real_backward_count 428980  28.926%\n",
      "layer   1  Sparsity: 78.3447%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 94.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40631.0\n",
      "lif layer 1 self.abs_max_v: 40631.0\n",
      "lif layer 2 self.abs_max_v: 807.0\n",
      "fc layer 2 self.abs_max_out: 810.0\n",
      "lif layer 2 self.abs_max_v: 810.0\n",
      "train - Value 0: 1872 occurrences\n",
      "train - Value 1: 2158 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-46  lr=['4.0000000'], tr/val_loss: 21.603497/ 21.640451, val:  50.00%, val_best:  84.07%, tr:  92.38%, tr_best:  93.03%, epoch time: 234.73 seconds, 3.91 minutes\n",
      "layer   1  Sparsity: 84.4117%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9100%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.2569%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1515280 real_backward_count 438284  28.924%\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40650.0\n",
      "lif layer 1 self.abs_max_v: 40650.0\n",
      "train - Value 0: 1919 occurrences\n",
      "train - Value 1: 2111 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 344 occurrences\n",
      "test - Value 1: 108 occurrences\n",
      "epoch-47  lr=['4.0000000'], tr/val_loss: 20.610556/ 16.902826, val:  73.45%, val_best:  84.07%, tr:  92.06%, tr_best:  93.03%, epoch time: 234.90 seconds, 3.91 minutes\n",
      "layer   1  Sparsity: 84.4089%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9153%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.2411%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1547520 real_backward_count 447604  28.924%\n",
      "layer   1  Sparsity: 92.3584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40663.0\n",
      "lif layer 1 self.abs_max_v: 40663.0\n",
      "train - Value 0: 1909 occurrences\n",
      "train - Value 1: 2121 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 3 self.abs_max_out: 121.0\n",
      "max_activation_accul updated: 731.00 at epoch 48, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-48  lr=['4.0000000'], tr/val_loss: 23.073431/ 26.651749, val:  50.00%, val_best:  84.07%, tr:  91.56%, tr_best:  93.03%, epoch time: 234.91 seconds, 3.92 minutes\n",
      "layer   1  Sparsity: 84.4086%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9055%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.2075%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1579760 real_backward_count 457018  28.930%\n",
      "layer   1  Sparsity: 87.5977%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40670.0\n",
      "lif layer 1 self.abs_max_v: 40670.0\n",
      "train - Value 0: 1911 occurrences\n",
      "train - Value 1: 2119 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 75 occurrences\n",
      "test - Value 1: 377 occurrences\n",
      "epoch-49  lr=['4.0000000'], tr/val_loss: 22.097889/ 17.094995, val:  64.82%, val_best:  84.07%, tr:  91.32%, tr_best:  93.03%, epoch time: 233.44 seconds, 3.89 minutes\n",
      "layer   1  Sparsity: 84.4096%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9178%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.2635%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1612000 real_backward_count 466334  28.929%\n",
      "layer   1  Sparsity: 86.7432%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40678.0\n",
      "lif layer 1 self.abs_max_v: 40678.0\n",
      "train - Value 0: 1949 occurrences\n",
      "train - Value 1: 2081 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 364 occurrences\n",
      "test - Value 1: 88 occurrences\n",
      "epoch-50  lr=['4.0000000'], tr/val_loss: 20.660439/ 17.745861, val:  69.47%, val_best:  84.07%, tr:  92.26%, tr_best:  93.03%, epoch time: 231.33 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9315%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.3358%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644240 real_backward_count 475537  28.921%\n",
      "layer   1  Sparsity: 89.1113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1931 occurrences\n",
      "train - Value 1: 2099 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-51  lr=['4.0000000'], tr/val_loss: 20.816402/ 22.122015, val:  50.00%, val_best:  84.07%, tr:  92.26%, tr_best:  93.03%, epoch time: 231.62 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4093%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9295%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.3247%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1676480 real_backward_count 484649  28.909%\n",
      "layer   1  Sparsity: 84.1309%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40692.0\n",
      "lif layer 1 self.abs_max_v: 40692.0\n",
      "train - Value 0: 1996 occurrences\n",
      "train - Value 1: 2034 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 390 occurrences\n",
      "test - Value 1: 62 occurrences\n",
      "epoch-52  lr=['4.0000000'], tr/val_loss: 19.743881/ 15.008661, val:  63.72%, val_best:  84.07%, tr:  92.08%, tr_best:  93.03%, epoch time: 232.24 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9209%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.2892%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1708720 real_backward_count 493897  28.905%\n",
      "layer   1  Sparsity: 87.8906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 97.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40704.0\n",
      "lif layer 1 self.abs_max_v: 40704.0\n",
      "train - Value 0: 1957 occurrences\n",
      "train - Value 1: 2073 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 181 occurrences\n",
      "test - Value 1: 271 occurrences\n",
      "epoch-53  lr=['4.0000000'], tr/val_loss: 18.690912/ 14.430540, val:  80.75%, val_best:  84.07%, tr:  91.32%, tr_best:  93.03%, epoch time: 233.38 seconds, 3.89 minutes\n",
      "layer   1  Sparsity: 84.4096%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9173%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.3601%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1740960 real_backward_count 503282  28.908%\n",
      "layer   1  Sparsity: 80.3711%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40716.0\n",
      "lif layer 1 self.abs_max_v: 40716.0\n",
      "train - Value 0: 1912 occurrences\n",
      "train - Value 1: 2118 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 87 occurrences\n",
      "test - Value 1: 365 occurrences\n",
      "epoch-54  lr=['4.0000000'], tr/val_loss: 18.075716/ 14.829712, val:  66.59%, val_best:  84.07%, tr:  91.24%, tr_best:  93.03%, epoch time: 232.02 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9207%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.3149%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1773200 real_backward_count 512581  28.907%\n",
      "layer   1  Sparsity: 83.0322%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 122.0\n",
      "fc layer 3 self.abs_max_out: 125.0\n",
      "fc layer 1 self.abs_max_out: 40729.0\n",
      "lif layer 1 self.abs_max_v: 40729.0\n",
      "train - Value 0: 1911 occurrences\n",
      "train - Value 1: 2119 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-55  lr=['4.0000000'], tr/val_loss: 13.954600/ 12.105490, val:  50.00%, val_best:  84.07%, tr:  92.51%, tr_best:  93.03%, epoch time: 233.22 seconds, 3.89 minutes\n",
      "layer   1  Sparsity: 84.4106%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.8606%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.3960%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1805440 real_backward_count 521995  28.912%\n",
      "layer   1  Sparsity: 80.0049%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40732.0\n",
      "lif layer 1 self.abs_max_v: 40732.0\n",
      "train - Value 0: 1849 occurrences\n",
      "train - Value 1: 2181 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 2 occurrences\n",
      "test - Value 1: 450 occurrences\n",
      "epoch-56  lr=['4.0000000'], tr/val_loss: 15.945325/ 25.917381, val:  50.44%, val_best:  84.07%, tr:  90.82%, tr_best:  93.03%, epoch time: 232.85 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.7458%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 95.8738%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1837680 real_backward_count 531511  28.923%\n",
      "layer   1  Sparsity: 82.4707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 95.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 129.0\n",
      "fc layer 3 self.abs_max_out: 137.0\n",
      "fc layer 1 self.abs_max_out: 40749.0\n",
      "lif layer 1 self.abs_max_v: 40749.0\n",
      "fc layer 3 self.abs_max_out: 138.0\n",
      "fc layer 3 self.abs_max_out: 141.0\n",
      "fc layer 3 self.abs_max_out: 147.0\n",
      "fc layer 3 self.abs_max_out: 148.0\n",
      "fc layer 3 self.abs_max_out: 153.0\n",
      "train - Value 0: 1909 occurrences\n",
      "train - Value 1: 2121 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 827.00 at epoch 57, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 254 occurrences\n",
      "test - Value 1: 198 occurrences\n",
      "epoch-57  lr=['4.0000000'], tr/val_loss: 24.388535/ 25.074123, val:  84.51%, val_best:  84.51%, tr:  91.56%, tr_best:  93.03%, epoch time: 233.88 seconds, 3.90 minutes\n",
      "layer   1  Sparsity: 84.4108%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.6515%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 94.3374%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869920 real_backward_count 541001  28.932%\n",
      "layer   1  Sparsity: 83.9355%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 154.0\n",
      "fc layer 3 self.abs_max_out: 161.0\n",
      "fc layer 3 self.abs_max_out: 167.0\n",
      "fc layer 1 self.abs_max_out: 40757.0\n",
      "lif layer 1 self.abs_max_v: 40757.0\n",
      "fc layer 3 self.abs_max_out: 178.0\n",
      "train - Value 0: 1979 occurrences\n",
      "train - Value 1: 2051 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 215 occurrences\n",
      "test - Value 1: 237 occurrences\n",
      "epoch-58  lr=['4.0000000'], tr/val_loss: 30.258421/ 24.196739, val:  81.64%, val_best:  84.51%, tr:  92.01%, tr_best:  93.03%, epoch time: 232.31 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.5977%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 93.6738%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1902160 real_backward_count 550314  28.931%\n",
      "layer   1  Sparsity: 78.9795%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 190.0\n",
      "fc layer 3 self.abs_max_out: 198.0\n",
      "fc layer 3 self.abs_max_out: 202.0\n",
      "train - Value 0: 1939 occurrences\n",
      "train - Value 1: 2091 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 835.00 at epoch 59, iter 4029\n",
      "max_activation_accul updated: 838.00 at epoch 59, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 416 occurrences\n",
      "test - Value 1: 36 occurrences\n",
      "epoch-59  lr=['4.0000000'], tr/val_loss: 54.613876/ 44.225426, val:  57.96%, val_best:  84.51%, tr:  91.02%, tr_best:  93.03%, epoch time: 232.96 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4116%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.5293%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.8836%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1934400 real_backward_count 559954  28.947%\n",
      "layer   1  Sparsity: 79.5410%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 215.0\n",
      "fc layer 1 self.abs_max_out: 40776.0\n",
      "lif layer 1 self.abs_max_v: 40776.0\n",
      "train - Value 0: 1860 occurrences\n",
      "train - Value 1: 2170 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 879.00 at epoch 60, iter 4029\n",
      "max_activation_accul updated: 881.00 at epoch 60, iter 4029\n",
      "max_activation_accul updated: 905.00 at epoch 60, iter 4029\n",
      "max_activation_accul updated: 983.00 at epoch 60, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 320 occurrences\n",
      "test - Value 1: 132 occurrences\n",
      "epoch-60  lr=['4.0000000'], tr/val_loss: 44.252655/ 55.828510, val:  77.88%, val_best:  84.51%, tr:  90.74%, tr_best:  93.03%, epoch time: 232.35 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4114%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.5579%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.5287%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1966640 real_backward_count 569536  28.960%\n",
      "layer   1  Sparsity: 77.0752%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40790.0\n",
      "lif layer 1 self.abs_max_v: 40790.0\n",
      "train - Value 0: 1938 occurrences\n",
      "train - Value 1: 2092 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 299 occurrences\n",
      "test - Value 1: 153 occurrences\n",
      "epoch-61  lr=['4.0000000'], tr/val_loss: 61.530422/ 56.748920, val:  82.08%, val_best:  84.51%, tr:  90.89%, tr_best:  93.03%, epoch time: 232.36 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4120%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.5219%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.3215%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1998880 real_backward_count 578915  28.962%\n",
      "layer   1  Sparsity: 80.2246%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40810.0\n",
      "lif layer 1 self.abs_max_v: 40810.0\n",
      "train - Value 0: 1965 occurrences\n",
      "train - Value 1: 2065 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 344 occurrences\n",
      "test - Value 1: 108 occurrences\n",
      "epoch-62  lr=['4.0000000'], tr/val_loss: 64.494102/ 40.575245, val:  73.89%, val_best:  84.51%, tr:  91.12%, tr_best:  93.03%, epoch time: 231.68 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.4900%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.2072%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2031120 real_backward_count 588403  28.969%\n",
      "layer   1  Sparsity: 84.2041%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1937 occurrences\n",
      "train - Value 1: 2093 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 1019.00 at epoch 63, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 343 occurrences\n",
      "test - Value 1: 109 occurrences\n",
      "epoch-63  lr=['4.0000000'], tr/val_loss: 55.636898/ 50.749901, val:  74.12%, val_best:  84.51%, tr:  90.67%, tr_best:  93.03%, epoch time: 231.32 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.3846%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.1664%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2063360 real_backward_count 597883  28.976%\n",
      "layer   1  Sparsity: 79.5166%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40849.0\n",
      "lif layer 1 self.abs_max_v: 40849.0\n",
      "fc layer 3 self.abs_max_out: 221.0\n",
      "train - Value 0: 1897 occurrences\n",
      "train - Value 1: 2133 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 1044.00 at epoch 64, iter 4029\n",
      "max_activation_accul updated: 1049.00 at epoch 64, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 348 occurrences\n",
      "test - Value 1: 104 occurrences\n",
      "epoch-64  lr=['4.0000000'], tr/val_loss: 53.713577/ 35.711166, val:  72.57%, val_best:  84.51%, tr:  92.16%, tr_best:  93.03%, epoch time: 232.56 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4114%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.2314%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.0577%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2095600 real_backward_count 607282  28.979%\n",
      "layer   1  Sparsity: 80.3223%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40855.0\n",
      "lif layer 1 self.abs_max_v: 40855.0\n",
      "fc layer 3 self.abs_max_out: 228.0\n",
      "train - Value 0: 1919 occurrences\n",
      "train - Value 1: 2111 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 159 occurrences\n",
      "test - Value 1: 293 occurrences\n",
      "epoch-65  lr=['4.0000000'], tr/val_loss: 50.706612/ 27.065876, val:  78.98%, val_best:  84.51%, tr:  92.85%, tr_best:  93.03%, epoch time: 232.04 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.2165%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.0296%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2127840 real_backward_count 616616  28.978%\n",
      "layer   1  Sparsity: 79.7119%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 816.0\n",
      "train - Value 0: 1938 occurrences\n",
      "train - Value 1: 2092 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 313 occurrences\n",
      "test - Value 1: 139 occurrences\n",
      "epoch-66  lr=['4.0000000'], tr/val_loss: 38.853123/ 23.621803, val:  78.10%, val_best:  84.51%, tr:  91.89%, tr_best:  93.03%, epoch time: 232.07 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4114%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.2376%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.8288%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2160080 real_backward_count 626155  28.988%\n",
      "layer   1  Sparsity: 93.0908%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 95.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 837.0\n",
      "lif layer 2 self.abs_max_v: 837.0\n",
      "fc layer 1 self.abs_max_out: 40885.0\n",
      "lif layer 1 self.abs_max_v: 40885.0\n",
      "train - Value 0: 1910 occurrences\n",
      "train - Value 1: 2120 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 1185.00 at epoch 67, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 4 occurrences\n",
      "test - Value 1: 448 occurrences\n",
      "epoch-67  lr=['4.0000000'], tr/val_loss: 50.418381/ 67.482475, val:  50.88%, val_best:  84.51%, tr:  91.04%, tr_best:  93.03%, epoch time: 233.67 seconds, 3.89 minutes\n",
      "layer   1  Sparsity: 84.4084%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.2206%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.1840%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2192320 real_backward_count 635596  28.992%\n",
      "layer   1  Sparsity: 77.2217%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40891.0\n",
      "lif layer 1 self.abs_max_v: 40891.0\n",
      "fc layer 3 self.abs_max_out: 235.0\n",
      "train - Value 0: 1935 occurrences\n",
      "train - Value 1: 2095 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 1352.00 at epoch 68, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-68  lr=['4.0000000'], tr/val_loss: 83.301376/ 73.707870, val:  50.00%, val_best:  84.51%, tr:  93.35%, tr_best:  93.35%, epoch time: 233.36 seconds, 3.89 minutes\n",
      "layer   1  Sparsity: 84.4119%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1278%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.1897%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2224560 real_backward_count 644995  28.994%\n",
      "layer   1  Sparsity: 88.2812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 93.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 847.0\n",
      "fc layer 1 self.abs_max_out: 40893.0\n",
      "lif layer 1 self.abs_max_v: 40893.0\n",
      "fc layer 3 self.abs_max_out: 253.0\n",
      "fc layer 2 self.abs_max_out: 843.0\n",
      "fc layer 2 self.abs_max_out: 846.0\n",
      "train - Value 0: 1940 occurrences\n",
      "train - Value 1: 2090 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 419 occurrences\n",
      "test - Value 1: 33 occurrences\n",
      "epoch-69  lr=['4.0000000'], tr/val_loss: 83.184517/ 79.072243, val:  57.30%, val_best:  84.51%, tr:  92.88%, tr_best:  93.35%, epoch time: 233.90 seconds, 3.90 minutes\n",
      "layer   1  Sparsity: 84.4095%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1257%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.1765%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2256800 real_backward_count 654348  28.995%\n",
      "layer   1  Sparsity: 80.9326%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 857.0\n",
      "lif layer 2 self.abs_max_v: 857.0\n",
      "fc layer 1 self.abs_max_out: 40929.0\n",
      "lif layer 1 self.abs_max_v: 40929.0\n",
      "fc layer 2 self.abs_max_out: 873.0\n",
      "lif layer 2 self.abs_max_v: 873.0\n",
      "fc layer 2 self.abs_max_out: 887.0\n",
      "lif layer 2 self.abs_max_v: 887.0\n",
      "fc layer 2 self.abs_max_out: 891.0\n",
      "lif layer 2 self.abs_max_v: 891.0\n",
      "train - Value 0: 1943 occurrences\n",
      "train - Value 1: 2087 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 199 occurrences\n",
      "test - Value 1: 253 occurrences\n",
      "epoch-70  lr=['4.0000000'], tr/val_loss: 82.338913/ 61.969795, val:  79.42%, val_best:  84.51%, tr:  93.45%, tr_best:  93.45%, epoch time: 233.56 seconds, 3.89 minutes\n",
      "layer   1  Sparsity: 84.4111%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1343%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.0832%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2289040 real_backward_count 663570  28.989%\n",
      "layer   1  Sparsity: 88.6719%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 93.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 896.0\n",
      "fc layer 2 self.abs_max_out: 943.0\n",
      "lif layer 2 self.abs_max_v: 943.0\n",
      "fc layer 2 self.abs_max_out: 962.0\n",
      "lif layer 2 self.abs_max_v: 962.0\n",
      "fc layer 1 self.abs_max_out: 40933.0\n",
      "lif layer 1 self.abs_max_v: 40933.0\n",
      "train - Value 0: 1925 occurrences\n",
      "train - Value 1: 2105 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 177 occurrences\n",
      "test - Value 1: 275 occurrences\n",
      "epoch-71  lr=['4.0000000'], tr/val_loss: 73.204140/ 57.820118, val:  79.42%, val_best:  84.51%, tr:  93.80%, tr_best:  93.80%, epoch time: 232.81 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.9897%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2321280 real_backward_count 672796  28.984%\n",
      "layer   1  Sparsity: 80.1514%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 976.0\n",
      "lif layer 2 self.abs_max_v: 976.0\n",
      "train - Value 0: 1945 occurrences\n",
      "train - Value 1: 2085 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 185 occurrences\n",
      "test - Value 1: 267 occurrences\n",
      "epoch-72  lr=['4.0000000'], tr/val_loss: 62.255421/ 59.648602, val:  79.87%, val_best:  84.51%, tr:  93.45%, tr_best:  93.80%, epoch time: 230.44 seconds, 3.84 minutes\n",
      "layer   1  Sparsity: 84.4113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0937%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.0445%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2353520 real_backward_count 682155  28.984%\n",
      "layer   1  Sparsity: 86.6211%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40952.0\n",
      "lif layer 1 self.abs_max_v: 40952.0\n",
      "train - Value 0: 1946 occurrences\n",
      "train - Value 1: 2084 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 345 occurrences\n",
      "test - Value 1: 107 occurrences\n",
      "epoch-73  lr=['4.0000000'], tr/val_loss: 83.809044/ 52.026562, val:  73.23%, val_best:  84.51%, tr:  93.33%, tr_best:  93.80%, epoch time: 231.92 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1014%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.7839%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2385760 real_backward_count 691383  28.980%\n",
      "layer   1  Sparsity: 86.0840%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40956.0\n",
      "lif layer 1 self.abs_max_v: 40956.0\n",
      "fc layer 3 self.abs_max_out: 262.0\n",
      "train - Value 0: 1934 occurrences\n",
      "train - Value 1: 2096 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 343 occurrences\n",
      "test - Value 1: 109 occurrences\n",
      "epoch-74  lr=['4.0000000'], tr/val_loss: 84.582352/ 70.526604, val:  73.67%, val_best:  84.51%, tr:  92.93%, tr_best:  93.80%, epoch time: 231.17 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0779%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.7088%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2418000 real_backward_count 700722  28.979%\n",
      "layer   1  Sparsity: 89.3799%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 95.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40965.0\n",
      "lif layer 1 self.abs_max_v: 40965.0\n",
      "train - Value 0: 1906 occurrences\n",
      "train - Value 1: 2124 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 1374.00 at epoch 75, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-75  lr=['4.0000000'], tr/val_loss: 69.870193/ 60.858341, val:  50.00%, val_best:  84.51%, tr:  93.13%, tr_best:  93.80%, epoch time: 230.96 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4092%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0855%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.0798%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2450240 real_backward_count 709911  28.973%\n",
      "layer   1  Sparsity: 91.6748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 95.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40969.0\n",
      "lif layer 1 self.abs_max_v: 40969.0\n",
      "train - Value 0: 1920 occurrences\n",
      "train - Value 1: 2110 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 397 occurrences\n",
      "test - Value 1: 55 occurrences\n",
      "epoch-76  lr=['4.0000000'], tr/val_loss: 75.326546/ 62.533768, val:  62.17%, val_best:  84.51%, tr:  92.78%, tr_best:  93.80%, epoch time: 232.45 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4087%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0882%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.7545%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2482480 real_backward_count 719287  28.975%\n",
      "layer   1  Sparsity: 88.7695%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 93.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40973.0\n",
      "lif layer 1 self.abs_max_v: 40973.0\n",
      "train - Value 0: 1935 occurrences\n",
      "train - Value 1: 2095 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 362 occurrences\n",
      "test - Value 1: 90 occurrences\n",
      "epoch-77  lr=['4.0000000'], tr/val_loss: 66.238716/ 41.066795, val:  69.47%, val_best:  84.51%, tr:  92.85%, tr_best:  93.80%, epoch time: 231.11 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0944%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.5934%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2514720 real_backward_count 728697  28.977%\n",
      "layer   1  Sparsity: 90.5029%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 93.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 271.0\n",
      "fc layer 2 self.abs_max_out: 979.0\n",
      "lif layer 2 self.abs_max_v: 979.0\n",
      "fc layer 1 self.abs_max_out: 40979.0\n",
      "lif layer 1 self.abs_max_v: 40979.0\n",
      "train - Value 0: 1906 occurrences\n",
      "train - Value 1: 2124 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 3 self.abs_max_out: 281.0\n",
      "max_activation_accul updated: 1400.00 at epoch 78, iter 4029\n",
      "max_activation_accul updated: 1555.00 at epoch 78, iter 4029\n",
      "max_activation_accul updated: 1935.00 at epoch 78, iter 4029\n",
      "fc layer 3 self.abs_max_out: 286.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-78  lr=['4.0000000'], tr/val_loss: 68.548767/ 98.637032, val:  50.00%, val_best:  84.51%, tr:  92.93%, tr_best:  93.80%, epoch time: 231.86 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4090%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0723%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.3871%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2546960 real_backward_count 738012  28.976%\n",
      "layer   1  Sparsity: 88.2324%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 40981.0\n",
      "lif layer 1 self.abs_max_v: 40981.0\n",
      "train - Value 0: 1958 occurrences\n",
      "train - Value 1: 2072 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 243 occurrences\n",
      "test - Value 1: 209 occurrences\n",
      "epoch-79  lr=['4.0000000'], tr/val_loss: 63.215244/ 43.643215, val:  88.72%, val_best:  88.72%, tr:  91.99%, tr_best:  93.80%, epoch time: 232.23 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4095%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1233%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.9522%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2579200 real_backward_count 747426  28.979%\n",
      "layer   1  Sparsity: 85.0098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1942 occurrences\n",
      "train - Value 1: 2088 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 23 occurrences\n",
      "test - Value 1: 429 occurrences\n",
      "epoch-80  lr=['4.0000000'], tr/val_loss: 62.291286/ 54.355686, val:  54.20%, val_best:  88.72%, tr:  92.73%, tr_best:  93.80%, epoch time: 231.13 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4102%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1462%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.5682%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2611440 real_backward_count 756743  28.978%\n",
      "layer   1  Sparsity: 77.2705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1017.0\n",
      "lif layer 2 self.abs_max_v: 1017.0\n",
      "fc layer 2 self.abs_max_out: 1030.0\n",
      "lif layer 2 self.abs_max_v: 1030.0\n",
      "train - Value 0: 1923 occurrences\n",
      "train - Value 1: 2107 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 91 occurrences\n",
      "test - Value 1: 361 occurrences\n",
      "epoch-81  lr=['4.0000000'], tr/val_loss: 61.351494/ 60.439178, val:  68.81%, val_best:  88.72%, tr:  92.90%, tr_best:  93.80%, epoch time: 232.13 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4119%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1499%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.8685%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2643680 real_backward_count 766086  28.978%\n",
      "layer   1  Sparsity: 87.2803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1032.0\n",
      "lif layer 2 self.abs_max_v: 1032.0\n",
      "train - Value 0: 1960 occurrences\n",
      "train - Value 1: 2070 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 3 occurrences\n",
      "test - Value 1: 449 occurrences\n",
      "epoch-82  lr=['4.0000000'], tr/val_loss: 55.993732/ 58.725849, val:  50.66%, val_best:  88.72%, tr:  94.32%, tr_best:  94.32%, epoch time: 232.72 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4097%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1911%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.8366%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2675920 real_backward_count 775193  28.969%\n",
      "layer   1  Sparsity: 90.7471%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 93.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1034.0\n",
      "lif layer 2 self.abs_max_v: 1034.0\n",
      "train - Value 0: 1917 occurrences\n",
      "train - Value 1: 2113 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 128 occurrences\n",
      "test - Value 1: 324 occurrences\n",
      "epoch-83  lr=['4.0000000'], tr/val_loss: 63.799587/ 50.975830, val:  76.11%, val_best:  88.72%, tr:  94.69%, tr_best:  94.69%, epoch time: 232.54 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4089%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1766%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.7721%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2708160 real_backward_count 784259  28.959%\n",
      "layer   1  Sparsity: 74.1943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1942 occurrences\n",
      "train - Value 1: 2088 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 133 occurrences\n",
      "test - Value 1: 319 occurrences\n",
      "epoch-84  lr=['4.0000000'], tr/val_loss: 60.484550/ 49.759064, val:  77.21%, val_best:  88.72%, tr:  94.37%, tr_best:  94.69%, epoch time: 233.31 seconds, 3.89 minutes\n",
      "layer   1  Sparsity: 84.4126%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.2509%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.2034%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2740400 real_backward_count 793478  28.955%\n",
      "layer   1  Sparsity: 83.3984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1942 occurrences\n",
      "train - Value 1: 2088 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 93 occurrences\n",
      "test - Value 1: 359 occurrences\n",
      "epoch-85  lr=['4.0000000'], tr/val_loss: 61.541504/ 54.322601, val:  67.92%, val_best:  88.72%, tr:  94.76%, tr_best:  94.76%, epoch time: 233.68 seconds, 3.89 minutes\n",
      "layer   1  Sparsity: 84.4106%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0565%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.8447%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2772640 real_backward_count 802565  28.946%\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 94.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1934 occurrences\n",
      "train - Value 1: 2096 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-86  lr=['4.0000000'], tr/val_loss: 58.170673/ 91.266975, val:  50.00%, val_best:  88.72%, tr:  93.92%, tr_best:  94.76%, epoch time: 233.17 seconds, 3.89 minutes\n",
      "layer   1  Sparsity: 84.4089%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1308%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.4320%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2804880 real_backward_count 811717  28.939%\n",
      "layer   1  Sparsity: 91.3330%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 289.0\n",
      "fc layer 3 self.abs_max_out: 292.0\n",
      "fc layer 3 self.abs_max_out: 298.0\n",
      "train - Value 0: 1967 occurrences\n",
      "train - Value 1: 2063 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 69 occurrences\n",
      "test - Value 1: 383 occurrences\n",
      "epoch-87  lr=['4.0000000'], tr/val_loss: 67.123329/ 64.711517, val:  63.94%, val_best:  88.72%, tr:  94.19%, tr_best:  94.76%, epoch time: 232.48 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1665%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.2765%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2837120 real_backward_count 820821  28.931%\n",
      "layer   1  Sparsity: 90.5029%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1939 occurrences\n",
      "train - Value 1: 2091 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 48 occurrences\n",
      "test - Value 1: 404 occurrences\n",
      "epoch-88  lr=['4.0000000'], tr/val_loss: 71.232239/ 58.386799, val:  58.85%, val_best:  88.72%, tr:  94.59%, tr_best:  94.76%, epoch time: 231.65 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4090%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1969%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.1163%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2869360 real_backward_count 829869  28.922%\n",
      "layer   1  Sparsity: 69.9951%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1050.0\n",
      "lif layer 2 self.abs_max_v: 1050.0\n",
      "train - Value 0: 1949 occurrences\n",
      "train - Value 1: 2081 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 13 occurrences\n",
      "test - Value 1: 439 occurrences\n",
      "epoch-89  lr=['4.0000000'], tr/val_loss: 70.117424/ 70.171570, val:  51.99%, val_best:  88.72%, tr:  94.84%, tr_best:  94.84%, epoch time: 232.41 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4136%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1728%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.0887%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2901600 real_backward_count 838952  28.913%\n",
      "layer   1  Sparsity: 91.3330%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1965 occurrences\n",
      "train - Value 1: 2065 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 252 occurrences\n",
      "test - Value 1: 200 occurrences\n",
      "epoch-90  lr=['4.0000000'], tr/val_loss: 73.229935/ 55.197784, val:  84.07%, val_best:  88.72%, tr:  94.74%, tr_best:  94.84%, epoch time: 231.81 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1674%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.8843%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2933840 real_backward_count 848064  28.906%\n",
      "layer   1  Sparsity: 76.6113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1959 occurrences\n",
      "train - Value 1: 2071 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 319 occurrences\n",
      "test - Value 1: 133 occurrences\n",
      "epoch-91  lr=['4.0000000'], tr/val_loss: 68.397705/ 54.900959, val:  78.10%, val_best:  88.72%, tr:  94.99%, tr_best:  94.99%, epoch time: 230.83 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4121%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1707%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.0373%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2966080 real_backward_count 857057  28.895%\n",
      "layer   1  Sparsity: 82.4463%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1920 occurrences\n",
      "train - Value 1: 2110 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 193 occurrences\n",
      "test - Value 1: 259 occurrences\n",
      "epoch-92  lr=['4.0000000'], tr/val_loss: 72.210770/ 51.685921, val:  85.18%, val_best:  88.72%, tr:  93.82%, tr_best:  94.99%, epoch time: 231.83 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4108%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1771%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.1756%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2998320 real_backward_count 866263  28.892%\n",
      "layer   1  Sparsity: 83.9355%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1945 occurrences\n",
      "train - Value 1: 2085 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-93  lr=['4.0000000'], tr/val_loss: 72.974342/ 79.317978, val:  50.00%, val_best:  88.72%, tr:  94.29%, tr_best:  94.99%, epoch time: 233.06 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1890%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.1110%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3030560 real_backward_count 875413  28.886%\n",
      "layer   1  Sparsity: 80.5664%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1073.0\n",
      "lif layer 2 self.abs_max_v: 1073.0\n",
      "train - Value 0: 1934 occurrences\n",
      "train - Value 1: 2096 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 383 occurrences\n",
      "test - Value 1: 69 occurrences\n",
      "epoch-94  lr=['4.0000000'], tr/val_loss: 76.285172/ 52.361935, val:  65.27%, val_best:  88.72%, tr:  94.86%, tr_best:  94.99%, epoch time: 230.69 seconds, 3.84 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1455%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.2138%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3062800 real_backward_count 884397  28.875%\n",
      "layer   1  Sparsity: 74.3896%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1927 occurrences\n",
      "train - Value 1: 2103 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 4 occurrences\n",
      "test - Value 1: 448 occurrences\n",
      "epoch-95  lr=['4.0000000'], tr/val_loss: 73.195396/ 74.711891, val:  50.88%, val_best:  88.72%, tr:  95.29%, tr_best:  95.29%, epoch time: 232.36 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4126%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0883%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.8951%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3095040 real_backward_count 893332  28.863%\n",
      "layer   1  Sparsity: 87.2803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1898 occurrences\n",
      "train - Value 1: 2132 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 128 occurrences\n",
      "test - Value 1: 324 occurrences\n",
      "epoch-96  lr=['4.0000000'], tr/val_loss: 76.498810/ 70.937859, val:  77.43%, val_best:  88.72%, tr:  94.22%, tr_best:  95.29%, epoch time: 232.44 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4097%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1091%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.9092%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3127280 real_backward_count 902419  28.856%\n",
      "layer   1  Sparsity: 80.3955%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 303.0\n",
      "train - Value 0: 1886 occurrences\n",
      "train - Value 1: 2144 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 259 occurrences\n",
      "test - Value 1: 193 occurrences\n",
      "epoch-97  lr=['4.0000000'], tr/val_loss: 90.652664/ 90.610275, val:  85.62%, val_best:  88.72%, tr:  94.27%, tr_best:  95.29%, epoch time: 233.91 seconds, 3.90 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1194%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.9082%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3159520 real_backward_count 911481  28.849%\n",
      "layer   1  Sparsity: 85.0098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 308.0\n",
      "train - Value 0: 1920 occurrences\n",
      "train - Value 1: 2110 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 4 occurrences\n",
      "test - Value 1: 448 occurrences\n",
      "epoch-98  lr=['4.0000000'], tr/val_loss:110.133247/ 95.360077, val:  50.88%, val_best:  88.72%, tr:  94.47%, tr_best:  95.29%, epoch time: 230.89 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4102%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0142%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.7903%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3191760 real_backward_count 920473  28.839%\n",
      "layer   1  Sparsity: 79.5898%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1125.0\n",
      "lif layer 2 self.abs_max_v: 1125.0\n",
      "train - Value 0: 1942 occurrences\n",
      "train - Value 1: 2088 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 9 occurrences\n",
      "test - Value 1: 443 occurrences\n",
      "epoch-99  lr=['4.0000000'], tr/val_loss: 96.275131/ 79.340530, val:  51.99%, val_best:  88.72%, tr:  94.37%, tr_best:  95.29%, epoch time: 232.07 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4114%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.9392%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.7287%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3224000 real_backward_count 929749  28.838%\n",
      "layer   1  Sparsity: 91.5527%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 93.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 317.0\n",
      "fc layer 3 self.abs_max_out: 318.0\n",
      "train - Value 0: 1901 occurrences\n",
      "train - Value 1: 2129 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-100 lr=['4.0000000'], tr/val_loss:103.165344/ 93.610031, val:  50.00%, val_best:  88.72%, tr:  94.00%, tr_best:  95.29%, epoch time: 231.51 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4087%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.9095%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.6126%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3256240 real_backward_count 938882  28.833%\n",
      "layer   1  Sparsity: 82.5684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 323.0\n",
      "train - Value 0: 1889 occurrences\n",
      "train - Value 1: 2141 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-101 lr=['4.0000000'], tr/val_loss: 88.265335/ 92.175102, val:  50.00%, val_best:  88.72%, tr:  93.60%, tr_best:  95.29%, epoch time: 232.87 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4108%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8914%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.1845%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3288480 real_backward_count 948117  28.831%\n",
      "layer   1  Sparsity: 77.9297%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 338.0\n",
      "train - Value 0: 1941 occurrences\n",
      "train - Value 1: 2089 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 62 occurrences\n",
      "test - Value 1: 390 occurrences\n",
      "epoch-102 lr=['4.0000000'], tr/val_loss: 90.307068/ 67.629776, val:  62.83%, val_best:  88.72%, tr:  93.15%, tr_best:  95.29%, epoch time: 233.63 seconds, 3.89 minutes\n",
      "layer   1  Sparsity: 84.4118%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8776%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.8556%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3320720 real_backward_count 957215  28.826%\n",
      "layer   1  Sparsity: 80.5176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 344.0\n",
      "fc layer 3 self.abs_max_out: 364.0\n",
      "train - Value 0: 1934 occurrences\n",
      "train - Value 1: 2096 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 12 occurrences\n",
      "test - Value 1: 440 occurrences\n",
      "epoch-103 lr=['4.0000000'], tr/val_loss: 81.889687/ 77.645340, val:  52.65%, val_best:  88.72%, tr:  95.01%, tr_best:  95.29%, epoch time: 231.87 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8809%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.7852%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3352960 real_backward_count 966212  28.817%\n",
      "layer   1  Sparsity: 89.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1145.0\n",
      "lif layer 2 self.abs_max_v: 1145.0\n",
      "train - Value 0: 1950 occurrences\n",
      "train - Value 1: 2080 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 304 occurrences\n",
      "test - Value 1: 148 occurrences\n",
      "epoch-104 lr=['4.0000000'], tr/val_loss: 74.100479/ 47.316620, val:  82.30%, val_best:  88.72%, tr:  94.96%, tr_best:  95.29%, epoch time: 232.21 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4093%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.9093%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.8043%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3385200 real_backward_count 975274  28.810%\n",
      "layer   1  Sparsity: 88.5010%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 93.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1146.0\n",
      "lif layer 2 self.abs_max_v: 1146.0\n",
      "fc layer 2 self.abs_max_out: 1174.0\n",
      "lif layer 2 self.abs_max_v: 1174.0\n",
      "train - Value 0: 1930 occurrences\n",
      "train - Value 1: 2100 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 50 occurrences\n",
      "test - Value 1: 402 occurrences\n",
      "epoch-105 lr=['4.0000000'], tr/val_loss: 65.437492/ 73.268066, val:  60.62%, val_best:  88.72%, tr:  94.52%, tr_best:  95.29%, epoch time: 226.39 seconds, 3.77 minutes\n",
      "layer   1  Sparsity: 84.4094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8640%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.6662%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3417440 real_backward_count 984329  28.803%\n",
      "layer   1  Sparsity: 87.8662%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1951 occurrences\n",
      "train - Value 1: 2079 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 13 occurrences\n",
      "test - Value 1: 439 occurrences\n",
      "epoch-106 lr=['4.0000000'], tr/val_loss: 84.715569/ 95.489578, val:  52.88%, val_best:  88.72%, tr:  94.44%, tr_best:  95.29%, epoch time: 232.12 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4096%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8688%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.8419%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3449680 real_backward_count 993528  28.801%\n",
      "layer   1  Sparsity: 90.1611%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1198.0\n",
      "lif layer 2 self.abs_max_v: 1198.0\n",
      "fc layer 2 self.abs_max_out: 1214.0\n",
      "lif layer 2 self.abs_max_v: 1214.0\n",
      "fc layer 2 self.abs_max_out: 1257.0\n",
      "lif layer 2 self.abs_max_v: 1257.0\n",
      "train - Value 0: 1960 occurrences\n",
      "train - Value 1: 2070 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 132 occurrences\n",
      "test - Value 1: 320 occurrences\n",
      "epoch-107 lr=['4.0000000'], tr/val_loss:109.303177/ 90.957138, val:  76.99%, val_best:  88.72%, tr:  94.22%, tr_best:  95.29%, epoch time: 231.63 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8641%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.7502%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3481920 real_backward_count 1002629  28.795%\n",
      "layer   1  Sparsity: 73.6328%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1267.0\n",
      "lif layer 2 self.abs_max_v: 1267.0\n",
      "train - Value 0: 1957 occurrences\n",
      "train - Value 1: 2073 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 70 occurrences\n",
      "test - Value 1: 382 occurrences\n",
      "epoch-108 lr=['4.0000000'], tr/val_loss:114.536667/ 89.158234, val:  64.60%, val_best:  88.72%, tr:  94.54%, tr_best:  95.29%, epoch time: 230.98 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4127%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8838%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.6141%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3514160 real_backward_count 1011607  28.787%\n",
      "layer   1  Sparsity: 92.4805%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 93.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1276.0\n",
      "lif layer 2 self.abs_max_v: 1276.0\n",
      "fc layer 2 self.abs_max_out: 1313.0\n",
      "lif layer 2 self.abs_max_v: 1313.0\n",
      "train - Value 0: 1938 occurrences\n",
      "train - Value 1: 2092 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 327 occurrences\n",
      "test - Value 1: 125 occurrences\n",
      "epoch-109 lr=['4.0000000'], tr/val_loss:108.783852/ 62.932777, val:  77.21%, val_best:  88.72%, tr:  94.57%, tr_best:  95.29%, epoch time: 228.19 seconds, 3.80 minutes\n",
      "layer   1  Sparsity: 84.4085%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8663%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.4715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3546400 real_backward_count 1020522  28.776%\n",
      "layer   1  Sparsity: 80.3711%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1358.0\n",
      "lif layer 2 self.abs_max_v: 1358.0\n",
      "train - Value 0: 1973 occurrences\n",
      "train - Value 1: 2057 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 52 occurrences\n",
      "test - Value 1: 400 occurrences\n",
      "epoch-110 lr=['4.0000000'], tr/val_loss: 99.781342/ 86.946129, val:  59.29%, val_best:  88.72%, tr:  94.14%, tr_best:  95.29%, epoch time: 231.77 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8690%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.3386%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3578640 real_backward_count 1029668  28.773%\n",
      "layer   1  Sparsity: 75.2197%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 374.0\n",
      "train - Value 0: 1952 occurrences\n",
      "train - Value 1: 2078 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 29 occurrences\n",
      "test - Value 1: 423 occurrences\n",
      "epoch-111 lr=['4.0000000'], tr/val_loss:102.650017/ 74.937508, val:  55.97%, val_best:  88.72%, tr:  94.91%, tr_best:  95.29%, epoch time: 230.36 seconds, 3.84 minutes\n",
      "layer   1  Sparsity: 84.4124%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8214%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.2675%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3610880 real_backward_count 1038793  28.768%\n",
      "layer   1  Sparsity: 82.6660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1969 occurrences\n",
      "train - Value 1: 2061 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 87 occurrences\n",
      "test - Value 1: 365 occurrences\n",
      "epoch-112 lr=['4.0000000'], tr/val_loss: 95.532555/ 78.588020, val:  67.48%, val_best:  88.72%, tr:  94.94%, tr_best:  95.29%, epoch time: 229.54 seconds, 3.83 minutes\n",
      "layer   1  Sparsity: 84.4107%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8401%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.3626%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3643120 real_backward_count 1047712  28.759%\n",
      "layer   1  Sparsity: 85.1318%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 375.0\n",
      "train - Value 0: 1920 occurrences\n",
      "train - Value 1: 2110 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 2108.00 at epoch 113, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 3 occurrences\n",
      "test - Value 1: 449 occurrences\n",
      "epoch-113 lr=['4.0000000'], tr/val_loss:107.622154/116.738724, val:  50.66%, val_best:  88.72%, tr:  94.62%, tr_best:  95.29%, epoch time: 232.32 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4102%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8327%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.2119%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3675360 real_backward_count 1056640  28.749%\n",
      "layer   1  Sparsity: 82.4707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 383.0\n",
      "fc layer 3 self.abs_max_out: 398.0\n",
      "fc layer 3 self.abs_max_out: 402.0\n",
      "train - Value 0: 1940 occurrences\n",
      "train - Value 1: 2090 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 328 occurrences\n",
      "test - Value 1: 124 occurrences\n",
      "epoch-114 lr=['4.0000000'], tr/val_loss:130.001816/119.863403, val:  77.43%, val_best:  88.72%, tr:  95.21%, tr_best:  95.29%, epoch time: 230.69 seconds, 3.84 minutes\n",
      "layer   1  Sparsity: 84.4108%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8241%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.2970%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3707600 real_backward_count 1065553  28.740%\n",
      "layer   1  Sparsity: 78.5889%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 404.0\n",
      "fc layer 2 self.abs_max_out: 1363.0\n",
      "lif layer 2 self.abs_max_v: 1363.0\n",
      "fc layer 3 self.abs_max_out: 408.0\n",
      "train - Value 0: 1951 occurrences\n",
      "train - Value 1: 2079 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 2186.00 at epoch 115, iter 4029\n",
      "max_activation_accul updated: 2226.00 at epoch 115, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-115 lr=['4.0000000'], tr/val_loss:136.143509/131.491241, val:  50.00%, val_best:  88.72%, tr:  95.24%, tr_best:  95.29%, epoch time: 231.13 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4116%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8259%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.5714%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3739840 real_backward_count 1074382  28.728%\n",
      "layer   1  Sparsity: 78.8330%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 412.0\n",
      "train - Value 0: 1928 occurrences\n",
      "train - Value 1: 2102 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 169 occurrences\n",
      "test - Value 1: 283 occurrences\n",
      "epoch-116 lr=['4.0000000'], tr/val_loss:135.400192/117.045395, val:  84.73%, val_best:  88.72%, tr:  94.96%, tr_best:  95.29%, epoch time: 233.02 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4116%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8599%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.5776%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3772080 real_backward_count 1083249  28.718%\n",
      "layer   1  Sparsity: 78.1250%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 417.0\n",
      "fc layer 3 self.abs_max_out: 429.0\n",
      "train - Value 0: 1954 occurrences\n",
      "train - Value 1: 2076 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 220 occurrences\n",
      "test - Value 1: 232 occurrences\n",
      "epoch-117 lr=['4.0000000'], tr/val_loss:129.875931/112.630867, val:  87.17%, val_best:  88.72%, tr:  94.91%, tr_best:  95.29%, epoch time: 230.85 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4117%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8310%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.6150%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3804320 real_backward_count 1092203  28.710%\n",
      "layer   1  Sparsity: 88.7695%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1376.0\n",
      "lif layer 2 self.abs_max_v: 1376.0\n",
      "train - Value 0: 1937 occurrences\n",
      "train - Value 1: 2093 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 283 occurrences\n",
      "test - Value 1: 169 occurrences\n",
      "epoch-118 lr=['4.0000000'], tr/val_loss:126.790993/105.891037, val:  84.29%, val_best:  88.72%, tr:  95.04%, tr_best:  95.29%, epoch time: 232.19 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8001%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.8360%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3836560 real_backward_count 1101202  28.703%\n",
      "layer   1  Sparsity: 82.7148%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1960 occurrences\n",
      "train - Value 1: 2070 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 197 occurrences\n",
      "test - Value 1: 255 occurrences\n",
      "epoch-119 lr=['4.0000000'], tr/val_loss:126.685516/118.679626, val:  83.85%, val_best:  88.72%, tr:  94.47%, tr_best:  95.29%, epoch time: 230.73 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4107%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8348%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.9883%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3868800 real_backward_count 1110178  28.696%\n",
      "layer   1  Sparsity: 87.4023%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1974 occurrences\n",
      "train - Value 1: 2056 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 4 occurrences\n",
      "test - Value 1: 448 occurrences\n",
      "epoch-120 lr=['4.0000000'], tr/val_loss:137.286728/108.867874, val:  50.88%, val_best:  88.72%, tr:  95.71%, tr_best:  95.71%, epoch time: 231.93 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4097%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8636%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.1736%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3901040 real_backward_count 1118817  28.680%\n",
      "layer   1  Sparsity: 88.1104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 271 occurrences\n",
      "test - Value 1: 181 occurrences\n",
      "epoch-121 lr=['4.0000000'], tr/val_loss:133.240417/130.057983, val:  85.62%, val_best:  88.72%, tr:  95.36%, tr_best:  95.71%, epoch time: 233.36 seconds, 3.89 minutes\n",
      "layer   1  Sparsity: 84.4095%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8296%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.0104%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3933280 real_backward_count 1127536  28.667%\n",
      "layer   1  Sparsity: 85.5957%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1980 occurrences\n",
      "train - Value 1: 2050 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-122 lr=['4.0000000'], tr/val_loss:134.415024/123.112190, val:  50.00%, val_best:  88.72%, tr:  95.26%, tr_best:  95.71%, epoch time: 233.64 seconds, 3.89 minutes\n",
      "layer   1  Sparsity: 84.4101%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8398%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.0746%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3965520 real_backward_count 1136255  28.653%\n",
      "layer   1  Sparsity: 89.9902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1990 occurrences\n",
      "train - Value 1: 2040 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 10 occurrences\n",
      "test - Value 1: 442 occurrences\n",
      "epoch-123 lr=['4.0000000'], tr/val_loss:136.888992/132.411758, val:  52.21%, val_best:  88.72%, tr:  95.51%, tr_best:  95.71%, epoch time: 232.69 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.7677%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.2299%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3997760 real_backward_count 1145031  28.642%\n",
      "layer   1  Sparsity: 87.2803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2025 occurrences\n",
      "train - Value 1: 2005 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 190 occurrences\n",
      "test - Value 1: 262 occurrences\n",
      "epoch-124 lr=['4.0000000'], tr/val_loss:149.511459/131.113663, val:  83.63%, val_best:  88.72%, tr:  95.24%, tr_best:  95.71%, epoch time: 233.24 seconds, 3.89 minutes\n",
      "layer   1  Sparsity: 84.4097%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.7935%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.9338%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4030000 real_backward_count 1153827  28.631%\n",
      "layer   1  Sparsity: 82.9834%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 445.0\n",
      "train - Value 0: 2003 occurrences\n",
      "train - Value 1: 2027 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 277 occurrences\n",
      "test - Value 1: 175 occurrences\n",
      "epoch-125 lr=['4.0000000'], tr/val_loss:154.764648/137.110001, val:  85.62%, val_best:  88.72%, tr:  95.83%, tr_best:  95.83%, epoch time: 232.35 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4107%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8095%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.5481%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4062240 real_backward_count 1162483  28.617%\n",
      "layer   1  Sparsity: 91.2354%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1383.0\n",
      "lif layer 2 self.abs_max_v: 1383.0\n",
      "fc layer 2 self.abs_max_out: 1386.0\n",
      "lif layer 2 self.abs_max_v: 1386.0\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2010 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 2 self.abs_max_out: 1400.0\n",
      "lif layer 2 self.abs_max_v: 1400.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 388 occurrences\n",
      "test - Value 1: 64 occurrences\n",
      "epoch-126 lr=['4.0000000'], tr/val_loss:168.485916/174.660553, val:  64.16%, val_best:  88.72%, tr:  95.31%, tr_best:  95.83%, epoch time: 231.92 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6091%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.2362%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4094480 real_backward_count 1171236  28.605%\n",
      "layer   1  Sparsity: 76.6113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1969 occurrences\n",
      "train - Value 1: 2061 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 7 occurrences\n",
      "test - Value 1: 445 occurrences\n",
      "epoch-127 lr=['4.0000000'], tr/val_loss:197.094437/178.816254, val:  51.11%, val_best:  88.72%, tr:  95.63%, tr_best:  95.83%, epoch time: 231.08 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4121%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5788%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.1418%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4126720 real_backward_count 1180009  28.594%\n",
      "layer   1  Sparsity: 88.9893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 1410.0\n",
      "train - Value 0: 1975 occurrences\n",
      "train - Value 1: 2055 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 191 occurrences\n",
      "test - Value 1: 261 occurrences\n",
      "epoch-128 lr=['4.0000000'], tr/val_loss:189.556931/156.382294, val:  80.31%, val_best:  88.72%, tr:  95.73%, tr_best:  95.83%, epoch time: 231.01 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4093%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5864%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.4759%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4158960 real_backward_count 1188740  28.583%\n",
      "layer   1  Sparsity: 74.6582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 95.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1404.0\n",
      "lif layer 2 self.abs_max_v: 1442.0\n",
      "train - Value 0: 1983 occurrences\n",
      "train - Value 1: 2047 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 197 occurrences\n",
      "test - Value 1: 255 occurrences\n",
      "epoch-129 lr=['4.0000000'], tr/val_loss:185.230286/161.241287, val:  81.64%, val_best:  88.72%, tr:  95.53%, tr_best:  95.83%, epoch time: 231.55 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5711%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.4017%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4191200 real_backward_count 1197445  28.570%\n",
      "layer   1  Sparsity: 91.6992%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 1484.5\n",
      "train - Value 0: 1968 occurrences\n",
      "train - Value 1: 2062 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 3 occurrences\n",
      "test - Value 1: 449 occurrences\n",
      "epoch-130 lr=['4.0000000'], tr/val_loss:179.994324/168.209106, val:  50.66%, val_best:  88.72%, tr:  95.81%, tr_best:  95.83%, epoch time: 231.68 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4087%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5607%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.5221%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4223440 real_backward_count 1206278  28.562%\n",
      "layer   1  Sparsity: 82.7148%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1981 occurrences\n",
      "train - Value 1: 2049 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 217 occurrences\n",
      "test - Value 1: 235 occurrences\n",
      "epoch-131 lr=['4.0000000'], tr/val_loss:170.086777/154.772614, val:  86.06%, val_best:  88.72%, tr:  94.99%, tr_best:  95.83%, epoch time: 230.87 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4107%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5541%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.3058%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4255680 real_backward_count 1214999  28.550%\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 93.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1969 occurrences\n",
      "train - Value 1: 2061 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 238 occurrences\n",
      "test - Value 1: 214 occurrences\n",
      "epoch-132 lr=['4.0000000'], tr/val_loss:181.892166/156.842743, val:  86.28%, val_best:  88.72%, tr:  95.98%, tr_best:  95.98%, epoch time: 233.78 seconds, 3.90 minutes\n",
      "layer   1  Sparsity: 84.4089%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5247%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.3292%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4287920 real_backward_count 1223737  28.539%\n",
      "layer   1  Sparsity: 84.6680%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1460.0\n",
      "train - Value 0: 1975 occurrences\n",
      "train - Value 1: 2055 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 311 occurrences\n",
      "test - Value 1: 141 occurrences\n",
      "epoch-133 lr=['4.0000000'], tr/val_loss:179.854431/141.051636, val:  79.42%, val_best:  88.72%, tr:  96.18%, tr_best:  96.18%, epoch time: 232.48 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4103%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5100%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.1473%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4320160 real_backward_count 1232459  28.528%\n",
      "layer   1  Sparsity: 80.4932%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 1505.5\n",
      "train - Value 0: 1965 occurrences\n",
      "train - Value 1: 2065 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 214 occurrences\n",
      "test - Value 1: 238 occurrences\n",
      "epoch-134 lr=['4.0000000'], tr/val_loss:175.419739/145.851166, val:  84.07%, val_best:  88.72%, tr:  95.68%, tr_best:  96.18%, epoch time: 231.72 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4977%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.0474%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4352400 real_backward_count 1241162  28.517%\n",
      "layer   1  Sparsity: 87.2559%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1953 occurrences\n",
      "train - Value 1: 2077 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 162 occurrences\n",
      "test - Value 1: 290 occurrences\n",
      "epoch-135 lr=['4.0000000'], tr/val_loss:174.421036/149.313904, val:  80.53%, val_best:  88.72%, tr:  95.88%, tr_best:  96.18%, epoch time: 232.79 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4097%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4951%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.1390%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4384640 real_backward_count 1249824  28.505%\n",
      "layer   1  Sparsity: 87.5488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1976 occurrences\n",
      "train - Value 1: 2054 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 9 occurrences\n",
      "test - Value 1: 443 occurrences\n",
      "epoch-136 lr=['4.0000000'], tr/val_loss:172.688599/151.036850, val:  51.99%, val_best:  88.72%, tr:  95.26%, tr_best:  96.18%, epoch time: 232.25 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4096%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4830%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.0765%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4416880 real_backward_count 1258570  28.495%\n",
      "layer   1  Sparsity: 74.5605%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 95.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1979 occurrences\n",
      "train - Value 1: 2051 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-137 lr=['4.0000000'], tr/val_loss:164.993469/156.920990, val:  50.00%, val_best:  88.72%, tr:  95.33%, tr_best:  96.18%, epoch time: 232.39 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4499%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.8838%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4449120 real_backward_count 1267308  28.484%\n",
      "layer   1  Sparsity: 72.5342%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1978 occurrences\n",
      "train - Value 1: 2052 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 239 occurrences\n",
      "test - Value 1: 213 occurrences\n",
      "epoch-138 lr=['4.0000000'], tr/val_loss:170.969849/151.432877, val:  84.73%, val_best:  88.72%, tr:  95.71%, tr_best:  96.18%, epoch time: 232.08 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4130%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4535%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.5799%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4481360 real_backward_count 1276034  28.474%\n",
      "layer   1  Sparsity: 76.4893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1966 occurrences\n",
      "train - Value 1: 2064 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 210 occurrences\n",
      "test - Value 1: 242 occurrences\n",
      "epoch-139 lr=['4.0000000'], tr/val_loss:172.479584/143.229248, val:  84.51%, val_best:  88.72%, tr:  95.86%, tr_best:  96.18%, epoch time: 232.07 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4121%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4515%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.4854%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4513600 real_backward_count 1284761  28.464%\n",
      "layer   1  Sparsity: 89.9658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1994 occurrences\n",
      "train - Value 1: 2036 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 50 occurrences\n",
      "test - Value 1: 402 occurrences\n",
      "epoch-140 lr=['4.0000000'], tr/val_loss:171.364105/148.686554, val:  60.62%, val_best:  88.72%, tr:  95.36%, tr_best:  96.18%, epoch time: 231.98 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4413%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.4572%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4545840 real_backward_count 1293581  28.456%\n",
      "layer   1  Sparsity: 88.6719%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1967 occurrences\n",
      "train - Value 1: 2063 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 44 occurrences\n",
      "test - Value 1: 408 occurrences\n",
      "epoch-141 lr=['4.0000000'], tr/val_loss:173.772324/156.540817, val:  59.73%, val_best:  88.72%, tr:  95.53%, tr_best:  96.18%, epoch time: 232.87 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4061%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.4947%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4578080 real_backward_count 1302364  28.448%\n",
      "layer   1  Sparsity: 81.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1994 occurrences\n",
      "train - Value 1: 2036 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 38 occurrences\n",
      "test - Value 1: 414 occurrences\n",
      "epoch-142 lr=['4.0000000'], tr/val_loss:177.846252/153.400848, val:  57.96%, val_best:  88.72%, tr:  95.16%, tr_best:  96.18%, epoch time: 232.20 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4111%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3439%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.5166%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4610320 real_backward_count 1311234  28.441%\n",
      "layer   1  Sparsity: 87.1826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1988 occurrences\n",
      "train - Value 1: 2042 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 348 occurrences\n",
      "test - Value 1: 104 occurrences\n",
      "epoch-143 lr=['4.0000000'], tr/val_loss:172.933243/164.630447, val:  73.01%, val_best:  88.72%, tr:  94.47%, tr_best:  96.18%, epoch time: 231.79 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4097%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3902%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.2068%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4642560 real_backward_count 1320144  28.436%\n",
      "layer   1  Sparsity: 84.3750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 1552.5\n",
      "train - Value 0: 1959 occurrences\n",
      "train - Value 1: 2071 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 63 occurrences\n",
      "test - Value 1: 389 occurrences\n",
      "epoch-144 lr=['4.0000000'], tr/val_loss:174.222885/152.167419, val:  62.61%, val_best:  88.72%, tr:  94.54%, tr_best:  96.18%, epoch time: 232.38 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4103%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4027%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0878%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4674800 real_backward_count 1329167  28.433%\n",
      "layer   1  Sparsity: 78.0762%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1973 occurrences\n",
      "train - Value 1: 2057 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 306 occurrences\n",
      "test - Value 1: 146 occurrences\n",
      "epoch-145 lr=['4.0000000'], tr/val_loss:172.635406/151.472717, val:  81.42%, val_best:  88.72%, tr:  94.69%, tr_best:  96.18%, epoch time: 231.54 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4118%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4340%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.3140%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4707040 real_backward_count 1338103  28.428%\n",
      "layer   1  Sparsity: 86.5967%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 1588.5\n",
      "train - Value 0: 1961 occurrences\n",
      "train - Value 1: 2069 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 35 occurrences\n",
      "test - Value 1: 417 occurrences\n",
      "epoch-146 lr=['4.0000000'], tr/val_loss:171.546890/155.036850, val:  57.74%, val_best:  88.72%, tr:  95.53%, tr_best:  96.18%, epoch time: 231.65 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4099%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4466%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.9884%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4739280 real_backward_count 1346972  28.421%\n",
      "layer   1  Sparsity: 90.0879%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1960 occurrences\n",
      "train - Value 1: 2070 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 328 occurrences\n",
      "test - Value 1: 124 occurrences\n",
      "epoch-147 lr=['4.0000000'], tr/val_loss:184.298538/156.081100, val:  76.99%, val_best:  88.72%, tr:  95.01%, tr_best:  96.18%, epoch time: 231.04 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4484%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.8431%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4771520 real_backward_count 1355816  28.415%\n",
      "layer   1  Sparsity: 84.1064%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 1622.0\n",
      "fc layer 2 self.abs_max_out: 1514.0\n",
      "train - Value 0: 1963 occurrences\n",
      "train - Value 1: 2067 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 2 self.abs_max_out: 1515.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 80 occurrences\n",
      "test - Value 1: 372 occurrences\n",
      "epoch-148 lr=['4.0000000'], tr/val_loss:176.217972/147.735474, val:  66.81%, val_best:  88.72%, tr:  94.84%, tr_best:  96.18%, epoch time: 231.10 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3774%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.6178%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4803760 real_backward_count 1364835  28.412%\n",
      "layer   1  Sparsity: 75.8789%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1949 occurrences\n",
      "train - Value 1: 2081 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 257 occurrences\n",
      "test - Value 1: 195 occurrences\n",
      "epoch-149 lr=['4.0000000'], tr/val_loss:171.604736/156.024933, val:  86.95%, val_best:  88.72%, tr:  94.29%, tr_best:  96.18%, epoch time: 231.79 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4122%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3652%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.5026%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4836000 real_backward_count 1373845  28.409%\n",
      "layer   1  Sparsity: 79.9316%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1970 occurrences\n",
      "train - Value 1: 2060 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 191 occurrences\n",
      "test - Value 1: 261 occurrences\n",
      "epoch-150 lr=['4.0000000'], tr/val_loss:187.336777/159.053940, val:  85.62%, val_best:  88.72%, tr:  93.72%, tr_best:  96.18%, epoch time: 231.40 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3177%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0858%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4868240 real_backward_count 1382850  28.406%\n",
      "layer   1  Sparsity: 87.3291%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 448.0\n",
      "train - Value 0: 1937 occurrences\n",
      "train - Value 1: 2093 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 115 occurrences\n",
      "test - Value 1: 337 occurrences\n",
      "epoch-151 lr=['4.0000000'], tr/val_loss:177.703110/159.118591, val:  74.12%, val_best:  88.72%, tr:  94.89%, tr_best:  96.18%, epoch time: 231.97 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4097%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3279%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9733%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4900480 real_backward_count 1391770  28.401%\n",
      "layer   1  Sparsity: 88.5010%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1921 occurrences\n",
      "train - Value 1: 2109 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-152 lr=['4.0000000'], tr/val_loss:177.567169/149.809494, val:  50.00%, val_best:  88.72%, tr:  95.14%, tr_best:  96.18%, epoch time: 231.38 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3163%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.1921%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4932720 real_backward_count 1400668  28.395%\n",
      "layer   1  Sparsity: 87.2559%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1939 occurrences\n",
      "train - Value 1: 2091 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 397 occurrences\n",
      "test - Value 1: 55 occurrences\n",
      "epoch-153 lr=['4.0000000'], tr/val_loss:173.648636/151.588455, val:  62.17%, val_best:  88.72%, tr:  95.09%, tr_best:  96.18%, epoch time: 232.13 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4097%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2935%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9359%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4964960 real_backward_count 1409425  28.387%\n",
      "layer   1  Sparsity: 89.6484%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1988 occurrences\n",
      "train - Value 1: 2042 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 136 occurrences\n",
      "test - Value 1: 316 occurrences\n",
      "epoch-154 lr=['4.0000000'], tr/val_loss:184.437332/163.011108, val:  76.55%, val_best:  88.72%, tr:  95.51%, tr_best:  96.18%, epoch time: 232.64 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4092%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2672%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.0182%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4997200 real_backward_count 1418361  28.383%\n",
      "layer   1  Sparsity: 81.5674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1973 occurrences\n",
      "train - Value 1: 2057 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 415 occurrences\n",
      "test - Value 1: 37 occurrences\n",
      "epoch-155 lr=['4.0000000'], tr/val_loss:186.812454/158.984680, val:  58.19%, val_best:  88.72%, tr:  94.79%, tr_best:  96.18%, epoch time: 232.77 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4110%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1925%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9062%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5029440 real_backward_count 1427281  28.379%\n",
      "layer   1  Sparsity: 73.9990%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 95.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 454.0\n",
      "train - Value 0: 1941 occurrences\n",
      "train - Value 1: 2089 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 2229.00 at epoch 156, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 36 occurrences\n",
      "test - Value 1: 416 occurrences\n",
      "epoch-156 lr=['4.0000000'], tr/val_loss:183.535690/154.244156, val:  57.96%, val_best:  88.72%, tr:  94.89%, tr_best:  96.18%, epoch time: 233.17 seconds, 3.89 minutes\n",
      "layer   1  Sparsity: 84.4127%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.0917%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9802%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5061680 real_backward_count 1436178  28.374%\n",
      "layer   1  Sparsity: 88.2568%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1952 occurrences\n",
      "train - Value 1: 2078 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 342 occurrences\n",
      "test - Value 1: 110 occurrences\n",
      "epoch-157 lr=['4.0000000'], tr/val_loss:173.086060/143.970901, val:  73.89%, val_best:  88.72%, tr:  94.96%, tr_best:  96.18%, epoch time: 232.76 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4095%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1458%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9768%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5093920 real_backward_count 1445020  28.368%\n",
      "layer   1  Sparsity: 86.7188%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1977 occurrences\n",
      "train - Value 1: 2053 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 427 occurrences\n",
      "test - Value 1: 25 occurrences\n",
      "epoch-158 lr=['4.0000000'], tr/val_loss:174.945679/161.414474, val:  55.53%, val_best:  88.72%, tr:  95.93%, tr_best:  96.18%, epoch time: 231.38 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1157%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.7497%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5126160 real_backward_count 1453874  28.362%\n",
      "layer   1  Sparsity: 88.5254%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1979 occurrences\n",
      "train - Value 1: 2051 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 315 occurrences\n",
      "test - Value 1: 137 occurrences\n",
      "epoch-159 lr=['4.0000000'], tr/val_loss:177.497406/142.645523, val:  78.54%, val_best:  88.72%, tr:  95.29%, tr_best:  96.18%, epoch time: 231.80 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.0917%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4709%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5158400 real_backward_count 1462665  28.355%\n",
      "layer   1  Sparsity: 81.3721%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1960 occurrences\n",
      "train - Value 1: 2070 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 259 occurrences\n",
      "test - Value 1: 193 occurrences\n",
      "epoch-160 lr=['4.0000000'], tr/val_loss:186.637161/163.196030, val:  85.18%, val_best:  88.72%, tr:  95.26%, tr_best:  96.18%, epoch time: 231.98 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4110%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.0873%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0664%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5190640 real_backward_count 1471512  28.349%\n",
      "layer   1  Sparsity: 88.9893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1949 occurrences\n",
      "train - Value 1: 2081 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 2261.00 at epoch 161, iter 4029\n",
      "max_activation_accul updated: 2278.00 at epoch 161, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-161 lr=['4.0000000'], tr/val_loss:189.575455/171.885834, val:  50.00%, val_best:  88.72%, tr:  94.59%, tr_best:  96.18%, epoch time: 232.92 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4093%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1455%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2147%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5222880 real_backward_count 1480305  28.343%\n",
      "layer   1  Sparsity: 81.2988%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 95.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1949 occurrences\n",
      "train - Value 1: 2081 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 63 occurrences\n",
      "test - Value 1: 389 occurrences\n",
      "epoch-162 lr=['4.0000000'], tr/val_loss:182.795395/160.026703, val:  63.94%, val_best:  88.72%, tr:  94.54%, tr_best:  96.18%, epoch time: 232.29 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4110%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1358%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2239%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5255120 real_backward_count 1489098  28.336%\n",
      "layer   1  Sparsity: 93.5791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1973 occurrences\n",
      "train - Value 1: 2057 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 36 occurrences\n",
      "test - Value 1: 416 occurrences\n",
      "epoch-163 lr=['4.0000000'], tr/val_loss:193.098206/166.869888, val:  57.96%, val_best:  88.72%, tr:  95.04%, tr_best:  96.18%, epoch time: 230.99 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4083%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1379%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1270%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5287360 real_backward_count 1497911  28.330%\n",
      "layer   1  Sparsity: 79.3213%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1964 occurrences\n",
      "train - Value 1: 2066 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 44 occurrences\n",
      "test - Value 1: 408 occurrences\n",
      "epoch-164 lr=['4.0000000'], tr/val_loss:196.214539/164.888412, val:  59.29%, val_best:  88.72%, tr:  95.31%, tr_best:  96.18%, epoch time: 231.81 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4115%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1559%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1719%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5319600 real_backward_count 1506648  28.323%\n",
      "layer   1  Sparsity: 78.8330%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1980 occurrences\n",
      "train - Value 1: 2050 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 431 occurrences\n",
      "test - Value 1: 21 occurrences\n",
      "epoch-165 lr=['4.0000000'], tr/val_loss:190.864883/177.023239, val:  54.65%, val_best:  88.72%, tr:  94.57%, tr_best:  96.18%, epoch time: 232.70 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4116%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1436%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0958%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5351840 real_backward_count 1515581  28.319%\n",
      "layer   1  Sparsity: 81.3721%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1957 occurrences\n",
      "train - Value 1: 2073 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 289 occurrences\n",
      "test - Value 1: 163 occurrences\n",
      "epoch-166 lr=['4.0000000'], tr/val_loss:194.272476/159.724197, val:  82.96%, val_best:  88.72%, tr:  94.99%, tr_best:  96.18%, epoch time: 232.60 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4110%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1502%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2963%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5384080 real_backward_count 1524374  28.313%\n",
      "layer   1  Sparsity: 91.5527%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1921 occurrences\n",
      "train - Value 1: 2109 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 173 occurrences\n",
      "test - Value 1: 279 occurrences\n",
      "epoch-167 lr=['4.0000000'], tr/val_loss:186.404510/164.919067, val:  81.64%, val_best:  88.72%, tr:  94.54%, tr_best:  96.18%, epoch time: 231.80 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4087%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1049%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6306%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5416320 real_backward_count 1533231  28.308%\n",
      "layer   1  Sparsity: 80.3467%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 1684.0\n",
      "train - Value 0: 1964 occurrences\n",
      "train - Value 1: 2066 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 339 occurrences\n",
      "test - Value 1: 113 occurrences\n",
      "epoch-168 lr=['4.0000000'], tr/val_loss:188.438370/158.881714, val:  74.56%, val_best:  88.72%, tr:  94.62%, tr_best:  96.18%, epoch time: 232.88 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.9775%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6821%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5448560 real_backward_count 1542132  28.303%\n",
      "layer   1  Sparsity: 84.3506%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1518.0\n",
      "train - Value 0: 1990 occurrences\n",
      "train - Value 1: 2040 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 16 occurrences\n",
      "test - Value 1: 436 occurrences\n",
      "epoch-169 lr=['4.0000000'], tr/val_loss:190.136292/171.788849, val:  53.10%, val_best:  88.72%, tr:  94.71%, tr_best:  96.18%, epoch time: 231.25 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.0262%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5350%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5480800 real_backward_count 1551040  28.300%\n",
      "layer   1  Sparsity: 60.5469%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 93.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1941 occurrences\n",
      "train - Value 1: 2089 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 151 occurrences\n",
      "test - Value 1: 301 occurrences\n",
      "epoch-170 lr=['4.0000000'], tr/val_loss:190.566544/148.261795, val:  78.54%, val_best:  88.72%, tr:  94.64%, tr_best:  96.18%, epoch time: 232.11 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4157%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.0898%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1381%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5513040 real_backward_count 1559962  28.296%\n",
      "layer   1  Sparsity: 79.0527%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1965 occurrences\n",
      "train - Value 1: 2065 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 229 occurrences\n",
      "test - Value 1: 223 occurrences\n",
      "epoch-171 lr=['4.0000000'], tr/val_loss:186.220566/160.920975, val:  86.95%, val_best:  88.72%, tr:  95.29%, tr_best:  96.18%, epoch time: 231.20 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4115%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1295%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3468%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5545280 real_backward_count 1568858  28.292%\n",
      "layer   1  Sparsity: 84.4727%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1946 occurrences\n",
      "train - Value 1: 2084 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 271 occurrences\n",
      "test - Value 1: 181 occurrences\n",
      "epoch-172 lr=['4.0000000'], tr/val_loss:186.475021/138.911926, val:  86.95%, val_best:  88.72%, tr:  95.01%, tr_best:  96.18%, epoch time: 233.47 seconds, 3.89 minutes\n",
      "layer   1  Sparsity: 84.4103%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1355%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3204%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5577520 real_backward_count 1577544  28.284%\n",
      "layer   1  Sparsity: 76.5381%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1978 occurrences\n",
      "train - Value 1: 2052 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 242 occurrences\n",
      "test - Value 1: 210 occurrences\n",
      "epoch-173 lr=['4.0000000'], tr/val_loss:185.414154/163.845596, val:  88.05%, val_best:  88.72%, tr:  94.81%, tr_best:  96.18%, epoch time: 232.62 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4121%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1037%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1991%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5609760 real_backward_count 1586496  28.281%\n",
      "layer   1  Sparsity: 92.9443%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1941 occurrences\n",
      "train - Value 1: 2089 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 316 occurrences\n",
      "test - Value 1: 136 occurrences\n",
      "epoch-174 lr=['4.0000000'], tr/val_loss:188.664368/158.738037, val:  78.32%, val_best:  88.72%, tr:  95.19%, tr_best:  96.18%, epoch time: 231.64 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4084%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.0862%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1803%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5642000 real_backward_count 1595351  28.276%\n",
      "layer   1  Sparsity: 81.3721%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1969 occurrences\n",
      "train - Value 1: 2061 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 19 occurrences\n",
      "test - Value 1: 433 occurrences\n",
      "epoch-175 lr=['4.0000000'], tr/val_loss:178.212830/162.522171, val:  53.76%, val_best:  88.72%, tr:  95.38%, tr_best:  96.18%, epoch time: 232.41 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4110%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1044%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0414%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5674240 real_backward_count 1604112  28.270%\n",
      "layer   1  Sparsity: 86.5479%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1953 occurrences\n",
      "train - Value 1: 2077 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 300 occurrences\n",
      "test - Value 1: 152 occurrences\n",
      "epoch-176 lr=['4.0000000'], tr/val_loss:189.972748/154.034149, val:  81.42%, val_best:  88.72%, tr:  95.58%, tr_best:  96.18%, epoch time: 230.62 seconds, 3.84 minutes\n",
      "layer   1  Sparsity: 84.4099%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1168%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8834%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5706480 real_backward_count 1612844  28.263%\n",
      "layer   1  Sparsity: 86.3770%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1956 occurrences\n",
      "train - Value 1: 2074 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-177 lr=['4.0000000'], tr/val_loss:181.866379/174.111938, val:  50.00%, val_best:  88.72%, tr:  95.26%, tr_best:  96.18%, epoch time: 231.87 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4099%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1249%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1005%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5738720 real_backward_count 1621704  28.259%\n",
      "layer   1  Sparsity: 84.7656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1966 occurrences\n",
      "train - Value 1: 2064 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 106 occurrences\n",
      "test - Value 1: 346 occurrences\n",
      "epoch-178 lr=['4.0000000'], tr/val_loss:172.128235/138.897873, val:  72.57%, val_best:  88.72%, tr:  95.71%, tr_best:  96.18%, epoch time: 231.56 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4103%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1317%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2314%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5770960 real_backward_count 1630385  28.252%\n",
      "layer   1  Sparsity: 79.1992%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1959 occurrences\n",
      "train - Value 1: 2071 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 80 occurrences\n",
      "test - Value 1: 372 occurrences\n",
      "epoch-179 lr=['4.0000000'], tr/val_loss:167.889252/145.571915, val:  66.81%, val_best:  88.72%, tr:  95.33%, tr_best:  96.18%, epoch time: 231.48 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4115%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1318%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1709%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5803200 real_backward_count 1639232  28.247%\n",
      "layer   1  Sparsity: 90.5029%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1978 occurrences\n",
      "train - Value 1: 2052 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 373 occurrences\n",
      "test - Value 1: 79 occurrences\n",
      "epoch-180 lr=['4.0000000'], tr/val_loss:169.814362/150.812943, val:  67.48%, val_best:  88.72%, tr:  95.56%, tr_best:  96.18%, epoch time: 232.02 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4090%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.0965%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0351%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5835440 real_backward_count 1647958  28.241%\n",
      "layer   1  Sparsity: 81.9336%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1950 occurrences\n",
      "train - Value 1: 2080 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 195 occurrences\n",
      "test - Value 1: 257 occurrences\n",
      "epoch-181 lr=['4.0000000'], tr/val_loss:181.188095/152.785217, val:  82.52%, val_best:  88.72%, tr:  95.01%, tr_best:  96.18%, epoch time: 232.32 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4109%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.0695%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9150%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5867680 real_backward_count 1656935  28.238%\n",
      "layer   1  Sparsity: 79.5410%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1985 occurrences\n",
      "train - Value 1: 2045 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 30 occurrences\n",
      "test - Value 1: 422 occurrences\n",
      "epoch-182 lr=['4.0000000'], tr/val_loss:180.449234/146.288345, val:  56.64%, val_best:  88.72%, tr:  95.04%, tr_best:  96.18%, epoch time: 232.64 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4114%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1112%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5899920 real_backward_count 1665887  28.236%\n",
      "layer   1  Sparsity: 76.9775%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 472.0\n",
      "train - Value 0: 1975 occurrences\n",
      "train - Value 1: 2055 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 6 occurrences\n",
      "test - Value 1: 446 occurrences\n",
      "epoch-183 lr=['4.0000000'], tr/val_loss:178.921890/158.146255, val:  51.33%, val_best:  88.72%, tr:  95.24%, tr_best:  96.18%, epoch time: 233.30 seconds, 3.89 minutes\n",
      "layer   1  Sparsity: 84.4120%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1353%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1973%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5932160 real_backward_count 1674752  28.232%\n",
      "layer   1  Sparsity: 78.9795%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 95.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1970 occurrences\n",
      "train - Value 1: 2060 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 128 occurrences\n",
      "test - Value 1: 324 occurrences\n",
      "epoch-184 lr=['4.0000000'], tr/val_loss:182.906662/154.292389, val:  75.22%, val_best:  88.72%, tr:  95.41%, tr_best:  96.18%, epoch time: 231.66 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4116%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1240%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2769%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5964400 real_backward_count 1683475  28.225%\n",
      "layer   1  Sparsity: 86.7188%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 477.0\n",
      "train - Value 0: 1961 occurrences\n",
      "train - Value 1: 2069 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 146 occurrences\n",
      "test - Value 1: 306 occurrences\n",
      "epoch-185 lr=['4.0000000'], tr/val_loss:176.278625/143.875671, val:  79.65%, val_best:  88.72%, tr:  95.48%, tr_best:  96.18%, epoch time: 231.97 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1335%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0127%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5996640 real_backward_count 1692306  28.221%\n",
      "layer   1  Sparsity: 79.8340%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1974 occurrences\n",
      "train - Value 1: 2056 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 340 occurrences\n",
      "test - Value 1: 112 occurrences\n",
      "epoch-186 lr=['4.0000000'], tr/val_loss:149.205688/124.286865, val:  73.89%, val_best:  88.72%, tr:  95.16%, tr_best:  96.18%, epoch time: 231.17 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4114%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1368%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9891%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6028880 real_backward_count 1701201  28.218%\n",
      "layer   1  Sparsity: 76.0498%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 95.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1962 occurrences\n",
      "train - Value 1: 2068 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-187 lr=['4.0000000'], tr/val_loss:138.209778/135.917358, val:  50.00%, val_best:  88.72%, tr:  94.71%, tr_best:  96.18%, epoch time: 234.02 seconds, 3.90 minutes\n",
      "layer   1  Sparsity: 84.4122%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2138%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1554%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6061120 real_backward_count 1710136  28.215%\n",
      "layer   1  Sparsity: 78.8330%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1969 occurrences\n",
      "train - Value 1: 2061 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 207 occurrences\n",
      "test - Value 1: 245 occurrences\n",
      "epoch-188 lr=['4.0000000'], tr/val_loss:119.736305/102.709656, val:  83.85%, val_best:  88.72%, tr:  94.79%, tr_best:  96.18%, epoch time: 231.72 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4116%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2651%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3134%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6093360 real_backward_count 1719074  28.212%\n",
      "layer   1  Sparsity: 77.0264%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 95.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1974 occurrences\n",
      "train - Value 1: 2056 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 337 occurrences\n",
      "test - Value 1: 115 occurrences\n",
      "epoch-189 lr=['4.0000000'], tr/val_loss:134.683914/102.732986, val:  74.56%, val_best:  88.72%, tr:  95.01%, tr_best:  96.18%, epoch time: 230.97 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4120%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2475%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1766%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6125600 real_backward_count 1727835  28.207%\n",
      "layer   1  Sparsity: 73.5352%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 482.0\n",
      "train - Value 0: 1979 occurrences\n",
      "train - Value 1: 2051 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 260 occurrences\n",
      "test - Value 1: 192 occurrences\n",
      "epoch-190 lr=['4.0000000'], tr/val_loss:136.669128/101.426743, val:  83.63%, val_best:  88.72%, tr:  95.04%, tr_best:  96.18%, epoch time: 230.89 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4128%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2530%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0483%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6157840 real_backward_count 1736678  28.203%\n",
      "layer   1  Sparsity: 89.1846%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1579.0\n",
      "train - Value 0: 2043 occurrences\n",
      "train - Value 1: 1987 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 82 occurrences\n",
      "test - Value 1: 370 occurrences\n",
      "epoch-191 lr=['4.0000000'], tr/val_loss:141.130524/147.791473, val:  67.70%, val_best:  88.72%, tr:  95.33%, tr_best:  96.18%, epoch time: 231.32 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4093%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1551%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1409%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6190080 real_backward_count 1745452  28.198%\n",
      "layer   1  Sparsity: 84.8145%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1600.0\n",
      "train - Value 0: 1985 occurrences\n",
      "train - Value 1: 2045 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 86 occurrences\n",
      "test - Value 1: 366 occurrences\n",
      "epoch-192 lr=['4.0000000'], tr/val_loss:166.318497/134.329437, val:  68.58%, val_best:  88.72%, tr:  95.78%, tr_best:  96.18%, epoch time: 232.00 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4103%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1647%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3379%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6222320 real_backward_count 1754200  28.192%\n",
      "layer   1  Sparsity: 90.9912%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 106 occurrences\n",
      "test - Value 1: 346 occurrences\n",
      "epoch-193 lr=['4.0000000'], tr/val_loss:136.994202/ 98.006721, val:  72.57%, val_best:  88.72%, tr:  96.03%, tr_best:  96.18%, epoch time: 230.78 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4089%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1714%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2244%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6254560 real_backward_count 1762831  28.185%\n",
      "layer   1  Sparsity: 88.3057%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1691.0\n",
      "lif layer 2 self.abs_max_v: 1691.0\n",
      "train - Value 0: 1983 occurrences\n",
      "train - Value 1: 2047 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 149 occurrences\n",
      "test - Value 1: 303 occurrences\n",
      "epoch-194 lr=['4.0000000'], tr/val_loss:140.356201/117.624763, val:  79.42%, val_best:  88.72%, tr:  95.48%, tr_best:  96.18%, epoch time: 230.98 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4095%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2394%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4408%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6286800 real_backward_count 1771505  28.178%\n",
      "layer   1  Sparsity: 75.4639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1998 occurrences\n",
      "train - Value 1: 2032 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 240 occurrences\n",
      "test - Value 1: 212 occurrences\n",
      "epoch-195 lr=['4.0000000'], tr/val_loss:152.275665/148.842697, val:  86.73%, val_best:  88.72%, tr:  96.40%, tr_best:  96.40%, epoch time: 230.29 seconds, 3.84 minutes\n",
      "layer   1  Sparsity: 84.4123%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1984%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4441%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6319040 real_backward_count 1780136  28.171%\n",
      "layer   1  Sparsity: 90.4053%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1976 occurrences\n",
      "train - Value 1: 2054 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 2392.00 at epoch 196, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 18 occurrences\n",
      "test - Value 1: 434 occurrences\n",
      "epoch-196 lr=['4.0000000'], tr/val_loss:161.195770/174.035477, val:  53.98%, val_best:  88.72%, tr:  95.96%, tr_best:  96.40%, epoch time: 231.41 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4090%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1980%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0745%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6351280 real_backward_count 1788700  28.163%\n",
      "layer   1  Sparsity: 83.6182%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1961 occurrences\n",
      "train - Value 1: 2069 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 276 occurrences\n",
      "test - Value 1: 176 occurrences\n",
      "epoch-197 lr=['4.0000000'], tr/val_loss:178.725662/140.329315, val:  85.40%, val_best:  88.72%, tr:  95.83%, tr_best:  96.40%, epoch time: 230.76 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4105%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1719%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2729%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6383520 real_backward_count 1797265  28.155%\n",
      "layer   1  Sparsity: 80.9326%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1968 occurrences\n",
      "train - Value 1: 2062 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 205 occurrences\n",
      "test - Value 1: 247 occurrences\n",
      "epoch-198 lr=['4.0000000'], tr/val_loss:164.438171/ 88.284760, val:  83.85%, val_best:  88.72%, tr:  95.96%, tr_best:  96.40%, epoch time: 231.18 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4111%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1868%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1310%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6415760 real_backward_count 1805948  28.149%\n",
      "layer   1  Sparsity: 93.9453%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 94.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2000 occurrences\n",
      "train - Value 1: 2030 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 40 occurrences\n",
      "test - Value 1: 412 occurrences\n",
      "epoch-199 lr=['4.0000000'], tr/val_loss:180.499451/173.296524, val:  58.41%, val_best:  88.72%, tr:  95.61%, tr_best:  96.40%, epoch time: 230.67 seconds, 3.84 minutes\n",
      "layer   1  Sparsity: 84.4082%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1552%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8835%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111e7bfeb38545199562e6e0d7fa052d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñá‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñÅ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñà‚ñÅ‚ñÜ‚ñÇ‚ñà‚ñà‚ñÅ‚ñÜ‚ñÅ‚ñÖ‚ñÜ‚ñÉ‚ñÖ‚ñÇ‚ñá‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÉ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñá</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñá‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñÅ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñà‚ñÅ‚ñÜ‚ñÇ‚ñà‚ñà‚ñÅ‚ñÜ‚ñÅ‚ñÖ‚ñÜ‚ñÉ‚ñÖ‚ñÇ‚ñá‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÉ</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>0.0</td></tr><tr><td>tr_acc</td><td>0.95608</td></tr><tr><td>tr_epoch_loss</td><td>180.49945</td></tr><tr><td>val_acc_best</td><td>0.88717</td></tr><tr><td>val_acc_now</td><td>0.58407</td></tr><tr><td>val_loss</td><td>173.29652</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">confused-sweep-22</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/ocd0ibmx' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/ocd0ibmx</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251224_202144-ocd0ibmx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: sbkllp1v with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 2048\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloser_encourage_mode: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttimestep_sums_threshold: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: n_tidigits_tonic\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251225_093054-sbkllp1v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/sbkllp1v' target=\"_blank\">neat-sweep-15</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9lv70ttl' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9lv70ttl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9lv70ttl' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9lv70ttl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/sbkllp1v' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/sbkllp1v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'timestep_sums_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'loser_encourage_mode' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251225_093103_147', 'my_seed': 42, 'TIME': 8, 'BATCH': 1, 'IMAGE_SIZE': 8, 'which_data': 'n_tidigits_tonic', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 2048, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 1, 'dvs_duration': 2, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': False, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 8, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'timestep_sums_threshold': 0, 'lif_layer_sg_width2': 2, 'lif_layer_v_threshold2': 64, 'init_scaling': [0.5, 0.25, 0.0625], 'learning_rate': 2, 'learning_rate2': 8, 'loser_encourage_mode': True} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target word: 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target word: 2\n",
      "\n",
      "\n",
      "\n",
      "train_dataset length = 4030, test_dataset length = 452\n",
      "\n",
      "len(train_loader): 4030 BATCH: 1 train_data_count: 4030\n",
      "len(test_loader): 452 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 2048\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 2, self.v_threshold 64\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=512, out_features=200, TIME=8, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.25, 0.0625])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=2048, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=8, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=8, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.25, 0.0625])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=64, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=8, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=8, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.25, 0.0625])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 144,400\n",
      "========================================================\n",
      "\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 2\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 1594.0\n",
      "lif layer 1 self.abs_max_v: 1594.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "lif layer 1 self.abs_max_v: 2165.0\n",
      "fc layer 2 self.abs_max_out: 32.0\n",
      "lif layer 2 self.abs_max_v: 32.0\n",
      "layer   1  Sparsity: 88.8916%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 100.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 1880.0\n",
      "lif layer 1 self.abs_max_v: 2231.5\n",
      "fc layer 1 self.abs_max_out: 1887.0\n",
      "lif layer 1 self.abs_max_v: 2327.0\n",
      "fc layer 1 self.abs_max_out: 1912.0\n",
      "lif layer 1 self.abs_max_v: 2341.0\n",
      "lif layer 1 self.abs_max_v: 2423.5\n",
      "fc layer 1 self.abs_max_out: 2156.0\n",
      "lif layer 1 self.abs_max_v: 2807.5\n",
      "fc layer 2 self.abs_max_out: 57.0\n",
      "lif layer 2 self.abs_max_v: 57.0\n",
      "fc layer 1 self.abs_max_out: 2190.0\n",
      "fc layer 2 self.abs_max_out: 76.0\n",
      "lif layer 2 self.abs_max_v: 76.0\n",
      "fc layer 3 self.abs_max_out: 6.0\n",
      "fc layer 2 self.abs_max_out: 102.0\n",
      "lif layer 2 self.abs_max_v: 102.0\n",
      "fc layer 3 self.abs_max_out: 19.0\n",
      "lif layer 1 self.abs_max_v: 2946.0\n",
      "fc layer 2 self.abs_max_out: 131.0\n",
      "lif layer 2 self.abs_max_v: 120.5\n",
      "fc layer 3 self.abs_max_out: 34.0\n",
      "fc layer 2 self.abs_max_out: 145.0\n",
      "lif layer 2 self.abs_max_v: 145.0\n",
      "fc layer 3 self.abs_max_out: 41.0\n",
      "lif layer 1 self.abs_max_v: 3047.0\n",
      "fc layer 1 self.abs_max_out: 2308.0\n",
      "lif layer 1 self.abs_max_v: 3132.0\n",
      "lif layer 2 self.abs_max_v: 151.0\n",
      "fc layer 1 self.abs_max_out: 2509.0\n",
      "lif layer 1 self.abs_max_v: 3133.0\n",
      "fc layer 1 self.abs_max_out: 2530.0\n",
      "lif layer 2 self.abs_max_v: 161.0\n",
      "fc layer 1 self.abs_max_out: 2749.0\n",
      "lif layer 1 self.abs_max_v: 3497.5\n",
      "fc layer 1 self.abs_max_out: 2928.0\n",
      "fc layer 3 self.abs_max_out: 42.0\n",
      "fc layer 2 self.abs_max_out: 170.0\n",
      "lif layer 2 self.abs_max_v: 177.5\n",
      "fc layer 2 self.abs_max_out: 174.0\n",
      "lif layer 2 self.abs_max_v: 189.0\n",
      "lif layer 1 self.abs_max_v: 3645.0\n",
      "fc layer 3 self.abs_max_out: 52.0\n",
      "fc layer 1 self.abs_max_out: 3182.0\n",
      "fc layer 2 self.abs_max_out: 195.0\n",
      "lif layer 2 self.abs_max_v: 219.5\n",
      "fc layer 1 self.abs_max_out: 3229.0\n",
      "lif layer 1 self.abs_max_v: 3783.0\n",
      "fc layer 3 self.abs_max_out: 57.0\n",
      "fc layer 2 self.abs_max_out: 196.0\n",
      "lif layer 2 self.abs_max_v: 242.5\n",
      "lif layer 2 self.abs_max_v: 248.5\n",
      "fc layer 2 self.abs_max_out: 208.0\n",
      "fc layer 2 self.abs_max_out: 209.0\n",
      "lif layer 2 self.abs_max_v: 293.0\n",
      "fc layer 1 self.abs_max_out: 3473.0\n",
      "lif layer 1 self.abs_max_v: 4474.5\n",
      "fc layer 1 self.abs_max_out: 3780.0\n",
      "lif layer 1 self.abs_max_v: 4628.0\n",
      "fc layer 1 self.abs_max_out: 3880.0\n",
      "fc layer 3 self.abs_max_out: 67.0\n",
      "fc layer 2 self.abs_max_out: 212.0\n",
      "fc layer 2 self.abs_max_out: 216.0\n",
      "fc layer 1 self.abs_max_out: 3917.0\n",
      "fc layer 2 self.abs_max_out: 217.0\n",
      "fc layer 2 self.abs_max_out: 219.0\n",
      "fc layer 1 self.abs_max_out: 4011.0\n",
      "lif layer 1 self.abs_max_v: 4755.0\n",
      "fc layer 2 self.abs_max_out: 222.0\n",
      "fc layer 1 self.abs_max_out: 4366.0\n",
      "lif layer 2 self.abs_max_v: 293.5\n",
      "lif layer 2 self.abs_max_v: 300.5\n",
      "fc layer 1 self.abs_max_out: 4846.0\n",
      "lif layer 1 self.abs_max_v: 4895.5\n",
      "fc layer 1 self.abs_max_out: 4927.0\n",
      "lif layer 1 self.abs_max_v: 4927.0\n",
      "fc layer 1 self.abs_max_out: 4962.0\n",
      "lif layer 1 self.abs_max_v: 4962.0\n",
      "fc layer 2 self.abs_max_out: 224.0\n",
      "fc layer 1 self.abs_max_out: 5059.0\n",
      "lif layer 1 self.abs_max_v: 5059.0\n",
      "lif layer 1 self.abs_max_v: 5596.5\n",
      "lif layer 2 self.abs_max_v: 326.0\n",
      "lif layer 2 self.abs_max_v: 329.5\n",
      "fc layer 2 self.abs_max_out: 233.0\n",
      "fc layer 3 self.abs_max_out: 76.0\n",
      "lif layer 1 self.abs_max_v: 5638.0\n",
      "fc layer 1 self.abs_max_out: 5132.0\n",
      "fc layer 1 self.abs_max_out: 5154.0\n",
      "lif layer 2 self.abs_max_v: 339.0\n",
      "fc layer 1 self.abs_max_out: 5181.0\n",
      "lif layer 2 self.abs_max_v: 346.5\n",
      "lif layer 2 self.abs_max_v: 348.0\n",
      "fc layer 2 self.abs_max_out: 236.0\n",
      "lif layer 1 self.abs_max_v: 5734.5\n",
      "fc layer 1 self.abs_max_out: 5257.0\n",
      "lif layer 1 self.abs_max_v: 5861.5\n",
      "fc layer 2 self.abs_max_out: 237.0\n",
      "fc layer 2 self.abs_max_out: 251.0\n",
      "fc layer 1 self.abs_max_out: 5791.0\n",
      "lif layer 1 self.abs_max_v: 5943.0\n",
      "lif layer 1 self.abs_max_v: 6190.0\n",
      "fc layer 2 self.abs_max_out: 259.0\n",
      "fc layer 1 self.abs_max_out: 5825.0\n",
      "fc layer 2 self.abs_max_out: 275.0\n",
      "lif layer 2 self.abs_max_v: 357.0\n",
      "fc layer 2 self.abs_max_out: 289.0\n",
      "lif layer 1 self.abs_max_v: 6561.0\n",
      "fc layer 1 self.abs_max_out: 6169.0\n",
      "lif layer 1 self.abs_max_v: 6651.0\n",
      "fc layer 1 self.abs_max_out: 6291.0\n",
      "fc layer 1 self.abs_max_out: 6475.0\n",
      "lif layer 2 self.abs_max_v: 393.0\n",
      "lif layer 1 self.abs_max_v: 6749.5\n",
      "lif layer 1 self.abs_max_v: 7139.0\n",
      "fc layer 1 self.abs_max_out: 6480.0\n",
      "fc layer 1 self.abs_max_out: 6637.0\n",
      "fc layer 1 self.abs_max_out: 6773.0\n",
      "lif layer 1 self.abs_max_v: 7183.5\n",
      "fc layer 1 self.abs_max_out: 7012.0\n",
      "fc layer 1 self.abs_max_out: 7133.0\n",
      "lif layer 1 self.abs_max_v: 7858.0\n",
      "fc layer 1 self.abs_max_out: 7218.0\n",
      "fc layer 1 self.abs_max_out: 7309.0\n",
      "lif layer 1 self.abs_max_v: 8034.5\n",
      "fc layer 1 self.abs_max_out: 7380.0\n",
      "fc layer 1 self.abs_max_out: 7684.0\n",
      "fc layer 1 self.abs_max_out: 7799.0\n",
      "fc layer 1 self.abs_max_out: 8103.0\n",
      "lif layer 1 self.abs_max_v: 8103.0\n",
      "fc layer 1 self.abs_max_out: 8214.0\n",
      "lif layer 1 self.abs_max_v: 8214.0\n",
      "lif layer 1 self.abs_max_v: 8424.0\n",
      "fc layer 2 self.abs_max_out: 300.0\n",
      "fc layer 1 self.abs_max_out: 8494.0\n",
      "lif layer 1 self.abs_max_v: 8494.0\n",
      "lif layer 1 self.abs_max_v: 8603.0\n",
      "fc layer 3 self.abs_max_out: 81.0\n",
      "fc layer 1 self.abs_max_out: 8538.0\n",
      "lif layer 2 self.abs_max_v: 400.0\n",
      "lif layer 1 self.abs_max_v: 9016.5\n",
      "fc layer 1 self.abs_max_out: 8922.0\n",
      "fc layer 1 self.abs_max_out: 9116.0\n",
      "lif layer 1 self.abs_max_v: 9116.0\n",
      "fc layer 1 self.abs_max_out: 9141.0\n",
      "lif layer 1 self.abs_max_v: 9141.0\n",
      "fc layer 1 self.abs_max_out: 9276.0\n",
      "lif layer 1 self.abs_max_v: 9276.0\n",
      "lif layer 1 self.abs_max_v: 9338.5\n",
      "lif layer 2 self.abs_max_v: 475.0\n",
      "fc layer 3 self.abs_max_out: 91.0\n",
      "fc layer 1 self.abs_max_out: 9335.0\n",
      "fc layer 1 self.abs_max_out: 9477.0\n",
      "lif layer 1 self.abs_max_v: 9477.0\n",
      "fc layer 2 self.abs_max_out: 305.0\n",
      "lif layer 1 self.abs_max_v: 9916.0\n",
      "fc layer 2 self.abs_max_out: 317.0\n",
      "lif layer 1 self.abs_max_v: 10022.0\n",
      "fc layer 1 self.abs_max_out: 9545.0\n",
      "fc layer 1 self.abs_max_out: 9575.0\n",
      "lif layer 1 self.abs_max_v: 10532.5\n",
      "fc layer 1 self.abs_max_out: 9855.0\n",
      "fc layer 1 self.abs_max_out: 10077.0\n",
      "fc layer 1 self.abs_max_out: 10090.0\n",
      "fc layer 1 self.abs_max_out: 10209.0\n",
      "fc layer 2 self.abs_max_out: 336.0\n",
      "lif layer 2 self.abs_max_v: 519.0\n",
      "fc layer 2 self.abs_max_out: 339.0\n",
      "fc layer 1 self.abs_max_out: 10340.0\n",
      "fc layer 2 self.abs_max_out: 353.0\n",
      "fc layer 2 self.abs_max_out: 355.0\n",
      "fc layer 2 self.abs_max_out: 362.0\n",
      "lif layer 2 self.abs_max_v: 537.5\n",
      "lif layer 2 self.abs_max_v: 568.0\n",
      "lif layer 2 self.abs_max_v: 573.5\n",
      "fc layer 2 self.abs_max_out: 369.0\n",
      "fc layer 3 self.abs_max_out: 93.0\n",
      "fc layer 2 self.abs_max_out: 370.0\n",
      "fc layer 2 self.abs_max_out: 384.0\n",
      "fc layer 1 self.abs_max_out: 10344.0\n",
      "fc layer 1 self.abs_max_out: 10509.0\n",
      "fc layer 2 self.abs_max_out: 401.0\n",
      "lif layer 2 self.abs_max_v: 631.5\n",
      "lif layer 1 self.abs_max_v: 10742.0\n",
      "lif layer 1 self.abs_max_v: 10774.5\n",
      "fc layer 1 self.abs_max_out: 11156.0\n",
      "lif layer 1 self.abs_max_v: 11156.0\n",
      "fc layer 2 self.abs_max_out: 418.0\n",
      "fc layer 2 self.abs_max_out: 433.0\n",
      "lif layer 2 self.abs_max_v: 654.0\n",
      "fc layer 2 self.abs_max_out: 438.0\n",
      "lif layer 2 self.abs_max_v: 656.5\n",
      "fc layer 2 self.abs_max_out: 445.0\n",
      "lif layer 2 self.abs_max_v: 717.0\n",
      "fc layer 1 self.abs_max_out: 11227.0\n",
      "lif layer 1 self.abs_max_v: 11227.0\n",
      "fc layer 1 self.abs_max_out: 11923.0\n",
      "lif layer 1 self.abs_max_v: 11923.0\n",
      "fc layer 2 self.abs_max_out: 455.0\n",
      "fc layer 1 self.abs_max_out: 12013.0\n",
      "lif layer 1 self.abs_max_v: 12013.0\n",
      "lif layer 2 self.abs_max_v: 742.0\n",
      "fc layer 2 self.abs_max_out: 481.0\n",
      "lif layer 2 self.abs_max_v: 789.5\n",
      "lif layer 2 self.abs_max_v: 795.5\n",
      "fc layer 2 self.abs_max_out: 488.0\n",
      "fc layer 1 self.abs_max_out: 12423.0\n",
      "lif layer 1 self.abs_max_v: 12423.0\n",
      "fc layer 2 self.abs_max_out: 490.0\n",
      "lif layer 1 self.abs_max_v: 12555.0\n",
      "fc layer 1 self.abs_max_out: 12578.0\n",
      "lif layer 1 self.abs_max_v: 12578.0\n",
      "fc layer 3 self.abs_max_out: 95.0\n",
      "fc layer 2 self.abs_max_out: 495.0\n",
      "fc layer 2 self.abs_max_out: 516.0\n",
      "fc layer 1 self.abs_max_out: 13077.0\n",
      "lif layer 1 self.abs_max_v: 13077.0\n",
      "fc layer 2 self.abs_max_out: 551.0\n",
      "fc layer 3 self.abs_max_out: 104.0\n",
      "fc layer 1 self.abs_max_out: 13143.0\n",
      "lif layer 1 self.abs_max_v: 13143.0\n",
      "fc layer 1 self.abs_max_out: 13251.0\n",
      "lif layer 1 self.abs_max_v: 14037.5\n",
      "fc layer 2 self.abs_max_out: 561.0\n",
      "lif layer 2 self.abs_max_v: 818.5\n",
      "lif layer 2 self.abs_max_v: 876.5\n",
      "fc layer 1 self.abs_max_out: 13422.0\n",
      "lif layer 1 self.abs_max_v: 14219.0\n",
      "fc layer 2 self.abs_max_out: 565.0\n",
      "fc layer 1 self.abs_max_out: 13575.0\n",
      "fc layer 3 self.abs_max_out: 105.0\n",
      "fc layer 3 self.abs_max_out: 107.0\n",
      "fc layer 3 self.abs_max_out: 110.0\n",
      "fc layer 1 self.abs_max_out: 13892.0\n",
      "lif layer 2 self.abs_max_v: 938.5\n",
      "fc layer 1 self.abs_max_out: 14241.0\n",
      "lif layer 1 self.abs_max_v: 14241.0\n",
      "lif layer 1 self.abs_max_v: 14775.0\n",
      "lif layer 2 self.abs_max_v: 1002.0\n",
      "lif layer 2 self.abs_max_v: 1016.5\n",
      "lif layer 2 self.abs_max_v: 1050.5\n",
      "fc layer 1 self.abs_max_out: 15108.0\n",
      "lif layer 1 self.abs_max_v: 15108.0\n",
      "fc layer 2 self.abs_max_out: 571.0\n",
      "fc layer 3 self.abs_max_out: 114.0\n",
      "fc layer 2 self.abs_max_out: 587.0\n",
      "fc layer 3 self.abs_max_out: 118.0\n",
      "fc layer 2 self.abs_max_out: 618.0\n",
      "fc layer 1 self.abs_max_out: 15505.0\n",
      "lif layer 1 self.abs_max_v: 15505.0\n",
      "fc layer 2 self.abs_max_out: 622.0\n",
      "lif layer 2 self.abs_max_v: 1066.0\n",
      "lif layer 2 self.abs_max_v: 1069.0\n",
      "lif layer 2 self.abs_max_v: 1075.5\n",
      "fc layer 3 self.abs_max_out: 126.0\n",
      "fc layer 1 self.abs_max_out: 15906.0\n",
      "lif layer 1 self.abs_max_v: 15906.0\n",
      "fc layer 3 self.abs_max_out: 127.0\n",
      "fc layer 1 self.abs_max_out: 16009.0\n",
      "lif layer 1 self.abs_max_v: 16009.0\n",
      "fc layer 1 self.abs_max_out: 16010.0\n",
      "lif layer 1 self.abs_max_v: 16010.0\n",
      "fc layer 2 self.abs_max_out: 635.0\n",
      "fc layer 1 self.abs_max_out: 16483.0\n",
      "lif layer 1 self.abs_max_v: 16483.0\n",
      "fc layer 2 self.abs_max_out: 659.0\n",
      "lif layer 2 self.abs_max_v: 1089.0\n",
      "fc layer 1 self.abs_max_out: 16521.0\n",
      "lif layer 1 self.abs_max_v: 16521.0\n",
      "fc layer 2 self.abs_max_out: 660.0\n",
      "fc layer 2 self.abs_max_out: 664.0\n",
      "fc layer 2 self.abs_max_out: 672.0\n",
      "fc layer 2 self.abs_max_out: 674.0\n",
      "lif layer 2 self.abs_max_v: 1146.5\n",
      "fc layer 2 self.abs_max_out: 680.0\n",
      "lif layer 2 self.abs_max_v: 1208.5\n",
      "fc layer 2 self.abs_max_out: 687.0\n",
      "lif layer 2 self.abs_max_v: 1235.0\n",
      "fc layer 2 self.abs_max_out: 695.0\n",
      "fc layer 2 self.abs_max_out: 707.0\n",
      "fc layer 2 self.abs_max_out: 721.0\n",
      "fc layer 1 self.abs_max_out: 16920.0\n",
      "lif layer 1 self.abs_max_v: 16920.0\n",
      "fc layer 2 self.abs_max_out: 732.0\n",
      "lif layer 2 self.abs_max_v: 1236.5\n",
      "lif layer 1 self.abs_max_v: 17536.0\n",
      "fc layer 1 self.abs_max_out: 16932.0\n",
      "lif layer 2 self.abs_max_v: 1253.5\n",
      "fc layer 1 self.abs_max_out: 17106.0\n",
      "fc layer 1 self.abs_max_out: 17404.0\n",
      "fc layer 2 self.abs_max_out: 733.0\n",
      "fc layer 2 self.abs_max_out: 734.0\n",
      "fc layer 2 self.abs_max_out: 754.0\n",
      "fc layer 2 self.abs_max_out: 766.0\n",
      "fc layer 1 self.abs_max_out: 17577.0\n",
      "lif layer 1 self.abs_max_v: 17577.0\n",
      "lif layer 2 self.abs_max_v: 1298.0\n",
      "fc layer 2 self.abs_max_out: 771.0\n",
      "fc layer 1 self.abs_max_out: 17894.0\n",
      "lif layer 1 self.abs_max_v: 17894.0\n",
      "fc layer 2 self.abs_max_out: 775.0\n",
      "fc layer 2 self.abs_max_out: 777.0\n",
      "lif layer 2 self.abs_max_v: 1328.0\n",
      "lif layer 2 self.abs_max_v: 1366.0\n",
      "lif layer 2 self.abs_max_v: 1425.0\n",
      "fc layer 2 self.abs_max_out: 778.0\n",
      "fc layer 2 self.abs_max_out: 781.0\n",
      "fc layer 2 self.abs_max_out: 785.0\n",
      "fc layer 2 self.abs_max_out: 822.0\n",
      "lif layer 2 self.abs_max_v: 1467.5\n",
      "fc layer 1 self.abs_max_out: 18191.0\n",
      "lif layer 1 self.abs_max_v: 18191.0\n",
      "lif layer 1 self.abs_max_v: 18520.5\n",
      "fc layer 1 self.abs_max_out: 18237.0\n",
      "fc layer 2 self.abs_max_out: 835.0\n",
      "fc layer 2 self.abs_max_out: 836.0\n",
      "fc layer 2 self.abs_max_out: 843.0\n",
      "lif layer 1 self.abs_max_v: 18699.5\n",
      "fc layer 2 self.abs_max_out: 845.0\n",
      "fc layer 1 self.abs_max_out: 18279.0\n",
      "fc layer 2 self.abs_max_out: 853.0\n",
      "fc layer 2 self.abs_max_out: 855.0\n",
      "train - Value 0: 1976 occurrences\n",
      "train - Value 1: 2054 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 90.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 241.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 243.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 264.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 286.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 313.00 at epoch 0, iter 4029\n",
      "lif layer 2 self.abs_max_v: 1495.0\n",
      "max_activation_accul updated: 348.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 432.00 at epoch 0, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 5 occurrences\n",
      "test - Value 1: 447 occurrences\n",
      "epoch-0   lr=['2.0000000'], tr/val_loss: 17.172630/ 16.586092, val:  50.66%, val_best:  50.66%, tr:  68.56%, tr_best:  68.56%, epoch time: 233.43 seconds, 3.89 minutes\n",
      "layer   1  Sparsity: 84.4093%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.0096%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.7386%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 32240 real_backward_count 12937  40.127%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "layer   1  Sparsity: 84.8389%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 1554.5\n",
      "lif layer 2 self.abs_max_v: 1563.5\n",
      "lif layer 2 self.abs_max_v: 1597.0\n",
      "lif layer 1 self.abs_max_v: 19091.5\n",
      "fc layer 2 self.abs_max_out: 871.0\n",
      "fc layer 1 self.abs_max_out: 18844.0\n",
      "fc layer 1 self.abs_max_out: 18902.0\n",
      "fc layer 2 self.abs_max_out: 885.0\n",
      "fc layer 1 self.abs_max_out: 19019.0\n",
      "lif layer 1 self.abs_max_v: 19596.5\n",
      "fc layer 1 self.abs_max_out: 19349.0\n",
      "fc layer 1 self.abs_max_out: 19417.0\n",
      "fc layer 1 self.abs_max_out: 19475.0\n",
      "lif layer 1 self.abs_max_v: 19904.5\n",
      "fc layer 1 self.abs_max_out: 19892.0\n",
      "fc layer 1 self.abs_max_out: 19973.0\n",
      "lif layer 1 self.abs_max_v: 19973.0\n",
      "fc layer 1 self.abs_max_out: 20205.0\n",
      "lif layer 1 self.abs_max_v: 20205.0\n",
      "fc layer 1 self.abs_max_out: 20486.0\n",
      "lif layer 1 self.abs_max_v: 20486.0\n",
      "fc layer 2 self.abs_max_out: 890.0\n",
      "fc layer 2 self.abs_max_out: 895.0\n",
      "fc layer 2 self.abs_max_out: 900.0\n",
      "fc layer 2 self.abs_max_out: 916.0\n",
      "fc layer 2 self.abs_max_out: 930.0\n",
      "fc layer 2 self.abs_max_out: 937.0\n",
      "lif layer 2 self.abs_max_v: 1599.0\n",
      "fc layer 2 self.abs_max_out: 938.0\n",
      "fc layer 1 self.abs_max_out: 20570.0\n",
      "lif layer 1 self.abs_max_v: 20570.0\n",
      "fc layer 2 self.abs_max_out: 949.0\n",
      "fc layer 1 self.abs_max_out: 20621.0\n",
      "lif layer 1 self.abs_max_v: 20621.0\n",
      "fc layer 2 self.abs_max_out: 951.0\n",
      "lif layer 2 self.abs_max_v: 1656.5\n",
      "fc layer 3 self.abs_max_out: 131.0\n",
      "fc layer 1 self.abs_max_out: 20859.0\n",
      "lif layer 1 self.abs_max_v: 20859.0\n",
      "fc layer 3 self.abs_max_out: 132.0\n",
      "fc layer 3 self.abs_max_out: 133.0\n",
      "fc layer 3 self.abs_max_out: 136.0\n",
      "fc layer 1 self.abs_max_out: 20936.0\n",
      "lif layer 1 self.abs_max_v: 20936.0\n",
      "lif layer 2 self.abs_max_v: 1662.5\n",
      "fc layer 2 self.abs_max_out: 953.0\n",
      "fc layer 2 self.abs_max_out: 966.0\n",
      "lif layer 2 self.abs_max_v: 1724.5\n",
      "fc layer 2 self.abs_max_out: 974.0\n",
      "fc layer 2 self.abs_max_out: 990.0\n",
      "lif layer 2 self.abs_max_v: 1746.5\n",
      "fc layer 2 self.abs_max_out: 995.0\n",
      "fc layer 2 self.abs_max_out: 1001.0\n",
      "lif layer 2 self.abs_max_v: 1768.0\n",
      "fc layer 1 self.abs_max_out: 21270.0\n",
      "lif layer 1 self.abs_max_v: 21270.0\n",
      "fc layer 1 self.abs_max_out: 22096.0\n",
      "lif layer 1 self.abs_max_v: 22096.0\n",
      "fc layer 2 self.abs_max_out: 1006.0\n",
      "fc layer 2 self.abs_max_out: 1011.0\n",
      "lif layer 2 self.abs_max_v: 1814.0\n",
      "fc layer 2 self.abs_max_out: 1036.0\n",
      "lif layer 2 self.abs_max_v: 1898.5\n",
      "fc layer 2 self.abs_max_out: 1045.0\n",
      "lif layer 2 self.abs_max_v: 1935.5\n",
      "fc layer 2 self.abs_max_out: 1076.0\n",
      "fc layer 2 self.abs_max_out: 1087.0\n",
      "fc layer 2 self.abs_max_out: 1091.0\n",
      "fc layer 2 self.abs_max_out: 1096.0\n",
      "fc layer 2 self.abs_max_out: 1102.0\n",
      "lif layer 2 self.abs_max_v: 1940.0\n",
      "lif layer 2 self.abs_max_v: 1957.0\n",
      "fc layer 2 self.abs_max_out: 1103.0\n",
      "fc layer 1 self.abs_max_out: 22226.0\n",
      "lif layer 1 self.abs_max_v: 22226.0\n",
      "fc layer 1 self.abs_max_out: 22374.0\n",
      "lif layer 1 self.abs_max_v: 22374.0\n",
      "fc layer 2 self.abs_max_out: 1105.0\n",
      "lif layer 2 self.abs_max_v: 1961.0\n",
      "lif layer 2 self.abs_max_v: 2046.5\n",
      "fc layer 2 self.abs_max_out: 1133.0\n",
      "fc layer 1 self.abs_max_out: 22411.0\n",
      "lif layer 1 self.abs_max_v: 22411.0\n",
      "fc layer 1 self.abs_max_out: 22480.0\n",
      "lif layer 1 self.abs_max_v: 22480.0\n",
      "fc layer 1 self.abs_max_out: 22498.0\n",
      "lif layer 1 self.abs_max_v: 22498.0\n",
      "fc layer 1 self.abs_max_out: 22647.0\n",
      "lif layer 1 self.abs_max_v: 22647.0\n",
      "fc layer 1 self.abs_max_out: 22665.0\n",
      "lif layer 1 self.abs_max_v: 22665.0\n",
      "fc layer 1 self.abs_max_out: 22763.0\n",
      "lif layer 1 self.abs_max_v: 22763.0\n",
      "fc layer 1 self.abs_max_out: 22828.0\n",
      "lif layer 1 self.abs_max_v: 22828.0\n",
      "train - Value 0: 1982 occurrences\n",
      "train - Value 1: 2048 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 440.00 at epoch 1, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 186 occurrences\n",
      "test - Value 1: 266 occurrences\n",
      "epoch-1   lr=['2.0000000'], tr/val_loss: 22.074635/ 22.190512, val:  71.68%, val_best:  71.68%, tr:  76.25%, tr_best:  76.25%, epoch time: 232.64 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4102%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.4974%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.2083%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 64480 real_backward_count 24274  37.646%\n",
      "layer   1  Sparsity: 84.0088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1141.0\n",
      "fc layer 2 self.abs_max_out: 1154.0\n",
      "fc layer 1 self.abs_max_out: 23006.0\n",
      "lif layer 1 self.abs_max_v: 23006.0\n",
      "fc layer 1 self.abs_max_out: 23207.0\n",
      "lif layer 1 self.abs_max_v: 23207.0\n",
      "fc layer 1 self.abs_max_out: 23395.0\n",
      "lif layer 1 self.abs_max_v: 23395.0\n",
      "fc layer 1 self.abs_max_out: 23486.0\n",
      "lif layer 1 self.abs_max_v: 23486.0\n",
      "fc layer 1 self.abs_max_out: 23605.0\n",
      "lif layer 1 self.abs_max_v: 23605.0\n",
      "fc layer 1 self.abs_max_out: 23668.0\n",
      "lif layer 1 self.abs_max_v: 23668.0\n",
      "fc layer 1 self.abs_max_out: 23803.0\n",
      "lif layer 1 self.abs_max_v: 23803.0\n",
      "fc layer 1 self.abs_max_out: 24503.0\n",
      "lif layer 1 self.abs_max_v: 24503.0\n",
      "fc layer 2 self.abs_max_out: 1164.0\n",
      "fc layer 2 self.abs_max_out: 1177.0\n",
      "fc layer 2 self.abs_max_out: 1183.0\n",
      "fc layer 2 self.abs_max_out: 1184.0\n",
      "fc layer 1 self.abs_max_out: 24614.0\n",
      "lif layer 1 self.abs_max_v: 24614.0\n",
      "fc layer 2 self.abs_max_out: 1204.0\n",
      "fc layer 1 self.abs_max_out: 24701.0\n",
      "lif layer 1 self.abs_max_v: 24701.0\n",
      "fc layer 1 self.abs_max_out: 24804.0\n",
      "lif layer 1 self.abs_max_v: 24804.0\n",
      "fc layer 1 self.abs_max_out: 24893.0\n",
      "lif layer 1 self.abs_max_v: 24893.0\n",
      "fc layer 2 self.abs_max_out: 1211.0\n",
      "fc layer 2 self.abs_max_out: 1257.0\n",
      "fc layer 2 self.abs_max_out: 1282.0\n",
      "fc layer 2 self.abs_max_out: 1299.0\n",
      "train - Value 0: 1896 occurrences\n",
      "train - Value 1: 2134 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 451 occurrences\n",
      "test - Value 1: 1 occurrences\n",
      "epoch-2   lr=['2.0000000'], tr/val_loss: 19.952002/ 11.122515, val:  50.22%, val_best:  71.68%, tr:  78.24%, tr_best:  78.24%, epoch time: 232.21 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.5479%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2752%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 96720 real_backward_count 35506  36.710%\n",
      "layer   1  Sparsity: 78.8330%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 25096.0\n",
      "lif layer 1 self.abs_max_v: 25096.0\n",
      "fc layer 2 self.abs_max_out: 1302.0\n",
      "fc layer 2 self.abs_max_out: 1316.0\n",
      "fc layer 1 self.abs_max_out: 25203.0\n",
      "lif layer 1 self.abs_max_v: 25203.0\n",
      "fc layer 1 self.abs_max_out: 25553.0\n",
      "lif layer 1 self.abs_max_v: 25553.0\n",
      "fc layer 2 self.abs_max_out: 1338.0\n",
      "fc layer 2 self.abs_max_out: 1346.0\n",
      "fc layer 1 self.abs_max_out: 25576.0\n",
      "lif layer 1 self.abs_max_v: 25576.0\n",
      "fc layer 1 self.abs_max_out: 25718.0\n",
      "lif layer 1 self.abs_max_v: 25718.0\n",
      "fc layer 2 self.abs_max_out: 1369.0\n",
      "fc layer 1 self.abs_max_out: 25859.0\n",
      "lif layer 1 self.abs_max_v: 25859.0\n",
      "fc layer 1 self.abs_max_out: 25866.0\n",
      "lif layer 1 self.abs_max_v: 25866.0\n",
      "fc layer 1 self.abs_max_out: 25894.0\n",
      "lif layer 1 self.abs_max_v: 25894.0\n",
      "fc layer 1 self.abs_max_out: 26088.0\n",
      "lif layer 1 self.abs_max_v: 26088.0\n",
      "fc layer 1 self.abs_max_out: 26109.0\n",
      "lif layer 1 self.abs_max_v: 26109.0\n",
      "fc layer 1 self.abs_max_out: 26832.0\n",
      "lif layer 1 self.abs_max_v: 26832.0\n",
      "train - Value 0: 1945 occurrences\n",
      "train - Value 1: 2085 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 448.00 at epoch 3, iter 4029\n",
      "max_activation_accul updated: 503.00 at epoch 3, iter 4029\n",
      "max_activation_accul updated: 518.00 at epoch 3, iter 4029\n",
      "max_activation_accul updated: 655.00 at epoch 3, iter 4029\n",
      "max_activation_accul updated: 697.00 at epoch 3, iter 4029\n",
      "max_activation_accul updated: 733.00 at epoch 3, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 1 occurrences\n",
      "test - Value 1: 451 occurrences\n",
      "epoch-3   lr=['2.0000000'], tr/val_loss: 22.593630/ 39.738079, val:  50.22%, val_best:  71.68%, tr:  82.03%, tr_best:  82.03%, epoch time: 232.37 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4116%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.5177%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.7534%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 128960 real_backward_count 46313  35.913%\n",
      "layer   1  Sparsity: 70.6787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 2048.0\n",
      "lif layer 2 self.abs_max_v: 2113.0\n",
      "lif layer 2 self.abs_max_v: 2176.0\n",
      "fc layer 1 self.abs_max_out: 27372.0\n",
      "lif layer 1 self.abs_max_v: 27372.0\n",
      "lif layer 2 self.abs_max_v: 2217.0\n",
      "fc layer 3 self.abs_max_out: 143.0\n",
      "lif layer 2 self.abs_max_v: 2225.5\n",
      "lif layer 2 self.abs_max_v: 2253.0\n",
      "fc layer 2 self.abs_max_out: 1374.0\n",
      "fc layer 1 self.abs_max_out: 27942.0\n",
      "lif layer 1 self.abs_max_v: 27942.0\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-4   lr=['2.0000000'], tr/val_loss: 25.178917/ 12.174772, val:  50.00%, val_best:  71.68%, tr:  82.18%, tr_best:  82.18%, epoch time: 230.28 seconds, 3.84 minutes\n",
      "layer   1  Sparsity: 84.4134%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.4277%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.6200%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 161200 real_backward_count 57044  35.387%\n",
      "layer   1  Sparsity: 84.2285%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1409.0\n",
      "fc layer 1 self.abs_max_out: 27989.0\n",
      "lif layer 1 self.abs_max_v: 27989.0\n",
      "fc layer 2 self.abs_max_out: 1485.0\n",
      "fc layer 1 self.abs_max_out: 28181.0\n",
      "lif layer 1 self.abs_max_v: 28181.0\n",
      "fc layer 2 self.abs_max_out: 1490.0\n",
      "fc layer 2 self.abs_max_out: 1491.0\n",
      "fc layer 2 self.abs_max_out: 1512.0\n",
      "fc layer 3 self.abs_max_out: 152.0\n",
      "fc layer 2 self.abs_max_out: 1519.0\n",
      "fc layer 2 self.abs_max_out: 1525.0\n",
      "fc layer 2 self.abs_max_out: 1543.0\n",
      "fc layer 2 self.abs_max_out: 1578.0\n",
      "fc layer 2 self.abs_max_out: 1583.0\n",
      "train - Value 0: 2034 occurrences\n",
      "train - Value 1: 1996 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 2 self.abs_max_out: 1601.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 48 occurrences\n",
      "test - Value 1: 404 occurrences\n",
      "epoch-5   lr=['2.0000000'], tr/val_loss: 24.235754/ 30.661627, val:  60.18%, val_best:  71.68%, tr:  84.24%, tr_best:  84.24%, epoch time: 231.42 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.4414%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.3267%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 193440 real_backward_count 67557  34.924%\n",
      "layer   1  Sparsity: 84.3262%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1627.0\n",
      "fc layer 2 self.abs_max_out: 1632.0\n",
      "fc layer 1 self.abs_max_out: 28216.0\n",
      "lif layer 1 self.abs_max_v: 28216.0\n",
      "fc layer 1 self.abs_max_out: 28297.0\n",
      "lif layer 1 self.abs_max_v: 28297.0\n",
      "fc layer 1 self.abs_max_out: 28321.0\n",
      "lif layer 1 self.abs_max_v: 28321.0\n",
      "fc layer 1 self.abs_max_out: 29105.0\n",
      "lif layer 1 self.abs_max_v: 29105.0\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 302 occurrences\n",
      "test - Value 1: 150 occurrences\n",
      "epoch-6   lr=['2.0000000'], tr/val_loss: 27.057367/ 20.806326, val:  80.09%, val_best:  80.09%, tr:  84.99%, tr_best:  84.99%, epoch time: 230.79 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.2836%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.9091%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225680 real_backward_count 78001  34.563%\n",
      "layer   1  Sparsity: 82.6172%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 163.0\n",
      "fc layer 3 self.abs_max_out: 173.0\n",
      "fc layer 1 self.abs_max_out: 29155.0\n",
      "lif layer 1 self.abs_max_v: 29155.0\n",
      "fc layer 1 self.abs_max_out: 29800.0\n",
      "lif layer 1 self.abs_max_v: 29800.0\n",
      "fc layer 1 self.abs_max_out: 30131.0\n",
      "lif layer 1 self.abs_max_v: 30131.0\n",
      "fc layer 3 self.abs_max_out: 178.0\n",
      "train - Value 0: 1956 occurrences\n",
      "train - Value 1: 2074 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 741.00 at epoch 7, iter 4029\n",
      "max_activation_accul updated: 829.00 at epoch 7, iter 4029\n",
      "max_activation_accul updated: 838.00 at epoch 7, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-7   lr=['2.0000000'], tr/val_loss: 32.511478/ 49.028748, val:  50.00%, val_best:  80.09%, tr:  86.33%, tr_best:  86.33%, epoch time: 231.93 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4107%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0933%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.6426%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 257920 real_backward_count 88107  34.161%\n",
      "layer   1  Sparsity: 71.6797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 190.0\n",
      "fc layer 1 self.abs_max_out: 30447.0\n",
      "lif layer 1 self.abs_max_v: 30447.0\n",
      "fc layer 1 self.abs_max_out: 30668.0\n",
      "lif layer 1 self.abs_max_v: 30668.0\n",
      "train - Value 0: 1956 occurrences\n",
      "train - Value 1: 2074 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 117 occurrences\n",
      "test - Value 1: 335 occurrences\n",
      "epoch-8   lr=['2.0000000'], tr/val_loss: 39.077187/ 35.938808, val:  69.69%, val_best:  80.09%, tr:  87.37%, tr_best:  87.37%, epoch time: 231.69 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4132%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0579%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.4724%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 290160 real_backward_count 98065  33.797%\n",
      "layer   1  Sparsity: 78.1250%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1644.0\n",
      "fc layer 2 self.abs_max_out: 1647.0\n",
      "fc layer 2 self.abs_max_out: 1656.0\n",
      "fc layer 2 self.abs_max_out: 1710.0\n",
      "fc layer 2 self.abs_max_out: 1726.0\n",
      "fc layer 3 self.abs_max_out: 204.0\n",
      "fc layer 3 self.abs_max_out: 208.0\n",
      "fc layer 3 self.abs_max_out: 209.0\n",
      "fc layer 1 self.abs_max_out: 31140.0\n",
      "lif layer 1 self.abs_max_v: 31140.0\n",
      "train - Value 0: 2004 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 309 occurrences\n",
      "test - Value 1: 143 occurrences\n",
      "epoch-9   lr=['2.0000000'], tr/val_loss: 47.126484/ 25.613867, val:  80.75%, val_best:  80.75%, tr:  88.46%, tr_best:  88.46%, epoch time: 231.25 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4117%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1041%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4196%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 322400 real_backward_count 107780  33.431%\n",
      "layer   1  Sparsity: 77.7588%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 217.0\n",
      "fc layer 3 self.abs_max_out: 236.0\n",
      "fc layer 1 self.abs_max_out: 31463.0\n",
      "lif layer 1 self.abs_max_v: 31463.0\n",
      "fc layer 1 self.abs_max_out: 31692.0\n",
      "lif layer 1 self.abs_max_v: 31692.0\n",
      "fc layer 3 self.abs_max_out: 242.0\n",
      "fc layer 3 self.abs_max_out: 258.0\n",
      "train - Value 0: 1938 occurrences\n",
      "train - Value 1: 2092 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 299 occurrences\n",
      "test - Value 1: 153 occurrences\n",
      "epoch-10  lr=['2.0000000'], tr/val_loss: 51.411392/ 31.258768, val:  80.75%, val_best:  80.75%, tr:  87.92%, tr_best:  88.46%, epoch time: 227.40 seconds, 3.79 minutes\n",
      "layer   1  Sparsity: 84.4118%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0273%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6567%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 354640 real_backward_count 117459  33.121%\n",
      "layer   1  Sparsity: 80.5664%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 2303.0\n",
      "fc layer 1 self.abs_max_out: 32008.0\n",
      "lif layer 1 self.abs_max_v: 32008.0\n",
      "train - Value 0: 1984 occurrences\n",
      "train - Value 1: 2046 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 873.00 at epoch 11, iter 4029\n",
      "max_activation_accul updated: 914.00 at epoch 11, iter 4029\n",
      "max_activation_accul updated: 945.00 at epoch 11, iter 4029\n",
      "max_activation_accul updated: 977.00 at epoch 11, iter 4029\n",
      "max_activation_accul updated: 1008.00 at epoch 11, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 374 occurrences\n",
      "test - Value 1: 78 occurrences\n",
      "epoch-11  lr=['2.0000000'], tr/val_loss: 54.862629/ 73.187119, val:  67.26%, val_best:  80.75%, tr:  89.01%, tr_best:  89.01%, epoch time: 232.26 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1156%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.8455%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 386880 real_backward_count 127069  32.845%\n",
      "layer   1  Sparsity: 89.9658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 32215.0\n",
      "lif layer 1 self.abs_max_v: 32215.0\n",
      "fc layer 1 self.abs_max_out: 32417.0\n",
      "lif layer 1 self.abs_max_v: 32417.0\n",
      "train - Value 0: 1900 occurrences\n",
      "train - Value 1: 2130 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 1063.00 at epoch 12, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 333 occurrences\n",
      "test - Value 1: 119 occurrences\n",
      "epoch-12  lr=['2.0000000'], tr/val_loss: 58.837620/ 39.424679, val:  76.33%, val_best:  80.75%, tr:  89.35%, tr_best:  89.35%, epoch time: 231.54 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0665%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.7725%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 419120 real_backward_count 136736  32.625%\n",
      "layer   1  Sparsity: 84.4727%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 32459.0\n",
      "lif layer 1 self.abs_max_v: 32459.0\n",
      "fc layer 3 self.abs_max_out: 271.0\n",
      "train - Value 0: 1937 occurrences\n",
      "train - Value 1: 2093 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 1293.00 at epoch 13, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 109 occurrences\n",
      "test - Value 1: 343 occurrences\n",
      "epoch-13  lr=['2.0000000'], tr/val_loss: 60.678608/ 60.947002, val:  72.35%, val_best:  80.75%, tr:  89.73%, tr_best:  89.73%, epoch time: 231.07 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4103%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8980%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.4760%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 451360 real_backward_count 146244  32.401%\n",
      "layer   1  Sparsity: 94.7998%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 32744.0\n",
      "lif layer 1 self.abs_max_v: 32744.0\n",
      "fc layer 3 self.abs_max_out: 274.0\n",
      "fc layer 1 self.abs_max_out: 32923.0\n",
      "lif layer 1 self.abs_max_v: 32923.0\n",
      "fc layer 3 self.abs_max_out: 281.0\n",
      "train - Value 0: 1928 occurrences\n",
      "train - Value 1: 2102 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-14  lr=['2.0000000'], tr/val_loss: 62.791325/ 73.654282, val:  50.00%, val_best:  80.75%, tr:  89.95%, tr_best:  89.95%, epoch time: 230.99 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4080%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.7992%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.7632%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 483600 real_backward_count 155812  32.219%\n",
      "layer   1  Sparsity: 87.0605%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 33047.0\n",
      "lif layer 1 self.abs_max_v: 33047.0\n",
      "fc layer 1 self.abs_max_out: 33071.0\n",
      "lif layer 1 self.abs_max_v: 33071.0\n",
      "train - Value 0: 1926 occurrences\n",
      "train - Value 1: 2104 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 3 occurrences\n",
      "test - Value 1: 449 occurrences\n",
      "epoch-15  lr=['2.0000000'], tr/val_loss: 57.966877/ 65.640884, val:  50.66%, val_best:  80.75%, tr:  90.10%, tr_best:  90.10%, epoch time: 231.15 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4097%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.7865%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.0420%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 515840 real_backward_count 165238  32.033%\n",
      "layer   1  Sparsity: 85.1074%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 33259.0\n",
      "lif layer 1 self.abs_max_v: 33259.0\n",
      "fc layer 3 self.abs_max_out: 295.0\n",
      "train - Value 0: 1924 occurrences\n",
      "train - Value 1: 2106 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 311 occurrences\n",
      "test - Value 1: 141 occurrences\n",
      "epoch-16  lr=['2.0000000'], tr/val_loss: 58.065098/ 47.280708, val:  80.31%, val_best:  80.75%, tr:  89.65%, tr_best:  90.10%, epoch time: 231.57 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4102%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.7511%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.4361%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548080 real_backward_count 174780  31.890%\n",
      "layer   1  Sparsity: 86.4014%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 33440.0\n",
      "lif layer 1 self.abs_max_v: 33440.0\n",
      "fc layer 1 self.abs_max_out: 33454.0\n",
      "lif layer 1 self.abs_max_v: 33454.0\n",
      "train - Value 0: 1864 occurrences\n",
      "train - Value 1: 2166 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 1295.00 at epoch 17, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 8 occurrences\n",
      "test - Value 1: 444 occurrences\n",
      "epoch-17  lr=['2.0000000'], tr/val_loss: 60.918095/ 45.045052, val:  51.77%, val_best:  80.75%, tr:  90.05%, tr_best:  90.10%, epoch time: 231.02 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4099%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8858%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.8502%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 580320 real_backward_count 184153  31.733%\n",
      "layer   1  Sparsity: 79.7119%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 33544.0\n",
      "lif layer 1 self.abs_max_v: 33544.0\n",
      "fc layer 3 self.abs_max_out: 303.0\n",
      "fc layer 3 self.abs_max_out: 313.0\n",
      "train - Value 0: 1897 occurrences\n",
      "train - Value 1: 2133 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 88 occurrences\n",
      "test - Value 1: 364 occurrences\n",
      "epoch-18  lr=['2.0000000'], tr/val_loss: 64.186478/ 52.131268, val:  68.14%, val_best:  80.75%, tr:  89.93%, tr_best:  90.10%, epoch time: 230.17 seconds, 3.84 minutes\n",
      "layer   1  Sparsity: 84.4114%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.9247%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.4390%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 612560 real_backward_count 193408  31.574%\n",
      "layer   1  Sparsity: 88.1348%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 33674.0\n",
      "lif layer 1 self.abs_max_v: 33674.0\n",
      "fc layer 1 self.abs_max_out: 33872.0\n",
      "lif layer 1 self.abs_max_v: 33872.0\n",
      "train - Value 0: 1915 occurrences\n",
      "train - Value 1: 2115 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 60 occurrences\n",
      "test - Value 1: 392 occurrences\n",
      "epoch-19  lr=['2.0000000'], tr/val_loss: 61.509525/ 50.194561, val:  62.39%, val_best:  80.75%, tr:  89.18%, tr_best:  90.10%, epoch time: 232.21 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4095%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.7868%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1097%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 644800 real_backward_count 202864  31.462%\n",
      "layer   1  Sparsity: 79.7363%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 33941.0\n",
      "lif layer 1 self.abs_max_v: 33941.0\n",
      "train - Value 0: 1943 occurrences\n",
      "train - Value 1: 2087 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 2 self.abs_max_out: 1779.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 402 occurrences\n",
      "test - Value 1: 50 occurrences\n",
      "epoch-20  lr=['2.0000000'], tr/val_loss: 60.431702/ 23.071123, val:  61.06%, val_best:  80.75%, tr:  90.72%, tr_best:  90.72%, epoch time: 231.11 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4114%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8943%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.3950%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 677040 real_backward_count 212294  31.356%\n",
      "layer   1  Sparsity: 88.6963%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 2375.0\n",
      "fc layer 2 self.abs_max_out: 1814.0\n",
      "fc layer 2 self.abs_max_out: 1862.0\n",
      "fc layer 2 self.abs_max_out: 1888.0\n",
      "fc layer 1 self.abs_max_out: 34037.0\n",
      "lif layer 1 self.abs_max_v: 34037.0\n",
      "lif layer 2 self.abs_max_v: 2408.0\n",
      "fc layer 2 self.abs_max_out: 1890.0\n",
      "train - Value 0: 1924 occurrences\n",
      "train - Value 1: 2106 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 353 occurrences\n",
      "test - Value 1: 99 occurrences\n",
      "epoch-21  lr=['2.0000000'], tr/val_loss: 60.122902/ 39.060261, val:  71.90%, val_best:  80.75%, tr:  89.90%, tr_best:  90.72%, epoch time: 232.85 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.7326%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.4547%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 709280 real_backward_count 221695  31.256%\n",
      "layer   1  Sparsity: 84.8389%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 34103.0\n",
      "lif layer 1 self.abs_max_v: 34103.0\n",
      "fc layer 2 self.abs_max_out: 1943.0\n",
      "fc layer 2 self.abs_max_out: 1992.0\n",
      "train - Value 0: 1954 occurrences\n",
      "train - Value 1: 2076 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 27 occurrences\n",
      "test - Value 1: 425 occurrences\n",
      "epoch-22  lr=['2.0000000'], tr/val_loss: 60.316204/110.877151, val:  55.97%, val_best:  80.75%, tr:  90.84%, tr_best:  90.84%, epoch time: 230.95 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4102%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6695%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.2179%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 741520 real_backward_count 230995  31.152%\n",
      "layer   1  Sparsity: 86.8896%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 2042.0\n",
      "fc layer 1 self.abs_max_out: 34297.0\n",
      "lif layer 1 self.abs_max_v: 34297.0\n",
      "train - Value 0: 1952 occurrences\n",
      "train - Value 1: 2078 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 393 occurrences\n",
      "test - Value 1: 59 occurrences\n",
      "epoch-23  lr=['2.0000000'], tr/val_loss: 61.022263/ 61.329716, val:  63.05%, val_best:  80.75%, tr:  91.24%, tr_best:  91.24%, epoch time: 231.99 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.7131%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.5973%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773760 real_backward_count 240210  31.045%\n",
      "layer   1  Sparsity: 78.1250%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 34316.0\n",
      "lif layer 1 self.abs_max_v: 34316.0\n",
      "fc layer 2 self.abs_max_out: 2070.0\n",
      "fc layer 2 self.abs_max_out: 2119.0\n",
      "fc layer 2 self.abs_max_out: 2171.0\n",
      "train - Value 0: 1999 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 333 occurrences\n",
      "test - Value 1: 119 occurrences\n",
      "epoch-24  lr=['2.0000000'], tr/val_loss: 62.373753/ 53.270306, val:  75.00%, val_best:  80.75%, tr:  92.95%, tr_best:  92.95%, epoch time: 233.82 seconds, 3.90 minutes\n",
      "layer   1  Sparsity: 84.4117%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.7322%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.5697%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 806000 real_backward_count 249167  30.914%\n",
      "layer   1  Sparsity: 87.6221%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 34356.0\n",
      "lif layer 1 self.abs_max_v: 34356.0\n",
      "fc layer 2 self.abs_max_out: 2173.0\n",
      "fc layer 2 self.abs_max_out: 2183.0\n",
      "train - Value 0: 1966 occurrences\n",
      "train - Value 1: 2064 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 1325.00 at epoch 25, iter 4029\n",
      "max_activation_accul updated: 1356.00 at epoch 25, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 435 occurrences\n",
      "test - Value 1: 17 occurrences\n",
      "epoch-25  lr=['2.0000000'], tr/val_loss: 66.040077/ 35.123711, val:  53.76%, val_best:  80.75%, tr:  91.89%, tr_best:  92.95%, epoch time: 233.02 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4096%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.7492%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3677%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 838240 real_backward_count 258266  30.811%\n",
      "layer   1  Sparsity: 84.4727%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 34547.0\n",
      "lif layer 1 self.abs_max_v: 34547.0\n",
      "train - Value 0: 1966 occurrences\n",
      "train - Value 1: 2064 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 208 occurrences\n",
      "test - Value 1: 244 occurrences\n",
      "epoch-26  lr=['2.0000000'], tr/val_loss: 59.885334/ 45.326279, val:  85.84%, val_best:  85.84%, tr:  91.39%, tr_best:  92.95%, epoch time: 232.82 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4103%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.7400%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.7141%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 870480 real_backward_count 267486  30.729%\n",
      "layer   1  Sparsity: 83.1055%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 34638.0\n",
      "lif layer 1 self.abs_max_v: 34638.0\n",
      "train - Value 0: 1959 occurrences\n",
      "train - Value 1: 2071 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 277 occurrences\n",
      "test - Value 1: 175 occurrences\n",
      "epoch-27  lr=['2.0000000'], tr/val_loss: 60.409351/ 23.889389, val:  84.73%, val_best:  85.84%, tr:  91.22%, tr_best:  92.95%, epoch time: 232.34 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4106%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8710%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.0706%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 902720 real_backward_count 276766  30.659%\n",
      "layer   1  Sparsity: 79.2480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 34755.0\n",
      "lif layer 1 self.abs_max_v: 34755.0\n",
      "train - Value 0: 1957 occurrences\n",
      "train - Value 1: 2073 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 264 occurrences\n",
      "test - Value 1: 188 occurrences\n",
      "epoch-28  lr=['2.0000000'], tr/val_loss: 57.954933/ 30.414194, val:  83.19%, val_best:  85.84%, tr:  91.07%, tr_best:  92.95%, epoch time: 231.69 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4115%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6978%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.8208%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 934960 real_backward_count 286096  30.600%\n",
      "layer   1  Sparsity: 88.5254%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 34843.0\n",
      "lif layer 1 self.abs_max_v: 34843.0\n",
      "train - Value 0: 1957 occurrences\n",
      "train - Value 1: 2073 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 335 occurrences\n",
      "test - Value 1: 117 occurrences\n",
      "epoch-29  lr=['2.0000000'], tr/val_loss: 62.237492/ 29.465237, val:  75.44%, val_best:  85.84%, tr:  91.76%, tr_best:  92.95%, epoch time: 231.78 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6090%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.6459%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 967200 real_backward_count 295468  30.549%\n",
      "layer   1  Sparsity: 85.9863%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 34929.0\n",
      "lif layer 1 self.abs_max_v: 34929.0\n",
      "train - Value 0: 1963 occurrences\n",
      "train - Value 1: 2067 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 397 occurrences\n",
      "test - Value 1: 55 occurrences\n",
      "epoch-30  lr=['2.0000000'], tr/val_loss: 61.297520/ 28.781414, val:  62.17%, val_best:  85.84%, tr:  92.61%, tr_best:  92.95%, epoch time: 230.23 seconds, 3.84 minutes\n",
      "layer   1  Sparsity: 84.4100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5846%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.2101%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 999440 real_backward_count 304645  30.482%\n",
      "layer   1  Sparsity: 82.9834%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 35049.0\n",
      "lif layer 1 self.abs_max_v: 35049.0\n",
      "train - Value 0: 1936 occurrences\n",
      "train - Value 1: 2094 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 280 occurrences\n",
      "test - Value 1: 172 occurrences\n",
      "epoch-31  lr=['2.0000000'], tr/val_loss: 66.377075/ 37.865105, val:  85.40%, val_best:  85.84%, tr:  93.03%, tr_best:  93.03%, epoch time: 232.47 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4107%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6062%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.2429%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1031680 real_backward_count 313607  30.398%\n",
      "layer   1  Sparsity: 78.1006%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 95.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 35059.0\n",
      "lif layer 1 self.abs_max_v: 35059.0\n",
      "train - Value 0: 1983 occurrences\n",
      "train - Value 1: 2047 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 65 occurrences\n",
      "test - Value 1: 387 occurrences\n",
      "epoch-32  lr=['2.0000000'], tr/val_loss: 67.940247/ 63.186138, val:  63.50%, val_best:  85.84%, tr:  92.01%, tr_best:  93.03%, epoch time: 232.79 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4117%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6050%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.6804%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1063920 real_backward_count 322712  30.332%\n",
      "layer   1  Sparsity: 82.4707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 35190.0\n",
      "lif layer 1 self.abs_max_v: 35190.0\n",
      "train - Value 0: 1974 occurrences\n",
      "train - Value 1: 2056 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 377 occurrences\n",
      "test - Value 1: 75 occurrences\n",
      "epoch-33  lr=['2.0000000'], tr/val_loss: 66.726959/ 44.105671, val:  66.59%, val_best:  85.84%, tr:  92.28%, tr_best:  93.03%, epoch time: 233.11 seconds, 3.89 minutes\n",
      "layer   1  Sparsity: 84.4108%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5952%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.9554%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096160 real_backward_count 331812  30.270%\n",
      "layer   1  Sparsity: 80.4199%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 35195.0\n",
      "lif layer 1 self.abs_max_v: 35195.0\n",
      "train - Value 0: 1964 occurrences\n",
      "train - Value 1: 2066 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 362 occurrences\n",
      "test - Value 1: 90 occurrences\n",
      "epoch-34  lr=['2.0000000'], tr/val_loss: 67.720901/ 28.745298, val:  69.91%, val_best:  85.84%, tr:  91.84%, tr_best:  93.03%, epoch time: 231.56 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5298%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0060%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1128400 real_backward_count 340938  30.214%\n",
      "layer   1  Sparsity: 80.4443%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 35254.0\n",
      "lif layer 1 self.abs_max_v: 35254.0\n",
      "train - Value 0: 1936 occurrences\n",
      "train - Value 1: 2094 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 203 occurrences\n",
      "test - Value 1: 249 occurrences\n",
      "epoch-35  lr=['2.0000000'], tr/val_loss: 69.914932/ 41.855526, val:  87.39%, val_best:  87.39%, tr:  91.64%, tr_best:  93.03%, epoch time: 232.25 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5992%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.0181%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1160640 real_backward_count 350079  30.163%\n",
      "layer   1  Sparsity: 92.4805%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 35355.0\n",
      "lif layer 1 self.abs_max_v: 35355.0\n",
      "train - Value 0: 1974 occurrences\n",
      "train - Value 1: 2056 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 372 occurrences\n",
      "test - Value 1: 80 occurrences\n",
      "epoch-36  lr=['2.0000000'], tr/val_loss: 67.676994/ 26.711159, val:  67.70%, val_best:  87.39%, tr:  92.68%, tr_best:  93.03%, epoch time: 232.08 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4085%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5854%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0416%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1192880 real_backward_count 359003  30.095%\n",
      "layer   1  Sparsity: 81.3721%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 35387.0\n",
      "lif layer 1 self.abs_max_v: 35387.0\n",
      "train - Value 0: 1943 occurrences\n",
      "train - Value 1: 2087 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 1623.00 at epoch 37, iter 4029\n",
      "max_activation_accul updated: 1946.00 at epoch 37, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-37  lr=['2.0000000'], tr/val_loss: 70.087219/114.346504, val:  50.00%, val_best:  87.39%, tr:  92.46%, tr_best:  93.03%, epoch time: 232.29 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4110%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6587%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1122%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1225120 real_backward_count 367920  30.031%\n",
      "layer   1  Sparsity: 80.6641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 35419.0\n",
      "lif layer 1 self.abs_max_v: 35419.0\n",
      "train - Value 0: 1892 occurrences\n",
      "train - Value 1: 2138 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 405 occurrences\n",
      "test - Value 1: 47 occurrences\n",
      "epoch-38  lr=['2.0000000'], tr/val_loss: 68.902184/ 32.485458, val:  60.40%, val_best:  87.39%, tr:  90.94%, tr_best:  93.03%, epoch time: 232.57 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6086%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.2416%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1257360 real_backward_count 376987  29.982%\n",
      "layer   1  Sparsity: 91.2842%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 336.0\n",
      "train - Value 0: 1918 occurrences\n",
      "train - Value 1: 2112 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 222 occurrences\n",
      "test - Value 1: 230 occurrences\n",
      "epoch-39  lr=['2.0000000'], tr/val_loss: 68.806137/ 45.755302, val:  88.50%, val_best:  88.50%, tr:  92.28%, tr_best:  93.03%, epoch time: 231.08 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5431%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8162%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1289600 real_backward_count 386135  29.942%\n",
      "layer   1  Sparsity: 77.5146%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 35463.0\n",
      "lif layer 1 self.abs_max_v: 35463.0\n",
      "train - Value 0: 1998 occurrences\n",
      "train - Value 1: 2032 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 169 occurrences\n",
      "test - Value 1: 283 occurrences\n",
      "epoch-40  lr=['2.0000000'], tr/val_loss: 61.447842/ 37.963001, val:  81.19%, val_best:  88.50%, tr:  92.83%, tr_best:  93.03%, epoch time: 233.65 seconds, 3.89 minutes\n",
      "layer   1  Sparsity: 84.4119%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5512%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.9084%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321840 real_backward_count 395037  29.885%\n",
      "layer   1  Sparsity: 83.3984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 35560.0\n",
      "lif layer 1 self.abs_max_v: 35560.0\n",
      "train - Value 0: 2005 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 182 occurrences\n",
      "test - Value 1: 270 occurrences\n",
      "epoch-41  lr=['2.0000000'], tr/val_loss: 56.674007/ 45.964718, val:  84.07%, val_best:  88.50%, tr:  93.00%, tr_best:  93.03%, epoch time: 231.81 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4106%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5135%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.3407%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1354080 real_backward_count 403946  29.832%\n",
      "layer   1  Sparsity: 86.5967%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 35576.0\n",
      "lif layer 1 self.abs_max_v: 35576.0\n",
      "train - Value 0: 1996 occurrences\n",
      "train - Value 1: 2034 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 392 occurrences\n",
      "test - Value 1: 60 occurrences\n",
      "epoch-42  lr=['2.0000000'], tr/val_loss: 57.505104/ 13.031301, val:  63.27%, val_best:  88.50%, tr:  93.23%, tr_best:  93.23%, epoch time: 232.68 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4099%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4737%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9579%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1386320 real_backward_count 412936  29.786%\n",
      "layer   1  Sparsity: 82.2510%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 35613.0\n",
      "lif layer 1 self.abs_max_v: 35613.0\n",
      "lif layer 2 self.abs_max_v: 2460.0\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 415 occurrences\n",
      "test - Value 1: 37 occurrences\n",
      "epoch-43  lr=['2.0000000'], tr/val_loss: 62.928844/ 30.636835, val:  58.19%, val_best:  88.50%, tr:  93.95%, tr_best:  93.95%, epoch time: 233.22 seconds, 3.89 minutes\n",
      "layer   1  Sparsity: 84.4108%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4329%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.4190%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1418560 real_backward_count 421941  29.744%\n",
      "layer   1  Sparsity: 91.2598%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 35692.0\n",
      "lif layer 1 self.abs_max_v: 35692.0\n",
      "train - Value 0: 1989 occurrences\n",
      "train - Value 1: 2041 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 438 occurrences\n",
      "test - Value 1: 14 occurrences\n",
      "epoch-44  lr=['2.0000000'], tr/val_loss: 68.610748/ 42.719551, val:  53.10%, val_best:  88.50%, tr:  93.10%, tr_best:  93.95%, epoch time: 231.56 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2994%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.1310%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1450800 real_backward_count 430803  29.694%\n",
      "layer   1  Sparsity: 92.9443%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 2470.0\n",
      "fc layer 1 self.abs_max_out: 35717.0\n",
      "lif layer 1 self.abs_max_v: 35717.0\n",
      "train - Value 0: 2025 occurrences\n",
      "train - Value 1: 2005 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-45  lr=['2.0000000'], tr/val_loss: 63.685051/ 87.920876, val:  50.00%, val_best:  88.50%, tr:  92.85%, tr_best:  93.95%, epoch time: 231.94 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4084%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3047%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.3107%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1483040 real_backward_count 439849  29.659%\n",
      "layer   1  Sparsity: 78.3447%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 35798.0\n",
      "lif layer 1 self.abs_max_v: 35798.0\n",
      "train - Value 0: 1991 occurrences\n",
      "train - Value 1: 2039 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 3 occurrences\n",
      "test - Value 1: 449 occurrences\n",
      "epoch-46  lr=['2.0000000'], tr/val_loss: 60.549236/ 80.563263, val:  50.66%, val_best:  88.50%, tr:  92.16%, tr_best:  93.95%, epoch time: 232.90 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4117%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2390%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.7916%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1515280 real_backward_count 448847  29.621%\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 2492.5\n",
      "fc layer 2 self.abs_max_out: 2194.0\n",
      "fc layer 2 self.abs_max_out: 2227.0\n",
      "fc layer 1 self.abs_max_out: 35831.0\n",
      "lif layer 1 self.abs_max_v: 35831.0\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 2 self.abs_max_out: 2240.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 318 occurrences\n",
      "test - Value 1: 134 occurrences\n",
      "epoch-47  lr=['2.0000000'], tr/val_loss: 61.656883/ 50.174885, val:  78.76%, val_best:  88.50%, tr:  93.10%, tr_best:  93.95%, epoch time: 232.51 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4089%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2538%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.0283%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1547520 real_backward_count 457728  29.578%\n",
      "layer   1  Sparsity: 92.3584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 2256.0\n",
      "fc layer 2 self.abs_max_out: 2272.0\n",
      "fc layer 2 self.abs_max_out: 2327.0\n",
      "fc layer 1 self.abs_max_out: 35854.0\n",
      "lif layer 1 self.abs_max_v: 35854.0\n",
      "train - Value 0: 2025 occurrences\n",
      "train - Value 1: 2005 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 16 occurrences\n",
      "test - Value 1: 436 occurrences\n",
      "epoch-48  lr=['2.0000000'], tr/val_loss: 58.899563/ 47.549335, val:  53.54%, val_best:  88.50%, tr:  93.60%, tr_best:  93.95%, epoch time: 233.18 seconds, 3.89 minutes\n",
      "layer   1  Sparsity: 84.4086%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3247%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.4385%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1579760 real_backward_count 466580  29.535%\n",
      "layer   1  Sparsity: 87.5977%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2036 occurrences\n",
      "train - Value 1: 1994 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 17 occurrences\n",
      "test - Value 1: 435 occurrences\n",
      "epoch-49  lr=['2.0000000'], tr/val_loss: 56.523964/ 56.551086, val:  53.76%, val_best:  88.50%, tr:  92.83%, tr_best:  93.95%, epoch time: 231.02 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4096%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4464%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.6532%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1612000 real_backward_count 475426  29.493%\n",
      "layer   1  Sparsity: 86.7432%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 35878.0\n",
      "lif layer 1 self.abs_max_v: 35878.0\n",
      "lif layer 2 self.abs_max_v: 2543.0\n",
      "train - Value 0: 2063 occurrences\n",
      "train - Value 1: 1967 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 156 occurrences\n",
      "test - Value 1: 296 occurrences\n",
      "epoch-50  lr=['2.0000000'], tr/val_loss: 55.969536/ 52.583183, val:  82.74%, val_best:  88.50%, tr:  93.10%, tr_best:  93.95%, epoch time: 231.87 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4753%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.0569%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644240 real_backward_count 484245  29.451%\n",
      "layer   1  Sparsity: 89.1113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 35903.0\n",
      "lif layer 1 self.abs_max_v: 35903.0\n",
      "train - Value 0: 1998 occurrences\n",
      "train - Value 1: 2032 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 8 occurrences\n",
      "test - Value 1: 444 occurrences\n",
      "epoch-51  lr=['2.0000000'], tr/val_loss: 60.661633/102.504486, val:  51.77%, val_best:  88.50%, tr:  93.52%, tr_best:  93.95%, epoch time: 232.32 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4093%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4304%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.8790%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1676480 real_backward_count 493112  29.414%\n",
      "layer   1  Sparsity: 84.1309%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 35953.0\n",
      "lif layer 1 self.abs_max_v: 35953.0\n",
      "train - Value 0: 2040 occurrences\n",
      "train - Value 1: 1990 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 450 occurrences\n",
      "test - Value 1: 2 occurrences\n",
      "epoch-52  lr=['2.0000000'], tr/val_loss: 54.345741/ 30.912790, val:  50.44%, val_best:  88.50%, tr:  94.81%, tr_best:  94.81%, epoch time: 233.58 seconds, 3.89 minutes\n",
      "layer   1  Sparsity: 84.4104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4421%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.3559%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1708720 real_backward_count 501836  29.369%\n",
      "layer   1  Sparsity: 87.8906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2026 occurrences\n",
      "train - Value 1: 2004 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 379 occurrences\n",
      "test - Value 1: 73 occurrences\n",
      "epoch-53  lr=['2.0000000'], tr/val_loss: 56.231228/ 28.155254, val:  66.15%, val_best:  88.50%, tr:  94.47%, tr_best:  94.81%, epoch time: 231.66 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4096%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4208%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.6353%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1740960 real_backward_count 510433  29.319%\n",
      "layer   1  Sparsity: 80.3711%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 205 occurrences\n",
      "test - Value 1: 247 occurrences\n",
      "epoch-54  lr=['2.0000000'], tr/val_loss: 57.249870/ 35.475277, val:  88.27%, val_best:  88.50%, tr:  94.39%, tr_best:  94.81%, epoch time: 232.35 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4267%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.1139%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1773200 real_backward_count 519196  29.280%\n",
      "layer   1  Sparsity: 83.0322%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 35961.0\n",
      "lif layer 1 self.abs_max_v: 35961.0\n",
      "train - Value 0: 2057 occurrences\n",
      "train - Value 1: 1973 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 54 occurrences\n",
      "test - Value 1: 398 occurrences\n",
      "epoch-55  lr=['2.0000000'], tr/val_loss: 51.197613/ 58.787144, val:  61.50%, val_best:  88.50%, tr:  94.94%, tr_best:  94.94%, epoch time: 233.65 seconds, 3.89 minutes\n",
      "layer   1  Sparsity: 84.4106%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4253%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5070%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1805440 real_backward_count 527820  29.235%\n",
      "layer   1  Sparsity: 80.0049%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 35980.0\n",
      "lif layer 1 self.abs_max_v: 35980.0\n",
      "lif layer 2 self.abs_max_v: 2597.5\n",
      "train - Value 0: 2056 occurrences\n",
      "train - Value 1: 1974 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 20 occurrences\n",
      "test - Value 1: 432 occurrences\n",
      "epoch-56  lr=['2.0000000'], tr/val_loss: 56.301373/ 54.609283, val:  53.98%, val_best:  88.50%, tr:  94.32%, tr_best:  94.94%, epoch time: 232.06 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3990%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.2422%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1837680 real_backward_count 536494  29.194%\n",
      "layer   1  Sparsity: 82.4707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 2699.0\n",
      "train - Value 0: 2045 occurrences\n",
      "train - Value 1: 1985 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 201 occurrences\n",
      "test - Value 1: 251 occurrences\n",
      "epoch-57  lr=['2.0000000'], tr/val_loss: 56.512993/ 47.889767, val:  86.06%, val_best:  88.50%, tr:  95.09%, tr_best:  95.09%, epoch time: 232.31 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4108%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4519%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.7468%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869920 real_backward_count 544965  29.144%\n",
      "layer   1  Sparsity: 83.9355%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2024 occurrences\n",
      "train - Value 1: 2006 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 358 occurrences\n",
      "test - Value 1: 94 occurrences\n",
      "epoch-58  lr=['2.0000000'], tr/val_loss: 55.764011/ 28.106153, val:  70.80%, val_best:  88.50%, tr:  95.01%, tr_best:  95.09%, epoch time: 229.21 seconds, 3.82 minutes\n",
      "layer   1  Sparsity: 84.4104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3477%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.7802%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1902160 real_backward_count 553448  29.096%\n",
      "layer   1  Sparsity: 78.9795%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 95.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 35992.0\n",
      "lif layer 1 self.abs_max_v: 35992.0\n",
      "train - Value 0: 2059 occurrences\n",
      "train - Value 1: 1971 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 182 occurrences\n",
      "test - Value 1: 270 occurrences\n",
      "epoch-59  lr=['2.0000000'], tr/val_loss: 57.807617/ 30.462910, val:  84.51%, val_best:  88.50%, tr:  94.84%, tr_best:  95.09%, epoch time: 232.35 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4116%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4254%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.7425%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1934400 real_backward_count 561927  29.049%\n",
      "layer   1  Sparsity: 79.5410%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36049.0\n",
      "lif layer 1 self.abs_max_v: 36049.0\n",
      "train - Value 0: 2047 occurrences\n",
      "train - Value 1: 1983 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 197 occurrences\n",
      "test - Value 1: 255 occurrences\n",
      "epoch-60  lr=['2.0000000'], tr/val_loss: 58.730175/ 50.512886, val:  82.96%, val_best:  88.50%, tr:  95.04%, tr_best:  95.09%, epoch time: 231.64 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4114%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4450%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.2209%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1966640 real_backward_count 570441  29.006%\n",
      "layer   1  Sparsity: 77.0752%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2010 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 450 occurrences\n",
      "test - Value 1: 2 occurrences\n",
      "epoch-61  lr=['2.0000000'], tr/val_loss: 61.154869/ 32.223591, val:  50.44%, val_best:  88.50%, tr:  95.01%, tr_best:  95.09%, epoch time: 230.95 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4120%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3979%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.5559%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1998880 real_backward_count 579016  28.967%\n",
      "layer   1  Sparsity: 80.2246%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36051.0\n",
      "lif layer 1 self.abs_max_v: 36051.0\n",
      "train - Value 0: 2067 occurrences\n",
      "train - Value 1: 1963 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 302 occurrences\n",
      "test - Value 1: 150 occurrences\n",
      "epoch-62  lr=['2.0000000'], tr/val_loss: 51.910229/ 23.235252, val:  81.42%, val_best:  88.50%, tr:  94.29%, tr_best:  95.09%, epoch time: 233.02 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4003%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.7581%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2031120 real_backward_count 587589  28.929%\n",
      "layer   1  Sparsity: 84.2041%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2050 occurrences\n",
      "train - Value 1: 1980 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 98 occurrences\n",
      "test - Value 1: 354 occurrences\n",
      "epoch-63  lr=['2.0000000'], tr/val_loss: 53.550846/ 60.604282, val:  70.35%, val_best:  88.50%, tr:  95.06%, tr_best:  95.09%, epoch time: 232.40 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4587%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.3549%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2063360 real_backward_count 596175  28.893%\n",
      "layer   1  Sparsity: 79.5166%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36061.0\n",
      "lif layer 1 self.abs_max_v: 36061.0\n",
      "train - Value 0: 2032 occurrences\n",
      "train - Value 1: 1998 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 197 occurrences\n",
      "test - Value 1: 255 occurrences\n",
      "epoch-64  lr=['2.0000000'], tr/val_loss: 57.722839/ 41.385551, val:  85.18%, val_best:  88.50%, tr:  95.66%, tr_best:  95.66%, epoch time: 233.06 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4114%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5120%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.8036%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2095600 real_backward_count 604702  28.856%\n",
      "layer   1  Sparsity: 80.3223%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2083 occurrences\n",
      "train - Value 1: 1947 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 188 occurrences\n",
      "test - Value 1: 264 occurrences\n",
      "epoch-65  lr=['2.0000000'], tr/val_loss: 51.935093/ 39.278278, val:  84.07%, val_best:  88.50%, tr:  95.73%, tr_best:  95.73%, epoch time: 232.17 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5403%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.6530%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2127840 real_backward_count 613146  28.815%\n",
      "layer   1  Sparsity: 79.7119%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36096.0\n",
      "lif layer 1 self.abs_max_v: 36096.0\n",
      "train - Value 0: 2028 occurrences\n",
      "train - Value 1: 2002 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 293 occurrences\n",
      "test - Value 1: 159 occurrences\n",
      "epoch-66  lr=['2.0000000'], tr/val_loss: 54.983910/ 46.282867, val:  82.52%, val_best:  88.50%, tr:  95.16%, tr_best:  95.73%, epoch time: 232.46 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4114%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4488%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.7239%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2160080 real_backward_count 621774  28.785%\n",
      "layer   1  Sparsity: 93.0908%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 74 occurrences\n",
      "test - Value 1: 378 occurrences\n",
      "epoch-67  lr=['2.0000000'], tr/val_loss: 54.443386/ 51.544380, val:  66.37%, val_best:  88.50%, tr:  95.41%, tr_best:  95.73%, epoch time: 232.45 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4084%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5552%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8218%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2192320 real_backward_count 630187  28.745%\n",
      "layer   1  Sparsity: 77.2217%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36122.0\n",
      "lif layer 1 self.abs_max_v: 36122.0\n",
      "train - Value 0: 2031 occurrences\n",
      "train - Value 1: 1999 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 146 occurrences\n",
      "test - Value 1: 306 occurrences\n",
      "epoch-68  lr=['2.0000000'], tr/val_loss: 51.716869/ 40.804367, val:  78.76%, val_best:  88.50%, tr:  95.73%, tr_best:  95.73%, epoch time: 233.11 seconds, 3.89 minutes\n",
      "layer   1  Sparsity: 84.4119%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4513%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.0075%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2224560 real_backward_count 638740  28.713%\n",
      "layer   1  Sparsity: 88.2812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36141.0\n",
      "lif layer 1 self.abs_max_v: 36141.0\n",
      "train - Value 0: 2043 occurrences\n",
      "train - Value 1: 1987 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 152 occurrences\n",
      "test - Value 1: 300 occurrences\n",
      "epoch-69  lr=['2.0000000'], tr/val_loss: 54.810253/ 32.839577, val:  81.42%, val_best:  88.50%, tr:  95.73%, tr_best:  95.73%, epoch time: 231.02 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4095%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4216%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.9534%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2256800 real_backward_count 647265  28.681%\n",
      "layer   1  Sparsity: 80.9326%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2043 occurrences\n",
      "train - Value 1: 1987 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 118 occurrences\n",
      "test - Value 1: 334 occurrences\n",
      "epoch-70  lr=['2.0000000'], tr/val_loss: 52.636326/ 44.518795, val:  73.01%, val_best:  88.50%, tr:  94.84%, tr_best:  95.73%, epoch time: 230.32 seconds, 3.84 minutes\n",
      "layer   1  Sparsity: 84.4111%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3537%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5336%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2289040 real_backward_count 655753  28.648%\n",
      "layer   1  Sparsity: 88.6719%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36183.0\n",
      "lif layer 1 self.abs_max_v: 36183.0\n",
      "train - Value 0: 2049 occurrences\n",
      "train - Value 1: 1981 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 98 occurrences\n",
      "test - Value 1: 354 occurrences\n",
      "epoch-71  lr=['2.0000000'], tr/val_loss: 53.179604/ 62.703159, val:  70.80%, val_best:  88.50%, tr:  94.39%, tr_best:  95.73%, epoch time: 232.31 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3914%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8840%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2321280 real_backward_count 664400  28.622%\n",
      "layer   1  Sparsity: 80.1514%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2041 occurrences\n",
      "train - Value 1: 1989 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 152 occurrences\n",
      "test - Value 1: 300 occurrences\n",
      "epoch-72  lr=['2.0000000'], tr/val_loss: 57.641087/ 52.940201, val:  80.97%, val_best:  88.50%, tr:  94.59%, tr_best:  95.73%, epoch time: 231.98 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4371%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.9923%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2353520 real_backward_count 673074  28.599%\n",
      "layer   1  Sparsity: 86.6211%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2041 occurrences\n",
      "train - Value 1: 1989 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 262 occurrences\n",
      "test - Value 1: 190 occurrences\n",
      "epoch-73  lr=['2.0000000'], tr/val_loss: 53.917164/ 45.997337, val:  82.30%, val_best:  88.50%, tr:  95.83%, tr_best:  95.83%, epoch time: 231.98 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4484%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.3405%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2385760 real_backward_count 681597  28.569%\n",
      "layer   1  Sparsity: 86.0840%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36231.0\n",
      "lif layer 1 self.abs_max_v: 36231.0\n",
      "train - Value 0: 2063 occurrences\n",
      "train - Value 1: 1967 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 441 occurrences\n",
      "test - Value 1: 11 occurrences\n",
      "epoch-74  lr=['2.0000000'], tr/val_loss: 53.871780/ 13.374052, val:  52.43%, val_best:  88.50%, tr:  95.38%, tr_best:  95.83%, epoch time: 231.48 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4296%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.0493%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2418000 real_backward_count 690159  28.543%\n",
      "layer   1  Sparsity: 89.3799%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2036 occurrences\n",
      "train - Value 1: 1994 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 301 occurrences\n",
      "test - Value 1: 151 occurrences\n",
      "epoch-75  lr=['2.0000000'], tr/val_loss: 60.823406/ 25.911518, val:  79.87%, val_best:  88.50%, tr:  94.91%, tr_best:  95.83%, epoch time: 231.63 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4092%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3449%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.4100%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2450240 real_backward_count 698687  28.515%\n",
      "layer   1  Sparsity: 91.6748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36239.0\n",
      "lif layer 1 self.abs_max_v: 36239.0\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 126 occurrences\n",
      "test - Value 1: 326 occurrences\n",
      "epoch-76  lr=['2.0000000'], tr/val_loss: 58.579014/ 54.787975, val:  76.11%, val_best:  88.50%, tr:  95.36%, tr_best:  95.83%, epoch time: 232.79 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4087%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3094%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.8447%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2482480 real_backward_count 707145  28.485%\n",
      "layer   1  Sparsity: 88.7695%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2045 occurrences\n",
      "train - Value 1: 1985 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 271 occurrences\n",
      "test - Value 1: 181 occurrences\n",
      "epoch-77  lr=['2.0000000'], tr/val_loss: 58.027020/ 41.291832, val:  83.85%, val_best:  88.50%, tr:  96.48%, tr_best:  96.48%, epoch time: 231.56 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2607%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.4157%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2514720 real_backward_count 715576  28.455%\n",
      "layer   1  Sparsity: 90.5029%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36294.0\n",
      "lif layer 1 self.abs_max_v: 36294.0\n",
      "train - Value 0: 2043 occurrences\n",
      "train - Value 1: 1987 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 171 occurrences\n",
      "test - Value 1: 281 occurrences\n",
      "epoch-78  lr=['2.0000000'], tr/val_loss: 57.951130/ 50.632454, val:  82.08%, val_best:  88.50%, tr:  95.63%, tr_best:  96.48%, epoch time: 231.54 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4090%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2725%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.9973%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2546960 real_backward_count 724073  28.429%\n",
      "layer   1  Sparsity: 88.2324%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2044 occurrences\n",
      "train - Value 1: 1986 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 247 occurrences\n",
      "test - Value 1: 205 occurrences\n",
      "epoch-79  lr=['2.0000000'], tr/val_loss: 63.120144/ 79.086723, val:  85.62%, val_best:  88.50%, tr:  95.46%, tr_best:  96.48%, epoch time: 231.81 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4095%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3840%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.6967%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2579200 real_backward_count 732379  28.396%\n",
      "layer   1  Sparsity: 85.0098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2002 occurrences\n",
      "train - Value 1: 2028 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 83 occurrences\n",
      "test - Value 1: 369 occurrences\n",
      "epoch-80  lr=['2.0000000'], tr/val_loss: 72.331787/ 72.672371, val:  68.36%, val_best:  88.50%, tr:  95.96%, tr_best:  96.48%, epoch time: 231.12 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4102%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3388%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.1124%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2611440 real_backward_count 740778  28.367%\n",
      "layer   1  Sparsity: 77.2705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 95.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 348.0\n",
      "train - Value 0: 1982 occurrences\n",
      "train - Value 1: 2048 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 13 occurrences\n",
      "test - Value 1: 439 occurrences\n",
      "epoch-81  lr=['2.0000000'], tr/val_loss: 74.637131/ 97.480171, val:  52.88%, val_best:  88.50%, tr:  95.21%, tr_best:  96.48%, epoch time: 231.98 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4119%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2715%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.1386%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2643680 real_backward_count 749191  28.339%\n",
      "layer   1  Sparsity: 87.2803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1967 occurrences\n",
      "train - Value 1: 2063 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 1 occurrences\n",
      "test - Value 1: 451 occurrences\n",
      "epoch-82  lr=['2.0000000'], tr/val_loss: 79.757820/111.411743, val:  50.22%, val_best:  88.50%, tr:  95.83%, tr_best:  96.48%, epoch time: 230.75 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4097%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2415%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.5757%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2675920 real_backward_count 757581  28.311%\n",
      "layer   1  Sparsity: 90.7471%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1987 occurrences\n",
      "train - Value 1: 2043 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 240 occurrences\n",
      "test - Value 1: 212 occurrences\n",
      "epoch-83  lr=['2.0000000'], tr/val_loss: 69.900520/ 60.459297, val:  87.61%, val_best:  88.50%, tr:  95.53%, tr_best:  96.48%, epoch time: 232.03 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4089%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1654%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.6118%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2708160 real_backward_count 766191  28.292%\n",
      "layer   1  Sparsity: 74.1943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 25.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2028 occurrences\n",
      "train - Value 1: 2002 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 121 occurrences\n",
      "test - Value 1: 331 occurrences\n",
      "epoch-84  lr=['2.0000000'], tr/val_loss: 74.743889/ 55.717026, val:  73.67%, val_best:  88.50%, tr:  96.35%, tr_best:  96.48%, epoch time: 229.93 seconds, 3.83 minutes\n",
      "layer   1  Sparsity: 84.4126%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1412%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.7226%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2740400 real_backward_count 774697  28.269%\n",
      "layer   1  Sparsity: 83.3984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1999 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 180 occurrences\n",
      "test - Value 1: 272 occurrences\n",
      "epoch-85  lr=['2.0000000'], tr/val_loss: 70.072769/ 43.394554, val:  81.86%, val_best:  88.50%, tr:  95.88%, tr_best:  96.48%, epoch time: 231.20 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4106%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1515%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.9431%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2772640 real_backward_count 783257  28.250%\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2051 occurrences\n",
      "train - Value 1: 1979 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 88 occurrences\n",
      "test - Value 1: 364 occurrences\n",
      "epoch-86  lr=['2.0000000'], tr/val_loss: 61.835335/ 36.535896, val:  69.03%, val_best:  88.50%, tr:  95.98%, tr_best:  96.48%, epoch time: 230.76 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4089%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1156%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.9902%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2804880 real_backward_count 791806  28.230%\n",
      "layer   1  Sparsity: 91.3330%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2032 occurrences\n",
      "train - Value 1: 1998 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 7 occurrences\n",
      "test - Value 1: 445 occurrences\n",
      "epoch-87  lr=['2.0000000'], tr/val_loss: 60.372295/ 82.896950, val:  51.55%, val_best:  88.50%, tr:  96.05%, tr_best:  96.48%, epoch time: 231.19 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1493%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.0761%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2837120 real_backward_count 800178  28.204%\n",
      "layer   1  Sparsity: 90.5029%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36320.0\n",
      "lif layer 1 self.abs_max_v: 36320.0\n",
      "train - Value 0: 2035 occurrences\n",
      "train - Value 1: 1995 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 171 occurrences\n",
      "test - Value 1: 281 occurrences\n",
      "epoch-88  lr=['2.0000000'], tr/val_loss: 54.382664/ 29.506636, val:  82.96%, val_best:  88.50%, tr:  96.33%, tr_best:  96.48%, epoch time: 231.97 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4090%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1963%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.1237%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2869360 real_backward_count 808523  28.178%\n",
      "layer   1  Sparsity: 69.9951%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36343.0\n",
      "lif layer 1 self.abs_max_v: 36343.0\n",
      "train - Value 0: 2059 occurrences\n",
      "train - Value 1: 1971 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 34 occurrences\n",
      "test - Value 1: 418 occurrences\n",
      "epoch-89  lr=['2.0000000'], tr/val_loss: 51.211708/ 68.124023, val:  57.52%, val_best:  88.50%, tr:  96.13%, tr_best:  96.48%, epoch time: 232.99 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4136%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2097%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.0599%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2901600 real_backward_count 816891  28.153%\n",
      "layer   1  Sparsity: 91.3330%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36353.0\n",
      "lif layer 1 self.abs_max_v: 36353.0\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 236 occurrences\n",
      "test - Value 1: 216 occurrences\n",
      "epoch-90  lr=['2.0000000'], tr/val_loss: 58.009243/ 46.028458, val:  85.84%, val_best:  88.50%, tr:  95.63%, tr_best:  96.48%, epoch time: 231.98 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2131%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.4098%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2933840 real_backward_count 825271  28.129%\n",
      "layer   1  Sparsity: 76.6113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36396.0\n",
      "lif layer 1 self.abs_max_v: 36396.0\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 290 occurrences\n",
      "test - Value 1: 162 occurrences\n",
      "epoch-91  lr=['2.0000000'], tr/val_loss: 57.242046/ 36.523468, val:  82.30%, val_best:  88.50%, tr:  95.83%, tr_best:  96.48%, epoch time: 231.97 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4121%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2020%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2502%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2966080 real_backward_count 833658  28.106%\n",
      "layer   1  Sparsity: 82.4463%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 346 occurrences\n",
      "test - Value 1: 106 occurrences\n",
      "epoch-92  lr=['2.0000000'], tr/val_loss: 64.358620/ 36.898994, val:  73.01%, val_best:  88.50%, tr:  95.73%, tr_best:  96.48%, epoch time: 231.91 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4108%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1895%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.2072%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2998320 real_backward_count 842188  28.089%\n",
      "layer   1  Sparsity: 83.9355%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 13 occurrences\n",
      "test - Value 1: 439 occurrences\n",
      "epoch-93  lr=['2.0000000'], tr/val_loss: 64.068466/ 81.325867, val:  52.88%, val_best:  88.50%, tr:  96.05%, tr_best:  96.48%, epoch time: 229.96 seconds, 3.83 minutes\n",
      "layer   1  Sparsity: 84.4104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1935%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.2137%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3030560 real_backward_count 850630  28.068%\n",
      "layer   1  Sparsity: 80.5664%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36415.0\n",
      "lif layer 1 self.abs_max_v: 36415.0\n",
      "train - Value 0: 2033 occurrences\n",
      "train - Value 1: 1997 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 288 occurrences\n",
      "test - Value 1: 164 occurrences\n",
      "epoch-94  lr=['2.0000000'], tr/val_loss: 61.975025/ 71.989204, val:  83.63%, val_best:  88.50%, tr:  95.38%, tr_best:  96.48%, epoch time: 231.36 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1890%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.1595%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3062800 real_backward_count 859045  28.048%\n",
      "layer   1  Sparsity: 74.3896%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36444.0\n",
      "lif layer 1 self.abs_max_v: 36444.0\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 65 occurrences\n",
      "test - Value 1: 387 occurrences\n",
      "epoch-95  lr=['2.0000000'], tr/val_loss: 66.755142/ 62.465294, val:  64.38%, val_best:  88.50%, tr:  96.40%, tr_best:  96.48%, epoch time: 231.35 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4126%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1626%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.2956%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3095040 real_backward_count 867253  28.021%\n",
      "layer   1  Sparsity: 87.2803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 45 occurrences\n",
      "test - Value 1: 407 occurrences\n",
      "epoch-96  lr=['2.0000000'], tr/val_loss: 66.365501/101.305809, val:  59.96%, val_best:  88.50%, tr:  96.10%, tr_best:  96.48%, epoch time: 231.78 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4097%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2239%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.9708%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3127280 real_backward_count 875547  27.997%\n",
      "layer   1  Sparsity: 80.3955%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36482.0\n",
      "lif layer 1 self.abs_max_v: 36482.0\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 161 occurrences\n",
      "test - Value 1: 291 occurrences\n",
      "epoch-97  lr=['2.0000000'], tr/val_loss: 63.880440/ 66.024117, val:  81.64%, val_best:  88.50%, tr:  96.43%, tr_best:  96.48%, epoch time: 232.35 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2303%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.7978%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3159520 real_backward_count 883821  27.973%\n",
      "layer   1  Sparsity: 85.0098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36496.0\n",
      "lif layer 1 self.abs_max_v: 36496.0\n",
      "train - Value 0: 1992 occurrences\n",
      "train - Value 1: 2038 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-98  lr=['2.0000000'], tr/val_loss: 63.587151/129.060501, val:  50.00%, val_best:  88.50%, tr:  95.91%, tr_best:  96.48%, epoch time: 232.17 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4102%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2401%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.5452%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3191760 real_backward_count 892105  27.950%\n",
      "layer   1  Sparsity: 79.5898%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 95.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 52 occurrences\n",
      "test - Value 1: 400 occurrences\n",
      "epoch-99  lr=['2.0000000'], tr/val_loss: 69.105446/ 37.375587, val:  61.50%, val_best:  88.50%, tr:  96.70%, tr_best:  96.70%, epoch time: 231.09 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4114%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3034%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8200%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3224000 real_backward_count 900298  27.925%\n",
      "layer   1  Sparsity: 91.5527%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36505.0\n",
      "lif layer 1 self.abs_max_v: 36505.0\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2009 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 69 occurrences\n",
      "test - Value 1: 383 occurrences\n",
      "epoch-100 lr=['2.0000000'], tr/val_loss: 64.764778/ 88.333542, val:  65.27%, val_best:  88.50%, tr:  96.48%, tr_best:  96.70%, epoch time: 231.83 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4087%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3450%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.5383%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3256240 real_backward_count 908593  27.903%\n",
      "layer   1  Sparsity: 82.5684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36509.0\n",
      "lif layer 1 self.abs_max_v: 36509.0\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 227 occurrences\n",
      "test - Value 1: 225 occurrences\n",
      "epoch-101 lr=['2.0000000'], tr/val_loss: 64.404037/ 34.676258, val:  82.08%, val_best:  88.50%, tr:  97.10%, tr_best:  97.10%, epoch time: 230.59 seconds, 3.84 minutes\n",
      "layer   1  Sparsity: 84.4108%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2725%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.2279%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3288480 real_backward_count 916692  27.876%\n",
      "layer   1  Sparsity: 77.9297%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 95.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36510.0\n",
      "lif layer 1 self.abs_max_v: 36510.0\n",
      "train - Value 0: 2022 occurrences\n",
      "train - Value 1: 2008 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 175 occurrences\n",
      "test - Value 1: 277 occurrences\n",
      "epoch-102 lr=['2.0000000'], tr/val_loss: 59.705360/ 67.930435, val:  84.29%, val_best:  88.50%, tr:  96.40%, tr_best:  97.10%, epoch time: 233.02 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4118%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2677%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.3238%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3320720 real_backward_count 924909  27.853%\n",
      "layer   1  Sparsity: 80.5176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 349.0\n",
      "fc layer 1 self.abs_max_out: 36542.0\n",
      "lif layer 1 self.abs_max_v: 36542.0\n",
      "train - Value 0: 2002 occurrences\n",
      "train - Value 1: 2028 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 214 occurrences\n",
      "test - Value 1: 238 occurrences\n",
      "epoch-103 lr=['2.0000000'], tr/val_loss: 66.765213/ 37.728813, val:  84.96%, val_best:  88.50%, tr:  96.35%, tr_best:  97.10%, epoch time: 232.47 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2112%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.8297%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3352960 real_backward_count 933141  27.830%\n",
      "layer   1  Sparsity: 89.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2031 occurrences\n",
      "train - Value 1: 1999 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-104 lr=['2.0000000'], tr/val_loss: 64.246742/ 20.214619, val:  50.00%, val_best:  88.50%, tr:  96.23%, tr_best:  97.10%, epoch time: 232.14 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4093%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2216%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.7800%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3385200 real_backward_count 941437  27.810%\n",
      "layer   1  Sparsity: 88.5010%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36580.0\n",
      "lif layer 1 self.abs_max_v: 36580.0\n",
      "train - Value 0: 2045 occurrences\n",
      "train - Value 1: 1985 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 270 occurrences\n",
      "test - Value 1: 182 occurrences\n",
      "epoch-105 lr=['2.0000000'], tr/val_loss: 60.581898/ 41.904427, val:  84.07%, val_best:  88.50%, tr:  97.12%, tr_best:  97.12%, epoch time: 230.87 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2808%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.8217%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3417440 real_backward_count 949580  27.786%\n",
      "layer   1  Sparsity: 87.8662%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1999 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 197 occurrences\n",
      "test - Value 1: 255 occurrences\n",
      "epoch-106 lr=['2.0000000'], tr/val_loss: 57.962074/ 53.567463, val:  85.62%, val_best:  88.50%, tr:  96.03%, tr_best:  97.12%, epoch time: 232.57 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4096%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2729%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8248%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3449680 real_backward_count 957867  27.767%\n",
      "layer   1  Sparsity: 90.1611%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36610.0\n",
      "lif layer 1 self.abs_max_v: 36610.0\n",
      "train - Value 0: 2064 occurrences\n",
      "train - Value 1: 1966 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 232 occurrences\n",
      "test - Value 1: 220 occurrences\n",
      "epoch-107 lr=['2.0000000'], tr/val_loss: 58.063942/ 34.401134, val:  88.05%, val_best:  88.50%, tr:  96.75%, tr_best:  97.12%, epoch time: 232.54 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2657%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.1321%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3481920 real_backward_count 966030  27.744%\n",
      "layer   1  Sparsity: 73.6328%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 95.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2042 occurrences\n",
      "train - Value 1: 1988 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 178 occurrences\n",
      "test - Value 1: 274 occurrences\n",
      "epoch-108 lr=['2.0000000'], tr/val_loss: 55.536526/ 60.051304, val:  84.51%, val_best:  88.50%, tr:  96.35%, tr_best:  97.12%, epoch time: 229.67 seconds, 3.83 minutes\n",
      "layer   1  Sparsity: 84.4127%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2420%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.9056%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3514160 real_backward_count 974313  27.725%\n",
      "layer   1  Sparsity: 92.4805%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2036 occurrences\n",
      "train - Value 1: 1994 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 320 occurrences\n",
      "test - Value 1: 132 occurrences\n",
      "epoch-109 lr=['2.0000000'], tr/val_loss: 63.773434/ 70.838585, val:  78.76%, val_best:  88.50%, tr:  96.80%, tr_best:  97.12%, epoch time: 230.68 seconds, 3.84 minutes\n",
      "layer   1  Sparsity: 84.4085%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2926%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.7152%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3546400 real_backward_count 982510  27.704%\n",
      "layer   1  Sparsity: 80.3711%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 383.0\n",
      "train - Value 0: 2038 occurrences\n",
      "train - Value 1: 1992 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 5 occurrences\n",
      "test - Value 1: 447 occurrences\n",
      "epoch-110 lr=['2.0000000'], tr/val_loss: 66.279053/ 76.191078, val:  51.11%, val_best:  88.50%, tr:  96.50%, tr_best:  97.12%, epoch time: 231.14 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2367%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.1490%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3578640 real_backward_count 990877  27.689%\n",
      "layer   1  Sparsity: 75.2197%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36622.0\n",
      "lif layer 1 self.abs_max_v: 36622.0\n",
      "train - Value 0: 2056 occurrences\n",
      "train - Value 1: 1974 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 60 occurrences\n",
      "test - Value 1: 392 occurrences\n",
      "epoch-111 lr=['2.0000000'], tr/val_loss: 64.272774/ 54.157150, val:  63.27%, val_best:  88.50%, tr:  96.90%, tr_best:  97.12%, epoch time: 231.43 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4124%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2211%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1608%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3610880 real_backward_count 998917  27.664%\n",
      "layer   1  Sparsity: 82.6660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2045 occurrences\n",
      "train - Value 1: 1985 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 54 occurrences\n",
      "test - Value 1: 398 occurrences\n",
      "epoch-112 lr=['2.0000000'], tr/val_loss: 60.222553/ 55.518600, val:  61.06%, val_best:  88.50%, tr:  96.48%, tr_best:  97.12%, epoch time: 231.80 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4107%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2232%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.1204%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3643120 real_backward_count 1007175  27.646%\n",
      "layer   1  Sparsity: 85.1318%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36649.0\n",
      "lif layer 1 self.abs_max_v: 36649.0\n",
      "train - Value 0: 2048 occurrences\n",
      "train - Value 1: 1982 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 145 occurrences\n",
      "test - Value 1: 307 occurrences\n",
      "epoch-113 lr=['2.0000000'], tr/val_loss: 60.439709/ 54.654327, val:  79.42%, val_best:  88.50%, tr:  96.30%, tr_best:  97.12%, epoch time: 230.66 seconds, 3.84 minutes\n",
      "layer   1  Sparsity: 84.4102%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2319%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.1100%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3675360 real_backward_count 1015334  27.625%\n",
      "layer   1  Sparsity: 82.4707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36669.0\n",
      "lif layer 1 self.abs_max_v: 36669.0\n",
      "train - Value 0: 2044 occurrences\n",
      "train - Value 1: 1986 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 212 occurrences\n",
      "test - Value 1: 240 occurrences\n",
      "epoch-114 lr=['2.0000000'], tr/val_loss: 57.355133/ 41.961742, val:  84.07%, val_best:  88.50%, tr:  95.91%, tr_best:  97.12%, epoch time: 230.50 seconds, 3.84 minutes\n",
      "layer   1  Sparsity: 84.4108%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2679%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.7403%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3707600 real_backward_count 1023703  27.611%\n",
      "layer   1  Sparsity: 78.5889%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2038 occurrences\n",
      "train - Value 1: 1992 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 26 occurrences\n",
      "test - Value 1: 426 occurrences\n",
      "epoch-115 lr=['2.0000000'], tr/val_loss: 54.548351/ 64.899452, val:  55.75%, val_best:  88.50%, tr:  96.40%, tr_best:  97.12%, epoch time: 231.84 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4116%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2225%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.1539%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3739840 real_backward_count 1032003  27.595%\n",
      "layer   1  Sparsity: 78.8330%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2022 occurrences\n",
      "train - Value 1: 2008 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 380 occurrences\n",
      "test - Value 1: 72 occurrences\n",
      "epoch-116 lr=['2.0000000'], tr/val_loss: 58.805077/ 62.310204, val:  65.49%, val_best:  88.50%, tr:  96.25%, tr_best:  97.12%, epoch time: 225.30 seconds, 3.75 minutes\n",
      "layer   1  Sparsity: 84.4116%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2074%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.0557%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3772080 real_backward_count 1040284  27.579%\n",
      "layer   1  Sparsity: 78.1250%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36697.0\n",
      "lif layer 1 self.abs_max_v: 36697.0\n",
      "train - Value 0: 2036 occurrences\n",
      "train - Value 1: 1994 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 291 occurrences\n",
      "test - Value 1: 161 occurrences\n",
      "epoch-117 lr=['2.0000000'], tr/val_loss: 55.679993/ 56.845646, val:  82.08%, val_best:  88.50%, tr:  96.60%, tr_best:  97.12%, epoch time: 231.12 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4117%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2173%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.4544%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3804320 real_backward_count 1048797  27.569%\n",
      "layer   1  Sparsity: 88.7695%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2059 occurrences\n",
      "train - Value 1: 1971 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 42 occurrences\n",
      "test - Value 1: 410 occurrences\n",
      "epoch-118 lr=['2.0000000'], tr/val_loss: 52.339046/ 77.969925, val:  58.85%, val_best:  88.50%, tr:  96.13%, tr_best:  97.12%, epoch time: 230.98 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3088%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6101%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3836560 real_backward_count 1057355  27.560%\n",
      "layer   1  Sparsity: 82.7148%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2024 occurrences\n",
      "train - Value 1: 2006 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 221 occurrences\n",
      "test - Value 1: 231 occurrences\n",
      "epoch-119 lr=['2.0000000'], tr/val_loss: 53.924805/ 43.648983, val:  83.85%, val_best:  88.50%, tr:  96.35%, tr_best:  97.12%, epoch time: 230.87 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4107%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2980%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2333%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3868800 real_backward_count 1065711  27.546%\n",
      "layer   1  Sparsity: 87.4023%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2034 occurrences\n",
      "train - Value 1: 1996 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 38 occurrences\n",
      "test - Value 1: 414 occurrences\n",
      "epoch-120 lr=['2.0000000'], tr/val_loss: 56.774654/ 54.301842, val:  58.41%, val_best:  88.50%, tr:  97.10%, tr_best:  97.12%, epoch time: 230.03 seconds, 3.83 minutes\n",
      "layer   1  Sparsity: 84.4097%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2635%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.1575%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3901040 real_backward_count 1074054  27.533%\n",
      "layer   1  Sparsity: 88.1104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36722.0\n",
      "lif layer 1 self.abs_max_v: 36722.0\n",
      "train - Value 0: 2059 occurrences\n",
      "train - Value 1: 1971 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 190 occurrences\n",
      "test - Value 1: 262 occurrences\n",
      "epoch-121 lr=['2.0000000'], tr/val_loss: 53.044415/ 28.575935, val:  85.84%, val_best:  88.50%, tr:  97.27%, tr_best:  97.27%, epoch time: 230.65 seconds, 3.84 minutes\n",
      "layer   1  Sparsity: 84.4095%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2179%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6820%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3933280 real_backward_count 1082284  27.516%\n",
      "layer   1  Sparsity: 85.5957%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36736.0\n",
      "lif layer 1 self.abs_max_v: 36736.0\n",
      "train - Value 0: 2061 occurrences\n",
      "train - Value 1: 1969 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 27 occurrences\n",
      "test - Value 1: 425 occurrences\n",
      "epoch-122 lr=['2.0000000'], tr/val_loss: 48.053413/ 58.942078, val:  55.97%, val_best:  88.50%, tr:  96.63%, tr_best:  97.27%, epoch time: 230.50 seconds, 3.84 minutes\n",
      "layer   1  Sparsity: 84.4101%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2532%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1724%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3965520 real_backward_count 1090614  27.502%\n",
      "layer   1  Sparsity: 89.9902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2063 occurrences\n",
      "train - Value 1: 1967 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 317 occurrences\n",
      "test - Value 1: 135 occurrences\n",
      "epoch-123 lr=['2.0000000'], tr/val_loss: 48.677490/ 56.508350, val:  76.77%, val_best:  88.50%, tr:  97.07%, tr_best:  97.27%, epoch time: 229.79 seconds, 3.83 minutes\n",
      "layer   1  Sparsity: 84.4091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2697%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8795%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3997760 real_backward_count 1098887  27.488%\n",
      "layer   1  Sparsity: 87.2803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36772.0\n",
      "lif layer 1 self.abs_max_v: 36772.0\n",
      "train - Value 0: 2069 occurrences\n",
      "train - Value 1: 1961 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 255 occurrences\n",
      "test - Value 1: 197 occurrences\n",
      "epoch-124 lr=['2.0000000'], tr/val_loss: 49.420765/ 40.706055, val:  86.95%, val_best:  88.50%, tr:  96.53%, tr_best:  97.27%, epoch time: 231.39 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4097%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2880%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4492%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4030000 real_backward_count 1107192  27.474%\n",
      "layer   1  Sparsity: 82.9834%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36777.0\n",
      "lif layer 1 self.abs_max_v: 36777.0\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 283 occurrences\n",
      "test - Value 1: 169 occurrences\n",
      "epoch-125 lr=['2.0000000'], tr/val_loss: 50.381428/ 25.754400, val:  84.73%, val_best:  88.50%, tr:  97.37%, tr_best:  97.37%, epoch time: 231.39 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4107%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2954%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9047%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4062240 real_backward_count 1115237  27.454%\n",
      "layer   1  Sparsity: 91.2354%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2028 occurrences\n",
      "train - Value 1: 2002 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 298 occurrences\n",
      "test - Value 1: 154 occurrences\n",
      "epoch-126 lr=['2.0000000'], tr/val_loss: 60.525261/ 46.092937, val:  80.97%, val_best:  88.50%, tr:  97.20%, tr_best:  97.37%, epoch time: 231.08 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2068%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.6714%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4094480 real_backward_count 1123214  27.432%\n",
      "layer   1  Sparsity: 76.6113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36805.0\n",
      "lif layer 1 self.abs_max_v: 36805.0\n",
      "train - Value 0: 2034 occurrences\n",
      "train - Value 1: 1996 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 60 occurrences\n",
      "test - Value 1: 392 occurrences\n",
      "epoch-127 lr=['2.0000000'], tr/val_loss: 59.091003/ 82.065956, val:  62.83%, val_best:  88.50%, tr:  97.54%, tr_best:  97.54%, epoch time: 231.27 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4121%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2309%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.4499%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4126720 real_backward_count 1131308  27.414%\n",
      "layer   1  Sparsity: 88.9893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36814.0\n",
      "lif layer 1 self.abs_max_v: 36814.0\n",
      "train - Value 0: 2037 occurrences\n",
      "train - Value 1: 1993 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 258 occurrences\n",
      "test - Value 1: 194 occurrences\n",
      "epoch-128 lr=['2.0000000'], tr/val_loss: 65.472572/ 40.659321, val:  85.84%, val_best:  88.50%, tr:  97.67%, tr_best:  97.67%, epoch time: 230.70 seconds, 3.84 minutes\n",
      "layer   1  Sparsity: 84.4093%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2261%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.4561%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4158960 real_backward_count 1139431  27.397%\n",
      "layer   1  Sparsity: 74.6582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2031 occurrences\n",
      "train - Value 1: 1999 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 281 occurrences\n",
      "test - Value 1: 171 occurrences\n",
      "epoch-129 lr=['2.0000000'], tr/val_loss: 64.513947/ 16.031042, val:  82.52%, val_best:  88.50%, tr:  96.92%, tr_best:  97.67%, epoch time: 231.71 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2604%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.3239%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4191200 real_backward_count 1147521  27.379%\n",
      "layer   1  Sparsity: 91.6992%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36820.0\n",
      "lif layer 1 self.abs_max_v: 36820.0\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-130 lr=['2.0000000'], tr/val_loss: 67.297485/ 99.439583, val:  50.00%, val_best:  88.50%, tr:  97.02%, tr_best:  97.67%, epoch time: 229.68 seconds, 3.83 minutes\n",
      "layer   1  Sparsity: 84.4087%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2499%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.7898%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4223440 real_backward_count 1155610  27.362%\n",
      "layer   1  Sparsity: 82.7148%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2031 occurrences\n",
      "train - Value 1: 1999 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 111 occurrences\n",
      "test - Value 1: 341 occurrences\n",
      "epoch-131 lr=['2.0000000'], tr/val_loss: 58.419632/ 57.358597, val:  73.67%, val_best:  88.50%, tr:  96.63%, tr_best:  97.67%, epoch time: 230.65 seconds, 3.84 minutes\n",
      "layer   1  Sparsity: 84.4107%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3071%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.0724%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4255680 real_backward_count 1163760  27.346%\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2000 occurrences\n",
      "train - Value 1: 2030 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 223 occurrences\n",
      "test - Value 1: 229 occurrences\n",
      "epoch-132 lr=['2.0000000'], tr/val_loss: 62.423862/ 64.163795, val:  86.50%, val_best:  88.50%, tr:  96.80%, tr_best:  97.67%, epoch time: 230.63 seconds, 3.84 minutes\n",
      "layer   1  Sparsity: 84.4089%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2744%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.7755%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4287920 real_backward_count 1172067  27.334%\n",
      "layer   1  Sparsity: 84.6680%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2028 occurrences\n",
      "train - Value 1: 2002 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 233 occurrences\n",
      "test - Value 1: 219 occurrences\n",
      "epoch-133 lr=['2.0000000'], tr/val_loss: 59.555717/ 54.951019, val:  88.27%, val_best:  88.50%, tr:  97.10%, tr_best:  97.67%, epoch time: 229.08 seconds, 3.82 minutes\n",
      "layer   1  Sparsity: 84.4103%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3074%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.6508%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4320160 real_backward_count 1180115  27.316%\n",
      "layer   1  Sparsity: 80.4932%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36832.0\n",
      "lif layer 1 self.abs_max_v: 36832.0\n",
      "train - Value 0: 2066 occurrences\n",
      "train - Value 1: 1964 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 261 occurrences\n",
      "test - Value 1: 191 occurrences\n",
      "epoch-134 lr=['2.0000000'], tr/val_loss: 53.696751/ 32.703125, val:  84.73%, val_best:  88.50%, tr:  96.45%, tr_best:  97.67%, epoch time: 231.17 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2438%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2538%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4352400 real_backward_count 1188226  27.300%\n",
      "layer   1  Sparsity: 87.2559%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36834.0\n",
      "lif layer 1 self.abs_max_v: 36834.0\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2009 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 272 occurrences\n",
      "test - Value 1: 180 occurrences\n",
      "epoch-135 lr=['2.0000000'], tr/val_loss: 55.324017/ 31.114725, val:  82.74%, val_best:  88.50%, tr:  96.28%, tr_best:  97.67%, epoch time: 229.52 seconds, 3.83 minutes\n",
      "layer   1  Sparsity: 84.4097%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1946%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6442%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4384640 real_backward_count 1196398  27.286%\n",
      "layer   1  Sparsity: 87.5488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 2703.5\n",
      "train - Value 0: 2065 occurrences\n",
      "train - Value 1: 1965 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 46 occurrences\n",
      "test - Value 1: 406 occurrences\n",
      "epoch-136 lr=['2.0000000'], tr/val_loss: 47.802666/ 73.881630, val:  60.18%, val_best:  88.50%, tr:  95.68%, tr_best:  97.67%, epoch time: 231.46 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4096%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1402%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2834%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4416880 real_backward_count 1204699  27.275%\n",
      "layer   1  Sparsity: 74.5605%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2059 occurrences\n",
      "train - Value 1: 1971 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 34 occurrences\n",
      "test - Value 1: 418 occurrences\n",
      "epoch-137 lr=['2.0000000'], tr/val_loss: 59.424061/ 88.493462, val:  57.52%, val_best:  88.50%, tr:  97.12%, tr_best:  97.67%, epoch time: 231.13 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1180%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.0270%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4449120 real_backward_count 1212856  27.261%\n",
      "layer   1  Sparsity: 72.5342%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36900.0\n",
      "lif layer 1 self.abs_max_v: 36900.0\n",
      "train - Value 0: 2052 occurrences\n",
      "train - Value 1: 1978 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 271 occurrences\n",
      "test - Value 1: 181 occurrences\n",
      "epoch-138 lr=['2.0000000'], tr/val_loss: 57.796646/ 24.731287, val:  85.62%, val_best:  88.50%, tr:  96.80%, tr_best:  97.67%, epoch time: 231.41 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4130%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2556%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.5377%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4481360 real_backward_count 1220861  27.243%\n",
      "layer   1  Sparsity: 76.4893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 95.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2030 occurrences\n",
      "train - Value 1: 2000 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-139 lr=['2.0000000'], tr/val_loss: 54.792377/ 59.630157, val:  50.00%, val_best:  88.50%, tr:  96.85%, tr_best:  97.67%, epoch time: 231.33 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4121%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2541%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.5688%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4513600 real_backward_count 1228980  27.228%\n",
      "layer   1  Sparsity: 89.9658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36906.0\n",
      "lif layer 1 self.abs_max_v: 36906.0\n",
      "train - Value 0: 2051 occurrences\n",
      "train - Value 1: 1979 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 175 occurrences\n",
      "test - Value 1: 277 occurrences\n",
      "epoch-140 lr=['2.0000000'], tr/val_loss: 56.029823/ 48.046749, val:  83.41%, val_best:  88.50%, tr:  96.97%, tr_best:  97.67%, epoch time: 231.47 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2558%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.9008%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4545840 real_backward_count 1237024  27.212%\n",
      "layer   1  Sparsity: 88.6719%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36940.0\n",
      "lif layer 1 self.abs_max_v: 36940.0\n",
      "train - Value 0: 2054 occurrences\n",
      "train - Value 1: 1976 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 80 occurrences\n",
      "test - Value 1: 372 occurrences\n",
      "epoch-141 lr=['2.0000000'], tr/val_loss: 50.138760/ 76.062431, val:  67.26%, val_best:  88.50%, tr:  97.20%, tr_best:  97.67%, epoch time: 232.09 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3066%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6432%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4578080 real_backward_count 1244971  27.194%\n",
      "layer   1  Sparsity: 81.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2054 occurrences\n",
      "train - Value 1: 1976 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 78 occurrences\n",
      "test - Value 1: 374 occurrences\n",
      "epoch-142 lr=['2.0000000'], tr/val_loss: 52.932323/ 75.747513, val:  66.37%, val_best:  88.50%, tr:  96.85%, tr_best:  97.67%, epoch time: 232.60 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4111%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3376%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.7445%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4610320 real_backward_count 1252994  27.178%\n",
      "layer   1  Sparsity: 87.1826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2034 occurrences\n",
      "train - Value 1: 1996 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 283 occurrences\n",
      "test - Value 1: 169 occurrences\n",
      "epoch-143 lr=['2.0000000'], tr/val_loss: 57.760994/ 21.445433, val:  83.41%, val_best:  88.50%, tr:  97.59%, tr_best:  97.67%, epoch time: 232.53 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4097%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3386%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5402%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4642560 real_backward_count 1260894  27.159%\n",
      "layer   1  Sparsity: 84.3750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2048 occurrences\n",
      "train - Value 1: 1982 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 163 occurrences\n",
      "test - Value 1: 289 occurrences\n",
      "epoch-144 lr=['2.0000000'], tr/val_loss: 55.689457/ 33.155293, val:  80.75%, val_best:  88.50%, tr:  97.34%, tr_best:  97.67%, epoch time: 231.27 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4103%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3532%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.6108%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4674800 real_backward_count 1268907  27.144%\n",
      "layer   1  Sparsity: 78.0762%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 2333.0\n",
      "train - Value 0: 2078 occurrences\n",
      "train - Value 1: 1952 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-145 lr=['2.0000000'], tr/val_loss: 51.098667/ 33.686642, val:  50.00%, val_best:  88.50%, tr:  96.65%, tr_best:  97.67%, epoch time: 231.86 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4118%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2900%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3501%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4707040 real_backward_count 1277096  27.132%\n",
      "layer   1  Sparsity: 86.5967%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2068 occurrences\n",
      "train - Value 1: 1962 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 156 occurrences\n",
      "test - Value 1: 296 occurrences\n",
      "epoch-146 lr=['2.0000000'], tr/val_loss: 47.769867/ 51.270466, val:  81.42%, val_best:  88.50%, tr:  97.44%, tr_best:  97.67%, epoch time: 231.14 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4099%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2484%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9924%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4739280 real_backward_count 1285135  27.117%\n",
      "layer   1  Sparsity: 90.0879%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2059 occurrences\n",
      "train - Value 1: 1971 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 169 occurrences\n",
      "test - Value 1: 283 occurrences\n",
      "epoch-147 lr=['2.0000000'], tr/val_loss: 51.964996/ 38.869671, val:  82.96%, val_best:  88.50%, tr:  97.32%, tr_best:  97.67%, epoch time: 230.87 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1800%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2611%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4771520 real_backward_count 1293271  27.104%\n",
      "layer   1  Sparsity: 84.1064%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 36977.0\n",
      "lif layer 1 self.abs_max_v: 36977.0\n",
      "train - Value 0: 2073 occurrences\n",
      "train - Value 1: 1957 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 149 occurrences\n",
      "test - Value 1: 303 occurrences\n",
      "epoch-148 lr=['2.0000000'], tr/val_loss: 52.827732/ 61.965153, val:  81.64%, val_best:  88.50%, tr:  97.72%, tr_best:  97.72%, epoch time: 231.45 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2200%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.0098%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4803760 real_backward_count 1301346  27.090%\n",
      "layer   1  Sparsity: 75.8789%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2048 occurrences\n",
      "train - Value 1: 1982 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 216 occurrences\n",
      "test - Value 1: 236 occurrences\n",
      "epoch-149 lr=['2.0000000'], tr/val_loss: 57.434689/ 61.519817, val:  85.84%, val_best:  88.50%, tr:  97.64%, tr_best:  97.72%, epoch time: 230.71 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4122%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2413%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8037%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4836000 real_backward_count 1309379  27.076%\n",
      "layer   1  Sparsity: 79.9316%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 37014.0\n",
      "lif layer 1 self.abs_max_v: 37014.0\n",
      "train - Value 0: 2030 occurrences\n",
      "train - Value 1: 2000 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 133 occurrences\n",
      "test - Value 1: 319 occurrences\n",
      "epoch-150 lr=['2.0000000'], tr/val_loss: 61.613407/ 75.745750, val:  77.65%, val_best:  88.50%, tr:  98.04%, tr_best:  98.04%, epoch time: 231.54 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2111%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.3005%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4868240 real_backward_count 1317315  27.059%\n",
      "layer   1  Sparsity: 87.3291%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2022 occurrences\n",
      "train - Value 1: 2008 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 296 occurrences\n",
      "test - Value 1: 156 occurrences\n",
      "epoch-151 lr=['2.0000000'], tr/val_loss: 68.240189/ 42.122032, val:  81.42%, val_best:  88.50%, tr:  98.09%, tr_best:  98.09%, epoch time: 230.20 seconds, 3.84 minutes\n",
      "layer   1  Sparsity: 84.4097%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2020%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.2090%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4900480 real_backward_count 1325179  27.042%\n",
      "layer   1  Sparsity: 88.5010%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 37015.0\n",
      "lif layer 1 self.abs_max_v: 37015.0\n",
      "train - Value 0: 2040 occurrences\n",
      "train - Value 1: 1990 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 205 occurrences\n",
      "test - Value 1: 247 occurrences\n",
      "epoch-152 lr=['2.0000000'], tr/val_loss: 68.709450/ 50.303173, val:  85.18%, val_best:  88.50%, tr:  97.99%, tr_best:  98.09%, epoch time: 229.98 seconds, 3.83 minutes\n",
      "layer   1  Sparsity: 84.4094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1906%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.3984%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4932720 real_backward_count 1333118  27.026%\n",
      "layer   1  Sparsity: 87.2559%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 2336.0\n",
      "fc layer 1 self.abs_max_out: 37041.0\n",
      "lif layer 1 self.abs_max_v: 37041.0\n",
      "train - Value 0: 2032 occurrences\n",
      "train - Value 1: 1998 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 204 occurrences\n",
      "test - Value 1: 248 occurrences\n",
      "epoch-153 lr=['2.0000000'], tr/val_loss: 65.578651/ 41.039196, val:  85.84%, val_best:  88.50%, tr:  96.50%, tr_best:  98.09%, epoch time: 232.23 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4097%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1713%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.1745%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4964960 real_backward_count 1341186  27.013%\n",
      "layer   1  Sparsity: 89.6484%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2031 occurrences\n",
      "train - Value 1: 1999 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 220 occurrences\n",
      "test - Value 1: 232 occurrences\n",
      "epoch-154 lr=['2.0000000'], tr/val_loss: 73.336655/ 38.063591, val:  87.61%, val_best:  88.50%, tr:  97.92%, tr_best:  98.09%, epoch time: 232.95 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4092%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1783%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.5739%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4997200 real_backward_count 1349188  26.999%\n",
      "layer   1  Sparsity: 81.5674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 2341.0\n",
      "train - Value 0: 2057 occurrences\n",
      "train - Value 1: 1973 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 211 occurrences\n",
      "test - Value 1: 241 occurrences\n",
      "epoch-155 lr=['2.0000000'], tr/val_loss: 63.495811/ 53.934528, val:  87.39%, val_best:  88.50%, tr:  97.42%, tr_best:  98.09%, epoch time: 231.80 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4110%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1999%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.8281%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5029440 real_backward_count 1357193  26.985%\n",
      "layer   1  Sparsity: 73.9990%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 21.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 2422.0\n",
      "fc layer 2 self.abs_max_out: 2442.0\n",
      "fc layer 1 self.abs_max_out: 37046.0\n",
      "lif layer 1 self.abs_max_v: 37046.0\n",
      "train - Value 0: 2032 occurrences\n",
      "train - Value 1: 1998 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 201 occurrences\n",
      "test - Value 1: 251 occurrences\n",
      "epoch-156 lr=['2.0000000'], tr/val_loss: 66.076805/ 55.483723, val:  86.06%, val_best:  88.50%, tr:  97.39%, tr_best:  98.09%, epoch time: 232.23 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4127%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2485%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.4454%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5061680 real_backward_count 1365206  26.971%\n",
      "layer   1  Sparsity: 88.2568%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2062 occurrences\n",
      "train - Value 1: 1968 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 225 occurrences\n",
      "test - Value 1: 227 occurrences\n",
      "epoch-157 lr=['2.0000000'], tr/val_loss: 59.986076/ 36.664036, val:  87.39%, val_best:  88.50%, tr:  97.34%, tr_best:  98.09%, epoch time: 229.78 seconds, 3.83 minutes\n",
      "layer   1  Sparsity: 84.4095%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2374%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.4637%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5093920 real_backward_count 1373233  26.958%\n",
      "layer   1  Sparsity: 86.7188%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 37050.0\n",
      "lif layer 1 self.abs_max_v: 37050.0\n",
      "train - Value 0: 2037 occurrences\n",
      "train - Value 1: 1993 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 149 occurrences\n",
      "test - Value 1: 303 occurrences\n",
      "epoch-158 lr=['2.0000000'], tr/val_loss: 64.299980/ 49.935474, val:  79.42%, val_best:  88.50%, tr:  97.12%, tr_best:  98.09%, epoch time: 231.84 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2128%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.3017%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5126160 real_backward_count 1381299  26.946%\n",
      "layer   1  Sparsity: 88.5254%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 37055.0\n",
      "lif layer 1 self.abs_max_v: 37055.0\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 203 occurrences\n",
      "test - Value 1: 249 occurrences\n",
      "epoch-159 lr=['2.0000000'], tr/val_loss: 69.076714/ 67.112831, val:  86.06%, val_best:  88.50%, tr:  97.42%, tr_best:  98.09%, epoch time: 229.99 seconds, 3.83 minutes\n",
      "layer   1  Sparsity: 84.4094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2684%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.0189%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5158400 real_backward_count 1389354  26.934%\n",
      "layer   1  Sparsity: 81.3721%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 37081.0\n",
      "lif layer 1 self.abs_max_v: 37081.0\n",
      "train - Value 0: 2027 occurrences\n",
      "train - Value 1: 2003 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 333 occurrences\n",
      "test - Value 1: 119 occurrences\n",
      "epoch-160 lr=['2.0000000'], tr/val_loss: 66.652718/ 43.564358, val:  74.56%, val_best:  88.50%, tr:  97.37%, tr_best:  98.09%, epoch time: 231.44 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4110%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2612%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.5290%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5190640 real_backward_count 1397341  26.920%\n",
      "layer   1  Sparsity: 88.9893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2048 occurrences\n",
      "train - Value 1: 1982 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 123 occurrences\n",
      "test - Value 1: 329 occurrences\n",
      "epoch-161 lr=['2.0000000'], tr/val_loss: 64.378288/ 48.255669, val:  74.56%, val_best:  88.50%, tr:  97.59%, tr_best:  98.09%, epoch time: 231.61 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4093%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2399%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.3953%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5222880 real_backward_count 1405180  26.904%\n",
      "layer   1  Sparsity: 81.2988%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 2738.0\n",
      "train - Value 0: 2058 occurrences\n",
      "train - Value 1: 1972 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "lif layer 2 self.abs_max_v: 2836.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 266 occurrences\n",
      "test - Value 1: 186 occurrences\n",
      "epoch-162 lr=['2.0000000'], tr/val_loss: 60.049534/ 29.108931, val:  82.74%, val_best:  88.50%, tr:  97.34%, tr_best:  98.09%, epoch time: 230.92 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4110%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2498%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.7022%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5255120 real_backward_count 1413079  26.890%\n",
      "layer   1  Sparsity: 93.5791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 37086.0\n",
      "lif layer 1 self.abs_max_v: 37086.0\n",
      "train - Value 0: 2034 occurrences\n",
      "train - Value 1: 1996 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 74 occurrences\n",
      "test - Value 1: 378 occurrences\n",
      "epoch-163 lr=['2.0000000'], tr/val_loss: 55.814583/ 52.286358, val:  66.37%, val_best:  88.50%, tr:  97.00%, tr_best:  98.09%, epoch time: 232.08 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4083%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2028%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.5906%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5287360 real_backward_count 1421004  26.875%\n",
      "layer   1  Sparsity: 79.3213%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2035 occurrences\n",
      "train - Value 1: 1995 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 294 occurrences\n",
      "test - Value 1: 158 occurrences\n",
      "epoch-164 lr=['2.0000000'], tr/val_loss: 63.504681/ 56.719242, val:  82.74%, val_best:  88.50%, tr:  97.22%, tr_best:  98.09%, epoch time: 231.24 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4115%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2389%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.1505%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5319600 real_backward_count 1429095  26.865%\n",
      "layer   1  Sparsity: 78.8330%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 95.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2070 occurrences\n",
      "train - Value 1: 1960 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 296 occurrences\n",
      "test - Value 1: 156 occurrences\n",
      "epoch-165 lr=['2.0000000'], tr/val_loss: 60.665504/ 31.063898, val:  81.42%, val_best:  88.50%, tr:  97.05%, tr_best:  98.09%, epoch time: 230.48 seconds, 3.84 minutes\n",
      "layer   1  Sparsity: 84.4116%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2745%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.3524%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5351840 real_backward_count 1437107  26.853%\n",
      "layer   1  Sparsity: 81.3721%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2045 occurrences\n",
      "train - Value 1: 1985 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 279 occurrences\n",
      "test - Value 1: 173 occurrences\n",
      "epoch-166 lr=['2.0000000'], tr/val_loss: 56.358608/ 47.686718, val:  85.18%, val_best:  88.50%, tr:  97.17%, tr_best:  98.09%, epoch time: 230.78 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4110%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2611%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.4856%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5384080 real_backward_count 1445147  26.841%\n",
      "layer   1  Sparsity: 91.5527%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2047 occurrences\n",
      "train - Value 1: 1983 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 67 occurrences\n",
      "test - Value 1: 385 occurrences\n",
      "epoch-167 lr=['2.0000000'], tr/val_loss: 58.303825/ 49.752296, val:  64.82%, val_best:  88.50%, tr:  97.77%, tr_best:  98.09%, epoch time: 231.14 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4087%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2314%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.4922%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5416320 real_backward_count 1453226  26.831%\n",
      "layer   1  Sparsity: 80.3467%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2043 occurrences\n",
      "train - Value 1: 1987 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 263 occurrences\n",
      "test - Value 1: 189 occurrences\n",
      "epoch-168 lr=['2.0000000'], tr/val_loss: 58.889469/ 34.642269, val:  85.62%, val_best:  88.50%, tr:  97.57%, tr_best:  98.09%, epoch time: 231.62 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2556%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.3881%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5448560 real_backward_count 1461235  26.819%\n",
      "layer   1  Sparsity: 84.3506%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2090 occurrences\n",
      "train - Value 1: 1940 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 208 occurrences\n",
      "test - Value 1: 244 occurrences\n",
      "epoch-169 lr=['2.0000000'], tr/val_loss: 55.416557/ 53.813515, val:  87.17%, val_best:  88.50%, tr:  97.30%, tr_best:  98.09%, epoch time: 230.66 seconds, 3.84 minutes\n",
      "layer   1  Sparsity: 84.4104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2607%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.4423%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5480800 real_backward_count 1469168  26.806%\n",
      "layer   1  Sparsity: 60.5469%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 91.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2053 occurrences\n",
      "train - Value 1: 1977 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 121 occurrences\n",
      "test - Value 1: 331 occurrences\n",
      "epoch-170 lr=['2.0000000'], tr/val_loss: 54.897514/ 33.998508, val:  74.56%, val_best:  88.50%, tr:  97.22%, tr_best:  98.09%, epoch time: 231.26 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4157%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2620%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8498%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5513040 real_backward_count 1476996  26.791%\n",
      "layer   1  Sparsity: 79.0527%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2071 occurrences\n",
      "train - Value 1: 1959 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 70 occurrences\n",
      "test - Value 1: 382 occurrences\n",
      "epoch-171 lr=['2.0000000'], tr/val_loss: 56.518433/ 61.273174, val:  65.49%, val_best:  88.50%, tr:  96.97%, tr_best:  98.09%, epoch time: 230.45 seconds, 3.84 minutes\n",
      "layer   1  Sparsity: 84.4115%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2736%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8727%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5545280 real_backward_count 1484966  26.779%\n",
      "layer   1  Sparsity: 84.4727%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2039 occurrences\n",
      "train - Value 1: 1991 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 101 occurrences\n",
      "test - Value 1: 351 occurrences\n",
      "epoch-172 lr=['2.0000000'], tr/val_loss: 64.680206/ 85.919136, val:  72.35%, val_best:  88.50%, tr:  97.97%, tr_best:  98.09%, epoch time: 229.67 seconds, 3.83 minutes\n",
      "layer   1  Sparsity: 84.4103%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2933%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.2054%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5577520 real_backward_count 1492788  26.764%\n",
      "layer   1  Sparsity: 76.5381%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 95.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2025 occurrences\n",
      "train - Value 1: 2005 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 179 occurrences\n",
      "test - Value 1: 273 occurrences\n",
      "epoch-173 lr=['2.0000000'], tr/val_loss: 67.307411/ 57.777046, val:  83.85%, val_best:  88.50%, tr:  97.62%, tr_best:  98.09%, epoch time: 230.48 seconds, 3.84 minutes\n",
      "layer   1  Sparsity: 84.4121%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2537%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.8336%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5609760 real_backward_count 1500760  26.753%\n",
      "layer   1  Sparsity: 92.9443%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 99.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2043 occurrences\n",
      "train - Value 1: 1987 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 259 occurrences\n",
      "test - Value 1: 193 occurrences\n",
      "epoch-174 lr=['2.0000000'], tr/val_loss: 70.029816/ 57.753418, val:  88.72%, val_best:  88.72%, tr:  97.07%, tr_best:  98.09%, epoch time: 230.88 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4084%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2322%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.3011%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5642000 real_backward_count 1508789  26.742%\n",
      "layer   1  Sparsity: 81.3721%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 21 occurrences\n",
      "test - Value 1: 431 occurrences\n",
      "epoch-175 lr=['2.0000000'], tr/val_loss: 60.650394/108.823326, val:  54.65%, val_best:  88.72%, tr:  97.02%, tr_best:  98.09%, epoch time: 230.91 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4110%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2016%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.6457%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5674240 real_backward_count 1516907  26.733%\n",
      "layer   1  Sparsity: 86.5479%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 193 occurrences\n",
      "test - Value 1: 259 occurrences\n",
      "epoch-176 lr=['2.0000000'], tr/val_loss: 68.073418/ 49.647434, val:  83.85%, val_best:  88.72%, tr:  97.30%, tr_best:  98.09%, epoch time: 230.36 seconds, 3.84 minutes\n",
      "layer   1  Sparsity: 84.4099%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2184%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.1413%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5706480 real_backward_count 1524869  26.722%\n",
      "layer   1  Sparsity: 86.3770%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2023 occurrences\n",
      "train - Value 1: 2007 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 24 occurrences\n",
      "test - Value 1: 428 occurrences\n",
      "epoch-177 lr=['2.0000000'], tr/val_loss: 64.995262/ 67.161545, val:  55.31%, val_best:  88.72%, tr:  97.87%, tr_best:  98.09%, epoch time: 230.09 seconds, 3.83 minutes\n",
      "layer   1  Sparsity: 84.4099%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2343%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.7251%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5738720 real_backward_count 1532789  26.710%\n",
      "layer   1  Sparsity: 84.7656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 2864.0\n",
      "train - Value 0: 2035 occurrences\n",
      "train - Value 1: 1995 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 110 occurrences\n",
      "test - Value 1: 342 occurrences\n",
      "epoch-178 lr=['2.0000000'], tr/val_loss: 64.927208/ 48.439095, val:  73.89%, val_best:  88.72%, tr:  97.47%, tr_best:  98.09%, epoch time: 229.91 seconds, 3.83 minutes\n",
      "layer   1  Sparsity: 84.4103%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2455%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.2940%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5770960 real_backward_count 1540846  26.700%\n",
      "layer   1  Sparsity: 79.1992%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2029 occurrences\n",
      "train - Value 1: 2001 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 77 occurrences\n",
      "test - Value 1: 375 occurrences\n",
      "epoch-179 lr=['2.0000000'], tr/val_loss: 60.244217/ 49.706150, val:  66.59%, val_best:  88.72%, tr:  98.06%, tr_best:  98.09%, epoch time: 232.12 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4115%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2599%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.8662%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5803200 real_backward_count 1548797  26.689%\n",
      "layer   1  Sparsity: 90.5029%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 37092.0\n",
      "lif layer 1 self.abs_max_v: 37092.0\n",
      "train - Value 0: 2041 occurrences\n",
      "train - Value 1: 1989 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 251 occurrences\n",
      "test - Value 1: 201 occurrences\n",
      "epoch-180 lr=['2.0000000'], tr/val_loss: 63.185459/ 66.494621, val:  87.83%, val_best:  88.72%, tr:  97.47%, tr_best:  98.09%, epoch time: 230.48 seconds, 3.84 minutes\n",
      "layer   1  Sparsity: 84.4090%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2648%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.1322%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5835440 real_backward_count 1556703  26.677%\n",
      "layer   1  Sparsity: 81.9336%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2032 occurrences\n",
      "train - Value 1: 1998 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 73 occurrences\n",
      "test - Value 1: 379 occurrences\n",
      "epoch-181 lr=['2.0000000'], tr/val_loss: 66.583145/ 63.151478, val:  66.15%, val_best:  88.72%, tr:  97.89%, tr_best:  98.09%, epoch time: 231.38 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4109%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2883%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.1014%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5867680 real_backward_count 1564504  26.663%\n",
      "layer   1  Sparsity: 79.5410%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2035 occurrences\n",
      "train - Value 1: 1995 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 195 occurrences\n",
      "test - Value 1: 257 occurrences\n",
      "epoch-182 lr=['2.0000000'], tr/val_loss: 58.108231/ 39.225433, val:  86.06%, val_best:  88.72%, tr:  98.21%, tr_best:  98.21%, epoch time: 230.26 seconds, 3.84 minutes\n",
      "layer   1  Sparsity: 84.4114%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2932%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.7409%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5899920 real_backward_count 1572217  26.648%\n",
      "layer   1  Sparsity: 76.9775%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 95.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2056 occurrences\n",
      "train - Value 1: 1974 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "lif layer 2 self.abs_max_v: 2986.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 208 occurrences\n",
      "test - Value 1: 244 occurrences\n",
      "epoch-183 lr=['2.0000000'], tr/val_loss: 52.713165/ 17.099495, val:  84.96%, val_best:  88.72%, tr:  97.59%, tr_best:  98.21%, epoch time: 231.54 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4120%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2763%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6741%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5932160 real_backward_count 1579847  26.632%\n",
      "layer   1  Sparsity: 78.9795%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 3026.5\n",
      "fc layer 1 self.abs_max_out: 37100.0\n",
      "lif layer 1 self.abs_max_v: 37100.0\n",
      "lif layer 2 self.abs_max_v: 3056.0\n",
      "train - Value 0: 2049 occurrences\n",
      "train - Value 1: 1981 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 168 occurrences\n",
      "test - Value 1: 284 occurrences\n",
      "epoch-184 lr=['2.0000000'], tr/val_loss: 52.013554/ 45.355942, val:  82.30%, val_best:  88.72%, tr:  97.87%, tr_best:  98.21%, epoch time: 230.26 seconds, 3.84 minutes\n",
      "layer   1  Sparsity: 84.4116%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2742%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5225%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5964400 real_backward_count 1587534  26.617%\n",
      "layer   1  Sparsity: 86.7188%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 37116.0\n",
      "lif layer 1 self.abs_max_v: 37116.0\n",
      "train - Value 0: 2048 occurrences\n",
      "train - Value 1: 1982 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 189 occurrences\n",
      "test - Value 1: 263 occurrences\n",
      "epoch-185 lr=['2.0000000'], tr/val_loss: 65.413658/ 66.559181, val:  84.29%, val_best:  88.72%, tr:  97.59%, tr_best:  98.21%, epoch time: 230.06 seconds, 3.83 minutes\n",
      "layer   1  Sparsity: 84.4098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3095%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.6950%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5996640 real_backward_count 1595300  26.603%\n",
      "layer   1  Sparsity: 79.8340%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 95.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2082 occurrences\n",
      "train - Value 1: 1948 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 125 occurrences\n",
      "test - Value 1: 327 occurrences\n",
      "epoch-186 lr=['2.0000000'], tr/val_loss: 53.461094/ 58.602509, val:  75.00%, val_best:  88.72%, tr:  96.70%, tr_best:  98.21%, epoch time: 229.80 seconds, 3.83 minutes\n",
      "layer   1  Sparsity: 84.4114%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2808%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.7150%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6028880 real_backward_count 1603320  26.594%\n",
      "layer   1  Sparsity: 76.0498%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 95.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 2461.0\n",
      "train - Value 0: 2076 occurrences\n",
      "train - Value 1: 1954 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 60 occurrences\n",
      "test - Value 1: 392 occurrences\n",
      "epoch-187 lr=['2.0000000'], tr/val_loss: 51.555111/ 66.900650, val:  63.27%, val_best:  88.72%, tr:  97.20%, tr_best:  98.21%, epoch time: 229.99 seconds, 3.83 minutes\n",
      "layer   1  Sparsity: 84.4122%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2229%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5711%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6061120 real_backward_count 1611117  26.581%\n",
      "layer   1  Sparsity: 78.8330%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 96.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2083 occurrences\n",
      "train - Value 1: 1947 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-188 lr=['2.0000000'], tr/val_loss: 47.348042/ 26.736605, val:  50.00%, val_best:  88.72%, tr:  97.37%, tr_best:  98.21%, epoch time: 230.02 seconds, 3.83 minutes\n",
      "layer   1  Sparsity: 84.4116%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2683%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3468%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6093360 real_backward_count 1618930  26.569%\n",
      "layer   1  Sparsity: 77.0264%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 95.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2068 occurrences\n",
      "train - Value 1: 1962 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 218 occurrences\n",
      "test - Value 1: 234 occurrences\n",
      "epoch-189 lr=['2.0000000'], tr/val_loss: 49.439793/ 38.281502, val:  88.50%, val_best:  88.72%, tr:  97.30%, tr_best:  98.21%, epoch time: 228.84 seconds, 3.81 minutes\n",
      "layer   1  Sparsity: 84.4120%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2837%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9810%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6125600 real_backward_count 1626727  26.556%\n",
      "layer   1  Sparsity: 73.5352%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 37120.0\n",
      "lif layer 1 self.abs_max_v: 37120.0\n",
      "train - Value 0: 2068 occurrences\n",
      "train - Value 1: 1962 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 260 occurrences\n",
      "test - Value 1: 192 occurrences\n",
      "epoch-190 lr=['2.0000000'], tr/val_loss: 52.611916/ 21.754728, val:  84.07%, val_best:  88.72%, tr:  97.54%, tr_best:  98.21%, epoch time: 231.40 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4128%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2312%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7318%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6157840 real_backward_count 1634528  26.544%\n",
      "layer   1  Sparsity: 89.1846%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2038 occurrences\n",
      "train - Value 1: 1992 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 112 occurrences\n",
      "test - Value 1: 340 occurrences\n",
      "epoch-191 lr=['2.0000000'], tr/val_loss: 61.040565/ 51.555237, val:  73.01%, val_best:  88.72%, tr:  98.04%, tr_best:  98.21%, epoch time: 231.34 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4093%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2629%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.9383%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6190080 real_backward_count 1642340  26.532%\n",
      "layer   1  Sparsity: 84.8145%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2058 occurrences\n",
      "train - Value 1: 1972 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 166 occurrences\n",
      "test - Value 1: 286 occurrences\n",
      "epoch-192 lr=['2.0000000'], tr/val_loss: 58.566647/ 34.994473, val:  79.65%, val_best:  88.72%, tr:  97.39%, tr_best:  98.21%, epoch time: 231.01 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4103%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2417%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.0202%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6222320 real_backward_count 1650215  26.521%\n",
      "layer   1  Sparsity: 90.9912%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2056 occurrences\n",
      "train - Value 1: 1974 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 151 occurrences\n",
      "test - Value 1: 301 occurrences\n",
      "epoch-193 lr=['2.0000000'], tr/val_loss: 54.691689/ 44.792690, val:  78.98%, val_best:  88.72%, tr:  97.59%, tr_best:  98.21%, epoch time: 232.27 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4089%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1962%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.8107%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6254560 real_backward_count 1657997  26.509%\n",
      "layer   1  Sparsity: 88.3057%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2038 occurrences\n",
      "train - Value 1: 1992 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 138 occurrences\n",
      "test - Value 1: 314 occurrences\n",
      "epoch-194 lr=['2.0000000'], tr/val_loss: 49.760433/ 41.721035, val:  76.99%, val_best:  88.72%, tr:  97.59%, tr_best:  98.21%, epoch time: 231.14 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4095%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2314%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1597%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6286800 real_backward_count 1665663  26.495%\n",
      "layer   1  Sparsity: 75.4639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 95.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2046 occurrences\n",
      "train - Value 1: 1984 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 178 occurrences\n",
      "test - Value 1: 274 occurrences\n",
      "epoch-195 lr=['2.0000000'], tr/val_loss: 52.861755/ 53.235115, val:  83.19%, val_best:  88.72%, tr:  98.04%, tr_best:  98.21%, epoch time: 230.82 seconds, 3.85 minutes\n",
      "layer   1  Sparsity: 84.4123%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2102%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7321%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6319040 real_backward_count 1673543  26.484%\n",
      "layer   1  Sparsity: 90.4053%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 37137.0\n",
      "lif layer 1 self.abs_max_v: 37137.0\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 216 occurrences\n",
      "test - Value 1: 236 occurrences\n",
      "epoch-196 lr=['2.0000000'], tr/val_loss: 55.564354/ 49.209209, val:  83.63%, val_best:  88.72%, tr:  97.67%, tr_best:  98.21%, epoch time: 231.53 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4090%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1214%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8175%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6351280 real_backward_count 1681434  26.474%\n",
      "layer   1  Sparsity: 83.6182%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2025 occurrences\n",
      "train - Value 1: 2005 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 286 occurrences\n",
      "test - Value 1: 166 occurrences\n",
      "epoch-197 lr=['2.0000000'], tr/val_loss: 69.015488/ 66.335884, val:  83.63%, val_best:  88.72%, tr:  97.57%, tr_best:  98.21%, epoch time: 232.07 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4105%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1324%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.6697%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6383520 real_backward_count 1689350  26.464%\n",
      "layer   1  Sparsity: 80.9326%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2046 occurrences\n",
      "train - Value 1: 1984 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 212 occurrences\n",
      "test - Value 1: 240 occurrences\n",
      "epoch-198 lr=['2.0000000'], tr/val_loss: 56.994511/ 57.788280, val:  88.94%, val_best:  88.94%, tr:  98.14%, tr_best:  98.21%, epoch time: 231.86 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4111%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.1973%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.3109%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6415760 real_backward_count 1697210  26.454%\n",
      "layer   1  Sparsity: 93.9453%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 98.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 37157.0\n",
      "lif layer 1 self.abs_max_v: 37157.0\n",
      "train - Value 0: 2028 occurrences\n",
      "train - Value 1: 2002 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 53 occurrences\n",
      "test - Value 1: 399 occurrences\n",
      "epoch-199 lr=['2.0000000'], tr/val_loss: 56.196697/ 52.599068, val:  61.73%, val_best:  88.94%, tr:  98.29%, tr_best:  98.29%, epoch time: 232.21 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4082%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 97.2208%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.6837%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b407df9e6744c44b74803fa04986465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÖ‚ñÉ‚ñÑ‚ñÅ‚ñÖ‚ñÇ‚ñá‚ñà‚ñá‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÑ‚ñÖ‚ñá‚ñÇ‚ñÅ‚ñá‚ñá‚ñá‚ñà‚ñÉ‚ñá‚ñà‚ñÉ‚ñà‚ñÇ‚ñá‚ñá‚ñà‚ñà‚ñÑ‚ñÑ‚ñá‚ñÇ‚ñá‚ñÉ‚ñÜ‚ñÉ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÅ‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÜ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÖ‚ñÉ‚ñÑ‚ñÅ‚ñÖ‚ñÇ‚ñá‚ñà‚ñá‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÑ‚ñÖ‚ñá‚ñÇ‚ñÅ‚ñá‚ñá‚ñá‚ñà‚ñÉ‚ñá‚ñà‚ñÉ‚ñà‚ñÇ‚ñá‚ñá‚ñà‚ñà‚ñÑ‚ñÑ‚ñá‚ñÇ‚ñá‚ñÉ‚ñÜ‚ñÉ</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÇ‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñá‚ñà‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñà‚ñÜ‚ñÉ‚ñÖ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÜ‚ñÑ‚ñá‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÅ‚ñÖ‚ñÉ‚ñÑ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.98288</td></tr><tr><td>tr_epoch_loss</td><td>56.1967</td></tr><tr><td>val_acc_best</td><td>0.88938</td></tr><tr><td>val_acc_now</td><td>0.61726</td></tr><tr><td>val_loss</td><td>52.59907</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">neat-sweep-15</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/sbkllp1v' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/sbkllp1v</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251225_093054-sbkllp1v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: s5byf0r1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloser_encourage_mode: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttimestep_sums_threshold: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: n_tidigits_tonic\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251225_222320-s5byf0r1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/s5byf0r1' target=\"_blank\">eager-sweep-22</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9lv70ttl' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9lv70ttl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9lv70ttl' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9lv70ttl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/s5byf0r1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/s5byf0r1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'timestep_sums_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'loser_encourage_mode' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251225_222328_941', 'my_seed': 42, 'TIME': 8, 'BATCH': 1, 'IMAGE_SIZE': 8, 'which_data': 'n_tidigits_tonic', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 128, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 1, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 1, 'dvs_duration': 2, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': False, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 8, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'timestep_sums_threshold': 0, 'lif_layer_sg_width2': 2, 'lif_layer_v_threshold2': 32, 'init_scaling': [0.5, 0.25, 0.0625], 'learning_rate': 1, 'learning_rate2': 1, 'loser_encourage_mode': False} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target word: 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target word: 2\n",
      "\n",
      "\n",
      "\n",
      "train_dataset length = 4030, test_dataset length = 452\n",
      "\n",
      "len(train_loader): 4030 BATCH: 1 train_data_count: 4030\n",
      "len(test_loader): 452 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 1, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 2, self.v_threshold 32\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=512, out_features=200, TIME=8, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.25, 0.0625])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=8, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=8, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.25, 0.0625])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=32, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=8, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=8, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.25, 0.0625])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 144,400\n",
      "========================================================\n",
      "\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 1594.0\n",
      "lif layer 1 self.abs_max_v: 1594.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 672.0\n",
      "lif layer 2 self.abs_max_v: 672.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 69.0\n",
      "lif layer 1 self.abs_max_v: 2073.0\n",
      "lif layer 2 self.abs_max_v: 846.0\n",
      "fc layer 3 self.abs_max_out: 70.0\n",
      "layer   1  Sparsity: 88.8916%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 82.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 74.0\n",
      "fc layer 3 self.abs_max_out: 78.0\n",
      "fc layer 1 self.abs_max_out: 1880.0\n",
      "lif layer 1 self.abs_max_v: 2088.0\n",
      "fc layer 3 self.abs_max_out: 87.0\n",
      "lif layer 1 self.abs_max_v: 2231.5\n",
      "lif layer 2 self.abs_max_v: 916.0\n",
      "fc layer 3 self.abs_max_out: 134.0\n",
      "fc layer 1 self.abs_max_out: 1887.0\n",
      "lif layer 1 self.abs_max_v: 2327.0\n",
      "fc layer 3 self.abs_max_out: 150.0\n",
      "fc layer 1 self.abs_max_out: 1912.0\n",
      "lif layer 2 self.abs_max_v: 974.5\n",
      "fc layer 3 self.abs_max_out: 173.0\n",
      "lif layer 1 self.abs_max_v: 2423.5\n",
      "fc layer 1 self.abs_max_out: 2129.0\n",
      "lif layer 1 self.abs_max_v: 2471.0\n",
      "fc layer 3 self.abs_max_out: 207.0\n",
      "fc layer 1 self.abs_max_out: 2353.0\n",
      "lif layer 1 self.abs_max_v: 2573.0\n",
      "lif layer 1 self.abs_max_v: 2719.0\n",
      "lif layer 1 self.abs_max_v: 2946.0\n",
      "lif layer 1 self.abs_max_v: 3428.5\n",
      "fc layer 3 self.abs_max_out: 208.0\n",
      "fc layer 3 self.abs_max_out: 210.0\n",
      "lif layer 2 self.abs_max_v: 983.0\n",
      "fc layer 2 self.abs_max_out: 729.0\n",
      "lif layer 2 self.abs_max_v: 1014.0\n",
      "fc layer 3 self.abs_max_out: 231.0\n",
      "fc layer 2 self.abs_max_out: 737.0\n",
      "fc layer 3 self.abs_max_out: 233.0\n",
      "fc layer 3 self.abs_max_out: 237.0\n",
      "fc layer 2 self.abs_max_out: 743.0\n",
      "lif layer 2 self.abs_max_v: 1076.0\n",
      "fc layer 2 self.abs_max_out: 794.0\n",
      "lif layer 2 self.abs_max_v: 1112.0\n",
      "fc layer 3 self.abs_max_out: 247.0\n",
      "fc layer 1 self.abs_max_out: 2424.0\n",
      "fc layer 2 self.abs_max_out: 803.0\n",
      "fc layer 1 self.abs_max_out: 2448.0\n",
      "fc layer 2 self.abs_max_out: 805.0\n",
      "fc layer 1 self.abs_max_out: 2589.0\n",
      "lif layer 1 self.abs_max_v: 3698.5\n",
      "fc layer 1 self.abs_max_out: 2606.0\n",
      "fc layer 1 self.abs_max_out: 2609.0\n",
      "fc layer 3 self.abs_max_out: 257.0\n",
      "fc layer 1 self.abs_max_out: 2851.0\n",
      "fc layer 1 self.abs_max_out: 3441.0\n",
      "fc layer 3 self.abs_max_out: 278.0\n",
      "fc layer 1 self.abs_max_out: 3515.0\n",
      "lif layer 2 self.abs_max_v: 1128.0\n",
      "lif layer 2 self.abs_max_v: 1128.5\n",
      "lif layer 2 self.abs_max_v: 1213.0\n",
      "fc layer 1 self.abs_max_out: 3546.0\n",
      "lif layer 1 self.abs_max_v: 3810.5\n",
      "lif layer 2 self.abs_max_v: 1228.0\n",
      "lif layer 2 self.abs_max_v: 1331.0\n",
      "lif layer 2 self.abs_max_v: 1336.0\n",
      "fc layer 2 self.abs_max_out: 843.0\n",
      "fc layer 2 self.abs_max_out: 871.0\n",
      "fc layer 3 self.abs_max_out: 281.0\n",
      "fc layer 3 self.abs_max_out: 282.0\n",
      "fc layer 3 self.abs_max_out: 285.0\n",
      "fc layer 3 self.abs_max_out: 293.0\n",
      "fc layer 3 self.abs_max_out: 295.0\n",
      "fc layer 3 self.abs_max_out: 300.0\n",
      "fc layer 3 self.abs_max_out: 318.0\n",
      "lif layer 2 self.abs_max_v: 1340.5\n",
      "lif layer 2 self.abs_max_v: 1352.5\n",
      "lif layer 2 self.abs_max_v: 1372.5\n",
      "lif layer 2 self.abs_max_v: 1420.5\n",
      "lif layer 1 self.abs_max_v: 3846.0\n",
      "lif layer 2 self.abs_max_v: 1470.0\n",
      "fc layer 1 self.abs_max_out: 3687.0\n",
      "lif layer 1 self.abs_max_v: 4098.5\n",
      "lif layer 1 self.abs_max_v: 4514.0\n",
      "fc layer 2 self.abs_max_out: 919.0\n",
      "lif layer 1 self.abs_max_v: 4604.5\n",
      "lif layer 1 self.abs_max_v: 4627.5\n",
      "lif layer 1 self.abs_max_v: 4994.0\n",
      "fc layer 2 self.abs_max_out: 939.0\n",
      "lif layer 2 self.abs_max_v: 1483.0\n",
      "lif layer 2 self.abs_max_v: 1493.5\n",
      "lif layer 2 self.abs_max_v: 1495.5\n",
      "fc layer 2 self.abs_max_out: 1011.0\n",
      "lif layer 2 self.abs_max_v: 1614.0\n",
      "lif layer 2 self.abs_max_v: 1617.5\n",
      "lif layer 2 self.abs_max_v: 1634.0\n",
      "lif layer 2 self.abs_max_v: 1783.0\n",
      "fc layer 2 self.abs_max_out: 1023.0\n",
      "fc layer 2 self.abs_max_out: 1067.0\n",
      "lif layer 2 self.abs_max_v: 1832.0\n",
      "fc layer 2 self.abs_max_out: 1111.0\n",
      "fc layer 2 self.abs_max_out: 1131.0\n",
      "fc layer 3 self.abs_max_out: 328.0\n",
      "fc layer 3 self.abs_max_out: 329.0\n",
      "lif layer 2 self.abs_max_v: 1865.5\n",
      "lif layer 2 self.abs_max_v: 1892.5\n",
      "lif layer 1 self.abs_max_v: 5041.5\n",
      "lif layer 2 self.abs_max_v: 1929.0\n",
      "lif layer 1 self.abs_max_v: 5131.5\n",
      "fc layer 2 self.abs_max_out: 1200.0\n",
      "fc layer 3 self.abs_max_out: 354.0\n",
      "fc layer 2 self.abs_max_out: 1247.0\n",
      "fc layer 2 self.abs_max_out: 1341.0\n",
      "fc layer 2 self.abs_max_out: 1445.0\n",
      "lif layer 1 self.abs_max_v: 5293.0\n",
      "fc layer 2 self.abs_max_out: 1457.0\n",
      "lif layer 2 self.abs_max_v: 2067.0\n",
      "fc layer 2 self.abs_max_out: 1459.0\n",
      "lif layer 2 self.abs_max_v: 2072.5\n",
      "fc layer 2 self.abs_max_out: 1467.0\n",
      "lif layer 2 self.abs_max_v: 2109.5\n",
      "lif layer 2 self.abs_max_v: 2182.0\n",
      "fc layer 2 self.abs_max_out: 1497.0\n",
      "fc layer 2 self.abs_max_out: 1558.0\n",
      "fc layer 3 self.abs_max_out: 379.0\n",
      "fc layer 2 self.abs_max_out: 1598.0\n",
      "lif layer 2 self.abs_max_v: 2188.0\n",
      "lif layer 2 self.abs_max_v: 2243.0\n",
      "fc layer 3 self.abs_max_out: 386.0\n",
      "fc layer 3 self.abs_max_out: 392.0\n",
      "fc layer 3 self.abs_max_out: 394.0\n",
      "lif layer 2 self.abs_max_v: 2307.0\n",
      "lif layer 1 self.abs_max_v: 5540.0\n",
      "lif layer 1 self.abs_max_v: 5637.0\n",
      "fc layer 2 self.abs_max_out: 1603.0\n",
      "lif layer 1 self.abs_max_v: 5810.5\n",
      "lif layer 2 self.abs_max_v: 2321.5\n",
      "lif layer 2 self.abs_max_v: 2338.0\n",
      "fc layer 3 self.abs_max_out: 405.0\n",
      "fc layer 3 self.abs_max_out: 412.0\n",
      "lif layer 2 self.abs_max_v: 2341.0\n",
      "lif layer 2 self.abs_max_v: 2384.5\n",
      "lif layer 1 self.abs_max_v: 5822.5\n",
      "fc layer 1 self.abs_max_out: 3728.0\n",
      "lif layer 1 self.abs_max_v: 5823.5\n",
      "fc layer 2 self.abs_max_out: 1688.0\n",
      "fc layer 1 self.abs_max_out: 3738.0\n",
      "fc layer 2 self.abs_max_out: 1725.0\n",
      "fc layer 2 self.abs_max_out: 1775.0\n",
      "fc layer 2 self.abs_max_out: 1833.0\n",
      "lif layer 2 self.abs_max_v: 2397.5\n",
      "fc layer 2 self.abs_max_out: 1899.0\n",
      "lif layer 2 self.abs_max_v: 2402.0\n",
      "fc layer 1 self.abs_max_out: 3771.0\n",
      "lif layer 2 self.abs_max_v: 2404.0\n",
      "lif layer 2 self.abs_max_v: 2525.0\n",
      "fc layer 1 self.abs_max_out: 3827.0\n",
      "lif layer 1 self.abs_max_v: 5928.0\n",
      "lif layer 2 self.abs_max_v: 2688.5\n",
      "lif layer 2 self.abs_max_v: 2704.5\n",
      "fc layer 1 self.abs_max_out: 3928.0\n",
      "lif layer 1 self.abs_max_v: 6024.5\n",
      "lif layer 1 self.abs_max_v: 6400.5\n",
      "fc layer 1 self.abs_max_out: 4144.0\n",
      "lif layer 1 self.abs_max_v: 6459.5\n",
      "lif layer 1 self.abs_max_v: 6823.0\n",
      "lif layer 2 self.abs_max_v: 2769.0\n",
      "fc layer 1 self.abs_max_out: 4210.0\n",
      "lif layer 1 self.abs_max_v: 6894.5\n",
      "lif layer 2 self.abs_max_v: 2798.5\n",
      "lif layer 2 self.abs_max_v: 2812.0\n",
      "lif layer 2 self.abs_max_v: 2858.0\n",
      "lif layer 2 self.abs_max_v: 2909.5\n",
      "lif layer 2 self.abs_max_v: 2994.5\n",
      "fc layer 2 self.abs_max_out: 1979.0\n",
      "lif layer 2 self.abs_max_v: 3005.5\n",
      "lif layer 2 self.abs_max_v: 3042.5\n",
      "fc layer 1 self.abs_max_out: 4242.0\n",
      "fc layer 1 self.abs_max_out: 4267.0\n",
      "lif layer 1 self.abs_max_v: 6954.0\n",
      "lif layer 2 self.abs_max_v: 3057.5\n",
      "fc layer 1 self.abs_max_out: 4315.0\n",
      "fc layer 1 self.abs_max_out: 4322.0\n",
      "fc layer 1 self.abs_max_out: 4327.0\n",
      "fc layer 1 self.abs_max_out: 4357.0\n",
      "lif layer 2 self.abs_max_v: 3115.5\n",
      "fc layer 3 self.abs_max_out: 431.0\n",
      "lif layer 2 self.abs_max_v: 3247.5\n",
      "fc layer 3 self.abs_max_out: 455.0\n",
      "fc layer 3 self.abs_max_out: 466.0\n",
      "lif layer 2 self.abs_max_v: 3306.5\n",
      "fc layer 1 self.abs_max_out: 4427.0\n",
      "fc layer 1 self.abs_max_out: 4429.0\n",
      "fc layer 1 self.abs_max_out: 4650.0\n",
      "lif layer 2 self.abs_max_v: 3327.0\n",
      "fc layer 1 self.abs_max_out: 4759.0\n",
      "fc layer 1 self.abs_max_out: 4835.0\n",
      "fc layer 1 self.abs_max_out: 4866.0\n",
      "fc layer 3 self.abs_max_out: 468.0\n",
      "lif layer 2 self.abs_max_v: 3374.5\n",
      "fc layer 3 self.abs_max_out: 481.0\n",
      "fc layer 3 self.abs_max_out: 487.0\n",
      "lif layer 2 self.abs_max_v: 3376.0\n",
      "fc layer 2 self.abs_max_out: 1986.0\n",
      "fc layer 2 self.abs_max_out: 1988.0\n",
      "fc layer 2 self.abs_max_out: 2072.0\n",
      "fc layer 3 self.abs_max_out: 491.0\n",
      "fc layer 3 self.abs_max_out: 502.0\n",
      "fc layer 2 self.abs_max_out: 2077.0\n",
      "fc layer 2 self.abs_max_out: 2084.0\n",
      "fc layer 2 self.abs_max_out: 2215.0\n",
      "fc layer 2 self.abs_max_out: 2264.0\n",
      "fc layer 2 self.abs_max_out: 2274.0\n",
      "fc layer 2 self.abs_max_out: 2339.0\n",
      "lif layer 1 self.abs_max_v: 7410.5\n",
      "fc layer 2 self.abs_max_out: 2440.0\n",
      "lif layer 1 self.abs_max_v: 7458.0\n",
      "fc layer 3 self.abs_max_out: 549.0\n",
      "fc layer 1 self.abs_max_out: 4904.0\n",
      "fc layer 1 self.abs_max_out: 5007.0\n",
      "fc layer 1 self.abs_max_out: 5040.0\n",
      "fc layer 1 self.abs_max_out: 5111.0\n",
      "train - Value 0: 1962 occurrences\n",
      "train - Value 1: 2068 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 848.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 1743.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 1932.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 2476.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 2608.00 at epoch 0, iter 4029\n",
      "fc layer 1 self.abs_max_out: 5176.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss:175.301590/229.979721, val:  50.00%, val_best:  50.00%, tr:  90.40%, tr_best:  90.40%, epoch time: 232.79 seconds, 3.88 minutes\n",
      "layer   1  Sparsity: 84.4093%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2583%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9449%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 32240 real_backward_count 8572  26.588%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "layer   1  Sparsity: 84.8389%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 3399.0\n",
      "lif layer 1 self.abs_max_v: 7565.5\n",
      "lif layer 2 self.abs_max_v: 3403.5\n",
      "lif layer 1 self.abs_max_v: 7595.0\n",
      "lif layer 1 self.abs_max_v: 7926.0\n",
      "fc layer 1 self.abs_max_out: 5313.0\n",
      "lif layer 1 self.abs_max_v: 8844.5\n",
      "fc layer 1 self.abs_max_out: 5317.0\n",
      "lif layer 2 self.abs_max_v: 3614.5\n",
      "lif layer 1 self.abs_max_v: 8870.5\n",
      "fc layer 1 self.abs_max_out: 5363.0\n",
      "fc layer 1 self.abs_max_out: 5570.0\n",
      "lif layer 2 self.abs_max_v: 3899.5\n",
      "lif layer 2 self.abs_max_v: 3933.5\n",
      "lif layer 1 self.abs_max_v: 8969.5\n",
      "lif layer 1 self.abs_max_v: 9172.0\n",
      "fc layer 1 self.abs_max_out: 5685.0\n",
      "lif layer 2 self.abs_max_v: 3972.5\n",
      "lif layer 2 self.abs_max_v: 4081.0\n",
      "lif layer 1 self.abs_max_v: 9333.0\n",
      "lif layer 1 self.abs_max_v: 9628.5\n",
      "lif layer 1 self.abs_max_v: 9921.5\n",
      "lif layer 2 self.abs_max_v: 4187.5\n",
      "fc layer 3 self.abs_max_out: 569.0\n",
      "fc layer 1 self.abs_max_out: 5718.0\n",
      "fc layer 3 self.abs_max_out: 590.0\n",
      "fc layer 3 self.abs_max_out: 610.0\n",
      "fc layer 3 self.abs_max_out: 611.0\n",
      "fc layer 1 self.abs_max_out: 5871.0\n",
      "fc layer 3 self.abs_max_out: 668.0\n",
      "fc layer 1 self.abs_max_out: 5898.0\n",
      "fc layer 3 self.abs_max_out: 669.0\n",
      "fc layer 3 self.abs_max_out: 678.0\n",
      "fc layer 3 self.abs_max_out: 726.0\n",
      "fc layer 3 self.abs_max_out: 744.0\n",
      "fc layer 3 self.abs_max_out: 772.0\n",
      "fc layer 3 self.abs_max_out: 850.0\n",
      "lif layer 1 self.abs_max_v: 10241.0\n",
      "fc layer 2 self.abs_max_out: 2611.0\n",
      "fc layer 2 self.abs_max_out: 2689.0\n",
      "fc layer 2 self.abs_max_out: 2690.0\n",
      "fc layer 2 self.abs_max_out: 2855.0\n",
      "fc layer 2 self.abs_max_out: 2930.0\n",
      "fc layer 2 self.abs_max_out: 3000.0\n",
      "train - Value 0: 1942 occurrences\n",
      "train - Value 1: 2088 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 2905.00 at epoch 1, iter 4029\n",
      "max_activation_accul updated: 2966.00 at epoch 1, iter 4029\n",
      "max_activation_accul updated: 3661.00 at epoch 1, iter 4029\n",
      "fc layer 2 self.abs_max_out: 3087.0\n",
      "max_activation_accul updated: 3762.00 at epoch 1, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 90 occurrences\n",
      "test - Value 1: 362 occurrences\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss:311.369995/326.322296, val:  66.81%, val_best:  66.81%, tr:  90.05%, tr_best:  90.40%, epoch time: 231.92 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4102%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4422%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.3936%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 64480 real_backward_count 16915  26.233%\n",
      "layer   1  Sparsity: 84.0088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3088.0\n",
      "fc layer 2 self.abs_max_out: 3097.0\n",
      "fc layer 2 self.abs_max_out: 3143.0\n",
      "fc layer 2 self.abs_max_out: 3212.0\n",
      "fc layer 2 self.abs_max_out: 3240.0\n",
      "fc layer 2 self.abs_max_out: 3298.0\n",
      "fc layer 1 self.abs_max_out: 5956.0\n",
      "fc layer 1 self.abs_max_out: 5994.0\n",
      "fc layer 1 self.abs_max_out: 6119.0\n",
      "fc layer 1 self.abs_max_out: 6281.0\n",
      "fc layer 1 self.abs_max_out: 6292.0\n",
      "fc layer 1 self.abs_max_out: 6342.0\n",
      "lif layer 1 self.abs_max_v: 10599.5\n",
      "fc layer 1 self.abs_max_out: 6348.0\n",
      "train - Value 0: 1991 occurrences\n",
      "train - Value 1: 2039 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 3807.00 at epoch 2, iter 4029\n",
      "max_activation_accul updated: 3848.00 at epoch 2, iter 4029\n",
      "fc layer 3 self.abs_max_out: 853.0\n",
      "max_activation_accul updated: 4140.00 at epoch 2, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 445 occurrences\n",
      "test - Value 1: 7 occurrences\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss:326.874969/429.392578, val:  51.55%, val_best:  66.81%, tr:  91.61%, tr_best:  91.61%, epoch time: 232.26 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7903%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.2334%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 96720 real_backward_count 25229  26.085%\n",
      "layer   1  Sparsity: 78.8330%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6411.0\n",
      "fc layer 1 self.abs_max_out: 6433.0\n",
      "fc layer 1 self.abs_max_out: 6605.0\n",
      "lif layer 1 self.abs_max_v: 10733.0\n",
      "lif layer 1 self.abs_max_v: 10831.5\n",
      "lif layer 1 self.abs_max_v: 11183.0\n",
      "fc layer 1 self.abs_max_out: 6744.0\n",
      "fc layer 3 self.abs_max_out: 1013.0\n",
      "train - Value 0: 2045 occurrences\n",
      "train - Value 1: 1985 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 2 self.abs_max_out: 3328.0\n",
      "max_activation_accul updated: 4148.00 at epoch 3, iter 4029\n",
      "max_activation_accul updated: 4730.00 at epoch 3, iter 4029\n",
      "max_activation_accul updated: 5280.00 at epoch 3, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:475.818939/495.502594, val:  50.00%, val_best:  66.81%, tr:  94.04%, tr_best:  94.04%, epoch time: 231.71 seconds, 3.86 minutes\n",
      "layer   1  Sparsity: 84.4116%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2262%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.5196%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 128960 real_backward_count 32922  25.529%\n",
      "layer   1  Sparsity: 70.6787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6850.0\n",
      "fc layer 1 self.abs_max_out: 7084.0\n",
      "lif layer 1 self.abs_max_v: 11391.5\n",
      "fc layer 1 self.abs_max_out: 7252.0\n",
      "fc layer 1 self.abs_max_out: 7498.0\n",
      "train - Value 0: 2040 occurrences\n",
      "train - Value 1: 1990 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 2 self.abs_max_out: 3401.0\n",
      "max_activation_accul updated: 5482.00 at epoch 4, iter 4029\n",
      "max_activation_accul updated: 5483.00 at epoch 4, iter 4029\n",
      "max_activation_accul updated: 5631.00 at epoch 4, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:520.683594/521.203491, val:  50.00%, val_best:  66.81%, tr:  94.12%, tr_best:  94.12%, epoch time: 232.47 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4134%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8334%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.6282%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 161200 real_backward_count 40485  25.115%\n",
      "layer   1  Sparsity: 84.2285%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3438.0\n",
      "fc layer 2 self.abs_max_out: 3478.0\n",
      "fc layer 2 self.abs_max_out: 3580.0\n",
      "fc layer 1 self.abs_max_out: 7767.0\n",
      "lif layer 2 self.abs_max_v: 4238.5\n",
      "lif layer 1 self.abs_max_v: 12534.0\n",
      "lif layer 1 self.abs_max_v: 12846.0\n",
      "fc layer 1 self.abs_max_out: 7965.0\n",
      "train - Value 0: 2024 occurrences\n",
      "train - Value 1: 2006 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 7976.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 63 occurrences\n",
      "test - Value 1: 389 occurrences\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:510.751099/468.578033, val:  63.05%, val_best:  66.81%, tr:  95.61%, tr_best:  95.61%, epoch time: 232.30 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1766%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.9548%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 193440 real_backward_count 47984  24.806%\n",
      "layer   1  Sparsity: 84.3262%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2003 occurrences\n",
      "train - Value 1: 2027 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 341 occurrences\n",
      "test - Value 1: 111 occurrences\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:473.990753/446.487000, val:  71.90%, val_best:  71.90%, tr:  94.99%, tr_best:  95.61%, epoch time: 232.03 seconds, 3.87 minutes\n",
      "layer   1  Sparsity: 84.4104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0523%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.4446%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225680 real_backward_count 55479  24.583%\n",
      "layer   1  Sparsity: 82.6172%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 13065.5\n",
      "lif layer 1 self.abs_max_v: 13176.0\n",
      "fc layer 1 self.abs_max_out: 7987.0\n",
      "fc layer 1 self.abs_max_out: 8423.0\n",
      "fc layer 1 self.abs_max_out: 8658.0\n",
      "train - Value 0: 1999 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 139 occurrences\n",
      "test - Value 1: 313 occurrences\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:506.510223/476.412659, val:  77.21%, val_best:  77.21%, tr:  95.38%, tr_best:  95.61%, epoch time: 236.13 seconds, 3.94 minutes\n",
      "layer   1  Sparsity: 84.4107%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7268%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.2493%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 257920 real_backward_count 62898  24.387%\n",
      "layer   1  Sparsity: 71.6797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3648.0\n",
      "fc layer 2 self.abs_max_out: 3664.0\n",
      "fc layer 2 self.abs_max_out: 3798.0\n",
      "fc layer 2 self.abs_max_out: 3827.0\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2009 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 151 occurrences\n",
      "test - Value 1: 301 occurrences\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:490.947876/472.091034, val:  78.54%, val_best:  78.54%, tr:  95.09%, tr_best:  95.61%, epoch time: 250.93 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 84.4132%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9694%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.0261%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 290160 real_backward_count 70427  24.272%\n",
      "layer   1  Sparsity: 78.1250%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3834.0\n",
      "fc layer 1 self.abs_max_out: 8691.0\n",
      "lif layer 2 self.abs_max_v: 4314.0\n",
      "train - Value 0: 2056 occurrences\n",
      "train - Value 1: 1974 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 2 self.abs_max_out: 3872.0\n",
      "fc layer 1 self.abs_max_out: 8991.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 226 occurrences\n",
      "test - Value 1: 226 occurrences\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:509.382812/399.812469, val:  82.30%, val_best:  82.30%, tr:  96.10%, tr_best:  96.10%, epoch time: 249.67 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4117%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8669%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.8642%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 322400 real_backward_count 77892  24.160%\n",
      "layer   1  Sparsity: 77.7588%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3911.0\n",
      "fc layer 2 self.abs_max_out: 3948.0\n",
      "fc layer 2 self.abs_max_out: 3982.0\n",
      "fc layer 2 self.abs_max_out: 3994.0\n",
      "fc layer 2 self.abs_max_out: 4002.0\n",
      "fc layer 2 self.abs_max_out: 4163.0\n",
      "fc layer 2 self.abs_max_out: 4204.0\n",
      "fc layer 2 self.abs_max_out: 4305.0\n",
      "fc layer 2 self.abs_max_out: 4326.0\n",
      "lif layer 2 self.abs_max_v: 4326.0\n",
      "fc layer 2 self.abs_max_out: 4378.0\n",
      "lif layer 2 self.abs_max_v: 4378.0\n",
      "fc layer 2 self.abs_max_out: 4503.0\n",
      "lif layer 2 self.abs_max_v: 4503.0\n",
      "lif layer 1 self.abs_max_v: 13269.5\n",
      "lif layer 1 self.abs_max_v: 13309.0\n",
      "fc layer 1 self.abs_max_out: 9022.0\n",
      "train - Value 0: 2066 occurrences\n",
      "train - Value 1: 1964 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 409 occurrences\n",
      "test - Value 1: 43 occurrences\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:436.793976/449.894501, val:  59.51%, val_best:  82.30%, tr:  94.57%, tr_best:  96.10%, epoch time: 250.18 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4118%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8549%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.0730%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 354640 real_backward_count 85647  24.150%\n",
      "layer   1  Sparsity: 80.5664%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 4942.0\n",
      "fc layer 3 self.abs_max_out: 1022.0\n",
      "fc layer 3 self.abs_max_out: 1057.0\n",
      "train - Value 0: 2039 occurrences\n",
      "train - Value 1: 1991 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 9360.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 318 occurrences\n",
      "test - Value 1: 134 occurrences\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:524.657837/477.081177, val:  78.76%, val_best:  82.30%, tr:  94.89%, tr_best:  96.10%, epoch time: 250.39 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7279%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.4578%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 386880 real_backward_count 93088  24.061%\n",
      "layer   1  Sparsity: 89.9658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1071.0\n",
      "fc layer 3 self.abs_max_out: 1078.0\n",
      "train - Value 0: 2002 occurrences\n",
      "train - Value 1: 2028 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 9433.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 314 occurrences\n",
      "test - Value 1: 138 occurrences\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:536.973816/471.446594, val:  79.20%, val_best:  82.30%, tr:  96.15%, tr_best:  96.15%, epoch time: 251.77 seconds, 4.20 minutes\n",
      "layer   1  Sparsity: 84.4091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8679%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.3969%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 419120 real_backward_count 100373  23.949%\n",
      "layer   1  Sparsity: 84.4727%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2060 occurrences\n",
      "train - Value 1: 1970 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 266 occurrences\n",
      "test - Value 1: 186 occurrences\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:504.426331/414.627777, val:  87.61%, val_best:  87.61%, tr:  95.91%, tr_best:  96.15%, epoch time: 250.99 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 84.4103%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6040%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.2141%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 451360 real_backward_count 107635  23.847%\n",
      "layer   1  Sparsity: 94.7998%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1091.0\n",
      "fc layer 3 self.abs_max_out: 1124.0\n",
      "fc layer 3 self.abs_max_out: 1146.0\n",
      "train - Value 0: 2064 occurrences\n",
      "train - Value 1: 1966 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 204 occurrences\n",
      "test - Value 1: 248 occurrences\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:474.180969/393.228851, val:  86.73%, val_best:  87.61%, tr:  95.71%, tr_best:  96.15%, epoch time: 249.26 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4080%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7486%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.5051%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 483600 real_backward_count 115098  23.800%\n",
      "layer   1  Sparsity: 87.0605%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 9481.0\n",
      "fc layer 1 self.abs_max_out: 9680.0\n",
      "train - Value 0: 2066 occurrences\n",
      "train - Value 1: 1964 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 48 occurrences\n",
      "test - Value 1: 404 occurrences\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:394.675568/329.460846, val:  60.18%, val_best:  87.61%, tr:  95.96%, tr_best:  96.15%, epoch time: 249.68 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4097%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6583%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.5888%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 515840 real_backward_count 122617  23.770%\n",
      "layer   1  Sparsity: 85.1074%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1214.0\n",
      "lif layer 1 self.abs_max_v: 13312.0\n",
      "lif layer 1 self.abs_max_v: 13475.0\n",
      "train - Value 0: 2025 occurrences\n",
      "train - Value 1: 2005 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 217 occurrences\n",
      "test - Value 1: 235 occurrences\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:340.529999/359.810577, val:  83.41%, val_best:  87.61%, tr:  95.73%, tr_best:  96.15%, epoch time: 249.10 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4102%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8201%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.0880%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548080 real_backward_count 130198  23.755%\n",
      "layer   1  Sparsity: 86.4014%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 9762.0\n",
      "fc layer 1 self.abs_max_out: 10071.0\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 113 occurrences\n",
      "test - Value 1: 339 occurrences\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:282.403076/323.352509, val:  71.46%, val_best:  87.61%, tr:  94.69%, tr_best:  96.15%, epoch time: 250.82 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 84.4099%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7565%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.0985%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 580320 real_backward_count 137943  23.770%\n",
      "layer   1  Sparsity: 79.7119%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 21 occurrences\n",
      "test - Value 1: 431 occurrences\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:396.901886/495.398132, val:  54.20%, val_best:  87.61%, tr:  94.96%, tr_best:  96.15%, epoch time: 251.38 seconds, 4.19 minutes\n",
      "layer   1  Sparsity: 84.4114%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1851%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.1797%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 612560 real_backward_count 145472  23.748%\n",
      "layer   1  Sparsity: 88.1348%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 5978.00 at epoch 19, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 35 occurrences\n",
      "test - Value 1: 417 occurrences\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:544.758972/506.727448, val:  57.30%, val_best:  87.61%, tr:  96.60%, tr_best:  96.60%, epoch time: 250.88 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 84.4095%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3168%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.5471%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 644800 real_backward_count 152714  23.684%\n",
      "layer   1  Sparsity: 79.7363%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4540.0\n",
      "train - Value 0: 2004 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 379 occurrences\n",
      "test - Value 1: 73 occurrences\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:344.698029/318.808197, val:  66.15%, val_best:  87.61%, tr:  96.55%, tr_best:  96.60%, epoch time: 249.22 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4114%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0169%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.2014%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 677040 real_backward_count 159835  23.608%\n",
      "layer   1  Sparsity: 88.6963%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4611.0\n",
      "lif layer 2 self.abs_max_v: 5197.0\n",
      "fc layer 3 self.abs_max_out: 1243.0\n",
      "train - Value 0: 2004 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 6151.00 at epoch 21, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 231 occurrences\n",
      "test - Value 1: 221 occurrences\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:360.462769/324.139679, val:  85.18%, val_best:  87.61%, tr:  97.15%, tr_best:  97.15%, epoch time: 252.18 seconds, 4.20 minutes\n",
      "layer   1  Sparsity: 84.4094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8460%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.8224%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 709280 real_backward_count 166913  23.533%\n",
      "layer   1  Sparsity: 84.8389%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2041 occurrences\n",
      "train - Value 1: 1989 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 32 occurrences\n",
      "test - Value 1: 420 occurrences\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:354.641815/350.961700, val:  57.08%, val_best:  87.61%, tr:  96.97%, tr_best:  97.15%, epoch time: 246.90 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 84.4102%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3536%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.2029%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 741520 real_backward_count 174106  23.480%\n",
      "layer   1  Sparsity: 86.8896%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4614.0\n",
      "train - Value 0: 2023 occurrences\n",
      "train - Value 1: 2007 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 328 occurrences\n",
      "test - Value 1: 124 occurrences\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:354.965607/287.990540, val:  76.99%, val_best:  87.61%, tr:  97.47%, tr_best:  97.47%, epoch time: 249.48 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5665%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1912%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773760 real_backward_count 181119  23.408%\n",
      "layer   1  Sparsity: 78.1250%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4668.0\n",
      "train - Value 0: 2033 occurrences\n",
      "train - Value 1: 1997 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 301 occurrences\n",
      "test - Value 1: 151 occurrences\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:348.518799/274.261841, val:  81.19%, val_best:  87.61%, tr:  97.12%, tr_best:  97.47%, epoch time: 249.97 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4117%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7908%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.5752%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 806000 real_backward_count 188222  23.353%\n",
      "layer   1  Sparsity: 87.6221%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4737.0\n",
      "train - Value 0: 2035 occurrences\n",
      "train - Value 1: 1995 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 377 occurrences\n",
      "test - Value 1: 75 occurrences\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:331.091553/253.135284, val:  66.59%, val_best:  87.61%, tr:  97.32%, tr_best:  97.47%, epoch time: 249.40 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4096%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7323%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.2611%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 838240 real_backward_count 195262  23.294%\n",
      "layer   1  Sparsity: 84.4727%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 5208.0\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2009 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 187 occurrences\n",
      "test - Value 1: 265 occurrences\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:406.575592/495.926849, val:  84.73%, val_best:  87.61%, tr:  96.58%, tr_best:  97.47%, epoch time: 249.98 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4103%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9986%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.9858%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 870480 real_backward_count 202434  23.255%\n",
      "layer   1  Sparsity: 83.1055%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2040 occurrences\n",
      "train - Value 1: 1990 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 285 occurrences\n",
      "test - Value 1: 167 occurrences\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:492.343964/486.366791, val:  85.62%, val_best:  87.61%, tr:  97.15%, tr_best:  97.47%, epoch time: 249.75 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4106%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5598%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1459%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 902720 real_backward_count 209495  23.207%\n",
      "layer   1  Sparsity: 79.2480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2036 occurrences\n",
      "train - Value 1: 1994 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 413 occurrences\n",
      "test - Value 1: 39 occurrences\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:412.215912/409.477783, val:  58.19%, val_best:  87.61%, tr:  97.34%, tr_best:  97.47%, epoch time: 250.21 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4115%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9713%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.9543%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 934960 real_backward_count 216634  23.170%\n",
      "layer   1  Sparsity: 88.5254%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 10326.0\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 207 occurrences\n",
      "test - Value 1: 245 occurrences\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:442.006989/351.546234, val:  86.06%, val_best:  87.61%, tr:  97.25%, tr_best:  97.47%, epoch time: 248.85 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4497%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.6881%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 967200 real_backward_count 223680  23.127%\n",
      "layer   1  Sparsity: 85.9863%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 5279.0\n",
      "lif layer 2 self.abs_max_v: 5317.0\n",
      "lif layer 2 self.abs_max_v: 5364.0\n",
      "lif layer 2 self.abs_max_v: 5409.0\n",
      "lif layer 2 self.abs_max_v: 5533.5\n",
      "lif layer 2 self.abs_max_v: 5668.0\n",
      "lif layer 2 self.abs_max_v: 5732.0\n",
      "lif layer 2 self.abs_max_v: 5763.5\n",
      "lif layer 2 self.abs_max_v: 5815.0\n",
      "lif layer 2 self.abs_max_v: 5910.5\n",
      "lif layer 2 self.abs_max_v: 5989.5\n",
      "lif layer 2 self.abs_max_v: 6265.0\n",
      "fc layer 3 self.abs_max_out: 1247.0\n",
      "train - Value 0: 2044 occurrences\n",
      "train - Value 1: 1986 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 400 occurrences\n",
      "test - Value 1: 52 occurrences\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:402.599182/154.942413, val:  61.50%, val_best:  87.61%, tr:  96.65%, tr_best:  97.47%, epoch time: 250.49 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2521%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.5713%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 999440 real_backward_count 230893  23.102%\n",
      "layer   1  Sparsity: 82.9834%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 6269.5\n",
      "lif layer 2 self.abs_max_v: 6271.0\n",
      "lif layer 2 self.abs_max_v: 6393.5\n",
      "lif layer 2 self.abs_max_v: 6463.5\n",
      "train - Value 0: 2033 occurrences\n",
      "train - Value 1: 1997 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 299 occurrences\n",
      "test - Value 1: 153 occurrences\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:418.191254/467.022247, val:  81.19%, val_best:  87.61%, tr:  97.12%, tr_best:  97.47%, epoch time: 249.99 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4107%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6215%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.8820%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1031680 real_backward_count 237945  23.064%\n",
      "layer   1  Sparsity: 78.1006%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 6599.0\n",
      "lif layer 2 self.abs_max_v: 6736.5\n",
      "train - Value 0: 2033 occurrences\n",
      "train - Value 1: 1997 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 75 occurrences\n",
      "test - Value 1: 377 occurrences\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:424.798157/420.809418, val:  65.71%, val_best:  87.61%, tr:  97.37%, tr_best:  97.47%, epoch time: 250.24 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4117%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4947%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.2108%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1063920 real_backward_count 244973  23.026%\n",
      "layer   1  Sparsity: 82.4707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 6818.5\n",
      "lif layer 2 self.abs_max_v: 7080.0\n",
      "lif layer 2 self.abs_max_v: 7091.0\n",
      "lif layer 2 self.abs_max_v: 7227.0\n",
      "fc layer 3 self.abs_max_out: 1252.0\n",
      "train - Value 0: 2029 occurrences\n",
      "train - Value 1: 2001 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 421 occurrences\n",
      "test - Value 1: 31 occurrences\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:359.241852/371.550140, val:  56.86%, val_best:  87.61%, tr:  97.12%, tr_best:  97.47%, epoch time: 249.76 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4108%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5637%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.6589%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096160 real_backward_count 252090  22.998%\n",
      "layer   1  Sparsity: 80.4199%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2033 occurrences\n",
      "train - Value 1: 1997 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 268 occurrences\n",
      "test - Value 1: 184 occurrences\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:415.158997/371.857727, val:  85.84%, val_best:  87.61%, tr:  97.57%, tr_best:  97.57%, epoch time: 249.46 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2774%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1352%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1128400 real_backward_count 259102  22.962%\n",
      "layer   1  Sparsity: 80.4443%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7299.5\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 284 occurrences\n",
      "test - Value 1: 168 occurrences\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:327.417145/448.347198, val:  84.96%, val_best:  87.61%, tr:  97.30%, tr_best:  97.57%, epoch time: 249.73 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0990%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.8524%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1160640 real_backward_count 266186  22.934%\n",
      "layer   1  Sparsity: 92.4805%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7323.5\n",
      "lif layer 2 self.abs_max_v: 7355.5\n",
      "fc layer 1 self.abs_max_out: 10429.0\n",
      "train - Value 0: 2024 occurrences\n",
      "train - Value 1: 2006 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 6183.00 at epoch 36, iter 4029\n",
      "max_activation_accul updated: 6675.00 at epoch 36, iter 4029\n",
      "max_activation_accul updated: 6721.00 at epoch 36, iter 4029\n",
      "max_activation_accul updated: 6905.00 at epoch 36, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 409 occurrences\n",
      "test - Value 1: 43 occurrences\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:573.694763/560.524353, val:  59.51%, val_best:  87.61%, tr:  98.04%, tr_best:  98.04%, epoch time: 249.75 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4085%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6064%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.7445%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1192880 real_backward_count 273103  22.894%\n",
      "layer   1  Sparsity: 81.3721%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 39 occurrences\n",
      "test - Value 1: 413 occurrences\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:506.032745/437.763123, val:  58.19%, val_best:  87.61%, tr:  97.74%, tr_best:  98.04%, epoch time: 249.06 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4110%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3606%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.6782%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1225120 real_backward_count 279923  22.849%\n",
      "layer   1  Sparsity: 80.6641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1999 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 6936.00 at epoch 38, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 239 occurrences\n",
      "test - Value 1: 213 occurrences\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:464.814362/548.555359, val:  85.62%, val_best:  87.61%, tr:  98.11%, tr_best:  98.11%, epoch time: 250.50 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4415%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.3146%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1257360 real_backward_count 286853  22.814%\n",
      "layer   1  Sparsity: 91.2842%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7368.5\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 164 occurrences\n",
      "test - Value 1: 288 occurrences\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:597.002319/468.462006, val:  82.30%, val_best:  87.61%, tr:  98.36%, tr_best:  98.36%, epoch time: 248.52 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 84.4088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4030%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.9177%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1289600 real_backward_count 293601  22.767%\n",
      "layer   1  Sparsity: 77.5146%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7393.5\n",
      "lif layer 2 self.abs_max_v: 7434.0\n",
      "train - Value 0: 2004 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 142 occurrences\n",
      "test - Value 1: 310 occurrences\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:462.090027/440.614624, val:  79.20%, val_best:  87.61%, tr:  97.84%, tr_best:  98.36%, epoch time: 251.93 seconds, 4.20 minutes\n",
      "layer   1  Sparsity: 84.4119%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1290%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.6585%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321840 real_backward_count 300549  22.737%\n",
      "layer   1  Sparsity: 83.3984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7494.0\n",
      "lif layer 2 self.abs_max_v: 7562.0\n",
      "lif layer 1 self.abs_max_v: 13526.0\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 145 occurrences\n",
      "test - Value 1: 307 occurrences\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:514.105774/554.973145, val:  79.87%, val_best:  87.61%, tr:  98.26%, tr_best:  98.36%, epoch time: 251.34 seconds, 4.19 minutes\n",
      "layer   1  Sparsity: 84.4106%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3956%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.5020%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1354080 real_backward_count 307313  22.695%\n",
      "layer   1  Sparsity: 86.5967%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7563.0\n",
      "lif layer 2 self.abs_max_v: 7652.5\n",
      "train - Value 0: 2037 occurrences\n",
      "train - Value 1: 1993 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 305 occurrences\n",
      "test - Value 1: 147 occurrences\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:591.260010/587.306274, val:  81.64%, val_best:  87.61%, tr:  97.97%, tr_best:  98.36%, epoch time: 251.82 seconds, 4.20 minutes\n",
      "layer   1  Sparsity: 84.4099%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3215%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2723%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1386320 real_backward_count 314152  22.661%\n",
      "layer   1  Sparsity: 82.2510%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 13987.5\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 307 occurrences\n",
      "test - Value 1: 145 occurrences\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:616.259033/423.907288, val:  81.64%, val_best:  87.61%, tr:  97.77%, tr_best:  98.36%, epoch time: 249.78 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4108%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3295%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2326%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1418560 real_backward_count 320997  22.628%\n",
      "layer   1  Sparsity: 91.2598%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7664.5\n",
      "lif layer 2 self.abs_max_v: 7707.0\n",
      "fc layer 1 self.abs_max_out: 10926.0\n",
      "fc layer 3 self.abs_max_out: 1319.0\n",
      "train - Value 0: 2003 occurrences\n",
      "train - Value 1: 2027 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 314 occurrences\n",
      "test - Value 1: 138 occurrences\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:544.436523/496.337341, val:  80.09%, val_best:  87.61%, tr:  97.87%, tr_best:  98.36%, epoch time: 247.89 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 84.4088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3867%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.6210%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1450800 real_backward_count 327822  22.596%\n",
      "layer   1  Sparsity: 92.9443%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 10959.0\n",
      "train - Value 0: 2002 occurrences\n",
      "train - Value 1: 2028 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 7715.00 at epoch 45, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 6 occurrences\n",
      "test - Value 1: 446 occurrences\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:661.108093/717.971497, val:  51.33%, val_best:  87.61%, tr:  98.04%, tr_best:  98.36%, epoch time: 250.35 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4084%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4837%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0739%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1483040 real_backward_count 334617  22.563%\n",
      "layer   1  Sparsity: 78.3447%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2029 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 40 occurrences\n",
      "test - Value 1: 412 occurrences\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:677.518188/676.482300, val:  58.85%, val_best:  87.61%, tr:  98.66%, tr_best:  98.66%, epoch time: 251.12 seconds, 4.19 minutes\n",
      "layer   1  Sparsity: 84.4117%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5386%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.9835%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1515280 real_backward_count 341294  22.523%\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1993 occurrences\n",
      "train - Value 1: 2037 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 227 occurrences\n",
      "test - Value 1: 225 occurrences\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:687.362122/622.649475, val:  87.39%, val_best:  87.61%, tr:  98.46%, tr_best:  98.66%, epoch time: 250.29 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4089%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5569%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.6863%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1547520 real_backward_count 347944  22.484%\n",
      "layer   1  Sparsity: 92.3584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1334.0\n",
      "lif layer 1 self.abs_max_v: 14024.5\n",
      "lif layer 1 self.abs_max_v: 14134.5\n",
      "lif layer 1 self.abs_max_v: 14170.0\n",
      "train - Value 0: 2002 occurrences\n",
      "train - Value 1: 2028 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 114 occurrences\n",
      "test - Value 1: 338 occurrences\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:685.175781/663.571960, val:  74.78%, val_best:  87.61%, tr:  98.78%, tr_best:  98.78%, epoch time: 249.83 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4086%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4405%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.7265%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1579760 real_backward_count 354578  22.445%\n",
      "layer   1  Sparsity: 87.5977%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 14301.5\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 81 occurrences\n",
      "test - Value 1: 371 occurrences\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:701.548645/674.983337, val:  67.48%, val_best:  87.61%, tr:  98.46%, tr_best:  98.78%, epoch time: 249.44 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4096%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4615%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.7088%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1612000 real_backward_count 361229  22.409%\n",
      "layer   1  Sparsity: 86.7432%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 14381.5\n",
      "fc layer 3 self.abs_max_out: 1382.0\n",
      "fc layer 2 self.abs_max_out: 4781.0\n",
      "fc layer 2 self.abs_max_out: 4791.0\n",
      "fc layer 2 self.abs_max_out: 4807.0\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 266 occurrences\n",
      "test - Value 1: 186 occurrences\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:697.192932/639.328369, val:  86.28%, val_best:  87.61%, tr:  98.73%, tr_best:  98.78%, epoch time: 250.68 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 84.4098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5505%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.6294%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644240 real_backward_count 367827  22.371%\n",
      "layer   1  Sparsity: 89.1113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 11043.0\n",
      "fc layer 2 self.abs_max_out: 4861.0\n",
      "train - Value 0: 2000 occurrences\n",
      "train - Value 1: 2030 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 2 self.abs_max_out: 4870.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 97 occurrences\n",
      "test - Value 1: 355 occurrences\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:663.441223/664.844177, val:  70.58%, val_best:  87.61%, tr:  98.78%, tr_best:  98.78%, epoch time: 249.15 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4093%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5218%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.6734%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1676480 real_backward_count 374358  22.330%\n",
      "layer   1  Sparsity: 84.1309%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7740.5\n",
      "fc layer 2 self.abs_max_out: 4891.0\n",
      "train - Value 0: 1999 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 226 occurrences\n",
      "test - Value 1: 226 occurrences\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:704.937988/649.765930, val:  88.94%, val_best:  88.94%, tr:  98.76%, tr_best:  98.78%, epoch time: 249.39 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5788%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.6733%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1708720 real_backward_count 380878  22.290%\n",
      "layer   1  Sparsity: 87.8906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1420.0\n",
      "lif layer 1 self.abs_max_v: 14619.0\n",
      "fc layer 2 self.abs_max_out: 4914.0\n",
      "train - Value 0: 1992 occurrences\n",
      "train - Value 1: 2038 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 185 occurrences\n",
      "test - Value 1: 267 occurrences\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:711.075745/671.406372, val:  85.62%, val_best:  88.94%, tr:  98.59%, tr_best:  98.78%, epoch time: 250.58 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 84.4096%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5217%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.5621%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1740960 real_backward_count 387502  22.258%\n",
      "layer   1  Sparsity: 80.3711%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1453.0\n",
      "fc layer 3 self.abs_max_out: 1468.0\n",
      "train - Value 0: 1998 occurrences\n",
      "train - Value 1: 2032 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 187 occurrences\n",
      "test - Value 1: 265 occurrences\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:718.488281/676.518982, val:  85.62%, val_best:  88.94%, tr:  98.83%, tr_best:  98.83%, epoch time: 250.08 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5475%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.7709%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1773200 real_backward_count 394063  22.223%\n",
      "layer   1  Sparsity: 83.0322%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 14707.0\n",
      "lif layer 1 self.abs_max_v: 14717.5\n",
      "train - Value 0: 1999 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 226 occurrences\n",
      "test - Value 1: 226 occurrences\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:721.568542/680.829590, val:  87.17%, val_best:  88.94%, tr:  98.96%, tr_best:  98.96%, epoch time: 250.64 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 84.4106%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5465%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.6271%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1805440 real_backward_count 400509  22.183%\n",
      "layer   1  Sparsity: 80.0049%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 14764.5\n",
      "lif layer 1 self.abs_max_v: 14930.0\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 39 occurrences\n",
      "test - Value 1: 413 occurrences\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:704.696167/661.899658, val:  58.63%, val_best:  88.94%, tr:  98.86%, tr_best:  98.96%, epoch time: 248.35 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 84.4113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6169%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.5170%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1837680 real_backward_count 407009  22.148%\n",
      "layer   1  Sparsity: 82.4707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 15033.0\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 171 occurrences\n",
      "test - Value 1: 281 occurrences\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:670.968140/602.622253, val:  84.73%, val_best:  88.94%, tr:  98.83%, tr_best:  98.96%, epoch time: 249.42 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4108%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5670%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.2552%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869920 real_backward_count 413766  22.127%\n",
      "layer   1  Sparsity: 83.9355%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4943.0\n",
      "lif layer 1 self.abs_max_v: 15439.5\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 294 occurrences\n",
      "test - Value 1: 158 occurrences\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:633.621948/561.669678, val:  84.07%, val_best:  88.94%, tr:  98.83%, tr_best:  98.96%, epoch time: 249.08 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5073%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8358%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1902160 real_backward_count 420322  22.097%\n",
      "layer   1  Sparsity: 78.9795%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 102 occurrences\n",
      "test - Value 1: 350 occurrences\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:599.253418/565.677795, val:  72.12%, val_best:  88.94%, tr:  98.66%, tr_best:  98.96%, epoch time: 248.90 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4116%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6311%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.6735%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1934400 real_backward_count 426925  22.070%\n",
      "layer   1  Sparsity: 79.5410%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1999 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 184 occurrences\n",
      "test - Value 1: 268 occurrences\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:475.577911/337.110229, val:  86.73%, val_best:  88.94%, tr:  98.36%, tr_best:  98.96%, epoch time: 249.82 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4114%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8765%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.4782%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1966640 real_backward_count 433594  22.047%\n",
      "layer   1  Sparsity: 77.0752%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1994 occurrences\n",
      "train - Value 1: 2036 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 92 occurrences\n",
      "test - Value 1: 360 occurrences\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:487.510498/518.630737, val:  69.03%, val_best:  88.94%, tr:  98.88%, tr_best:  98.96%, epoch time: 250.74 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 84.4120%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0262%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.7568%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1998880 real_backward_count 440277  22.026%\n",
      "layer   1  Sparsity: 80.2246%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1981 occurrences\n",
      "train - Value 1: 2049 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 319 occurrences\n",
      "test - Value 1: 133 occurrences\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:539.842468/445.496826, val:  77.65%, val_best:  88.94%, tr:  98.51%, tr_best:  98.96%, epoch time: 249.86 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0366%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3283%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2031120 real_backward_count 446916  22.003%\n",
      "layer   1  Sparsity: 84.2041%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7805.5\n",
      "lif layer 2 self.abs_max_v: 7823.0\n",
      "train - Value 0: 1956 occurrences\n",
      "train - Value 1: 2074 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 164 occurrences\n",
      "test - Value 1: 288 occurrences\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:475.158081/402.710114, val:  84.96%, val_best:  88.94%, tr:  97.79%, tr_best:  98.96%, epoch time: 248.63 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 84.4104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1987%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3215%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2063360 real_backward_count 453779  21.992%\n",
      "layer   1  Sparsity: 79.5166%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 15443.5\n",
      "train - Value 0: 1993 occurrences\n",
      "train - Value 1: 2037 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 206 occurrences\n",
      "test - Value 1: 246 occurrences\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:527.380066/588.481323, val:  87.17%, val_best:  88.94%, tr:  98.36%, tr_best:  98.96%, epoch time: 247.54 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 84.4114%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4641%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.1601%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2095600 real_backward_count 460516  21.975%\n",
      "layer   1  Sparsity: 80.3223%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 15781.5\n",
      "train - Value 0: 2002 occurrences\n",
      "train - Value 1: 2028 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 160 occurrences\n",
      "test - Value 1: 292 occurrences\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:579.065979/500.276062, val:  81.42%, val_best:  88.94%, tr:  98.29%, tr_best:  98.96%, epoch time: 249.62 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4783%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.9157%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2127840 real_backward_count 467328  21.963%\n",
      "layer   1  Sparsity: 79.7119%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1998 occurrences\n",
      "train - Value 1: 2032 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 116 occurrences\n",
      "test - Value 1: 336 occurrences\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:542.659119/516.965698, val:  73.89%, val_best:  88.94%, tr:  98.29%, tr_best:  98.96%, epoch time: 249.98 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4114%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3336%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3918%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2160080 real_backward_count 474176  21.952%\n",
      "layer   1  Sparsity: 93.0908%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4959.0\n",
      "fc layer 2 self.abs_max_out: 4983.0\n",
      "fc layer 1 self.abs_max_out: 11071.0\n",
      "train - Value 0: 1989 occurrences\n",
      "train - Value 1: 2041 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 76 occurrences\n",
      "test - Value 1: 376 occurrences\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:549.381409/464.024750, val:  66.81%, val_best:  88.94%, tr:  97.87%, tr_best:  98.96%, epoch time: 248.80 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4084%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3393%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.5164%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2192320 real_backward_count 480966  21.939%\n",
      "layer   1  Sparsity: 77.2217%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7824.0\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 153 occurrences\n",
      "test - Value 1: 299 occurrences\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:557.735413/549.779785, val:  81.64%, val_best:  88.94%, tr:  98.73%, tr_best:  98.96%, epoch time: 250.68 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 84.4119%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3799%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.1535%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2224560 real_backward_count 487641  21.921%\n",
      "layer   1  Sparsity: 88.2812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 5051.0\n",
      "lif layer 1 self.abs_max_v: 15811.0\n",
      "lif layer 1 self.abs_max_v: 15983.5\n",
      "train - Value 0: 1999 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 196 occurrences\n",
      "test - Value 1: 256 occurrences\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:609.592224/596.336121, val:  88.05%, val_best:  88.94%, tr:  98.91%, tr_best:  98.96%, epoch time: 249.86 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4095%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5170%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.9893%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2256800 real_backward_count 494220  21.899%\n",
      "layer   1  Sparsity: 80.9326%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 5153.0\n",
      "fc layer 2 self.abs_max_out: 5180.0\n",
      "lif layer 1 self.abs_max_v: 16102.0\n",
      "train - Value 0: 1991 occurrences\n",
      "train - Value 1: 2039 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 204 occurrences\n",
      "test - Value 1: 248 occurrences\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:643.027100/592.901428, val:  88.94%, val_best:  88.94%, tr:  98.36%, tr_best:  98.96%, epoch time: 249.68 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4111%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6025%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.0083%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2289040 real_backward_count 500925  21.884%\n",
      "layer   1  Sparsity: 88.6719%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 16208.5\n",
      "lif layer 1 self.abs_max_v: 16209.5\n",
      "train - Value 0: 2002 occurrences\n",
      "train - Value 1: 2028 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 142 occurrences\n",
      "test - Value 1: 310 occurrences\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:693.053650/633.292786, val:  79.20%, val_best:  88.94%, tr:  98.78%, tr_best:  98.96%, epoch time: 249.51 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5772%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.5612%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2321280 real_backward_count 507459  21.861%\n",
      "layer   1  Sparsity: 80.1514%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1504.0\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 158 occurrences\n",
      "test - Value 1: 294 occurrences\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:773.299927/667.540649, val:  80.97%, val_best:  88.94%, tr:  99.06%, tr_best:  99.06%, epoch time: 249.53 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4597%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.9031%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2353520 real_backward_count 513960  21.838%\n",
      "layer   1  Sparsity: 86.6211%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 5256.0\n",
      "fc layer 2 self.abs_max_out: 5287.0\n",
      "fc layer 2 self.abs_max_out: 5329.0\n",
      "fc layer 2 self.abs_max_out: 5383.0\n",
      "fc layer 2 self.abs_max_out: 5479.0\n",
      "fc layer 2 self.abs_max_out: 5523.0\n",
      "fc layer 1 self.abs_max_out: 11331.0\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 245 occurrences\n",
      "test - Value 1: 207 occurrences\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:684.400391/583.881409, val:  86.95%, val_best:  88.94%, tr:  98.91%, tr_best:  99.06%, epoch time: 250.42 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5845%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.9423%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2385760 real_backward_count 520382  21.812%\n",
      "layer   1  Sparsity: 86.0840%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 11357.0\n",
      "fc layer 2 self.abs_max_out: 5572.0\n",
      "fc layer 2 self.abs_max_out: 5578.0\n",
      "fc layer 2 self.abs_max_out: 5579.0\n",
      "fc layer 2 self.abs_max_out: 5619.0\n",
      "fc layer 2 self.abs_max_out: 5770.0\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 241 occurrences\n",
      "test - Value 1: 211 occurrences\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:700.624451/677.040649, val:  87.39%, val_best:  88.94%, tr:  98.96%, tr_best:  99.06%, epoch time: 250.49 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7567%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.1155%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2418000 real_backward_count 526867  21.789%\n",
      "layer   1  Sparsity: 89.3799%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 11477.0\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 171 occurrences\n",
      "test - Value 1: 281 occurrences\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:743.225769/677.090027, val:  83.85%, val_best:  88.94%, tr:  98.93%, tr_best:  99.06%, epoch time: 248.03 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 84.4092%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7496%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.8455%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2450240 real_backward_count 533227  21.762%\n",
      "layer   1  Sparsity: 91.6748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 5911.0\n",
      "fc layer 1 self.abs_max_out: 11644.0\n",
      "lif layer 2 self.abs_max_v: 7911.0\n",
      "train - Value 0: 2004 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 215 occurrences\n",
      "test - Value 1: 237 occurrences\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:729.147522/697.702881, val:  88.72%, val_best:  88.94%, tr:  99.13%, tr_best:  99.13%, epoch time: 249.75 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4087%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6172%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.5329%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2482480 real_backward_count 539600  21.736%\n",
      "layer   1  Sparsity: 88.7695%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 16264.0\n",
      "fc layer 2 self.abs_max_out: 5959.0\n",
      "train - Value 0: 2005 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 158 occurrences\n",
      "test - Value 1: 294 occurrences\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:724.177979/620.995728, val:  83.19%, val_best:  88.94%, tr:  99.16%, tr_best:  99.16%, epoch time: 248.99 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7641%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.6929%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2514720 real_backward_count 546049  21.714%\n",
      "layer   1  Sparsity: 90.5029%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2004 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 95 occurrences\n",
      "test - Value 1: 357 occurrences\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:709.108887/665.556458, val:  70.58%, val_best:  88.94%, tr:  98.88%, tr_best:  99.16%, epoch time: 249.15 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4090%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5758%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.8930%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2546960 real_backward_count 552581  21.696%\n",
      "layer   1  Sparsity: 88.2324%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 11683.0\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 131 occurrences\n",
      "test - Value 1: 321 occurrences\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:702.830505/660.620972, val:  78.10%, val_best:  88.94%, tr:  98.88%, tr_best:  99.16%, epoch time: 250.12 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4095%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6645%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.9369%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2579200 real_backward_count 559021  21.674%\n",
      "layer   1  Sparsity: 85.0098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7988.5\n",
      "lif layer 2 self.abs_max_v: 8028.0\n",
      "fc layer 1 self.abs_max_out: 11833.0\n",
      "lif layer 2 self.abs_max_v: 8045.0\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2029 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 186 occurrences\n",
      "test - Value 1: 266 occurrences\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:716.039917/656.710571, val:  85.40%, val_best:  88.94%, tr:  98.91%, tr_best:  99.16%, epoch time: 249.92 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4102%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6825%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.2456%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2611440 real_backward_count 565567  21.657%\n",
      "layer   1  Sparsity: 77.2705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 8050.0\n",
      "lif layer 2 self.abs_max_v: 8061.5\n",
      "lif layer 2 self.abs_max_v: 8062.5\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2029 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 127 occurrences\n",
      "test - Value 1: 325 occurrences\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:743.378967/717.994751, val:  76.77%, val_best:  88.94%, tr:  99.16%, tr_best:  99.16%, epoch time: 249.69 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4119%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4736%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.8911%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2643680 real_backward_count 572064  21.639%\n",
      "layer   1  Sparsity: 87.2803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1512.0\n",
      "lif layer 2 self.abs_max_v: 8148.0\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 7986.00 at epoch 82, iter 4029\n",
      "max_activation_accul updated: 8015.00 at epoch 82, iter 4029\n",
      "lif layer 2 self.abs_max_v: 8352.0\n",
      "max_activation_accul updated: 8018.00 at epoch 82, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 86 occurrences\n",
      "test - Value 1: 366 occurrences\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:772.149170/767.421875, val:  69.03%, val_best:  88.94%, tr:  98.88%, tr_best:  99.16%, epoch time: 249.35 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4097%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5557%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.3448%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2675920 real_backward_count 578469  21.618%\n",
      "layer   1  Sparsity: 90.7471%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2003 occurrences\n",
      "train - Value 1: 2027 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 154 occurrences\n",
      "test - Value 1: 298 occurrences\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:791.701599/764.484192, val:  81.86%, val_best:  88.94%, tr:  98.91%, tr_best:  99.16%, epoch time: 248.77 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4089%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8647%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.9703%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2708160 real_backward_count 584936  21.599%\n",
      "layer   1  Sparsity: 74.1943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 29.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1994 occurrences\n",
      "train - Value 1: 2036 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 124 occurrences\n",
      "test - Value 1: 328 occurrences\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:852.138794/815.343201, val:  76.11%, val_best:  88.94%, tr:  98.98%, tr_best:  99.16%, epoch time: 248.11 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 84.4126%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8281%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.1496%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2740400 real_backward_count 591381  21.580%\n",
      "layer   1  Sparsity: 83.3984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2003 occurrences\n",
      "train - Value 1: 2027 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 158 occurrences\n",
      "test - Value 1: 294 occurrences\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:876.134460/769.143494, val:  81.86%, val_best:  88.94%, tr:  99.06%, tr_best:  99.16%, epoch time: 248.73 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4106%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7035%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.0026%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2772640 real_backward_count 597854  21.563%\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 89 occurrences\n",
      "test - Value 1: 363 occurrences\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:819.352173/791.598450, val:  69.69%, val_best:  88.94%, tr:  98.83%, tr_best:  99.16%, epoch time: 249.72 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4089%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6589%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.9989%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2804880 real_backward_count 604323  21.545%\n",
      "layer   1  Sparsity: 91.3330%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 8461.0\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 3 self.abs_max_out: 1538.0\n",
      "max_activation_accul updated: 8107.00 at epoch 87, iter 4029\n",
      "max_activation_accul updated: 9133.00 at epoch 87, iter 4029\n",
      "max_activation_accul updated: 9259.00 at epoch 87, iter 4029\n",
      "max_activation_accul updated: 9294.00 at epoch 87, iter 4029\n",
      "max_activation_accul updated: 9315.00 at epoch 87, iter 4029\n",
      "max_activation_accul updated: 10284.00 at epoch 87, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 49 occurrences\n",
      "test - Value 1: 403 occurrences\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:869.554565/902.225403, val:  60.84%, val_best:  88.94%, tr:  98.86%, tr_best:  99.16%, epoch time: 250.18 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6451%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.2758%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2837120 real_backward_count 610717  21.526%\n",
      "layer   1  Sparsity: 90.5029%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1997 occurrences\n",
      "train - Value 1: 2033 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 156 occurrences\n",
      "test - Value 1: 296 occurrences\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:883.943970/804.243774, val:  83.63%, val_best:  88.94%, tr:  98.91%, tr_best:  99.16%, epoch time: 249.94 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4090%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8443%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.5506%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2869360 real_backward_count 617085  21.506%\n",
      "layer   1  Sparsity: 69.9951%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1586.0\n",
      "fc layer 1 self.abs_max_out: 11931.0\n",
      "train - Value 0: 2004 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 19 occurrences\n",
      "test - Value 1: 433 occurrences\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:912.068420/922.562256, val:  54.20%, val_best:  88.94%, tr:  99.03%, tr_best:  99.16%, epoch time: 250.98 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 84.4136%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8477%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.6453%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2901600 real_backward_count 623458  21.487%\n",
      "layer   1  Sparsity: 91.3330%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 12070.0\n",
      "train - Value 0: 2002 occurrences\n",
      "train - Value 1: 2028 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 164 occurrences\n",
      "test - Value 1: 288 occurrences\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:908.831299/837.481140, val:  83.19%, val_best:  88.94%, tr:  99.18%, tr_best:  99.18%, epoch time: 249.60 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7953%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.5073%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2933840 real_backward_count 629815  21.467%\n",
      "layer   1  Sparsity: 76.6113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1601.0\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 201 occurrences\n",
      "test - Value 1: 251 occurrences\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:856.381958/746.537292, val:  87.83%, val_best:  88.94%, tr:  99.06%, tr_best:  99.18%, epoch time: 250.13 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4121%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8044%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.3135%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2966080 real_backward_count 636179  21.448%\n",
      "layer   1  Sparsity: 82.4463%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 12129.0\n",
      "fc layer 1 self.abs_max_out: 12163.0\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 187 occurrences\n",
      "test - Value 1: 265 occurrences\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:843.900024/724.292908, val:  84.73%, val_best:  88.94%, tr:  99.13%, tr_best:  99.18%, epoch time: 248.99 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4108%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8324%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.4243%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2998320 real_backward_count 642595  21.432%\n",
      "layer   1  Sparsity: 83.9355%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 12239.0\n",
      "fc layer 1 self.abs_max_out: 12445.0\n",
      "train - Value 0: 1997 occurrences\n",
      "train - Value 1: 2033 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 23 occurrences\n",
      "test - Value 1: 429 occurrences\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:854.619812/890.284973, val:  55.09%, val_best:  88.94%, tr:  98.96%, tr_best:  99.18%, epoch time: 249.41 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8894%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.5657%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3030560 real_backward_count 649047  21.417%\n",
      "layer   1  Sparsity: 80.5664%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 12467.0\n",
      "fc layer 1 self.abs_max_out: 12828.0\n",
      "fc layer 1 self.abs_max_out: 12938.0\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2029 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 160 occurrences\n",
      "test - Value 1: 292 occurrences\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:853.731750/755.420044, val:  84.07%, val_best:  88.94%, tr:  98.96%, tr_best:  99.18%, epoch time: 249.95 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8705%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.7083%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3062800 real_backward_count 655420  21.399%\n",
      "layer   1  Sparsity: 74.3896%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1614.0\n",
      "train - Value 0: 1999 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 120 occurrences\n",
      "test - Value 1: 332 occurrences\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:849.165833/788.060486, val:  72.57%, val_best:  88.94%, tr:  99.26%, tr_best:  99.26%, epoch time: 250.31 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4126%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7231%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.3555%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3095040 real_backward_count 661877  21.385%\n",
      "layer   1  Sparsity: 87.2803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 8645.5\n",
      "fc layer 3 self.abs_max_out: 1618.0\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 202 occurrences\n",
      "test - Value 1: 250 occurrences\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:865.302246/813.550171, val:  86.28%, val_best:  88.94%, tr:  99.11%, tr_best:  99.26%, epoch time: 249.47 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4097%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9068%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.1868%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3127280 real_backward_count 668205  21.367%\n",
      "layer   1  Sparsity: 80.3955%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2003 occurrences\n",
      "train - Value 1: 2027 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 179 occurrences\n",
      "test - Value 1: 273 occurrences\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:865.530762/856.886719, val:  86.50%, val_best:  88.94%, tr:  98.96%, tr_best:  99.26%, epoch time: 249.16 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0699%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.9119%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3159520 real_backward_count 674493  21.348%\n",
      "layer   1  Sparsity: 85.0098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1737.0\n",
      "train - Value 0: 2002 occurrences\n",
      "train - Value 1: 2028 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 133 occurrences\n",
      "test - Value 1: 319 occurrences\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:856.984985/822.668274, val:  78.10%, val_best:  88.94%, tr:  99.08%, tr_best:  99.26%, epoch time: 249.64 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4102%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8062%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.7737%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3191760 real_backward_count 680808  21.330%\n",
      "layer   1  Sparsity: 79.5898%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 16357.0\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 130 occurrences\n",
      "test - Value 1: 322 occurrences\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:845.523499/762.421204, val:  77.88%, val_best:  88.94%, tr:  99.03%, tr_best:  99.26%, epoch time: 248.31 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 84.4114%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7410%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.6676%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3224000 real_backward_count 687206  21.315%\n",
      "layer   1  Sparsity: 91.5527%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 8753.5\n",
      "lif layer 2 self.abs_max_v: 8798.5\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 48 occurrences\n",
      "test - Value 1: 404 occurrences\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:849.562927/946.874329, val:  60.62%, val_best:  88.94%, tr:  99.23%, tr_best:  99.26%, epoch time: 249.30 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4087%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8098%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.4922%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3256240 real_backward_count 693657  21.302%\n",
      "layer   1  Sparsity: 82.5684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1991 occurrences\n",
      "train - Value 1: 2039 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 268 occurrences\n",
      "test - Value 1: 184 occurrences\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:863.575378/878.673584, val:  86.73%, val_best:  88.94%, tr:  99.01%, tr_best:  99.26%, epoch time: 249.81 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4108%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8473%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.3551%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3288480 real_backward_count 699987  21.286%\n",
      "layer   1  Sparsity: 77.9297%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1995 occurrences\n",
      "train - Value 1: 2035 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 44 occurrences\n",
      "test - Value 1: 408 occurrences\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:869.227173/860.643677, val:  59.73%, val_best:  88.94%, tr:  98.71%, tr_best:  99.26%, epoch time: 249.10 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4118%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8595%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.6053%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3320720 real_backward_count 706374  21.272%\n",
      "layer   1  Sparsity: 80.5176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1998 occurrences\n",
      "train - Value 1: 2032 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 185 occurrences\n",
      "test - Value 1: 267 occurrences\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:907.313110/812.617554, val:  86.50%, val_best:  88.94%, tr:  99.18%, tr_best:  99.26%, epoch time: 249.98 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6886%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.3843%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3352960 real_backward_count 712715  21.256%\n",
      "layer   1  Sparsity: 89.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2029 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 175 occurrences\n",
      "test - Value 1: 277 occurrences\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:864.437439/769.162170, val:  84.29%, val_best:  88.94%, tr:  99.16%, tr_best:  99.26%, epoch time: 251.11 seconds, 4.19 minutes\n",
      "layer   1  Sparsity: 84.4093%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7794%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.2829%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3385200 real_backward_count 719028  21.240%\n",
      "layer   1  Sparsity: 88.5010%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1834.0\n",
      "train - Value 0: 2004 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 166 occurrences\n",
      "test - Value 1: 286 occurrences\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:861.985046/778.666748, val:  84.96%, val_best:  88.94%, tr:  99.18%, tr_best:  99.26%, epoch time: 249.37 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8094%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.3559%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3417440 real_backward_count 725347  21.225%\n",
      "layer   1  Sparsity: 87.8662%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2009 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 194 occurrences\n",
      "test - Value 1: 258 occurrences\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:843.976685/743.061340, val:  86.73%, val_best:  88.94%, tr:  99.06%, tr_best:  99.26%, epoch time: 250.01 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4096%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8301%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.4728%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3449680 real_backward_count 731689  21.210%\n",
      "layer   1  Sparsity: 90.1611%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1996 occurrences\n",
      "train - Value 1: 2034 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 182 occurrences\n",
      "test - Value 1: 270 occurrences\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:824.930176/752.622070, val:  86.73%, val_best:  88.94%, tr:  98.64%, tr_best:  99.26%, epoch time: 248.84 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8595%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.5309%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3481920 real_backward_count 738055  21.197%\n",
      "layer   1  Sparsity: 73.6328%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2004 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 72 occurrences\n",
      "test - Value 1: 380 occurrences\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:857.119202/835.696289, val:  65.93%, val_best:  88.94%, tr:  99.13%, tr_best:  99.26%, epoch time: 249.43 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4127%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9229%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.0058%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3514160 real_backward_count 744430  21.184%\n",
      "layer   1  Sparsity: 92.4805%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1997 occurrences\n",
      "train - Value 1: 2033 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 161 occurrences\n",
      "test - Value 1: 291 occurrences\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:937.423889/778.821289, val:  83.41%, val_best:  88.94%, tr:  99.16%, tr_best:  99.26%, epoch time: 249.67 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4085%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9171%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.4986%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3546400 real_backward_count 750746  21.169%\n",
      "layer   1  Sparsity: 80.3711%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1846.0\n",
      "train - Value 0: 1994 occurrences\n",
      "train - Value 1: 2036 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 178 occurrences\n",
      "test - Value 1: 274 occurrences\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:893.944336/791.786865, val:  85.84%, val_best:  88.94%, tr:  98.93%, tr_best:  99.26%, epoch time: 249.09 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0959%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.3451%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3578640 real_backward_count 757128  21.157%\n",
      "layer   1  Sparsity: 75.2197%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 203 occurrences\n",
      "test - Value 1: 249 occurrences\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:946.139526/894.390686, val:  87.83%, val_best:  88.94%, tr:  99.53%, tr_best:  99.53%, epoch time: 248.43 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 84.4124%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0495%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.3501%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3610880 real_backward_count 763418  21.142%\n",
      "layer   1  Sparsity: 82.6660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2029 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 119 occurrences\n",
      "test - Value 1: 333 occurrences\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:898.059814/806.692200, val:  75.88%, val_best:  88.94%, tr:  99.06%, tr_best:  99.53%, epoch time: 245.87 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 84.4107%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.2869%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3643120 real_backward_count 769627  21.125%\n",
      "layer   1  Sparsity: 85.1318%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1996 occurrences\n",
      "train - Value 1: 2034 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 103 occurrences\n",
      "test - Value 1: 349 occurrences\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:869.635925/789.164368, val:  72.35%, val_best:  88.94%, tr:  99.13%, tr_best:  99.53%, epoch time: 250.64 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 84.4102%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8903%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.4630%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3675360 real_backward_count 775900  21.111%\n",
      "layer   1  Sparsity: 82.4707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2000 occurrences\n",
      "train - Value 1: 2030 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 135 occurrences\n",
      "test - Value 1: 317 occurrences\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:854.545349/756.117859, val:  78.98%, val_best:  88.94%, tr:  98.98%, tr_best:  99.53%, epoch time: 249.12 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4108%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0268%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.5315%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3707600 real_backward_count 782177  21.097%\n",
      "layer   1  Sparsity: 78.5889%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1999 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 3 occurrences\n",
      "test - Value 1: 449 occurrences\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:837.714600/890.918274, val:  50.66%, val_best:  88.94%, tr:  99.11%, tr_best:  99.53%, epoch time: 250.03 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4116%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1046%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.6466%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3739840 real_backward_count 788448  21.082%\n",
      "layer   1  Sparsity: 78.8330%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2002 occurrences\n",
      "train - Value 1: 2028 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 134 occurrences\n",
      "test - Value 1: 318 occurrences\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:896.878967/834.475769, val:  78.76%, val_best:  88.94%, tr:  99.33%, tr_best:  99.53%, epoch time: 249.87 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4116%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1699%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.7845%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3772080 real_backward_count 794683  21.068%\n",
      "layer   1  Sparsity: 78.1250%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2005 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 170 occurrences\n",
      "test - Value 1: 282 occurrences\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:849.324646/783.725098, val:  85.84%, val_best:  88.94%, tr:  99.21%, tr_best:  99.53%, epoch time: 250.15 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4117%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9637%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.4564%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3804320 real_backward_count 800921  21.053%\n",
      "layer   1  Sparsity: 88.7695%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1999 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 126 occurrences\n",
      "test - Value 1: 326 occurrences\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:828.201538/735.591797, val:  76.99%, val_best:  88.94%, tr:  99.21%, tr_best:  99.53%, epoch time: 249.34 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8428%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.4652%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3836560 real_backward_count 807265  21.041%\n",
      "layer   1  Sparsity: 82.7148%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 134 occurrences\n",
      "test - Value 1: 318 occurrences\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:862.950745/804.024414, val:  78.32%, val_best:  88.94%, tr:  99.31%, tr_best:  99.53%, epoch time: 249.33 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4107%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1863%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.8881%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3868800 real_backward_count 813509  21.027%\n",
      "layer   1  Sparsity: 87.4023%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2004 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 5 occurrences\n",
      "test - Value 1: 447 occurrences\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:817.650024/873.149963, val:  51.11%, val_best:  88.94%, tr:  99.23%, tr_best:  99.53%, epoch time: 249.49 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4097%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0306%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.9556%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3901040 real_backward_count 819765  21.014%\n",
      "layer   1  Sparsity: 88.1104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 111 occurrences\n",
      "test - Value 1: 341 occurrences\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:881.186462/860.752075, val:  74.12%, val_best:  88.94%, tr:  99.28%, tr_best:  99.53%, epoch time: 250.13 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4095%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.9021%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3933280 real_backward_count 825921  20.998%\n",
      "layer   1  Sparsity: 85.5957%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1999 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 63 occurrences\n",
      "test - Value 1: 389 occurrences\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:906.390015/849.262329, val:  63.94%, val_best:  88.94%, tr:  99.26%, tr_best:  99.53%, epoch time: 248.44 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 84.4101%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0763%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.9695%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3965520 real_backward_count 832049  20.982%\n",
      "layer   1  Sparsity: 89.9902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 255 occurrences\n",
      "test - Value 1: 197 occurrences\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:877.896240/761.596558, val:  87.83%, val_best:  88.94%, tr:  99.53%, tr_best:  99.53%, epoch time: 248.75 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0147%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.5607%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3997760 real_backward_count 838309  20.969%\n",
      "layer   1  Sparsity: 87.2803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 95 occurrences\n",
      "test - Value 1: 357 occurrences\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:853.644348/833.082703, val:  71.02%, val_best:  88.94%, tr:  99.31%, tr_best:  99.53%, epoch time: 249.32 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4097%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0499%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3007%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4030000 real_backward_count 844522  20.956%\n",
      "layer   1  Sparsity: 82.9834%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 250 occurrences\n",
      "test - Value 1: 202 occurrences\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:910.356934/830.113220, val:  86.28%, val_best:  88.94%, tr:  99.33%, tr_best:  99.53%, epoch time: 249.18 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4107%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1308%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.2927%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4062240 real_backward_count 850635  20.940%\n",
      "layer   1  Sparsity: 91.2354%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 82.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1996 occurrences\n",
      "train - Value 1: 2034 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 199 occurrences\n",
      "test - Value 1: 253 occurrences\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:869.350281/799.054626, val:  90.93%, val_best:  90.93%, tr:  99.23%, tr_best:  99.53%, epoch time: 249.12 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0776%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.2853%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4094480 real_backward_count 856694  20.923%\n",
      "layer   1  Sparsity: 76.6113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2029 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 116 occurrences\n",
      "test - Value 1: 336 occurrences\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:863.762451/782.210571, val:  75.22%, val_best:  90.93%, tr:  99.40%, tr_best:  99.53%, epoch time: 245.99 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 84.4121%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8889%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3038%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4126720 real_backward_count 862709  20.905%\n",
      "layer   1  Sparsity: 88.9893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 172 occurrences\n",
      "test - Value 1: 280 occurrences\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:818.837769/735.279907, val:  85.84%, val_best:  90.93%, tr:  99.35%, tr_best:  99.53%, epoch time: 249.19 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4093%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8129%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.0978%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4158960 real_backward_count 868809  20.890%\n",
      "layer   1  Sparsity: 74.6582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2004 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 244 occurrences\n",
      "test - Value 1: 208 occurrences\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:793.227295/742.549133, val:  86.28%, val_best:  90.93%, tr:  99.13%, tr_best:  99.53%, epoch time: 249.86 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6762%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.7244%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4191200 real_backward_count 875106  20.880%\n",
      "layer   1  Sparsity: 91.6992%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 87 occurrences\n",
      "test - Value 1: 365 occurrences\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:782.820374/709.027161, val:  68.81%, val_best:  90.93%, tr:  99.31%, tr_best:  99.53%, epoch time: 249.07 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4087%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6759%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4223440 real_backward_count 881363  20.868%\n",
      "layer   1  Sparsity: 82.7148%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 200 occurrences\n",
      "test - Value 1: 252 occurrences\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:793.153625/759.054810, val:  88.05%, val_best:  90.93%, tr:  99.33%, tr_best:  99.53%, epoch time: 249.31 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4107%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6036%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.4420%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4255680 real_backward_count 887541  20.855%\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 9043.0\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 63 occurrences\n",
      "test - Value 1: 389 occurrences\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:795.031372/755.563232, val:  63.94%, val_best:  90.93%, tr:  99.58%, tr_best:  99.58%, epoch time: 249.62 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4089%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6319%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.9768%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4287920 real_backward_count 893586  20.840%\n",
      "layer   1  Sparsity: 84.6680%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 194 occurrences\n",
      "test - Value 1: 258 occurrences\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:765.753601/755.804565, val:  89.82%, val_best:  90.93%, tr:  99.23%, tr_best:  99.58%, epoch time: 249.82 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4103%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6720%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.2199%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4320160 real_backward_count 899704  20.826%\n",
      "layer   1  Sparsity: 80.4932%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2003 occurrences\n",
      "train - Value 1: 2027 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 122 occurrences\n",
      "test - Value 1: 330 occurrences\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:752.406311/693.505737, val:  76.55%, val_best:  90.93%, tr:  99.16%, tr_best:  99.58%, epoch time: 248.85 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7101%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3951%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4352400 real_backward_count 905802  20.812%\n",
      "layer   1  Sparsity: 87.2559%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2002 occurrences\n",
      "train - Value 1: 2028 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 155 occurrences\n",
      "test - Value 1: 297 occurrences\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:746.714172/623.706055, val:  82.08%, val_best:  90.93%, tr:  99.28%, tr_best:  99.58%, epoch time: 249.50 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4097%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8179%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3054%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4384640 real_backward_count 911925  20.798%\n",
      "layer   1  Sparsity: 87.5488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 13095.0\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 74 occurrences\n",
      "test - Value 1: 378 occurrences\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:702.728760/646.262756, val:  66.37%, val_best:  90.93%, tr:  99.18%, tr_best:  99.58%, epoch time: 248.37 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 84.4096%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7778%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.7314%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4416880 real_backward_count 918180  20.788%\n",
      "layer   1  Sparsity: 74.5605%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 13179.0\n",
      "fc layer 1 self.abs_max_out: 13356.0\n",
      "fc layer 1 self.abs_max_out: 13375.0\n",
      "train - Value 0: 2023 occurrences\n",
      "train - Value 1: 2007 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 133 occurrences\n",
      "test - Value 1: 319 occurrences\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:710.863953/657.750122, val:  78.54%, val_best:  90.93%, tr:  99.11%, tr_best:  99.58%, epoch time: 250.14 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.4228%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4449120 real_backward_count 924429  20.778%\n",
      "layer   1  Sparsity: 72.5342%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 157 occurrences\n",
      "test - Value 1: 295 occurrences\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:741.572510/685.312622, val:  83.41%, val_best:  90.93%, tr:  99.21%, tr_best:  99.58%, epoch time: 249.13 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4130%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8907%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.0569%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4481360 real_backward_count 930652  20.767%\n",
      "layer   1  Sparsity: 76.4893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 297 occurrences\n",
      "test - Value 1: 155 occurrences\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:741.206299/671.624329, val:  82.96%, val_best:  90.93%, tr:  98.73%, tr_best:  99.58%, epoch time: 248.63 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 84.4121%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6876%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.9013%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4513600 real_backward_count 937035  20.760%\n",
      "layer   1  Sparsity: 89.9658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 82.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 151 occurrences\n",
      "test - Value 1: 301 occurrences\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:719.932251/688.508850, val:  80.75%, val_best:  90.93%, tr:  98.96%, tr_best:  99.58%, epoch time: 249.73 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5570%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.4493%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4545840 real_backward_count 943484  20.755%\n",
      "layer   1  Sparsity: 88.6719%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 4 occurrences\n",
      "test - Value 1: 448 occurrences\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:737.531189/729.825317, val:  50.88%, val_best:  90.93%, tr:  99.23%, tr_best:  99.58%, epoch time: 248.61 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 84.4094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7838%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.7760%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4578080 real_backward_count 949697  20.744%\n",
      "layer   1  Sparsity: 81.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 100 occurrences\n",
      "test - Value 1: 352 occurrences\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:729.084351/677.403503, val:  72.12%, val_best:  90.93%, tr:  99.21%, tr_best:  99.58%, epoch time: 248.32 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 84.4111%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7768%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.1675%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4610320 real_backward_count 955927  20.735%\n",
      "layer   1  Sparsity: 87.1826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 293 occurrences\n",
      "test - Value 1: 159 occurrences\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:704.848145/660.781494, val:  82.96%, val_best:  90.93%, tr:  98.88%, tr_best:  99.58%, epoch time: 248.38 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 84.4097%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9242%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.1598%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4642560 real_backward_count 962262  20.727%\n",
      "layer   1  Sparsity: 84.3750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 140 occurrences\n",
      "test - Value 1: 312 occurrences\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:732.282715/680.032471, val:  79.20%, val_best:  90.93%, tr:  98.83%, tr_best:  99.58%, epoch time: 250.41 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4103%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.8636%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4674800 real_backward_count 968720  20.722%\n",
      "layer   1  Sparsity: 78.0762%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 230 occurrences\n",
      "test - Value 1: 222 occurrences\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:752.244019/652.811401, val:  88.94%, val_best:  90.93%, tr:  99.21%, tr_best:  99.58%, epoch time: 249.51 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4118%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8406%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.6008%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4707040 real_backward_count 975030  20.714%\n",
      "layer   1  Sparsity: 86.5967%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 242 occurrences\n",
      "test - Value 1: 210 occurrences\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:735.480530/662.117859, val:  87.61%, val_best:  90.93%, tr:  98.78%, tr_best:  99.58%, epoch time: 248.81 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4099%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5713%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1555%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4739280 real_backward_count 981503  20.710%\n",
      "layer   1  Sparsity: 90.0879%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1995 occurrences\n",
      "train - Value 1: 2035 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 314 occurrences\n",
      "test - Value 1: 138 occurrences\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:713.157593/651.944580, val:  79.20%, val_best:  90.93%, tr:  98.66%, tr_best:  99.58%, epoch time: 248.42 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 84.4091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4546%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0601%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4771520 real_backward_count 988038  20.707%\n",
      "layer   1  Sparsity: 84.1064%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 100 occurrences\n",
      "test - Value 1: 352 occurrences\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:684.136414/585.535706, val:  71.68%, val_best:  90.93%, tr:  99.01%, tr_best:  99.58%, epoch time: 249.44 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5229%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.5561%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4803760 real_backward_count 994588  20.704%\n",
      "layer   1  Sparsity: 75.8789%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1997 occurrences\n",
      "train - Value 1: 2033 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 194 occurrences\n",
      "test - Value 1: 258 occurrences\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:727.890442/624.351440, val:  88.94%, val_best:  90.93%, tr:  99.16%, tr_best:  99.58%, epoch time: 248.31 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 84.4122%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8298%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.1400%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4836000 real_backward_count 1001063  20.700%\n",
      "layer   1  Sparsity: 79.9316%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 167 occurrences\n",
      "test - Value 1: 285 occurrences\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:727.018677/661.810852, val:  84.29%, val_best:  90.93%, tr:  99.28%, tr_best:  99.58%, epoch time: 248.81 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1268%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.1372%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4868240 real_backward_count 1007451  20.694%\n",
      "layer   1  Sparsity: 87.3291%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 104 occurrences\n",
      "test - Value 1: 348 occurrences\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:740.197327/666.080444, val:  72.57%, val_best:  90.93%, tr:  99.18%, tr_best:  99.58%, epoch time: 249.58 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4097%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7405%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.7166%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4900480 real_backward_count 1013884  20.689%\n",
      "layer   1  Sparsity: 88.5010%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 154 occurrences\n",
      "test - Value 1: 298 occurrences\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:718.186951/610.829651, val:  82.74%, val_best:  90.93%, tr:  99.58%, tr_best:  99.58%, epoch time: 250.62 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 84.4094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8210%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.4834%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4932720 real_backward_count 1020241  20.683%\n",
      "layer   1  Sparsity: 87.2559%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 331 occurrences\n",
      "test - Value 1: 121 occurrences\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:671.643066/632.504211, val:  76.33%, val_best:  90.93%, tr:  99.08%, tr_best:  99.58%, epoch time: 249.80 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4097%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9682%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8773%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4964960 real_backward_count 1026543  20.676%\n",
      "layer   1  Sparsity: 89.6484%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 193 occurrences\n",
      "test - Value 1: 259 occurrences\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:691.869202/643.722473, val:  86.50%, val_best:  90.93%, tr:  99.13%, tr_best:  99.58%, epoch time: 248.74 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4092%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8647%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0576%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4997200 real_backward_count 1032813  20.668%\n",
      "layer   1  Sparsity: 81.5674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 166 occurrences\n",
      "test - Value 1: 286 occurrences\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:711.890564/591.839355, val:  84.07%, val_best:  90.93%, tr:  99.13%, tr_best:  99.58%, epoch time: 249.69 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4110%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9288%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.5110%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5029440 real_backward_count 1039093  20.660%\n",
      "layer   1  Sparsity: 73.9990%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 138 occurrences\n",
      "test - Value 1: 314 occurrences\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:647.762634/636.738159, val:  77.88%, val_best:  90.93%, tr:  98.64%, tr_best:  99.58%, epoch time: 249.90 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4127%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1299%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0854%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5061680 real_backward_count 1045502  20.655%\n",
      "layer   1  Sparsity: 88.2568%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 6061.0\n",
      "fc layer 2 self.abs_max_out: 6335.0\n",
      "train - Value 0: 1995 occurrences\n",
      "train - Value 1: 2035 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 374 occurrences\n",
      "test - Value 1: 78 occurrences\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:511.658112/394.848236, val:  66.81%, val_best:  90.93%, tr:  98.01%, tr_best:  99.58%, epoch time: 249.27 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4095%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3597%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.4488%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5093920 real_backward_count 1052316  20.658%\n",
      "layer   1  Sparsity: 86.7188%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 246 occurrences\n",
      "test - Value 1: 206 occurrences\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:466.340393/413.127075, val:  88.94%, val_best:  90.93%, tr:  98.76%, tr_best:  99.58%, epoch time: 250.60 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 84.4098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3332%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.6876%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5126160 real_backward_count 1058944  20.658%\n",
      "layer   1  Sparsity: 88.5254%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2009 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 175 occurrences\n",
      "test - Value 1: 277 occurrences\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:483.460632/445.652924, val:  85.62%, val_best:  90.93%, tr:  99.06%, tr_best:  99.58%, epoch time: 249.92 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5774%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3556%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5158400 real_backward_count 1065287  20.652%\n",
      "layer   1  Sparsity: 81.3721%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 111 occurrences\n",
      "test - Value 1: 341 occurrences\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:485.056000/481.696320, val:  74.12%, val_best:  90.93%, tr:  98.81%, tr_best:  99.58%, epoch time: 249.99 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4110%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4746%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.5114%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5190640 real_backward_count 1071634  20.646%\n",
      "layer   1  Sparsity: 88.9893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2022 occurrences\n",
      "train - Value 1: 2008 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 143 occurrences\n",
      "test - Value 1: 309 occurrences\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:496.394379/479.667725, val:  79.87%, val_best:  90.93%, tr:  99.03%, tr_best:  99.58%, epoch time: 251.28 seconds, 4.19 minutes\n",
      "layer   1  Sparsity: 84.4093%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4565%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.2166%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5222880 real_backward_count 1077941  20.639%\n",
      "layer   1  Sparsity: 81.2988%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 6366.0\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2009 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 162 occurrences\n",
      "test - Value 1: 290 occurrences\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:486.956146/479.532043, val:  82.74%, val_best:  90.93%, tr:  99.40%, tr_best:  99.58%, epoch time: 250.49 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4110%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4848%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.2841%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5255120 real_backward_count 1084299  20.633%\n",
      "layer   1  Sparsity: 93.5791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2029 occurrences\n",
      "train - Value 1: 2001 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 58 occurrences\n",
      "test - Value 1: 394 occurrences\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:493.585724/602.214844, val:  62.83%, val_best:  90.93%, tr:  99.35%, tr_best:  99.58%, epoch time: 251.14 seconds, 4.19 minutes\n",
      "layer   1  Sparsity: 84.4083%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4810%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.0267%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5287360 real_backward_count 1090652  20.628%\n",
      "layer   1  Sparsity: 79.3213%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2024 occurrences\n",
      "train - Value 1: 2006 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 242 occurrences\n",
      "test - Value 1: 210 occurrences\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:497.388123/456.095886, val:  87.17%, val_best:  90.93%, tr:  99.33%, tr_best:  99.58%, epoch time: 250.81 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 84.4115%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8184%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.7872%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5319600 real_backward_count 1097001  20.622%\n",
      "layer   1  Sparsity: 78.8330%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 259 occurrences\n",
      "test - Value 1: 193 occurrences\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:491.420471/462.774902, val:  86.95%, val_best:  90.93%, tr:  98.96%, tr_best:  99.58%, epoch time: 250.90 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 84.4116%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8509%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1827%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5351840 real_backward_count 1103446  20.618%\n",
      "layer   1  Sparsity: 81.3721%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2010 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 177 occurrences\n",
      "test - Value 1: 275 occurrences\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:503.986877/508.333282, val:  84.73%, val_best:  90.93%, tr:  99.08%, tr_best:  99.58%, epoch time: 248.95 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4110%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9439%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.9056%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5384080 real_backward_count 1109778  20.612%\n",
      "layer   1  Sparsity: 91.5527%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 83.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2009 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 123 occurrences\n",
      "test - Value 1: 329 occurrences\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:495.957764/454.787994, val:  76.33%, val_best:  90.93%, tr:  99.21%, tr_best:  99.58%, epoch time: 249.77 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4087%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6538%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8889%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5416320 real_backward_count 1116061  20.606%\n",
      "layer   1  Sparsity: 80.3467%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 253 occurrences\n",
      "test - Value 1: 199 occurrences\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:475.747192/406.940277, val:  87.39%, val_best:  90.93%, tr:  98.81%, tr_best:  99.58%, epoch time: 250.54 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6855%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.9977%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5448560 real_backward_count 1122412  20.600%\n",
      "layer   1  Sparsity: 84.3506%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2023 occurrences\n",
      "train - Value 1: 2007 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 102 occurrences\n",
      "test - Value 1: 350 occurrences\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:468.969818/487.297211, val:  72.12%, val_best:  90.93%, tr:  98.96%, tr_best:  99.58%, epoch time: 249.41 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4230%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.9846%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5480800 real_backward_count 1128864  20.597%\n",
      "layer   1  Sparsity: 60.5469%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 47.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2026 occurrences\n",
      "train - Value 1: 2004 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 124 occurrences\n",
      "test - Value 1: 328 occurrences\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:481.443146/430.351227, val:  76.11%, val_best:  90.93%, tr:  98.88%, tr_best:  99.58%, epoch time: 250.29 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4157%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1410%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2134%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5513040 real_backward_count 1135241  20.592%\n",
      "layer   1  Sparsity: 79.0527%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 87 occurrences\n",
      "test - Value 1: 365 occurrences\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:476.171692/455.936829, val:  68.81%, val_best:  90.93%, tr:  99.21%, tr_best:  99.58%, epoch time: 249.17 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4115%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2983%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.9581%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5545280 real_backward_count 1141554  20.586%\n",
      "layer   1  Sparsity: 84.4727%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 117 occurrences\n",
      "test - Value 1: 335 occurrences\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:475.739288/475.044647, val:  74.56%, val_best:  90.93%, tr:  99.28%, tr_best:  99.58%, epoch time: 249.96 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4103%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0510%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.9029%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5577520 real_backward_count 1147746  20.578%\n",
      "layer   1  Sparsity: 76.5381%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 229 occurrences\n",
      "test - Value 1: 223 occurrences\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:483.095093/431.667236, val:  89.60%, val_best:  90.93%, tr:  99.03%, tr_best:  99.58%, epoch time: 250.00 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4121%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2535%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8323%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5609760 real_backward_count 1154049  20.572%\n",
      "layer   1  Sparsity: 92.9443%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 134 occurrences\n",
      "test - Value 1: 318 occurrences\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:454.502563/423.592560, val:  78.32%, val_best:  90.93%, tr:  99.31%, tr_best:  99.58%, epoch time: 251.93 seconds, 4.20 minutes\n",
      "layer   1  Sparsity: 84.4084%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1924%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.7902%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5642000 real_backward_count 1160397  20.567%\n",
      "layer   1  Sparsity: 81.3721%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 64 occurrences\n",
      "test - Value 1: 388 occurrences\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:478.587189/527.700134, val:  64.16%, val_best:  90.93%, tr:  99.08%, tr_best:  99.58%, epoch time: 250.37 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4110%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2084%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.4125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5674240 real_backward_count 1166719  20.562%\n",
      "layer   1  Sparsity: 86.5479%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2010 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 134 occurrences\n",
      "test - Value 1: 318 occurrences\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:485.156036/497.210968, val:  78.32%, val_best:  90.93%, tr:  99.18%, tr_best:  99.58%, epoch time: 250.05 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4099%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1093%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0241%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5706480 real_backward_count 1173052  20.556%\n",
      "layer   1  Sparsity: 86.3770%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 31 occurrences\n",
      "test - Value 1: 421 occurrences\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:489.565582/538.274109, val:  56.86%, val_best:  90.93%, tr:  99.01%, tr_best:  99.58%, epoch time: 249.49 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4099%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2671%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.6338%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5738720 real_backward_count 1179331  20.550%\n",
      "layer   1  Sparsity: 84.7656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 35 occurrences\n",
      "test - Value 1: 417 occurrences\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:489.329132/551.612305, val:  57.30%, val_best:  90.93%, tr:  98.96%, tr_best:  99.58%, epoch time: 250.30 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4103%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2739%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.9195%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5770960 real_backward_count 1185637  20.545%\n",
      "layer   1  Sparsity: 79.1992%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 89 occurrences\n",
      "test - Value 1: 363 occurrences\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:487.325623/490.661285, val:  69.25%, val_best:  90.93%, tr:  99.01%, tr_best:  99.58%, epoch time: 248.91 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4115%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2371%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.4495%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5803200 real_backward_count 1192006  20.540%\n",
      "layer   1  Sparsity: 90.5029%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2009 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 216 occurrences\n",
      "test - Value 1: 236 occurrences\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:481.910828/438.719971, val:  89.38%, val_best:  90.93%, tr:  99.16%, tr_best:  99.58%, epoch time: 249.27 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4090%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8021%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8560%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5835440 real_backward_count 1198217  20.533%\n",
      "layer   1  Sparsity: 81.9336%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 218 occurrences\n",
      "test - Value 1: 234 occurrences\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:492.532532/448.349304, val:  89.82%, val_best:  90.93%, tr:  99.28%, tr_best:  99.58%, epoch time: 250.36 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4109%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4717%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.2975%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5867680 real_backward_count 1204316  20.525%\n",
      "layer   1  Sparsity: 79.5410%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 195 occurrences\n",
      "test - Value 1: 257 occurrences\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:496.787567/476.164551, val:  87.39%, val_best:  90.93%, tr:  99.18%, tr_best:  99.58%, epoch time: 249.23 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4114%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7614%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3190%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5899920 real_backward_count 1210559  20.518%\n",
      "layer   1  Sparsity: 76.9775%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 207 occurrences\n",
      "test - Value 1: 245 occurrences\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:476.323425/455.265564, val:  88.72%, val_best:  90.93%, tr:  99.11%, tr_best:  99.58%, epoch time: 249.64 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4120%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1410%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0967%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5932160 real_backward_count 1216824  20.512%\n",
      "layer   1  Sparsity: 78.9795%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 102 occurrences\n",
      "test - Value 1: 350 occurrences\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:474.838593/477.511993, val:  72.12%, val_best:  90.93%, tr:  99.33%, tr_best:  99.58%, epoch time: 250.74 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 84.4116%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2153%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.4822%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5964400 real_backward_count 1223009  20.505%\n",
      "layer   1  Sparsity: 86.7188%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 139 occurrences\n",
      "test - Value 1: 313 occurrences\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:472.020599/492.669434, val:  80.31%, val_best:  90.93%, tr:  99.35%, tr_best:  99.58%, epoch time: 250.22 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1048%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.2900%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5996640 real_backward_count 1229180  20.498%\n",
      "layer   1  Sparsity: 79.8340%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2026 occurrences\n",
      "train - Value 1: 2004 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 149 occurrences\n",
      "test - Value 1: 303 occurrences\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:465.078247/455.435791, val:  82.08%, val_best:  90.93%, tr:  99.38%, tr_best:  99.58%, epoch time: 251.15 seconds, 4.19 minutes\n",
      "layer   1  Sparsity: 84.4114%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9343%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.2289%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6028880 real_backward_count 1235323  20.490%\n",
      "layer   1  Sparsity: 76.0498%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 97 occurrences\n",
      "test - Value 1: 355 occurrences\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:477.517670/490.015503, val:  71.02%, val_best:  90.93%, tr:  99.35%, tr_best:  99.58%, epoch time: 250.15 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4122%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9304%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8092%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6061120 real_backward_count 1241463  20.482%\n",
      "layer   1  Sparsity: 78.8330%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 117 occurrences\n",
      "test - Value 1: 335 occurrences\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:465.643127/435.777740, val:  75.44%, val_best:  90.93%, tr:  99.28%, tr_best:  99.58%, epoch time: 249.38 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4116%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9618%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.7821%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6093360 real_backward_count 1247648  20.476%\n",
      "layer   1  Sparsity: 77.0264%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 154 occurrences\n",
      "test - Value 1: 298 occurrences\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:456.795746/382.490662, val:  83.19%, val_best:  90.93%, tr:  99.13%, tr_best:  99.58%, epoch time: 250.09 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4120%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8041%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8958%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6125600 real_backward_count 1253896  20.470%\n",
      "layer   1  Sparsity: 73.5352%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 186 occurrences\n",
      "test - Value 1: 266 occurrences\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:448.261688/426.263733, val:  88.05%, val_best:  90.93%, tr:  99.21%, tr_best:  99.58%, epoch time: 249.96 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4128%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8724%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0420%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6157840 real_backward_count 1260198  20.465%\n",
      "layer   1  Sparsity: 89.1846%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 82 occurrences\n",
      "test - Value 1: 370 occurrences\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:448.059052/459.530609, val:  68.14%, val_best:  90.93%, tr:  99.21%, tr_best:  99.58%, epoch time: 249.29 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4093%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7595%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.9520%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6190080 real_backward_count 1266529  20.461%\n",
      "layer   1  Sparsity: 84.8145%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 26 occurrences\n",
      "test - Value 1: 426 occurrences\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:434.307587/488.270996, val:  55.75%, val_best:  90.93%, tr:  99.31%, tr_best:  99.58%, epoch time: 251.71 seconds, 4.20 minutes\n",
      "layer   1  Sparsity: 84.4103%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0254%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1730%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6222320 real_backward_count 1272881  20.457%\n",
      "layer   1  Sparsity: 90.9912%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 47 occurrences\n",
      "test - Value 1: 405 occurrences\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:433.796814/457.037598, val:  59.96%, val_best:  90.93%, tr:  99.21%, tr_best:  99.58%, epoch time: 251.51 seconds, 4.19 minutes\n",
      "layer   1  Sparsity: 84.4089%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1461%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.1519%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6254560 real_backward_count 1279121  20.451%\n",
      "layer   1  Sparsity: 88.3057%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2027 occurrences\n",
      "train - Value 1: 2003 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 71 occurrences\n",
      "test - Value 1: 381 occurrences\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:429.991791/470.020813, val:  65.27%, val_best:  90.93%, tr:  99.26%, tr_best:  99.58%, epoch time: 249.26 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4095%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5978%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8556%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6286800 real_backward_count 1285420  20.446%\n",
      "layer   1  Sparsity: 75.4639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2010 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 315 occurrences\n",
      "test - Value 1: 137 occurrences\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:430.474152/377.103455, val:  78.10%, val_best:  90.93%, tr:  99.33%, tr_best:  99.58%, epoch time: 249.48 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4123%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5759%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.1724%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6319040 real_backward_count 1291823  20.443%\n",
      "layer   1  Sparsity: 90.4053%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2010 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 66 occurrences\n",
      "test - Value 1: 386 occurrences\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:447.828918/449.637421, val:  64.60%, val_best:  90.93%, tr:  99.13%, tr_best:  99.58%, epoch time: 248.14 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 84.4090%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5137%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0933%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6351280 real_backward_count 1298159  20.439%\n",
      "layer   1  Sparsity: 83.6182%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 98 occurrences\n",
      "test - Value 1: 354 occurrences\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:473.441132/450.371552, val:  71.68%, val_best:  90.93%, tr:  99.08%, tr_best:  99.58%, epoch time: 248.04 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 84.4105%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0796%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.8721%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6383520 real_backward_count 1304490  20.435%\n",
      "layer   1  Sparsity: 80.9326%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1999 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 159 occurrences\n",
      "test - Value 1: 293 occurrences\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:465.117188/439.421417, val:  83.41%, val_best:  90.93%, tr:  99.21%, tr_best:  99.58%, epoch time: 249.22 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4111%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.6966%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.4107%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6415760 real_backward_count 1310842  20.432%\n",
      "layer   1  Sparsity: 93.9453%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2024 occurrences\n",
      "train - Value 1: 2006 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 131 occurrences\n",
      "test - Value 1: 321 occurrences\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:523.307434/463.149597, val:  78.10%, val_best:  90.93%, tr:  99.28%, tr_best:  99.58%, epoch time: 249.40 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4082%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6510%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.6786%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8efa8573e2b497faee5276e3d102939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÑ‚ñÉ‚ñÜ‚ñÉ‚ñá‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÅ‚ñÖ‚ñà‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÉ‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÜ‚ñà‚ñÇ‚ñà‚ñÖ‚ñÉ‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÑ‚ñÉ‚ñÜ‚ñÉ‚ñá‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÅ‚ñÖ‚ñà‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÉ‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÜ‚ñà‚ñÇ‚ñà‚ñÖ‚ñÉ‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñà‚ñà‚ñÜ‚ñà‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.9928</td></tr><tr><td>tr_epoch_loss</td><td>523.30743</td></tr><tr><td>val_acc_best</td><td>0.90929</td></tr><tr><td>val_acc_now</td><td>0.78097</td></tr><tr><td>val_loss</td><td>463.1496</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">eager-sweep-22</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/s5byf0r1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/s5byf0r1</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251225_222320-s5byf0r1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: f3h2vw90 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloser_encourage_mode: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttimestep_sums_threshold: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: n_tidigits_tonic\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251226_121411-f3h2vw90</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/f3h2vw90' target=\"_blank\">dry-sweep-29</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9lv70ttl' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9lv70ttl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9lv70ttl' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9lv70ttl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/f3h2vw90' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/f3h2vw90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'timestep_sums_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'loser_encourage_mode' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251226_121419_786', 'my_seed': 42, 'TIME': 8, 'BATCH': 1, 'IMAGE_SIZE': 8, 'which_data': 'n_tidigits_tonic', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 512, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 1, 'dvs_duration': 2, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': False, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 8, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'timestep_sums_threshold': 0, 'lif_layer_sg_width2': 8, 'lif_layer_v_threshold2': 32, 'init_scaling': [0.5, 0.25, 0.0625], 'learning_rate': 1, 'learning_rate2': 1, 'loser_encourage_mode': False} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target word: 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target word: 2\n",
      "\n",
      "\n",
      "\n",
      "train_dataset length = 4030, test_dataset length = 452\n",
      "\n",
      "len(train_loader): 4030 BATCH: 1 train_data_count: 4030\n",
      "len(test_loader): 452 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 4, self.v_threshold 512\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 8, self.v_threshold 32\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=512, out_features=200, TIME=8, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.25, 0.0625])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=512, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=8, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=8, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.25, 0.0625])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=32, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=8, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=8, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.25, 0.0625])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 144,400\n",
      "========================================================\n",
      "\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 1594.0\n",
      "lif layer 1 self.abs_max_v: 1594.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 440.0\n",
      "lif layer 2 self.abs_max_v: 440.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 83.0\n",
      "lif layer 1 self.abs_max_v: 2073.0\n",
      "fc layer 3 self.abs_max_out: 91.0\n",
      "layer   1  Sparsity: 88.8916%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 94.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 1880.0\n",
      "fc layer 2 self.abs_max_out: 488.0\n",
      "lif layer 2 self.abs_max_v: 604.5\n",
      "fc layer 3 self.abs_max_out: 100.0\n",
      "fc layer 3 self.abs_max_out: 148.0\n",
      "lif layer 1 self.abs_max_v: 2231.5\n",
      "fc layer 1 self.abs_max_out: 1887.0\n",
      "lif layer 1 self.abs_max_v: 2327.0\n",
      "lif layer 2 self.abs_max_v: 650.0\n",
      "fc layer 1 self.abs_max_out: 1912.0\n",
      "fc layer 3 self.abs_max_out: 162.0\n",
      "lif layer 1 self.abs_max_v: 2423.5\n",
      "fc layer 1 self.abs_max_out: 2121.0\n",
      "lif layer 1 self.abs_max_v: 2543.0\n",
      "fc layer 3 self.abs_max_out: 164.0\n",
      "fc layer 1 self.abs_max_out: 2190.0\n",
      "lif layer 1 self.abs_max_v: 2573.0\n",
      "fc layer 3 self.abs_max_out: 174.0\n",
      "fc layer 2 self.abs_max_out: 533.0\n",
      "lif layer 1 self.abs_max_v: 2946.0\n",
      "lif layer 1 self.abs_max_v: 3038.5\n",
      "fc layer 1 self.abs_max_out: 2289.0\n",
      "lif layer 2 self.abs_max_v: 704.0\n",
      "lif layer 2 self.abs_max_v: 762.5\n",
      "fc layer 3 self.abs_max_out: 197.0\n",
      "fc layer 3 self.abs_max_out: 199.0\n",
      "fc layer 2 self.abs_max_out: 536.0\n",
      "fc layer 1 self.abs_max_out: 2375.0\n",
      "fc layer 3 self.abs_max_out: 203.0\n",
      "fc layer 3 self.abs_max_out: 221.0\n",
      "fc layer 3 self.abs_max_out: 265.0\n",
      "lif layer 1 self.abs_max_v: 3247.5\n",
      "lif layer 1 self.abs_max_v: 3250.0\n",
      "fc layer 1 self.abs_max_out: 2542.0\n",
      "fc layer 2 self.abs_max_out: 578.0\n",
      "lif layer 2 self.abs_max_v: 791.0\n",
      "lif layer 1 self.abs_max_v: 3362.5\n",
      "lif layer 2 self.abs_max_v: 844.5\n",
      "fc layer 1 self.abs_max_out: 2555.0\n",
      "fc layer 1 self.abs_max_out: 2645.0\n",
      "lif layer 2 self.abs_max_v: 857.0\n",
      "fc layer 2 self.abs_max_out: 592.0\n",
      "fc layer 2 self.abs_max_out: 606.0\n",
      "fc layer 2 self.abs_max_out: 610.0\n",
      "fc layer 2 self.abs_max_out: 650.0\n",
      "fc layer 2 self.abs_max_out: 654.0\n",
      "fc layer 2 self.abs_max_out: 668.0\n",
      "lif layer 2 self.abs_max_v: 899.5\n",
      "fc layer 2 self.abs_max_out: 680.0\n",
      "lif layer 2 self.abs_max_v: 980.5\n",
      "fc layer 2 self.abs_max_out: 683.0\n",
      "lif layer 2 self.abs_max_v: 982.5\n",
      "fc layer 1 self.abs_max_out: 2650.0\n",
      "fc layer 1 self.abs_max_out: 2713.0\n",
      "fc layer 2 self.abs_max_out: 715.0\n",
      "fc layer 1 self.abs_max_out: 2895.0\n",
      "fc layer 1 self.abs_max_out: 2914.0\n",
      "lif layer 2 self.abs_max_v: 986.0\n",
      "lif layer 2 self.abs_max_v: 1093.5\n",
      "lif layer 2 self.abs_max_v: 1125.5\n",
      "fc layer 2 self.abs_max_out: 747.0\n",
      "fc layer 1 self.abs_max_out: 2968.0\n",
      "fc layer 2 self.abs_max_out: 794.0\n",
      "fc layer 3 self.abs_max_out: 274.0\n",
      "fc layer 3 self.abs_max_out: 282.0\n",
      "fc layer 3 self.abs_max_out: 294.0\n",
      "fc layer 3 self.abs_max_out: 302.0\n",
      "fc layer 1 self.abs_max_out: 3124.0\n",
      "fc layer 1 self.abs_max_out: 3215.0\n",
      "fc layer 1 self.abs_max_out: 3558.0\n",
      "lif layer 1 self.abs_max_v: 3558.0\n",
      "fc layer 2 self.abs_max_out: 884.0\n",
      "fc layer 3 self.abs_max_out: 312.0\n",
      "fc layer 3 self.abs_max_out: 331.0\n",
      "lif layer 2 self.abs_max_v: 1154.5\n",
      "fc layer 2 self.abs_max_out: 894.0\n",
      "fc layer 2 self.abs_max_out: 897.0\n",
      "fc layer 2 self.abs_max_out: 948.0\n",
      "fc layer 3 self.abs_max_out: 332.0\n",
      "fc layer 3 self.abs_max_out: 355.0\n",
      "fc layer 3 self.abs_max_out: 370.0\n",
      "fc layer 3 self.abs_max_out: 379.0\n",
      "fc layer 3 self.abs_max_out: 380.0\n",
      "fc layer 3 self.abs_max_out: 387.0\n",
      "fc layer 3 self.abs_max_out: 398.0\n",
      "lif layer 2 self.abs_max_v: 1157.5\n",
      "lif layer 2 self.abs_max_v: 1173.0\n",
      "fc layer 3 self.abs_max_out: 404.0\n",
      "fc layer 3 self.abs_max_out: 428.0\n",
      "fc layer 3 self.abs_max_out: 431.0\n",
      "fc layer 3 self.abs_max_out: 464.0\n",
      "fc layer 3 self.abs_max_out: 537.0\n",
      "fc layer 1 self.abs_max_out: 3739.0\n",
      "lif layer 1 self.abs_max_v: 3739.0\n",
      "fc layer 1 self.abs_max_out: 3859.0\n",
      "lif layer 1 self.abs_max_v: 3859.0\n",
      "fc layer 1 self.abs_max_out: 3891.0\n",
      "lif layer 1 self.abs_max_v: 3891.0\n",
      "fc layer 1 self.abs_max_out: 3926.0\n",
      "lif layer 1 self.abs_max_v: 3926.0\n",
      "lif layer 2 self.abs_max_v: 1215.5\n",
      "fc layer 1 self.abs_max_out: 4116.0\n",
      "lif layer 1 self.abs_max_v: 4116.0\n",
      "fc layer 1 self.abs_max_out: 4317.0\n",
      "lif layer 1 self.abs_max_v: 4317.0\n",
      "fc layer 1 self.abs_max_out: 4390.0\n",
      "lif layer 1 self.abs_max_v: 4390.0\n",
      "fc layer 1 self.abs_max_out: 4439.0\n",
      "lif layer 1 self.abs_max_v: 4439.0\n",
      "fc layer 1 self.abs_max_out: 4696.0\n",
      "lif layer 1 self.abs_max_v: 4696.0\n",
      "lif layer 2 self.abs_max_v: 1282.0\n",
      "lif layer 2 self.abs_max_v: 1331.0\n",
      "fc layer 1 self.abs_max_out: 4769.0\n",
      "lif layer 1 self.abs_max_v: 4769.0\n",
      "fc layer 1 self.abs_max_out: 4968.0\n",
      "lif layer 1 self.abs_max_v: 4968.0\n",
      "fc layer 1 self.abs_max_out: 4993.0\n",
      "lif layer 1 self.abs_max_v: 4993.0\n",
      "fc layer 1 self.abs_max_out: 5002.0\n",
      "lif layer 1 self.abs_max_v: 5002.0\n",
      "fc layer 3 self.abs_max_out: 566.0\n",
      "fc layer 3 self.abs_max_out: 575.0\n",
      "lif layer 2 self.abs_max_v: 1377.0\n",
      "fc layer 1 self.abs_max_out: 5047.0\n",
      "lif layer 1 self.abs_max_v: 5047.0\n",
      "fc layer 1 self.abs_max_out: 5098.0\n",
      "lif layer 1 self.abs_max_v: 5098.0\n",
      "fc layer 1 self.abs_max_out: 5123.0\n",
      "fc layer 1 self.abs_max_out: 5132.0\n",
      "lif layer 1 self.abs_max_v: 5132.0\n",
      "fc layer 1 self.abs_max_out: 5202.0\n",
      "lif layer 1 self.abs_max_v: 5202.0\n",
      "fc layer 1 self.abs_max_out: 5216.0\n",
      "fc layer 3 self.abs_max_out: 603.0\n",
      "fc layer 1 self.abs_max_out: 5219.0\n",
      "lif layer 1 self.abs_max_v: 5219.0\n",
      "fc layer 1 self.abs_max_out: 5265.0\n",
      "lif layer 1 self.abs_max_v: 5299.0\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 661.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 2024.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 2138.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 2232.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 3068.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 3144.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 3319.00 at epoch 0, iter 4029\n",
      "fc layer 1 self.abs_max_out: 5271.0\n",
      "fc layer 1 self.abs_max_out: 5329.0\n",
      "lif layer 1 self.abs_max_v: 5329.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss:134.984634/175.084610, val:  50.00%, val_best:  50.00%, tr:  86.35%, tr_best:  86.35%, epoch time: 249.37 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4093%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 88.0468%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.7633%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 32240 real_backward_count 9751  30.245%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "layer   1  Sparsity: 84.8389%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 87.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 5332.0\n",
      "fc layer 1 self.abs_max_out: 5400.0\n",
      "lif layer 1 self.abs_max_v: 5400.0\n",
      "fc layer 2 self.abs_max_out: 956.0\n",
      "fc layer 1 self.abs_max_out: 5480.0\n",
      "lif layer 1 self.abs_max_v: 5459.5\n",
      "lif layer 1 self.abs_max_v: 5549.0\n",
      "lif layer 1 self.abs_max_v: 5554.5\n",
      "fc layer 3 self.abs_max_out: 607.0\n",
      "fc layer 1 self.abs_max_out: 5529.0\n",
      "lif layer 1 self.abs_max_v: 5602.5\n",
      "lif layer 2 self.abs_max_v: 1393.0\n",
      "fc layer 3 self.abs_max_out: 612.0\n",
      "fc layer 1 self.abs_max_out: 5588.0\n",
      "lif layer 2 self.abs_max_v: 1408.5\n",
      "lif layer 2 self.abs_max_v: 1410.5\n",
      "fc layer 1 self.abs_max_out: 5603.0\n",
      "lif layer 1 self.abs_max_v: 5603.0\n",
      "lif layer 2 self.abs_max_v: 1430.5\n",
      "fc layer 1 self.abs_max_out: 5645.0\n",
      "lif layer 1 self.abs_max_v: 5645.0\n",
      "lif layer 2 self.abs_max_v: 1452.5\n",
      "fc layer 1 self.abs_max_out: 5654.0\n",
      "lif layer 1 self.abs_max_v: 5654.0\n",
      "fc layer 1 self.abs_max_out: 5746.0\n",
      "lif layer 1 self.abs_max_v: 5856.0\n",
      "fc layer 3 self.abs_max_out: 656.0\n",
      "fc layer 1 self.abs_max_out: 5832.0\n",
      "fc layer 1 self.abs_max_out: 5891.0\n",
      "lif layer 1 self.abs_max_v: 6015.5\n",
      "fc layer 1 self.abs_max_out: 5936.0\n",
      "fc layer 1 self.abs_max_out: 6106.0\n",
      "lif layer 1 self.abs_max_v: 6106.0\n",
      "fc layer 3 self.abs_max_out: 664.0\n",
      "lif layer 1 self.abs_max_v: 6175.5\n",
      "fc layer 2 self.abs_max_out: 960.0\n",
      "fc layer 2 self.abs_max_out: 964.0\n",
      "fc layer 2 self.abs_max_out: 974.0\n",
      "fc layer 2 self.abs_max_out: 990.0\n",
      "fc layer 2 self.abs_max_out: 991.0\n",
      "fc layer 2 self.abs_max_out: 1020.0\n",
      "fc layer 2 self.abs_max_out: 1026.0\n",
      "fc layer 2 self.abs_max_out: 1046.0\n",
      "fc layer 3 self.abs_max_out: 687.0\n",
      "fc layer 3 self.abs_max_out: 724.0\n",
      "fc layer 1 self.abs_max_out: 6161.0\n",
      "fc layer 3 self.abs_max_out: 726.0\n",
      "fc layer 3 self.abs_max_out: 732.0\n",
      "fc layer 3 self.abs_max_out: 743.0\n",
      "fc layer 2 self.abs_max_out: 1057.0\n",
      "fc layer 2 self.abs_max_out: 1062.0\n",
      "fc layer 2 self.abs_max_out: 1063.0\n",
      "fc layer 1 self.abs_max_out: 6173.0\n",
      "fc layer 1 self.abs_max_out: 6342.0\n",
      "lif layer 1 self.abs_max_v: 6345.0\n",
      "fc layer 2 self.abs_max_out: 1080.0\n",
      "fc layer 2 self.abs_max_out: 1113.0\n",
      "fc layer 1 self.abs_max_out: 6424.0\n",
      "lif layer 1 self.abs_max_v: 6432.5\n",
      "fc layer 1 self.abs_max_out: 6429.0\n",
      "fc layer 1 self.abs_max_out: 6523.0\n",
      "lif layer 1 self.abs_max_v: 6523.0\n",
      "fc layer 3 self.abs_max_out: 773.0\n",
      "fc layer 1 self.abs_max_out: 6774.0\n",
      "lif layer 1 self.abs_max_v: 6774.0\n",
      "fc layer 3 self.abs_max_out: 792.0\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 3403.00 at epoch 1, iter 4029\n",
      "max_activation_accul updated: 3582.00 at epoch 1, iter 4029\n",
      "max_activation_accul updated: 3874.00 at epoch 1, iter 4029\n",
      "lif layer 1 self.abs_max_v: 6838.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 298 occurrences\n",
      "test - Value 1: 154 occurrences\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss:197.711868/187.371613, val:  80.53%, val_best:  80.53%, tr:  91.54%, tr_best:  91.54%, epoch time: 250.28 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 84.4102%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.5439%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.2347%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 64480 real_backward_count 18708  29.014%\n",
      "layer   1  Sparsity: 84.0088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 802.0\n",
      "fc layer 3 self.abs_max_out: 812.0\n",
      "fc layer 1 self.abs_max_out: 6894.0\n",
      "lif layer 1 self.abs_max_v: 6926.0\n",
      "lif layer 2 self.abs_max_v: 1478.0\n",
      "lif layer 2 self.abs_max_v: 1507.0\n",
      "fc layer 2 self.abs_max_out: 1132.0\n",
      "fc layer 2 self.abs_max_out: 1147.0\n",
      "fc layer 2 self.abs_max_out: 1176.0\n",
      "fc layer 1 self.abs_max_out: 6898.0\n",
      "fc layer 1 self.abs_max_out: 7027.0\n",
      "lif layer 1 self.abs_max_v: 7065.0\n",
      "fc layer 2 self.abs_max_out: 1194.0\n",
      "fc layer 2 self.abs_max_out: 1206.0\n",
      "fc layer 1 self.abs_max_out: 7091.0\n",
      "lif layer 1 self.abs_max_v: 7131.0\n",
      "fc layer 3 self.abs_max_out: 866.0\n",
      "fc layer 3 self.abs_max_out: 879.0\n",
      "fc layer 1 self.abs_max_out: 7302.0\n",
      "lif layer 1 self.abs_max_v: 7302.0\n",
      "fc layer 2 self.abs_max_out: 1208.0\n",
      "fc layer 2 self.abs_max_out: 1217.0\n",
      "fc layer 2 self.abs_max_out: 1219.0\n",
      "fc layer 2 self.abs_max_out: 1228.0\n",
      "fc layer 3 self.abs_max_out: 902.0\n",
      "fc layer 1 self.abs_max_out: 7377.0\n",
      "lif layer 1 self.abs_max_v: 7424.0\n",
      "fc layer 2 self.abs_max_out: 1229.0\n",
      "fc layer 1 self.abs_max_out: 7447.0\n",
      "lif layer 1 self.abs_max_v: 7447.0\n",
      "lif layer 2 self.abs_max_v: 1509.5\n",
      "lif layer 2 self.abs_max_v: 1517.0\n",
      "lif layer 2 self.abs_max_v: 1535.0\n",
      "fc layer 2 self.abs_max_out: 1274.0\n",
      "lif layer 2 self.abs_max_v: 1556.5\n",
      "lif layer 2 self.abs_max_v: 1585.5\n",
      "fc layer 1 self.abs_max_out: 7454.0\n",
      "lif layer 1 self.abs_max_v: 7454.0\n",
      "lif layer 2 self.abs_max_v: 1608.0\n",
      "fc layer 1 self.abs_max_out: 7462.0\n",
      "lif layer 1 self.abs_max_v: 7462.0\n",
      "fc layer 1 self.abs_max_out: 7589.0\n",
      "lif layer 1 self.abs_max_v: 7624.5\n",
      "lif layer 2 self.abs_max_v: 1631.0\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 4391.00 at epoch 2, iter 4029\n",
      "lif layer 2 self.abs_max_v: 1696.5\n",
      "max_activation_accul updated: 4607.00 at epoch 2, iter 4029\n",
      "lif layer 2 self.abs_max_v: 1697.5\n",
      "lif layer 2 self.abs_max_v: 1707.5\n",
      "fc layer 1 self.abs_max_out: 7608.0\n",
      "max_activation_accul updated: 4654.00 at epoch 2, iter 4029\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 447 occurrences\n",
      "test - Value 1: 5 occurrences\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss:325.267029/321.413483, val:  51.11%, val_best:  80.53%, tr:  93.97%, tr_best:  93.97%, epoch time: 250.54 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 84.4104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 82.5398%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.0693%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 96720 real_backward_count 27226  28.149%\n",
      "layer   1  Sparsity: 78.8330%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 7618.0\n",
      "fc layer 1 self.abs_max_out: 7895.0\n",
      "lif layer 1 self.abs_max_v: 7895.0\n",
      "lif layer 2 self.abs_max_v: 1708.0\n",
      "lif layer 2 self.abs_max_v: 1710.5\n",
      "fc layer 1 self.abs_max_out: 7897.0\n",
      "lif layer 1 self.abs_max_v: 7897.0\n",
      "fc layer 3 self.abs_max_out: 943.0\n",
      "fc layer 3 self.abs_max_out: 965.0\n",
      "lif layer 2 self.abs_max_v: 1733.0\n",
      "fc layer 2 self.abs_max_out: 1281.0\n",
      "fc layer 2 self.abs_max_out: 1298.0\n",
      "fc layer 2 self.abs_max_out: 1327.0\n",
      "fc layer 1 self.abs_max_out: 8053.0\n",
      "lif layer 1 self.abs_max_v: 8091.5\n",
      "fc layer 3 self.abs_max_out: 1005.0\n",
      "fc layer 2 self.abs_max_out: 1337.0\n",
      "fc layer 2 self.abs_max_out: 1366.0\n",
      "fc layer 2 self.abs_max_out: 1384.0\n",
      "fc layer 2 self.abs_max_out: 1385.0\n",
      "fc layer 3 self.abs_max_out: 1012.0\n",
      "fc layer 2 self.abs_max_out: 1417.0\n",
      "fc layer 1 self.abs_max_out: 8082.0\n",
      "fc layer 2 self.abs_max_out: 1420.0\n",
      "fc layer 3 self.abs_max_out: 1109.0\n",
      "fc layer 1 self.abs_max_out: 8145.0\n",
      "lif layer 1 self.abs_max_v: 8183.0\n",
      "fc layer 2 self.abs_max_out: 1432.0\n",
      "fc layer 1 self.abs_max_out: 8228.0\n",
      "lif layer 1 self.abs_max_v: 8271.0\n",
      "fc layer 2 self.abs_max_out: 1455.0\n",
      "fc layer 1 self.abs_max_out: 8263.0\n",
      "lif layer 1 self.abs_max_v: 8301.0\n",
      "fc layer 2 self.abs_max_out: 1495.0\n",
      "fc layer 1 self.abs_max_out: 8493.0\n",
      "lif layer 1 self.abs_max_v: 8493.0\n",
      "fc layer 2 self.abs_max_out: 1496.0\n",
      "fc layer 2 self.abs_max_out: 1503.0\n",
      "fc layer 2 self.abs_max_out: 1581.0\n",
      "fc layer 2 self.abs_max_out: 1589.0\n",
      "fc layer 2 self.abs_max_out: 1605.0\n",
      "fc layer 2 self.abs_max_out: 1629.0\n",
      "fc layer 2 self.abs_max_out: 1632.0\n",
      "fc layer 2 self.abs_max_out: 1717.0\n",
      "fc layer 2 self.abs_max_out: 1772.0\n",
      "lif layer 2 self.abs_max_v: 1772.0\n",
      "lif layer 1 self.abs_max_v: 8509.0\n",
      "fc layer 2 self.abs_max_out: 1777.0\n",
      "lif layer 2 self.abs_max_v: 1777.0\n",
      "train - Value 0: 2004 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 2 self.abs_max_out: 1787.0\n",
      "lif layer 2 self.abs_max_v: 1787.0\n",
      "max_activation_accul updated: 4924.00 at epoch 3, iter 4029\n",
      "max_activation_accul updated: 6060.00 at epoch 3, iter 4029\n",
      "fc layer 1 self.abs_max_out: 8502.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:423.822540/457.340149, val:  50.00%, val_best:  80.53%, tr:  94.52%, tr_best:  94.52%, epoch time: 249.53 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 84.4116%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.5548%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.5632%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 128960 real_backward_count 35623  27.623%\n",
      "layer   1  Sparsity: 70.6787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 32.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1812.0\n",
      "lif layer 2 self.abs_max_v: 1812.0\n",
      "fc layer 1 self.abs_max_out: 8739.0\n",
      "lif layer 1 self.abs_max_v: 8739.0\n",
      "fc layer 1 self.abs_max_out: 8791.0\n",
      "lif layer 1 self.abs_max_v: 8791.0\n",
      "fc layer 2 self.abs_max_out: 1844.0\n",
      "lif layer 2 self.abs_max_v: 1844.0\n",
      "fc layer 2 self.abs_max_out: 1861.0\n",
      "lif layer 2 self.abs_max_v: 1861.0\n",
      "fc layer 1 self.abs_max_out: 8957.0\n",
      "lif layer 1 self.abs_max_v: 8957.0\n",
      "fc layer 2 self.abs_max_out: 1883.0\n",
      "lif layer 2 self.abs_max_v: 1883.0\n",
      "fc layer 2 self.abs_max_out: 1897.0\n",
      "lif layer 2 self.abs_max_v: 1897.0\n",
      "fc layer 2 self.abs_max_out: 1922.0\n",
      "lif layer 2 self.abs_max_v: 1922.0\n",
      "fc layer 3 self.abs_max_out: 1219.0\n",
      "fc layer 1 self.abs_max_out: 9210.0\n",
      "lif layer 1 self.abs_max_v: 9210.0\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 296 occurrences\n",
      "test - Value 1: 156 occurrences\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:463.621521/381.473969, val:  82.30%, val_best:  82.30%, tr:  95.31%, tr_best:  95.31%, epoch time: 249.06 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4134%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.1778%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.9523%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 161200 real_backward_count 43778  27.158%\n",
      "layer   1  Sparsity: 84.2285%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 9346.0\n",
      "lif layer 1 self.abs_max_v: 9346.0\n",
      "fc layer 1 self.abs_max_out: 9394.0\n",
      "lif layer 1 self.abs_max_v: 9394.0\n",
      "lif layer 2 self.abs_max_v: 1934.0\n",
      "lif layer 2 self.abs_max_v: 1954.0\n",
      "lif layer 2 self.abs_max_v: 1970.5\n",
      "lif layer 2 self.abs_max_v: 1990.0\n",
      "lif layer 2 self.abs_max_v: 1995.0\n",
      "lif layer 2 self.abs_max_v: 2036.5\n",
      "lif layer 2 self.abs_max_v: 2061.0\n",
      "lif layer 2 self.abs_max_v: 2092.5\n",
      "lif layer 1 self.abs_max_v: 9430.0\n",
      "fc layer 1 self.abs_max_out: 9610.0\n",
      "lif layer 1 self.abs_max_v: 9610.0\n",
      "lif layer 2 self.abs_max_v: 2106.0\n",
      "lif layer 2 self.abs_max_v: 2169.0\n",
      "lif layer 2 self.abs_max_v: 2263.5\n",
      "lif layer 1 self.abs_max_v: 9672.5\n",
      "fc layer 1 self.abs_max_out: 9785.0\n",
      "lif layer 1 self.abs_max_v: 9785.0\n",
      "fc layer 1 self.abs_max_out: 9803.0\n",
      "lif layer 1 self.abs_max_v: 9967.0\n",
      "fc layer 1 self.abs_max_out: 9811.0\n",
      "lif layer 1 self.abs_max_v: 9975.5\n",
      "train - Value 0: 2022 occurrences\n",
      "train - Value 1: 2008 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 109 occurrences\n",
      "test - Value 1: 343 occurrences\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:506.331543/349.698303, val:  72.35%, val_best:  82.30%, tr:  94.62%, tr_best:  95.31%, epoch time: 248.78 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.8899%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1734%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 193440 real_backward_count 51818  26.788%\n",
      "layer   1  Sparsity: 84.3262%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 9902.0\n",
      "lif layer 1 self.abs_max_v: 10055.0\n",
      "lif layer 1 self.abs_max_v: 10063.0\n",
      "fc layer 1 self.abs_max_out: 10010.0\n",
      "fc layer 1 self.abs_max_out: 10026.0\n",
      "lif layer 1 self.abs_max_v: 10172.5\n",
      "lif layer 2 self.abs_max_v: 2319.5\n",
      "fc layer 1 self.abs_max_out: 10057.0\n",
      "lif layer 1 self.abs_max_v: 10229.5\n",
      "fc layer 3 self.abs_max_out: 1237.0\n",
      "fc layer 3 self.abs_max_out: 1265.0\n",
      "fc layer 3 self.abs_max_out: 1354.0\n",
      "fc layer 1 self.abs_max_out: 10231.0\n",
      "lif layer 1 self.abs_max_v: 10231.0\n",
      "fc layer 1 self.abs_max_out: 10251.0\n",
      "lif layer 1 self.abs_max_v: 10251.0\n",
      "lif layer 1 self.abs_max_v: 10358.5\n",
      "train - Value 0: 2028 occurrences\n",
      "train - Value 1: 2002 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 129 occurrences\n",
      "test - Value 1: 323 occurrences\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:512.312744/423.672729, val:  75.44%, val_best:  82.30%, tr:  95.91%, tr_best:  95.91%, epoch time: 250.95 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 84.4104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.4397%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2268%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225680 real_backward_count 59762  26.481%\n",
      "layer   1  Sparsity: 82.6172%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1386.0\n",
      "fc layer 1 self.abs_max_out: 10381.0\n",
      "lif layer 1 self.abs_max_v: 10381.0\n",
      "fc layer 1 self.abs_max_out: 10395.0\n",
      "lif layer 1 self.abs_max_v: 10395.0\n",
      "lif layer 1 self.abs_max_v: 10555.0\n",
      "lif layer 1 self.abs_max_v: 10555.5\n",
      "fc layer 1 self.abs_max_out: 10505.0\n",
      "fc layer 3 self.abs_max_out: 1410.0\n",
      "lif layer 2 self.abs_max_v: 2346.5\n",
      "fc layer 1 self.abs_max_out: 10652.0\n",
      "lif layer 1 self.abs_max_v: 10652.0\n",
      "lif layer 1 self.abs_max_v: 10701.0\n",
      "lif layer 1 self.abs_max_v: 10716.5\n",
      "fc layer 1 self.abs_max_out: 10659.0\n",
      "lif layer 1 self.abs_max_v: 10762.0\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "lif layer 2 self.abs_max_v: 2349.0\n",
      "lif layer 2 self.abs_max_v: 2513.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 77 occurrences\n",
      "test - Value 1: 375 occurrences\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:562.536438/508.607666, val:  66.15%, val_best:  82.30%, tr:  96.90%, tr_best:  96.90%, epoch time: 249.19 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4107%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.0348%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.3864%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 257920 real_backward_count 67456  26.154%\n",
      "layer   1  Sparsity: 71.6797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 10734.0\n",
      "fc layer 1 self.abs_max_out: 10800.0\n",
      "lif layer 1 self.abs_max_v: 10800.0\n",
      "fc layer 1 self.abs_max_out: 10818.0\n",
      "lif layer 1 self.abs_max_v: 10818.0\n",
      "lif layer 1 self.abs_max_v: 10916.5\n",
      "fc layer 1 self.abs_max_out: 10912.0\n",
      "lif layer 1 self.abs_max_v: 10968.5\n",
      "lif layer 1 self.abs_max_v: 10974.0\n",
      "lif layer 1 self.abs_max_v: 11000.5\n",
      "fc layer 1 self.abs_max_out: 10978.0\n",
      "lif layer 1 self.abs_max_v: 11060.5\n",
      "lif layer 2 self.abs_max_v: 2548.0\n",
      "fc layer 2 self.abs_max_out: 1938.0\n",
      "lif layer 1 self.abs_max_v: 11120.5\n",
      "fc layer 2 self.abs_max_out: 1979.0\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 58 occurrences\n",
      "test - Value 1: 394 occurrences\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:551.484436/458.259979, val:  62.83%, val_best:  82.30%, tr:  96.85%, tr_best:  96.90%, epoch time: 247.88 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 84.4132%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9871%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1846%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 290160 real_backward_count 75221  25.924%\n",
      "layer   1  Sparsity: 78.1250%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 11138.5\n",
      "fc layer 2 self.abs_max_out: 2002.0\n",
      "fc layer 2 self.abs_max_out: 2016.0\n",
      "lif layer 1 self.abs_max_v: 11208.5\n",
      "fc layer 2 self.abs_max_out: 2025.0\n",
      "fc layer 1 self.abs_max_out: 10995.0\n",
      "lif layer 1 self.abs_max_v: 11232.5\n",
      "fc layer 1 self.abs_max_out: 11135.0\n",
      "fc layer 2 self.abs_max_out: 2082.0\n",
      "lif layer 1 self.abs_max_v: 11290.0\n",
      "lif layer 1 self.abs_max_v: 11302.0\n",
      "fc layer 3 self.abs_max_out: 1452.0\n",
      "fc layer 1 self.abs_max_out: 11251.0\n",
      "fc layer 1 self.abs_max_out: 11303.0\n",
      "lif layer 1 self.abs_max_v: 11303.0\n",
      "lif layer 1 self.abs_max_v: 11424.0\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "lif layer 1 self.abs_max_v: 11543.5\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 59 occurrences\n",
      "test - Value 1: 393 occurrences\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:558.119873/481.396332, val:  63.05%, val_best:  82.30%, tr:  96.35%, tr_best:  96.90%, epoch time: 248.41 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 84.4117%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.8393%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8460%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 322400 real_backward_count 82875  25.706%\n",
      "layer   1  Sparsity: 77.7588%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 11362.0\n",
      "fc layer 1 self.abs_max_out: 11427.0\n",
      "fc layer 2 self.abs_max_out: 2115.0\n",
      "fc layer 2 self.abs_max_out: 2165.0\n",
      "fc layer 1 self.abs_max_out: 11451.0\n",
      "fc layer 1 self.abs_max_out: 11464.0\n",
      "fc layer 3 self.abs_max_out: 1454.0\n",
      "fc layer 1 self.abs_max_out: 11497.0\n",
      "fc layer 3 self.abs_max_out: 1511.0\n",
      "train - Value 0: 2005 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 7352.00 at epoch 10, iter 4029\n",
      "fc layer 3 self.abs_max_out: 1520.0\n",
      "max_activation_accul updated: 8335.00 at epoch 10, iter 4029\n",
      "fc layer 3 self.abs_max_out: 1530.0\n",
      "fc layer 3 self.abs_max_out: 1662.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 450 occurrences\n",
      "test - Value 1: 2 occurrences\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:612.273987/629.136108, val:  50.44%, val_best:  82.30%, tr:  97.02%, tr_best:  97.02%, epoch time: 249.07 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 84.4118%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9505%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.4739%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 354640 real_backward_count 90408  25.493%\n",
      "layer   1  Sparsity: 80.5664%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 11665.0\n",
      "lif layer 1 self.abs_max_v: 11665.0\n",
      "fc layer 1 self.abs_max_out: 11684.0\n",
      "lif layer 1 self.abs_max_v: 11684.0\n",
      "fc layer 1 self.abs_max_out: 11712.0\n",
      "lif layer 1 self.abs_max_v: 11712.0\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "lif layer 2 self.abs_max_v: 2575.5\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 293 occurrences\n",
      "test - Value 1: 159 occurrences\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:673.220215/511.427765, val:  82.52%, val_best:  82.52%, tr:  97.69%, tr_best:  97.69%, epoch time: 250.71 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 84.4112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9854%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3696%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 386880 real_backward_count 97819  25.284%\n",
      "layer   1  Sparsity: 89.9658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 11902.0\n",
      "lif layer 1 self.abs_max_v: 11902.0\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "target_word=2\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}_targetword{target_word}_new251129',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [8]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [8]},\n",
    "        \"which_data\": {\"values\": ['n_tidigits_tonic']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [2048.0,1024.0,512.0,256.0,128.0,64.0,32.0]},\n",
    "        \"lif_layer_v_threshold2\": {\"values\": [2048.0,1024.0,512.0,256.0,128.0,64.0,32.0]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [0.25, 0.5,1.0, 2.0, 4.0, 8.0,]},\n",
    "        \"lif_layer_sg_width2\": {\"values\": [1.0, 2.0,4.0, 8.0, 16.0,32.0,64.0]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [4.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "        \"learning_rate\": {\"values\": [1.0, 2.0, 4.0, 8.0]},\n",
    "        \"learning_rate2\": {\"values\": [1.0, 2.0, 4.0, 8.0]}, \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [1]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [target_word]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [False]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [9]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [False]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [8]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [0]},\n",
    "        # # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "\n",
    "        \"scale_exp_2w\": {\"values\": [0]},\n",
    "        # # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "\n",
    "        \"scale_exp_3w\": {\"values\": [0]},\n",
    "        # # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "\n",
    "        \"timestep_sums_threshold\": {\"values\": [0]},\n",
    "\n",
    "        \"loser_encourage_mode\": {\"values\": [True, False]},\n",
    "        \n",
    "        \"init_scaling_0\": {\"values\": [1/2]},\n",
    "        \"init_scaling_1\": {\"values\": [1/4]},\n",
    "        \"init_scaling_2\": {\"values\": [1/16]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"5\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_2w,wandb.config.scale_exp_2w],[wandb.config.scale_exp_3w,wandb.config.scale_exp_3w]],\n",
    "        timestep_sums_threshold  =  wandb.config.timestep_sums_threshold,\n",
    "        loser_encourage_mode  =  wandb.config.loser_encourage_mode,\n",
    "        lif_layer_sg_width2  =  wandb.config.lif_layer_sg_width2,\n",
    "        lif_layer_v_threshold2  =  wandb.config.lif_layer_v_threshold2,\n",
    "        learning_rate2  =  wandb.config.learning_rate2,\n",
    "        init_scaling = [wandb.config.init_scaling_0,wandb.config.init_scaling_1,wandb.config.init_scaling_2],\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = '9lv70ttl'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn NTIDIGITS SWEEP LOSER ONOFF new251129')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn NTIDIGITS SWEEP LOSER ONOFF new251129')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
