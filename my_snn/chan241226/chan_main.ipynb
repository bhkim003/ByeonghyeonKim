{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import io\n",
    "import os\n",
    "\n",
    "os.chdir(\"./../data/\")\n",
    "'''\n",
    "filename = [\"C_Easy1_noise005.mat\", \"C_Easy1_noise01.mat\", \"C_Easy1_noise015.mat\", \"C_Easy1_noise02.mat\",\n",
    "            \"C_Easy2_noise005.mat\", \"C_Easy2_noise01.mat\", \"C_Easy2_noise015.mat\", \"C_Easy2_noise02.mat\",\n",
    "            \"C_Difficult1_noise005.mat\", \"C_Difficult1_noise01.mat\", \"C_Difficult1_noise015.mat\", \"C_Difficult1_noise02.mat\",\n",
    "            \"C_Difficult2_noise005.mat\", \"C_Difficult2_noise01.mat\", \"C_Difficult2_noise015.mat\", \"C_Difficult2_noise02.mat\"]\n",
    "\n",
    "'''\n",
    "filename = [\"C_Easy1_noise005.mat\", \"C_Easy1_noise01.mat\", \"C_Easy1_noise015.mat\", \"C_Easy1_noise02.mat\",\n",
    "            \"C_Easy1_noise025.mat\", \"C_Easy1_noise03.mat\", \"C_Easy1_noise035.mat\", \"C_Easy1_noise04.mat\",\n",
    "            \"C_Easy2_noise005.mat\", \"C_Easy2_noise01.mat\", \"C_Easy2_noise015.mat\", \"C_Easy2_noise02.mat\",\n",
    "            \"C_Difficult1_noise005.mat\", \"C_Difficult1_noise01.mat\", \"C_Difficult1_noise015.mat\", \"C_Difficult1_noise02.mat\",\n",
    "            \"C_Difficult2_noise005.mat\", \"C_Difficult2_noise01.mat\", \"C_Difficult2_noise015.mat\", \"C_Difficult2_noise02.mat\"]\n",
    "\n",
    "dataset_num = 20\n",
    "training_num = 2400\n",
    "spike_length = 50\n",
    "\n",
    "training_spike_group = np.zeros((dataset_num, training_num, spike_length))\n",
    "\n",
    "for ds in range(20):\n",
    "    mat1 = io.loadmat(filename[ds])\n",
    "    raw = mat1['data'][0]\n",
    "    ans_times = mat1['spike_times'][0][0][0]\n",
    "    ans_cluster = mat1['spike_class'][0][0][0]\n",
    "\n",
    "    slope = np.zeros(len(raw)-2)\n",
    "    for i in range(len(raw)-2):\n",
    "        slope[i] = raw[i+1] - raw[i]\n",
    "\n",
    "    spike_group = np.zeros((len(ans_times), spike_length))\n",
    "\n",
    "    for i in range(len(ans_times)):\n",
    "        max_slope_index = ans_times[i] + np.argmax(slope[ans_times[i] : ans_times[i] + 25])\n",
    "        spike_group[i, :] = raw[max_slope_index - 10 : max_slope_index + 40]\n",
    "\n",
    "    '''#max_slope_idx_check\n",
    "    max_slope_index = np.zeros(len(ans_times))\n",
    "    \n",
    "    for i in range(len(ans_times)):\n",
    "        slope_value = np.zeros(49)\n",
    "        for j in range(49):\n",
    "            slope_value[j] = abs(spike_group[i, j+1] - spike_group[i, j])\n",
    "        max_slope_index[i] = np.argmax(slope_value)\n",
    "    print(max_slope_index)\n",
    "\n",
    "    x = np.arange(0, 50, 1)\n",
    "    plt.figure()\n",
    "    plt.plot(x, spike_group[0, :])\n",
    "    plt.savefig('./../result_net/etc/spike.svg')\n",
    "\n",
    "    '''\n",
    "    training_spike_group[ds, :, :] = spike_group[:training_num, :]\n",
    "\n",
    "training_spike_group_reshape = training_spike_group.reshape(-1, spike_length)\n",
    "\n",
    "np.random.shuffle(training_spike_group_reshape)\n",
    "\n",
    "# check dataset\n",
    "x = np.arange(0, 50, 1)\n",
    "plt.figure()\n",
    "plt.plot(x, training_spike_group_reshape[1, :])\n",
    "plt.savefig('./../result_net/etc/spike.svg')\n",
    "print(np.shape(training_spike_group_reshape))\n",
    "\n",
    "\n",
    "np.save('./../data/training_dataset_20', training_spike_group_reshape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ssp_ITM_quant.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "\n",
    "os.chdir('./../data')\n",
    "\n",
    "from scipy import io\n",
    "\n",
    "filename = [\"C_Easy1_noise005.mat\", \"C_Easy1_noise01.mat\", \"C_Easy1_noise015.mat\", \"C_Easy1_noise02.mat\",\n",
    "            \"C_Easy1_noise025.mat\", \"C_Easy1_noise03.mat\", \"C_Easy1_noise035.mat\", \"C_Easy1_noise04.mat\",\n",
    "            \"C_Easy2_noise005.mat\", \"C_Easy2_noise01.mat\", \"C_Easy2_noise015.mat\", \"C_Easy2_noise02.mat\",\n",
    "            \"C_Difficult1_noise005.mat\", \"C_Difficult1_noise01.mat\", \"C_Difficult1_noise015.mat\", \"C_Difficult1_noise02.mat\",\n",
    "            \"C_Difficult2_noise005.mat\", \"C_Difficult2_noise01.mat\", \"C_Difficult2_noise015.mat\", \"C_Difficult2_noise02.mat\"]\n",
    "\n",
    "template =  [\"Spike_TEMPLATE_e1n005.npy\", \"Spike_TEMPLATE_e1n010.npy\", \"Spike_TEMPLATE_e1n015.npy\", \"Spike_TEMPLATE_e1n020.npy\",\n",
    "             \"Spike_TEMPLATE_e1n025.npy\", \"Spike_TEMPLATE_e1n030.npy\", \"Spike_TEMPLATE_e1n035.npy\", \"Spike_TEMPLATE_e1n040.npy\",\n",
    "             \"Spike_TEMPLATE_e2n005.npy\", \"Spike_TEMPLATE_e2n010.npy\", \"Spike_TEMPLATE_e2n015.npy\", \"Spike_TEMPLATE_e2n020.npy\",\n",
    "             \"Spike_TEMPLATE_d1n005.npy\", \"Spike_TEMPLATE_d1n010.npy\", \"Spike_TEMPLATE_d1n015.npy\", \"Spike_TEMPLATE_d1n020.npy\",\n",
    "             \"Spike_TEMPLATE_d2n005.npy\", \"Spike_TEMPLATE_d2n010.npy\", \"Spike_TEMPLATE_d2n015.npy\", \"Spike_TEMPLATE_d2n020.npy\"]\n",
    "\n",
    "\n",
    "def making_naive_template():\n",
    "    for ds in range(20):\n",
    "        print(\"\")\n",
    "        print(\"data\", ds)\n",
    "        mat1 = io.loadmat(filename[ds])\n",
    "        raw = mat1['data'][0]\n",
    "        thr = 0.9\n",
    "        \n",
    "        wait = 0\n",
    "        spike_index = 0\n",
    "        spike = np.zeros((10000, 50))\n",
    "        spike_times = np.zeros(10000)\n",
    "        training_cycle = 100\n",
    "        slope = np.zeros(len(raw)-2)\n",
    "        for i in range(len(raw)-2):\n",
    "            slope[i]=raw[i+1]-raw[i]\n",
    "            \n",
    "        for i in range(len(raw)-2):\n",
    "            wait += 1\n",
    "            if(21 <= wait):\n",
    "                if(raw[i+1] < raw[i+2] and raw[i+1] <= raw[i] and raw[i+1] < -thr) or (raw[i+1] > raw[i+2] and raw[i] <= raw[i+1] and raw[i+1] > thr):\n",
    "                    \n",
    "                    max_slope_index = i + np.argmax(slope[i - 8 : i + 5]) - 8\n",
    "                    spike[spike_index, :] = raw[max_slope_index - 10 : max_slope_index + 40]\n",
    "                    spike_times[spike_index] = i-20\n",
    "                    spike_index += 1        \n",
    "                    wait = 0\n",
    "        \n",
    "        num_cluster = 3\n",
    "        Cluster = np.zeros((num_cluster, 50))\n",
    "        distance_size = 0\n",
    "        cluster_num = np.zeros(num_cluster)\n",
    "        \n",
    "        for i in range(num_cluster):\n",
    "            distance_size += i+1    \n",
    "        distance = np.zeros(distance_size)\n",
    "        \n",
    "        for spike_index in range(training_cycle):\n",
    "            spike_n = spike[spike_index, :]\n",
    "            \n",
    "            if(spike_index == 0):\n",
    "                Cluster[0, :] = spike_n\n",
    "                Cl_num = 1\n",
    "                cluster_num[0] += 1\n",
    "                        \n",
    "            else:\n",
    "                for i in range(num_cluster):\n",
    "                    distance[i] = np.sum(abs(Cluster[i, 5:25] - spike_n[5:25])) * 17 + np.sum(abs(Cluster[i, 0:5] - spike_n[0:5])) * 2 + np.sum(abs(Cluster[i, 25:50] - spike_n[25:50])) * 2\n",
    "                #if(spike_index == 4):\n",
    "                   # print(distance)\n",
    "                k = 0\n",
    "                for j in range(1, num_cluster):\n",
    "                    k = k + j\n",
    "                    for i in range(j, num_cluster):\n",
    "\n",
    "                        if(spike_index < 30):\n",
    "                            mer_thr = 1.5\n",
    "                        else:\n",
    "                            mer_thr = 2.5\n",
    "                            \n",
    "                        if(cluster_num[j-1]>10) or (cluster_num[i]>10):\n",
    "                            distance[i + j * num_cluster - k] = 1500000000000\n",
    "                        else:\n",
    "                            distance[i + j * num_cluster - k] = np.sum(abs(Cluster[j - 1, 5:25] - Cluster[i, 5:25])) * 17 + np.sum(abs(Cluster[j - 1, 0:5] - Cluster[i, 0:5])) * 2 + np.sum(abs(Cluster[j - 1, 25:50] - Cluster[i, 25:50])) * 2\n",
    "                            distance[i + j * num_cluster - k] = distance[i + j * num_cluster - k] * mer_thr\n",
    "                            \n",
    "                m = np.argmin(distance)\n",
    "                if(m < num_cluster):\n",
    "                    \n",
    "                    Cluster[m, :] = (Cluster[m, :] * 15 + spike_n)/16\n",
    "                    cluster_num[m] += 1\n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    x = num_cluster\n",
    "                    i\n",
    "                    for i in range(1, num_cluster):\n",
    "                        y = x + num_cluster - i\n",
    "                        if(x <= m and m < y):\n",
    "                            Cluster[i - 1, :] = (Cluster[i - 1, :] + Cluster[m - x + i, :])/2\n",
    "                            cluster_num[i-1] = cluster_num[i-1] + cluster_num[m-x+i]\n",
    "                            Cluster[m - x + i, :] = spike_n\n",
    "                            cluster_num[m-x+i] = 1\n",
    "                        x = y\n",
    "        np.save(template[ds], Cluster)\n",
    "        \n",
    "making_naive_template()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ssp.train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "os.chdir(\"./../data/\")\n",
    "\n",
    "filename = [\"C_Easy1_noise005.mat\", \"C_Easy1_noise01.mat\", \"C_Easy1_noise015.mat\", \"C_Easy1_noise02.mat\",\n",
    "            \"C_Easy2_noise005.mat\", \"C_Easy2_noise01.mat\", \"C_Easy2_noise015.mat\", \"C_Easy2_noise02.mat\",\n",
    "            \"C_Difficult1_noise005.mat\", \"C_Difficult1_noise01.mat\", \"C_Difficult1_noise015.mat\", \"C_Difficult1_noise02.mat\",\n",
    "            \"C_Difficult2_noise005.mat\", \"C_Difficult2_noise01.mat\", \"C_Difficult2_noise015.mat\", \"C_Difficult2_noise02.mat\"]\n",
    "\n",
    "template =  [\"Spike_TEMPLATE_e1n005_new.npy\", \"Spike_TEMPLATE_e1n010_new.npy\", \"Spike_TEMPLATE_e1n015_new.npy\", \"Spike_TEMPLATE_e1n020_new.npy\",\n",
    "             \"Spike_TEMPLATE_e2n005_new.npy\", \"Spike_TEMPLATE_e2n010_new.npy\", \"Spike_TEMPLATE_e2n015_new.npy\", \"Spike_TEMPLATE_e2n020_new.npy\",\n",
    "             \"Spike_TEMPLATE_d1n005_new.npy\", \"Spike_TEMPLATE_d1n010_new.npy\", \"Spike_TEMPLATE_d1n015_new.npy\", \"Spike_TEMPLATE_d1n020_new.npy\",\n",
    "             \"Spike_TEMPLATE_d2n005_new.npy\", \"Spike_TEMPLATE_d2n010_new.npy\", \"Spike_TEMPLATE_d2n015_new.npy\", \"Spike_TEMPLATE_d2n020_new.npy\"]\n",
    "\n",
    "spike_tot = [\"Spike_e1n005.npy\", \"Spike_e1n010.npy\", \"Spike_e1n015.npy\", \"Spike_e1n020.npy\",\n",
    "            \"Spike_e2n005.npy\", \"Spike_e2n010.npy\", \"Spike_e2n015.npy\", \"Spike_e2n020.npy\",\n",
    "            \"Spike_d1n005.npy\", \"Spike_d1n010.npy\", \"Spike_d1n015.npy\", \"Spike_d1n020.npy\",\n",
    "            \"Spike_d2n005.npy\", \"Spike_d2n010.npy\", \"Spike_d2n015.npy\", \"Spike_d2n020.npy\"]\n",
    "\n",
    "times_tot = ['Spike_e1n005_times.npy', 'Spike_e1n010_times.npy', 'Spike_e1n015_times.npy', 'Spike_e1n020_times.npy',\n",
    "             'Spike_e2n005_times.npy', 'Spike_e2n010_times.npy', 'Spike_e2n015_times.npy', 'Spike_e2n020_times.npy',\n",
    "             'Spike_d1n005_times.npy', 'Spike_d1n010_times.npy', 'Spike_d1n015_times.npy', 'Spike_d1n020_times.npy',\n",
    "             'Spike_d2n005_times.npy', 'Spike_d2n010_times.npy', 'Spike_d2n015_times.npy', 'Spike_d2n020_times.npy']\n",
    "\n",
    "thr_tot = np.array([0.5, 0.5, 0.55, 0.7, 0.5, 0.5, 0.55, 0.7, 0.5, 0.5, 0.55, 0.7, 0.5, 0.5, 0.55, 0.7])\n",
    "cos_thr = np.array([0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.85, 0.95, 0.9, 0.8, 0.95, 0.95, 0.95, 0.95, 0.8])\n",
    "#%%\n",
    "\n",
    "from scipy import io\n",
    "\n",
    "cluster_ans = np.zeros((16, 4000))\n",
    "\n",
    "for i in range(16):\n",
    "    mat1 = io.loadmat(filename[i])        \n",
    "    cluster_ans[i, :len(mat1['spike_class'][0][0][0])] = mat1['spike_class'][0][0][0]\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class spikedataset(Dataset):\n",
    "\n",
    "    def __init__(self, path, transform = None):    \n",
    "        \n",
    "        self.transform = transform\n",
    "        spike_h = np.load(path)\n",
    "        self.spike = spike_h\n",
    "        self.len = len(self.spike)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        spike = self.spike[index]            \n",
    "        if self.transform is not None:\n",
    "            spike = self.transform(spike)\n",
    "        return spike\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "train_dataset = spikedataset('training_dataset_20.npy')\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = 32, shuffle = True)\n",
    "batch_size = 32\n",
    "#%%\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AE(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AE, self).__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.conv1 = nn.Conv1d(1, 32, 3, stride = 2, bias = False) # 24\n",
    "        self.conv2 = nn.Conv1d(32, 64, 3, stride = 2, bias = False) # 11\n",
    "        self.conv3 = nn.Conv1d(64, 96, 3, stride = 2, bias = False) # 4 # 병현: 여기 5인데?\n",
    "        self.fc1 = nn.Linear(96 * 5, 4, bias = False)\n",
    "        \n",
    "        # decoder\n",
    "        self.fc4 = nn.Linear(4, 5 * 96, bias = False)\n",
    "        self.deconv3 = nn.ConvTranspose1d(96, 64, 3, stride = 2, bias = False) #6 + 2 + 1= 9\n",
    "        self.deconv1 = nn.ConvTranspose1d(64, 32, 3, stride = 2, output_padding=1, bias = False) #16(9-1)*stride + 4(kernel-1) + 1 = 21\n",
    "        self.deconv2 = nn.ConvTranspose1d(32, 1, 3, stride = 2, output_padding=1, bias = False) #40 + 4 + 1 = 45\n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # encoder\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(-1, 96 * 5)\n",
    "        mid = self.fc1(x)\n",
    "        norm = torch.sqrt(torch.sum(torch.pow(mid, 2), dim = 1))\n",
    "        h = (mid.t()/norm).t()\n",
    "\n",
    "        # decoder\n",
    "        z = F.relu(self.fc4(h))\n",
    "        z = z.view(-1, 96, 5)\n",
    "        z = F.relu(self.deconv3(z))\n",
    "        z = F.relu(self.deconv1(z))\n",
    "        z = self.deconv2(z)\n",
    "\n",
    "        return h, z\n",
    "\n",
    "net = AE()\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "#net.load_state_dict(torch.load('allnobiasv2_7000.pth' , map_location = device))\n",
    "#net.eval()\n",
    "device_num = 0\n",
    "net.cuda(device_num)\n",
    "#%%\n",
    "import torch.optim as optim\n",
    "from scipy import io\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.001, momentum = 0.9)\n",
    "\n",
    "training_cycle = 2400\n",
    "\n",
    "\n",
    "num_cluster = 3\n",
    "distance_size = 0\n",
    "for i in range(num_cluster):\n",
    "    distance_size += i+1\n",
    "tau = np.zeros(3)\n",
    "\n",
    "\n",
    "acc_y = np.zeros(50000)\n",
    "acc_y2 = np.zeros(50000)\n",
    "acc_y_test = np.zeros(50000)\n",
    "\n",
    "acc_update = 0\n",
    "#%%\n",
    "max_epoch = 7000\n",
    "for epoch in range(max_epoch):\n",
    "\n",
    "    \n",
    "    \n",
    "    cluster_acc_data = np.zeros(16)\n",
    "    cluster_acc_data2 = np.zeros(16)\n",
    "    cluster_test = np.zeros(16)    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if(epoch == 0 or epoch == 1 or epoch % 1000 == 999): \n",
    "    \n",
    "        for ds in range(16):\n",
    "            thr = thr_tot[ds]\n",
    "            spike_template = np.load(template[ds])\n",
    "            spike = np.load(spike_tot[ds])\n",
    "            ans_cluster = cluster_ans[ds]\n",
    "            times = np.load(times_tot[ds])\n",
    "            print(ds+1)\n",
    "            \n",
    "            Cluster = np.zeros((3, 4))\n",
    "            \n",
    "            for i in range(3):\n",
    "                spike_torch = torch.from_numpy(spike_template[i, :])\n",
    "                spike_torch = spike_torch.view(1, 1, 50)\n",
    "                inner_inf, spike_class = net(spike_torch.float().cuda(device_num))\n",
    "                \n",
    "                Cluster[i, :] = inner_inf.cpu().detach().numpy()\n",
    "                \n",
    "            b = np.zeros((len(spike), 4))\n",
    "            \n",
    "            \n",
    "            for i in range(len(spike)):\n",
    "                spike_torch = torch.from_numpy(spike[i, :])\n",
    "                spike_torch = spike_torch.view(1, 1, 50)\n",
    "                inner_inf, spike_class = net(spike_torch.float().cuda(device_num))\n",
    "            \n",
    "                b[i, :] = inner_inf.cpu().detach().numpy()\n",
    "                \n",
    "            spike_id = np.zeros(len(spike))\n",
    "            distance = np.zeros(distance_size)\n",
    "            tau = np.zeros(3)\n",
    "            \n",
    "            cos = cos_thr[ds]\n",
    "            \n",
    "            for spike_index in range(2400):\n",
    "                ft1, ft2, ft3, ft4 = b[spike_index, :]\n",
    "                for q in range(3):\n",
    "                    tau[q] = np.sum(b[spike_index, :] * Cluster[q, :])\n",
    "                for i in range(num_cluster):\n",
    "                    distance[i] = pow(abs(Cluster[i, 0] - ft1), 2) + pow(abs(Cluster[i, 1] - ft2), 2) + pow(abs(Cluster[i, 2] - ft3), 2) + pow(abs(Cluster[i, 3] - ft4), 2)\n",
    "            \n",
    "                if(np.max(tau)>=cos):\n",
    "                    k = 0\n",
    "                    for j in range(1, num_cluster):\n",
    "                        k = k + j\n",
    "                        for i in range(j, num_cluster):\n",
    "                            distance[i + j * num_cluster - k] = np.sum(pow(Cluster[j-1, :] - Cluster[i, :], 2))*1500\n",
    "                            \n",
    "                    m = np.argmin(distance)\n",
    "                    if(m < num_cluster):\n",
    "                        Cluster[m, 0] = (Cluster[m, 0] * 15 + ft1)/16\n",
    "                        Cluster[m, 1] = (Cluster[m, 1] * 15 + ft2)/16\n",
    "                        Cluster[m, 2] = (Cluster[m, 2] * 15 + ft3)/16\n",
    "                        Cluster[m, 3] = (Cluster[m, 3] * 15 + ft4)/16\n",
    "                        \n",
    "                    else:\n",
    "                        x = num_cluster\n",
    "                                        \n",
    "                        for i in range(1, num_cluster):\n",
    "                            y = x + num_cluster - i\n",
    "                            if(x <= m and m < y):\n",
    "                                Cluster[i - 1, :] = (Cluster[i - 1, :] * 15 + Cluster[m - x + i, :])/16\n",
    "                                Cluster[m - x + i, :] = (ft1, ft2, ft3, ft4)\n",
    "                            x = y\n",
    "                            \n",
    "            for spike_index in range(len(spike)):\n",
    "                ft1, ft2, ft3, ft4 = b[spike_index, :]\n",
    "                for q in range(3):\n",
    "                    tau[q] = np.sum(b[spike_index, :] * Cluster[q, :])\n",
    "                for i in range(num_cluster):\n",
    "                    distance[i] = pow(abs(Cluster[i, 0] - ft1), 2) + pow(abs(Cluster[i, 1] - ft2), 2) + pow(abs(Cluster[i, 2] - ft3), 2) + pow(abs(Cluster[i, 3] - ft4), 2)\n",
    "                \n",
    "                if(np.max(tau)< -1):\n",
    "                    spike_id[spike_index] = 4\n",
    "                else:\n",
    "                    m = np.argmin(distance[0:num_cluster])\n",
    "                    spike_id[spike_index] = m + 1\n",
    "            \n",
    "            cluster_accuracy = np.zeros(6)\n",
    "            cluster_accuracy2 = np.zeros(6)\n",
    "            cluster_accuracy3 = np.zeros(6)\n",
    "            \n",
    "            for ep in range(6):\n",
    "                \n",
    "                if(ep == 1 or ep == 4):\n",
    "                    for i in range(len(spike)):\n",
    "                        if(spike_id[i] == 3):\n",
    "                            spike_id[i] = 2\n",
    "                        elif(spike_id[i] == 2):\n",
    "                            spike_id[i] = 3\n",
    "                            \n",
    "                elif(ep == 2 or ep == 5):\n",
    "                    for i in range(len(spike)):\n",
    "                        if(spike_id[i] == 1):\n",
    "                            spike_id[i] = 2\n",
    "                        elif(spike_id[i] == 2):\n",
    "                            spike_id[i] = 1\n",
    "                elif(ep == 3):\n",
    "                    for i in range(len(spike)):\n",
    "                        if(spike_id[i] == 1):\n",
    "                            spike_id[i] = 3\n",
    "                        elif(spike_id[i] == 3):\n",
    "                            spike_id[i] = 1\n",
    "                \n",
    "                true_cluster = 0\n",
    "                for i in range(int(times[0, 2])):\n",
    "                    if(times[i, 0] == 0) and (times[i + 1, 0] == 0):\n",
    "                        print(\"break\")\n",
    "                        break\n",
    "                    \n",
    "                    else:\n",
    "                        if(spike_id[int(times[i, 0])]==ans_cluster[int(times[i, 1])]):\n",
    "                            true_cluster += 1\n",
    "                        \n",
    "                cluster_accuracy[ep] = true_cluster*100/times[0, 2]\n",
    "                            \n",
    "                true_cluster2 = 0\n",
    "                for i in range(int(times[0, 2]), int(times[1, 2])):\n",
    "                    if(times[i, 0] == 0) and (times[i + 1, 0] == 0):\n",
    "                        print(\"break\")\n",
    "                        break\n",
    "                    else:\n",
    "                        if(spike_id[int(times[i, 0])]==ans_cluster[int(times[i, 1])]):\n",
    "                            true_cluster2 += 1\n",
    "                cluster_accuracy2[ep] = true_cluster2*100/(times[1, 2]-times[0, 2])\n",
    "                cluster_accuracy3[ep] = (true_cluster + true_cluster2)/times[1, 2]\n",
    "                \n",
    "            cluster_acc_data[ds] = max(cluster_accuracy)\n",
    "            print(\"training acc : \", max(cluster_accuracy))\n",
    "            print('test acc : ', cluster_accuracy2[np.argmax(cluster_accuracy)])\n",
    "            cluster_acc_data2[ds] = cluster_accuracy3[np.argmax(cluster_accuracy)]\n",
    "            cluster_test[ds] = cluster_accuracy2[np.argmax(cluster_accuracy)]\n",
    "        print(\"training acc : \", np.sum(cluster_acc_data)/16)\n",
    "        print(\"acc : \", np.sum(cluster_acc_data2)/16)\n",
    "        \n",
    "        acc_y[epoch] = np.sum(cluster_acc_data)/16\n",
    "        acc_y2[epoch] = np.sum(cluster_acc_data2)/16\n",
    "        acc_y_test[epoch] = np.sum(cluster_test)/16\n",
    "        \n",
    "        torch.save(net.state_dict(), './../result_net/original/new_data_net_%depoch_%0.3f.pth' % (epoch, np.sum(cluster_acc_data2)/16))\n",
    "\n",
    "    running_loss = 0.0\n",
    "   # for i in range(len(spike_normed)):\n",
    "    for i in range(int(2400*20/batch_size)):\n",
    "        optimizer.zero_grad()\n",
    "        spike_torch = next(iter(train_loader)).cuda(device_num)\n",
    "        spike_torch = spike_torch.view(-1, 1,  50)\n",
    "        spike_torch = spike_torch.float()\n",
    "        inner_inf, spike_class = net(spike_torch)\n",
    "        \n",
    "        loss1 = criterion(spike_class[:, 0, 5:25], spike_torch[:, 0, 5:25])\n",
    "        loss2 = criterion(spike_class[:, 0, 0:5], spike_torch[:, 0, 0:5])\n",
    "        loss3 = criterion(spike_class[:, 0, 25:50], spike_torch[:, 0, 25:50])\n",
    "        loss = loss1 * 2.125 + (loss2 + loss3)/4\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "#%%\n",
    "x = np.arange(0, 9, 1)\n",
    "y = np.zeros(9)\n",
    "z = np.zeros(9)\n",
    "w = np.zeros(9)\n",
    "\n",
    "y[0] = acc_y[0]\n",
    "y[1] = acc_y[1]\n",
    "y[2] = acc_y[1000-1]\n",
    "y[3] = acc_y[2000-1]\n",
    "y[4] = acc_y[3000-1]\n",
    "y[5] = acc_y[4000-1]\n",
    "y[6] = acc_y[5000-1]\n",
    "y[7] = acc_y[6000-1]\n",
    "y[8] = acc_y[7000-1]\n",
    "\n",
    "z[0] = acc_y2[0]\n",
    "z[1] = acc_y2[1]\n",
    "z[2] = acc_y2[1000-1]\n",
    "z[3] = acc_y2[2000-1]\n",
    "z[4] = acc_y2[3000-1]\n",
    "z[5] = acc_y2[4000-1]\n",
    "z[6] = acc_y2[5000-1]\n",
    "z[7] = acc_y2[6000-1]\n",
    "z[8] = acc_y2[7000-1]\n",
    "\n",
    "print(np.max(z), np.argmax(z))\n",
    "\n",
    "w[0] = acc_y_test[0]\n",
    "w[1] = acc_y_test[1]\n",
    "w[2] = acc_y_test[1000-1]\n",
    "w[3] = acc_y_test[2000-1]\n",
    "w[4] = acc_y_test[3000-1]\n",
    "w[5] = acc_y_test[4000-1]\n",
    "w[6] = acc_y_test[5000-1]\n",
    "w[7] = acc_y_test[6000-1]\n",
    "w[8] = acc_y_test[7000-1]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y)\n",
    "plt.figure()\n",
    "plt.plot(x, z)\n",
    "#for i, v in enumerate(x):\n",
    "#    plt.text(v, z[i], str(z[i]), fontsize = 10, weight = 'bold', color = \"red\", horizontalalignment='center', verticalalignment='bottom')\n",
    "plt.savefig('./../result_net/original/accuracy.svg')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# acc_metric.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def acc_det(spike_index, spike_times, ans_times):\n",
    "        k = 0\n",
    "        FN = 0\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        spike_true_index = np.zeros(10000)\n",
    "        spike_false_index = np.zeros(10000)\n",
    "        ans_index = np.zeros(10000)\n",
    "        spike_times[spike_times == 0] = 1500000\n",
    "        det_win = 20\n",
    "\n",
    "        '''\n",
    "        f = open('./../result/TP_index.txt', 'w')\n",
    "        g = open('./../result/FP_index.txt', 'w')\n",
    "        h = open('./../result/FN_index.txt', 'w')\n",
    "        '''\n",
    "        for j in range(len(ans_times)):\n",
    "                if(ans_times[j] + det_win >= spike_times[k] and spike_times[k] >= ans_times[j] - det_win):\n",
    "                        spike_true_index[TP] = k\n",
    "                        ans_index[TP] = j\n",
    "                        #f.write('%7d %7d'%(ans_times[j], spike_times[k]) +\"\\n\")\n",
    "                        TP = TP + 1\n",
    "                        k = k + 1\n",
    "                elif(ans_times[j] + det_win < spike_times[k]):\n",
    "                        FN = FN + 1\n",
    "                        #h.write('%7d'%(ans_times[j]) +\"\\n\")\n",
    "                else:\n",
    "                        while(1):\n",
    "                                spike_false_index[FP] = k\n",
    "                                FP = FP + 1\n",
    "                                #g.write('%7d'%(spike_times[k]) + \"\\n\")\n",
    "                                k = k + 1\n",
    "                                if(ans_times[j] - det_win <= spike_times[k]):\n",
    "                                        break\n",
    "                        if(ans_times[j] + det_win >= spike_times[k]):\n",
    "                                spike_true_index[TP] = k\n",
    "                                ans_index[TP] = j\n",
    "                                #f.write('%7d %7d'%(ans_times[j], spike_times[k]) +\"\\n\")\n",
    "                                TP = TP + 1\n",
    "                                k = k + 1\n",
    "                        else:\n",
    "                                FN = FN + 1\n",
    "\t\t\t\t#h.write('%7d'%(ans_times[j]) +\"\\n\")\n",
    "        print(\"# of ans : \", len(ans_times))\n",
    "        print(\"# of TP ; \", TP)\n",
    "        print(\"# of FP ; \", FP)\n",
    "        print(\"# of FN : \", FN)\n",
    "        print(\"Det acc : \", TP/len(ans_times))\n",
    "        return spike_true_index, spike_false_index, ans_index, TP, TP/len(ans_times)\n",
    "''''\n",
    "def acc_clu(numspike, spike_id, TP, spike_true_index, ans_index, ans_cluster):\n",
    "\tcluster_accuracy = np.zeros(6)\n",
    "\tfor ep in range(6):\n",
    "\t\tif(ep == 1 or ep == 4):\n",
    "\t\t\tfor i in range(numspike):\n",
    "\t\t\t\tif(spike_id[i] == 3):\n",
    "\t\t\t\t\tspike_id[i] = 2\n",
    "\t\t\t\telif(spike_id[i] == 2):\n",
    "\t\t\t\t\tspike_id[i] = 3\n",
    "\t\telif(ep == 2 or ep == 5):\n",
    "\t\t\tfor i in range(numspike):\n",
    "\t\t\t\tif(spike_id[i] == 1):\n",
    "\t\t\t\t\tspike_id[i] = 2\n",
    "\t\t\t\telif(spike_id[i] == 2):\n",
    "\t\t\t\t\tspike_id[i] = 1\n",
    "\t\telif(ep == 3):\n",
    "\t\t\tfor i in range(numspike):\n",
    "\t\t\t\tif(spike_id[i] == 1):\n",
    "\t\t\t\t\tspike_id[i] = 3\n",
    "\t\t\t\telif(spike_id[i] == 3):\n",
    "\t\t\t\t\tspike_id[i] = 1\n",
    "\t\ttrue_cluster = 0\n",
    "\t\tfor i in range(TP):\n",
    "\t\t\tif(spike_true_index[i] == 0) and (spike_true_index[i+1] == 0):\n",
    "\t\t\t\tprint(\"break\")\n",
    "\t\t\t\tbreak\n",
    "\t\t\telse:\n",
    "\t\t\t\tif(spike_id[int(spike_true_index[i])] == ans_cluster[int(ans_index[i])]):\n",
    "\t\t\t\t\ttrue_cluster += 1\n",
    "\t\tcluster_accuracy[ep] = true_cluster*100/TP\n",
    "\tprint('Clu acc : ', max(cluster_accuracy))\n",
    "\treturn max(cluster_accuracy)\n",
    "'''\n",
    "\n",
    "def acc(spike_index, spike_times, ans_times, spike_id, ans_cluster, training = 0):\n",
    "        k = 0\n",
    "        FN = 0\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        spike_times[spike_times == 0] = 1500000\n",
    "        det_win = 20\n",
    "        id_ssp = np.zeros(10000)\n",
    "        id_ans = np.zeros(10000)\n",
    "        id_false = np.zeros(10000)\n",
    "        training_TP = 0\n",
    "        training_ans = 0\n",
    "        training_cycle = 100\n",
    "        for j in range(len(ans_times)):\n",
    "                if(ans_times[j] + det_win >= spike_times[k] and spike_times[k] >= ans_times[j] - det_win):\n",
    "                        id_ssp[TP] = spike_id[k]\n",
    "                        id_ans[TP] = ans_cluster[j]\t\t\n",
    "                        TP = TP + 1\n",
    "                        k = k + 1\n",
    "                        if(k == training_cycle and training == 1):\n",
    "                                training_TP = TP\n",
    "                                training_ans = j\n",
    "                elif(ans_times[j] + det_win < spike_times[k]):\n",
    "                        FN = FN + 1\n",
    "                else:\n",
    "                        while(1):\n",
    "                                id_false[FP] = spike_id[k]\n",
    "                                FP = FP + 1\n",
    "\n",
    "                                k = k + 1\n",
    "                                if(k == training_cycle and training == 1):\n",
    "                                        training_TP = TP\n",
    "                                        training_ans = j\n",
    "\n",
    "                                if(ans_times[j] - det_win <= spike_times[k]):\n",
    "                                        break\n",
    "                        if(ans_times[j] + det_win >= spike_times[k]):\n",
    "                                id_ssp[TP] = spike_id[k]\n",
    "                                id_ans[TP] = ans_cluster[j]\n",
    "                                TP = TP + 1\n",
    "                                k = k + 1\n",
    "                                if(k == training_cycle and training == 1):\n",
    "                                        training_TP = TP\n",
    "                                        training_ans = j\n",
    "                        else:\n",
    "                                FN = FN + 1\n",
    "        #print(training_TP, training_ans)\n",
    "        print(\"# of ans : \", len(ans_times))\n",
    "        print(\"# of TP ; \", TP)\n",
    "        print('training miss : ', training_TP)\n",
    "        print('# of Error : ', len(ans_times)-(TP-training_TP))\n",
    "        \n",
    "        #print(\"# of FP ; \", FP)\n",
    "        print(\"# of FN : \", FN)\n",
    "        print(\"Det acc : \", (TP-training_TP)/(len(ans_times)-training_ans))\n",
    "\n",
    "        filtered_spike = 0\n",
    "        filtered_noise = 0\n",
    "        cluster_accuracy = np.zeros(6)\n",
    "        true_clusters = np.zeros(6)\n",
    "        noise = 0\n",
    "        for i in range(TP):\n",
    "                if(id_ssp[i] == 4):\n",
    "                        filtered_spike += 1\n",
    "        for i in range(FP):\n",
    "                if(id_false[i] == 4):\n",
    "                        filtered_noise += 1\n",
    "        for ep in range(6):\n",
    "                if(ep == 1 or ep == 4):\n",
    "                        for i in range(spike_index):\n",
    "                                if(id_ssp[i] == 3):\n",
    "                                        id_ssp[i] = 2\n",
    "                                elif(id_ssp[i] == 2):\n",
    "                                        id_ssp[i] = 3\n",
    "                elif(ep == 2 or ep == 5):\n",
    "                        for i in range(spike_index):\n",
    "                                if(id_ssp[i] == 1):\n",
    "                                        id_ssp[i] = 2\n",
    "                                elif(id_ssp[i] == 2):\n",
    "                                        id_ssp[i] = 1\n",
    "                elif(ep == 3):\n",
    "                        for i in range(spike_index):\n",
    "                                if(id_ssp[i] == 1):\n",
    "                                        id_ssp[i] = 3\n",
    "                                elif(id_ssp[i] == 3):\n",
    "                                        id_ssp[i] = 1\n",
    "                true_cluster = 0\n",
    "                for i in range(training_TP, TP):\n",
    "                        if(id_ssp[i] == id_ans[i]):\n",
    "                                true_cluster += 1\n",
    "                \n",
    "                cluster_accuracy[ep] = true_cluster*100/(TP-filtered_spike-training_TP)\n",
    "                true_clusters[ep] = true_cluster\n",
    "        #print('filtered noise : ', filtered_noise)\n",
    "        print('filtered spike : ', filtered_spike)\n",
    "        print(\"true cluster : \", max(true_clusters))\n",
    "        print('filtered FP : ', FP-filtered_noise)\n",
    "        print('Final det acc : ', (TP-filtered_spike-training_TP)/(len(ans_times)-training_ans))\n",
    "\n",
    "        print('Clu acc : ', max(cluster_accuracy))\n",
    "\t\n",
    "        return (TP-training_TP-filtered_spike)/(len(ans_times)-training_ans), max(cluster_accuracy), max(true_clusters)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
