{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28748/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    lif_layer_sg_width2 = None,\n",
    "                    lif_layer_v_threshold2 = None,\n",
    "                    learning_rate2 = None,\n",
    "                    init_scaling = None,\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp,\n",
    "                    ANPI_MODE=False,\n",
    "                    lif_layer_sg_width2=lif_layer_sg_width2,\n",
    "                    lif_layer_v_threshold2=lif_layer_v_threshold2,\n",
    "                    init_scaling=init_scaling).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                # lr = group['lr']\n",
    "\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        lr = learning_rate\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        lr = learning_rate2\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        lr = 1.0\n",
    "\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "                now_T = inputs.shape[1]\n",
    "                now_time_steps = temporal_filter*TIME\n",
    "                # start_idx = random.randint(0, now_T - now_time_steps)\n",
    "                start_idx = random.choice(range(0, now_T - now_time_steps + 1, now_time_steps))\n",
    "                # start_idx = random.choice([i for i in range(0, now_T - now_time_steps + 1, now_time_steps)])\n",
    "                inputs = inputs[:, start_idx : start_idx + now_time_steps]\n",
    "                if dvs_clipping != 0:\n",
    "                    inputs[inputs<dvs_clipping] = 0.0\n",
    "                    inputs[inputs>=dvs_clipping] = 1.0\n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            now_T = inputs_val.shape[1]\n",
    "                            now_time_steps = temporal_filter*TIME\n",
    "                            start_idx = 0\n",
    "                            inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if dvs_clipping != 0:\n",
    "                                inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "\n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    assert val_acc_best > 0.2\n",
    "                elif epoch > 10:\n",
    "                    assert val_acc_best > 0.4\n",
    "                elif epoch > 30:\n",
    "                    assert val_acc_best > 0.5\n",
    "                elif epoch > 100:\n",
    "                    assert val_acc_best > 0.6\n",
    "                    \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"5\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 2871,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 8, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 200,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = -1, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[0,0],[0,0],[0,0]], \n",
    "#                 lif_layer_sg_width2 = 4.0,\n",
    "#                 lif_layer_v_threshold2 = 8,\n",
    "#                 learning_rate2 = 8,\n",
    "#                 init_scaling = [1/2,1/2,1/2],\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: e1m59f1o\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5q330707 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.046875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0234375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_010553-5q330707</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5q330707' target=\"_blank\">usual-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5q330707' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5q330707</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': '20251214_010601_888', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 32, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 0.5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 0.5, 'lif_layer_v_threshold2': 64, 'init_scaling': [0.03125, 0.046875, 0.0234375], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 0.5, self.v_threshold 32\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 0.5, self.v_threshold 64\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=32, v_reset=10000, sg_width=0.5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=64, v_reset=10000, sg_width=0.5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 46.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 6.0\n",
      "lif layer 2 self.abs_max_v: 6.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 59.5\n",
      "fc layer 2 self.abs_max_out: 36.0\n",
      "lif layer 2 self.abs_max_v: 35.5\n",
      "fc layer 1 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 69.0\n",
      "fc layer 2 self.abs_max_out: 48.0\n",
      "lif layer 2 self.abs_max_v: 53.0\n",
      "fc layer 1 self.abs_max_out: 62.0\n",
      "lif layer 1 self.abs_max_v: 90.0\n",
      "lif layer 2 self.abs_max_v: 59.5\n",
      "fc layer 1 self.abs_max_out: 101.0\n",
      "lif layer 1 self.abs_max_v: 138.0\n",
      "fc layer 2 self.abs_max_out: 54.0\n",
      "lif layer 2 self.abs_max_v: 72.0\n",
      "fc layer 3 self.abs_max_out: 2.0\n",
      "fc layer 1 self.abs_max_out: 147.0\n",
      "lif layer 1 self.abs_max_v: 210.0\n",
      "fc layer 2 self.abs_max_out: 76.0\n",
      "lif layer 2 self.abs_max_v: 112.0\n",
      "fc layer 3 self.abs_max_out: 6.0\n",
      "fc layer 1 self.abs_max_out: 200.0\n",
      "lif layer 1 self.abs_max_v: 270.0\n",
      "fc layer 2 self.abs_max_out: 82.0\n",
      "lif layer 2 self.abs_max_v: 137.0\n",
      "fc layer 3 self.abs_max_out: 14.0\n",
      "fc layer 1 self.abs_max_out: 212.0\n",
      "lif layer 1 self.abs_max_v: 330.0\n",
      "lif layer 2 self.abs_max_v: 138.5\n",
      "fc layer 1 self.abs_max_out: 234.0\n",
      "fc layer 2 self.abs_max_out: 98.0\n",
      "fc layer 3 self.abs_max_out: 18.0\n",
      "fc layer 2 self.abs_max_out: 117.0\n",
      "fc layer 2 self.abs_max_out: 125.0\n",
      "fc layer 3 self.abs_max_out: 20.0\n",
      "fc layer 1 self.abs_max_out: 281.0\n",
      "fc layer 2 self.abs_max_out: 129.0\n",
      "fc layer 1 self.abs_max_out: 380.0\n",
      "lif layer 1 self.abs_max_v: 380.0\n",
      "lif layer 2 self.abs_max_v: 139.5\n",
      "fc layer 2 self.abs_max_out: 150.0\n",
      "lif layer 2 self.abs_max_v: 151.5\n",
      "lif layer 2 self.abs_max_v: 165.5\n",
      "fc layer 3 self.abs_max_out: 24.0\n",
      "lif layer 1 self.abs_max_v: 382.5\n",
      "lif layer 2 self.abs_max_v: 171.5\n",
      "lif layer 2 self.abs_max_v: 176.5\n",
      "lif layer 2 self.abs_max_v: 207.5\n",
      "fc layer 3 self.abs_max_out: 25.0\n",
      "fc layer 3 self.abs_max_out: 29.0\n",
      "fc layer 2 self.abs_max_out: 154.0\n",
      "lif layer 1 self.abs_max_v: 390.5\n",
      "lif layer 2 self.abs_max_v: 210.0\n",
      "lif layer 1 self.abs_max_v: 392.5\n",
      "lif layer 1 self.abs_max_v: 432.5\n",
      "fc layer 2 self.abs_max_out: 171.0\n",
      "fc layer 3 self.abs_max_out: 30.0\n",
      "lif layer 2 self.abs_max_v: 212.5\n",
      "lif layer 1 self.abs_max_v: 475.0\n",
      "lif layer 2 self.abs_max_v: 226.5\n",
      "fc layer 3 self.abs_max_out: 31.0\n",
      "lif layer 1 self.abs_max_v: 505.5\n",
      "fc layer 2 self.abs_max_out: 185.0\n",
      "lif layer 2 self.abs_max_v: 248.5\n",
      "fc layer 3 self.abs_max_out: 38.0\n",
      "lif layer 2 self.abs_max_v: 253.0\n",
      "lif layer 2 self.abs_max_v: 263.5\n",
      "fc layer 1 self.abs_max_out: 441.0\n",
      "fc layer 3 self.abs_max_out: 45.0\n",
      "fc layer 2 self.abs_max_out: 195.0\n",
      "lif layer 2 self.abs_max_v: 274.5\n",
      "lif layer 2 self.abs_max_v: 324.5\n",
      "lif layer 1 self.abs_max_v: 508.0\n",
      "lif layer 1 self.abs_max_v: 546.0\n",
      "lif layer 1 self.abs_max_v: 554.0\n",
      "lif layer 2 self.abs_max_v: 337.0\n",
      "fc layer 3 self.abs_max_out: 70.0\n",
      "fc layer 2 self.abs_max_out: 201.0\n",
      "fc layer 2 self.abs_max_out: 229.0\n",
      "lif layer 1 self.abs_max_v: 598.5\n",
      "fc layer 1 self.abs_max_out: 503.0\n",
      "lif layer 1 self.abs_max_v: 802.5\n",
      "lif layer 2 self.abs_max_v: 367.5\n",
      "lif layer 1 self.abs_max_v: 862.5\n",
      "lif layer 2 self.abs_max_v: 386.5\n",
      "lif layer 1 self.abs_max_v: 909.0\n",
      "fc layer 2 self.abs_max_out: 239.0\n",
      "lif layer 2 self.abs_max_v: 430.5\n",
      "fc layer 1 self.abs_max_out: 597.0\n",
      "lif layer 1 self.abs_max_v: 1019.5\n",
      "lif layer 2 self.abs_max_v: 431.5\n",
      "lif layer 1 self.abs_max_v: 1062.0\n",
      "fc layer 2 self.abs_max_out: 247.0\n",
      "fc layer 3 self.abs_max_out: 72.0\n",
      "fc layer 3 self.abs_max_out: 80.0\n",
      "lif layer 2 self.abs_max_v: 450.5\n",
      "fc layer 3 self.abs_max_out: 81.0\n",
      "fc layer 3 self.abs_max_out: 88.0\n",
      "fc layer 2 self.abs_max_out: 258.0\n",
      "lif layer 2 self.abs_max_v: 457.5\n",
      "fc layer 2 self.abs_max_out: 269.0\n",
      "fc layer 2 self.abs_max_out: 285.0\n",
      "fc layer 2 self.abs_max_out: 325.0\n",
      "fc layer 3 self.abs_max_out: 92.0\n",
      "fc layer 1 self.abs_max_out: 617.0\n",
      "fc layer 1 self.abs_max_out: 621.0\n",
      "fc layer 1 self.abs_max_out: 705.0\n",
      "fc layer 3 self.abs_max_out: 97.0\n",
      "fc layer 3 self.abs_max_out: 101.0\n",
      "fc layer 3 self.abs_max_out: 123.0\n",
      "lif layer 1 self.abs_max_v: 1095.5\n",
      "lif layer 2 self.abs_max_v: 471.0\n",
      "lif layer 1 self.abs_max_v: 1190.5\n",
      "fc layer 2 self.abs_max_out: 328.0\n",
      "fc layer 2 self.abs_max_out: 344.0\n",
      "fc layer 3 self.abs_max_out: 129.0\n",
      "fc layer 2 self.abs_max_out: 379.0\n",
      "lif layer 2 self.abs_max_v: 493.0\n",
      "lif layer 2 self.abs_max_v: 522.5\n",
      "lif layer 2 self.abs_max_v: 527.5\n",
      "lif layer 2 self.abs_max_v: 533.5\n",
      "lif layer 2 self.abs_max_v: 561.5\n",
      "lif layer 2 self.abs_max_v: 575.5\n",
      "lif layer 2 self.abs_max_v: 579.0\n",
      "lif layer 2 self.abs_max_v: 588.5\n",
      "lif layer 2 self.abs_max_v: 644.0\n",
      "lif layer 2 self.abs_max_v: 663.0\n",
      "fc layer 3 self.abs_max_out: 145.0\n",
      "lif layer 2 self.abs_max_v: 701.5\n",
      "lif layer 2 self.abs_max_v: 706.0\n",
      "fc layer 2 self.abs_max_out: 382.0\n",
      "lif layer 2 self.abs_max_v: 735.0\n",
      "fc layer 3 self.abs_max_out: 148.0\n",
      "fc layer 2 self.abs_max_out: 391.0\n",
      "fc layer 3 self.abs_max_out: 154.0\n",
      "fc layer 2 self.abs_max_out: 423.0\n",
      "fc layer 3 self.abs_max_out: 162.0\n",
      "fc layer 3 self.abs_max_out: 168.0\n",
      "fc layer 2 self.abs_max_out: 460.0\n",
      "fc layer 2 self.abs_max_out: 497.0\n",
      "fc layer 2 self.abs_max_out: 500.0\n",
      "fc layer 1 self.abs_max_out: 714.0\n",
      "fc layer 1 self.abs_max_out: 850.0\n",
      "lif layer 1 self.abs_max_v: 1383.5\n",
      "fc layer 2 self.abs_max_out: 591.0\n",
      "lif layer 2 self.abs_max_v: 736.0\n",
      "fc layer 3 self.abs_max_out: 173.0\n",
      "lif layer 2 self.abs_max_v: 767.5\n",
      "lif layer 2 self.abs_max_v: 805.0\n",
      "lif layer 2 self.abs_max_v: 825.0\n",
      "lif layer 2 self.abs_max_v: 832.0\n",
      "fc layer 3 self.abs_max_out: 190.0\n",
      "fc layer 3 self.abs_max_out: 192.0\n",
      "lif layer 2 self.abs_max_v: 846.5\n",
      "lif layer 2 self.abs_max_v: 871.0\n",
      "lif layer 2 self.abs_max_v: 902.5\n",
      "lif layer 2 self.abs_max_v: 918.5\n",
      "lif layer 2 self.abs_max_v: 926.5\n",
      "lif layer 2 self.abs_max_v: 930.5\n",
      "fc layer 3 self.abs_max_out: 227.0\n",
      "lif layer 2 self.abs_max_v: 953.0\n",
      "fc layer 3 self.abs_max_out: 229.0\n",
      "fc layer 3 self.abs_max_out: 239.0\n",
      "fc layer 3 self.abs_max_out: 253.0\n",
      "fc layer 3 self.abs_max_out: 255.0\n",
      "lif layer 2 self.abs_max_v: 1024.5\n",
      "fc layer 1 self.abs_max_out: 908.0\n",
      "fc layer 1 self.abs_max_out: 981.0\n",
      "fc layer 1 self.abs_max_out: 1006.0\n",
      "lif layer 2 self.abs_max_v: 1034.5\n",
      "lif layer 2 self.abs_max_v: 1056.0\n",
      "fc layer 2 self.abs_max_out: 603.0\n",
      "lif layer 2 self.abs_max_v: 1083.5\n",
      "lif layer 2 self.abs_max_v: 1126.0\n",
      "lif layer 2 self.abs_max_v: 1157.0\n",
      "fc layer 1 self.abs_max_out: 1048.0\n",
      "fc layer 1 self.abs_max_out: 1172.0\n",
      "lif layer 1 self.abs_max_v: 1579.0\n",
      "lif layer 1 self.abs_max_v: 1780.5\n",
      "fc layer 1 self.abs_max_out: 1257.0\n",
      "fc layer 1 self.abs_max_out: 1263.0\n",
      "fc layer 1 self.abs_max_out: 1342.0\n",
      "fc layer 1 self.abs_max_out: 1366.0\n",
      "fc layer 1 self.abs_max_out: 1410.0\n",
      "fc layer 1 self.abs_max_out: 1451.0\n",
      "fc layer 1 self.abs_max_out: 1466.0\n",
      "fc layer 1 self.abs_max_out: 1487.0\n",
      "fc layer 1 self.abs_max_out: 1578.0\n",
      "fc layer 3 self.abs_max_out: 256.0\n",
      "fc layer 2 self.abs_max_out: 653.0\n",
      "fc layer 2 self.abs_max_out: 732.0\n",
      "fc layer 1 self.abs_max_out: 1612.0\n",
      "fc layer 1 self.abs_max_out: 1642.0\n",
      "fc layer 1 self.abs_max_out: 1729.0\n",
      "fc layer 2 self.abs_max_out: 748.0\n",
      "lif layer 2 self.abs_max_v: 1195.0\n",
      "lif layer 2 self.abs_max_v: 1228.5\n",
      "lif layer 2 self.abs_max_v: 1245.5\n",
      "fc layer 2 self.abs_max_out: 805.0\n",
      "fc layer 1 self.abs_max_out: 1747.0\n",
      "fc layer 1 self.abs_max_out: 1971.0\n",
      "lif layer 1 self.abs_max_v: 1971.0\n",
      "fc layer 1 self.abs_max_out: 2042.0\n",
      "lif layer 1 self.abs_max_v: 2042.0\n",
      "fc layer 3 self.abs_max_out: 271.0\n",
      "lif layer 1 self.abs_max_v: 2065.0\n",
      "lif layer 2 self.abs_max_v: 1250.0\n",
      "lif layer 2 self.abs_max_v: 1302.0\n",
      "lif layer 2 self.abs_max_v: 1328.0\n",
      "fc layer 3 self.abs_max_out: 303.0\n",
      "fc layer 2 self.abs_max_out: 838.0\n",
      "lif layer 2 self.abs_max_v: 1362.0\n",
      "lif layer 2 self.abs_max_v: 1438.0\n",
      "fc layer 2 self.abs_max_out: 849.0\n",
      "fc layer 2 self.abs_max_out: 862.0\n",
      "lif layer 1 self.abs_max_v: 2104.5\n",
      "fc layer 2 self.abs_max_out: 930.0\n",
      "lif layer 1 self.abs_max_v: 2206.5\n",
      "lif layer 1 self.abs_max_v: 2488.5\n",
      "lif layer 1 self.abs_max_v: 2490.5\n",
      "fc layer 2 self.abs_max_out: 942.0\n",
      "fc layer 2 self.abs_max_out: 973.0\n",
      "fc layer 2 self.abs_max_out: 1020.0\n",
      "fc layer 1 self.abs_max_out: 2051.0\n",
      "fc layer 1 self.abs_max_out: 2098.0\n",
      "fc layer 1 self.abs_max_out: 2156.0\n",
      "fc layer 1 self.abs_max_out: 2181.0\n",
      "fc layer 3 self.abs_max_out: 330.0\n",
      "fc layer 3 self.abs_max_out: 348.0\n",
      "fc layer 1 self.abs_max_out: 2223.0\n",
      "fc layer 2 self.abs_max_out: 1055.0\n",
      "fc layer 2 self.abs_max_out: 1064.0\n",
      "lif layer 2 self.abs_max_v: 1504.0\n",
      "fc layer 1 self.abs_max_out: 2240.0\n",
      "fc layer 1 self.abs_max_out: 2382.0\n",
      "fc layer 1 self.abs_max_out: 2533.0\n",
      "lif layer 1 self.abs_max_v: 2533.0\n",
      "fc layer 1 self.abs_max_out: 2547.0\n",
      "lif layer 1 self.abs_max_v: 2547.0\n",
      "fc layer 1 self.abs_max_out: 2752.0\n",
      "lif layer 1 self.abs_max_v: 2752.0\n",
      "fc layer 1 self.abs_max_out: 2805.0\n",
      "lif layer 1 self.abs_max_v: 2805.0\n",
      "fc layer 1 self.abs_max_out: 2871.0\n",
      "lif layer 1 self.abs_max_v: 2871.0\n",
      "lif layer 2 self.abs_max_v: 1598.0\n",
      "lif layer 2 self.abs_max_v: 1658.5\n",
      "lif layer 1 self.abs_max_v: 2929.5\n",
      "lif layer 1 self.abs_max_v: 3017.0\n",
      "lif layer 1 self.abs_max_v: 3115.5\n",
      "fc layer 1 self.abs_max_out: 2995.0\n",
      "fc layer 2 self.abs_max_out: 1232.0\n",
      "fc layer 3 self.abs_max_out: 364.0\n",
      "fc layer 3 self.abs_max_out: 389.0\n",
      "fc layer 2 self.abs_max_out: 1253.0\n",
      "fc layer 2 self.abs_max_out: 1266.0\n",
      "fc layer 2 self.abs_max_out: 1296.0\n",
      "fc layer 2 self.abs_max_out: 1379.0\n",
      "fc layer 2 self.abs_max_out: 1393.0\n",
      "lif layer 2 self.abs_max_v: 1679.0\n",
      "lif layer 2 self.abs_max_v: 1804.5\n",
      "lif layer 2 self.abs_max_v: 1867.5\n",
      "lif layer 2 self.abs_max_v: 1910.0\n",
      "fc layer 2 self.abs_max_out: 1516.0\n",
      "fc layer 2 self.abs_max_out: 1554.0\n",
      "lif layer 1 self.abs_max_v: 3182.0\n",
      "lif layer 1 self.abs_max_v: 3271.0\n",
      "fc layer 2 self.abs_max_out: 1593.0\n",
      "lif layer 2 self.abs_max_v: 1941.5\n",
      "lif layer 2 self.abs_max_v: 1961.0\n",
      "lif layer 2 self.abs_max_v: 1970.5\n",
      "fc layer 2 self.abs_max_out: 1594.0\n",
      "lif layer 1 self.abs_max_v: 3581.5\n",
      "lif layer 1 self.abs_max_v: 3654.5\n",
      "fc layer 1 self.abs_max_out: 3032.0\n",
      "lif layer 1 self.abs_max_v: 3702.5\n",
      "lif layer 1 self.abs_max_v: 3716.0\n",
      "lif layer 1 self.abs_max_v: 3802.0\n",
      "fc layer 1 self.abs_max_out: 3083.0\n",
      "fc layer 1 self.abs_max_out: 3098.0\n",
      "fc layer 1 self.abs_max_out: 3213.0\n",
      "fc layer 1 self.abs_max_out: 3340.0\n",
      "fc layer 1 self.abs_max_out: 3350.0\n",
      "lif layer 1 self.abs_max_v: 4046.0\n",
      "fc layer 2 self.abs_max_out: 1612.0\n",
      "fc layer 1 self.abs_max_out: 3381.0\n",
      "fc layer 1 self.abs_max_out: 3401.0\n",
      "fc layer 3 self.abs_max_out: 390.0\n",
      "lif layer 2 self.abs_max_v: 1993.0\n",
      "lif layer 2 self.abs_max_v: 2047.5\n",
      "fc layer 2 self.abs_max_out: 1677.0\n",
      "lif layer 2 self.abs_max_v: 2078.0\n",
      "fc layer 1 self.abs_max_out: 3403.0\n",
      "fc layer 2 self.abs_max_out: 1695.0\n",
      "fc layer 1 self.abs_max_out: 3407.0\n",
      "fc layer 1 self.abs_max_out: 3413.0\n",
      "fc layer 1 self.abs_max_out: 3482.0\n",
      "fc layer 1 self.abs_max_out: 3557.0\n",
      "fc layer 1 self.abs_max_out: 3616.0\n",
      "fc layer 2 self.abs_max_out: 1697.0\n",
      "lif layer 2 self.abs_max_v: 2092.0\n",
      "lif layer 2 self.abs_max_v: 2100.0\n",
      "lif layer 2 self.abs_max_v: 2104.0\n",
      "lif layer 2 self.abs_max_v: 2106.0\n",
      "lif layer 1 self.abs_max_v: 4073.0\n",
      "lif layer 1 self.abs_max_v: 4248.5\n",
      "lif layer 1 self.abs_max_v: 4321.5\n",
      "lif layer 1 self.abs_max_v: 4559.5\n",
      "fc layer 2 self.abs_max_out: 1836.0\n",
      "fc layer 2 self.abs_max_out: 1865.0\n",
      "fc layer 3 self.abs_max_out: 414.0\n",
      "fc layer 2 self.abs_max_out: 1894.0\n",
      "lif layer 1 self.abs_max_v: 4749.0\n",
      "lif layer 1 self.abs_max_v: 5246.5\n",
      "lif layer 1 self.abs_max_v: 5332.5\n",
      "lif layer 1 self.abs_max_v: 5537.0\n",
      "fc layer 1 self.abs_max_out: 3620.0\n",
      "fc layer 1 self.abs_max_out: 3802.0\n",
      "fc layer 1 self.abs_max_out: 3896.0\n",
      "fc layer 1 self.abs_max_out: 3923.0\n",
      "fc layer 1 self.abs_max_out: 4014.0\n",
      "fc layer 1 self.abs_max_out: 4120.0\n",
      "lif layer 1 self.abs_max_v: 5543.5\n",
      "lif layer 1 self.abs_max_v: 5802.5\n",
      "lif layer 1 self.abs_max_v: 6305.5\n",
      "fc layer 1 self.abs_max_out: 4237.0\n",
      "fc layer 1 self.abs_max_out: 4279.0\n",
      "fc layer 1 self.abs_max_out: 4322.0\n",
      "fc layer 1 self.abs_max_out: 4326.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 11.519385/ 79.351784, val:  36.25%, val_best:  36.25%, tr:  98.77%, tr_best:  98.77%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6905%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.1886%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 1556  15.894%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1913.0\n",
      "fc layer 2 self.abs_max_out: 1914.0\n",
      "fc layer 2 self.abs_max_out: 1971.0\n",
      "fc layer 2 self.abs_max_out: 2067.0\n",
      "fc layer 2 self.abs_max_out: 2089.0\n",
      "fc layer 2 self.abs_max_out: 2147.0\n",
      "lif layer 2 self.abs_max_v: 2147.0\n",
      "fc layer 2 self.abs_max_out: 2209.0\n",
      "lif layer 2 self.abs_max_v: 2209.0\n",
      "lif layer 1 self.abs_max_v: 6642.5\n",
      "fc layer 2 self.abs_max_out: 2224.0\n",
      "lif layer 2 self.abs_max_v: 2224.0\n",
      "fc layer 2 self.abs_max_out: 2245.0\n",
      "lif layer 2 self.abs_max_v: 2245.0\n",
      "fc layer 2 self.abs_max_out: 2250.0\n",
      "lif layer 2 self.abs_max_v: 2250.0\n",
      "fc layer 2 self.abs_max_out: 2267.0\n",
      "lif layer 2 self.abs_max_v: 2267.0\n",
      "fc layer 1 self.abs_max_out: 4345.0\n",
      "fc layer 1 self.abs_max_out: 4430.0\n",
      "fc layer 2 self.abs_max_out: 2337.0\n",
      "lif layer 2 self.abs_max_v: 2337.0\n",
      "fc layer 2 self.abs_max_out: 2382.0\n",
      "lif layer 2 self.abs_max_v: 2382.0\n",
      "lif layer 2 self.abs_max_v: 2386.5\n",
      "lif layer 2 self.abs_max_v: 2389.5\n",
      "lif layer 2 self.abs_max_v: 2442.0\n",
      "lif layer 2 self.abs_max_v: 2522.0\n",
      "lif layer 2 self.abs_max_v: 2642.0\n",
      "fc layer 2 self.abs_max_out: 2390.0\n",
      "fc layer 2 self.abs_max_out: 2410.0\n",
      "fc layer 2 self.abs_max_out: 2490.0\n",
      "fc layer 2 self.abs_max_out: 2500.0\n",
      "lif layer 2 self.abs_max_v: 2733.0\n",
      "lif layer 2 self.abs_max_v: 2748.5\n",
      "lif layer 2 self.abs_max_v: 2832.5\n",
      "lif layer 2 self.abs_max_v: 2874.5\n",
      "lif layer 2 self.abs_max_v: 2895.5\n",
      "fc layer 2 self.abs_max_out: 2525.0\n",
      "fc layer 2 self.abs_max_out: 2555.0\n",
      "lif layer 1 self.abs_max_v: 6728.0\n",
      "fc layer 1 self.abs_max_out: 4604.0\n",
      "lif layer 1 self.abs_max_v: 6794.0\n",
      "fc layer 1 self.abs_max_out: 4954.0\n",
      "fc layer 2 self.abs_max_out: 2602.0\n",
      "fc layer 2 self.abs_max_out: 2639.0\n",
      "fc layer 2 self.abs_max_out: 2702.0\n",
      "lif layer 2 self.abs_max_v: 2911.5\n",
      "lif layer 2 self.abs_max_v: 2934.5\n",
      "lif layer 2 self.abs_max_v: 3035.0\n",
      "lif layer 2 self.abs_max_v: 3050.0\n",
      "fc layer 2 self.abs_max_out: 2742.0\n",
      "fc layer 2 self.abs_max_out: 2822.0\n",
      "lif layer 1 self.abs_max_v: 6824.5\n",
      "lif layer 2 self.abs_max_v: 3089.0\n",
      "fc layer 1 self.abs_max_out: 5106.0\n",
      "lif layer 2 self.abs_max_v: 3118.0\n",
      "lif layer 2 self.abs_max_v: 3177.0\n",
      "lif layer 2 self.abs_max_v: 3202.0\n",
      "lif layer 1 self.abs_max_v: 7311.0\n",
      "lif layer 1 self.abs_max_v: 7976.5\n",
      "fc layer 1 self.abs_max_out: 5472.0\n",
      "fc layer 1 self.abs_max_out: 5505.0\n",
      "lif layer 2 self.abs_max_v: 3214.0\n",
      "lif layer 2 self.abs_max_v: 3227.5\n",
      "lif layer 1 self.abs_max_v: 8060.5\n",
      "lif layer 1 self.abs_max_v: 8552.0\n",
      "lif layer 1 self.abs_max_v: 8960.5\n",
      "lif layer 1 self.abs_max_v: 9100.5\n",
      "fc layer 2 self.abs_max_out: 2844.0\n",
      "fc layer 2 self.abs_max_out: 2896.0\n",
      "fc layer 2 self.abs_max_out: 2980.0\n",
      "lif layer 2 self.abs_max_v: 3327.0\n",
      "fc layer 2 self.abs_max_out: 3033.0\n",
      "lif layer 2 self.abs_max_v: 3456.5\n",
      "lif layer 2 self.abs_max_v: 3527.5\n",
      "fc layer 2 self.abs_max_out: 3045.0\n",
      "lif layer 2 self.abs_max_v: 3537.5\n",
      "lif layer 2 self.abs_max_v: 3554.0\n",
      "lif layer 2 self.abs_max_v: 3660.0\n",
      "lif layer 2 self.abs_max_v: 3745.0\n",
      "fc layer 2 self.abs_max_out: 3054.0\n",
      "fc layer 2 self.abs_max_out: 3174.0\n",
      "fc layer 2 self.abs_max_out: 3187.0\n",
      "fc layer 2 self.abs_max_out: 3240.0\n",
      "lif layer 2 self.abs_max_v: 3834.5\n",
      "lif layer 2 self.abs_max_v: 3893.5\n",
      "lif layer 2 self.abs_max_v: 3930.0\n",
      "lif layer 2 self.abs_max_v: 3941.0\n",
      "lif layer 2 self.abs_max_v: 3946.5\n",
      "lif layer 2 self.abs_max_v: 3949.5\n",
      "fc layer 2 self.abs_max_out: 3332.0\n",
      "fc layer 1 self.abs_max_out: 6041.0\n",
      "fc layer 2 self.abs_max_out: 3349.0\n",
      "lif layer 1 self.abs_max_v: 9253.0\n",
      "lif layer 1 self.abs_max_v: 9344.0\n",
      "lif layer 1 self.abs_max_v: 9554.0\n",
      "lif layer 1 self.abs_max_v: 9712.0\n",
      "lif layer 1 self.abs_max_v: 10022.5\n",
      "lif layer 1 self.abs_max_v: 11015.5\n",
      "fc layer 1 self.abs_max_out: 6327.0\n",
      "lif layer 1 self.abs_max_v: 11835.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 13.388314/ 73.464302, val:  27.50%, val_best:  36.25%, tr:  99.18%, tr_best:  99.18%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.0439%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2368%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3096  15.812%\n",
      "fc layer 2 self.abs_max_out: 3381.0\n",
      "fc layer 2 self.abs_max_out: 3494.0\n",
      "fc layer 2 self.abs_max_out: 3594.0\n",
      "lif layer 2 self.abs_max_v: 3986.5\n",
      "lif layer 2 self.abs_max_v: 4073.5\n",
      "lif layer 2 self.abs_max_v: 4100.5\n",
      "lif layer 2 self.abs_max_v: 4247.5\n",
      "lif layer 2 self.abs_max_v: 4321.0\n",
      "lif layer 2 self.abs_max_v: 4357.5\n",
      "lif layer 2 self.abs_max_v: 4376.0\n",
      "lif layer 2 self.abs_max_v: 4385.0\n",
      "lif layer 2 self.abs_max_v: 4412.5\n",
      "fc layer 2 self.abs_max_out: 3814.0\n",
      "fc layer 2 self.abs_max_out: 3834.0\n",
      "lif layer 2 self.abs_max_v: 4424.0\n",
      "lif layer 2 self.abs_max_v: 4623.0\n",
      "lif layer 2 self.abs_max_v: 4723.5\n",
      "fc layer 1 self.abs_max_out: 6490.0\n",
      "fc layer 2 self.abs_max_out: 3860.0\n",
      "fc layer 2 self.abs_max_out: 3871.0\n",
      "lif layer 2 self.abs_max_v: 4782.0\n",
      "lif layer 2 self.abs_max_v: 4807.0\n",
      "lif layer 2 self.abs_max_v: 4817.5\n",
      "lif layer 2 self.abs_max_v: 4865.0\n",
      "fc layer 3 self.abs_max_out: 422.0\n",
      "fc layer 3 self.abs_max_out: 444.0\n",
      "fc layer 2 self.abs_max_out: 3987.0\n",
      "fc layer 2 self.abs_max_out: 4115.0\n",
      "lif layer 2 self.abs_max_v: 5015.5\n",
      "lif layer 2 self.abs_max_v: 5153.5\n",
      "lif layer 2 self.abs_max_v: 5260.0\n",
      "lif layer 2 self.abs_max_v: 5296.0\n",
      "lif layer 2 self.abs_max_v: 5314.0\n",
      "fc layer 2 self.abs_max_out: 4159.0\n",
      "fc layer 1 self.abs_max_out: 6535.0\n",
      "fc layer 1 self.abs_max_out: 6688.0\n",
      "lif layer 2 self.abs_max_v: 5364.5\n",
      "lif layer 2 self.abs_max_v: 5427.5\n",
      "fc layer 2 self.abs_max_out: 4184.0\n",
      "lif layer 2 self.abs_max_v: 5454.0\n",
      "fc layer 3 self.abs_max_out: 463.0\n",
      "lif layer 2 self.abs_max_v: 5501.5\n",
      "lif layer 2 self.abs_max_v: 5523.5\n",
      "lif layer 2 self.abs_max_v: 5547.5\n",
      "lif layer 2 self.abs_max_v: 5684.0\n",
      "lif layer 2 self.abs_max_v: 5771.5\n",
      "lif layer 2 self.abs_max_v: 5823.0\n",
      "lif layer 2 self.abs_max_v: 5841.5\n",
      "lif layer 2 self.abs_max_v: 5851.0\n",
      "lif layer 2 self.abs_max_v: 5855.5\n",
      "lif layer 2 self.abs_max_v: 5895.0\n",
      "lif layer 2 self.abs_max_v: 5957.0\n",
      "lif layer 2 self.abs_max_v: 6015.5\n",
      "lif layer 2 self.abs_max_v: 6045.0\n",
      "fc layer 1 self.abs_max_out: 6710.0\n",
      "fc layer 1 self.abs_max_out: 6797.0\n",
      "fc layer 1 self.abs_max_out: 6811.0\n",
      "fc layer 1 self.abs_max_out: 6967.0\n",
      "lif layer 1 self.abs_max_v: 12016.5\n",
      "lif layer 1 self.abs_max_v: 12114.0\n",
      "fc layer 1 self.abs_max_out: 6971.0\n",
      "lif layer 1 self.abs_max_v: 12338.5\n",
      "fc layer 1 self.abs_max_out: 7452.0\n",
      "lif layer 1 self.abs_max_v: 13618.5\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 13.618240/ 73.879913, val:  35.00%, val_best:  36.25%, tr:  98.57%, tr_best:  99.18%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8923%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.4580%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 4630  15.764%\n",
      "fc layer 2 self.abs_max_out: 4207.0\n",
      "fc layer 2 self.abs_max_out: 4217.0\n",
      "fc layer 1 self.abs_max_out: 7516.0\n",
      "fc layer 1 self.abs_max_out: 7634.0\n",
      "fc layer 1 self.abs_max_out: 7731.0\n",
      "lif layer 1 self.abs_max_v: 14031.0\n",
      "fc layer 1 self.abs_max_out: 7812.0\n",
      "fc layer 1 self.abs_max_out: 8360.0\n",
      "lif layer 1 self.abs_max_v: 15064.5\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss: 12.760816/ 74.980766, val:  32.08%, val_best:  36.25%, tr:  98.26%, tr_best:  99.18%, epoch time: 75.89 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.5127%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.5709%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6159  15.728%\n",
      "fc layer 1 self.abs_max_out: 8519.0\n",
      "lif layer 2 self.abs_max_v: 6070.5\n",
      "lif layer 2 self.abs_max_v: 6109.0\n",
      "lif layer 2 self.abs_max_v: 6136.0\n",
      "lif layer 2 self.abs_max_v: 6160.0\n",
      "lif layer 2 self.abs_max_v: 6172.0\n",
      "lif layer 2 self.abs_max_v: 6178.0\n",
      "lif layer 2 self.abs_max_v: 6248.5\n",
      "lif layer 2 self.abs_max_v: 6278.5\n",
      "lif layer 2 self.abs_max_v: 6300.5\n",
      "lif layer 2 self.abs_max_v: 6325.5\n",
      "lif layer 2 self.abs_max_v: 6338.0\n",
      "lif layer 2 self.abs_max_v: 6344.0\n",
      "lif layer 1 self.abs_max_v: 15097.5\n",
      "fc layer 1 self.abs_max_out: 8536.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss: 12.414070/ 98.009178, val:  25.42%, val_best:  36.25%, tr:  98.77%, tr_best:  99.18%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.7488%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.0439%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 7672  15.673%\n",
      "lif layer 2 self.abs_max_v: 6384.0\n",
      "lif layer 2 self.abs_max_v: 6404.0\n",
      "fc layer 2 self.abs_max_out: 4222.0\n",
      "lif layer 2 self.abs_max_v: 6446.0\n",
      "lif layer 2 self.abs_max_v: 6635.0\n",
      "lif layer 2 self.abs_max_v: 6729.5\n",
      "lif layer 2 self.abs_max_v: 6777.0\n",
      "lif layer 2 self.abs_max_v: 6800.5\n",
      "fc layer 1 self.abs_max_out: 9641.0\n",
      "lif layer 1 self.abs_max_v: 15371.5\n",
      "lif layer 1 self.abs_max_v: 16766.5\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss: 11.901056/ 55.407085, val:  28.33%, val_best:  36.25%, tr:  98.37%, tr_best:  99.18%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7073%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.4312%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9143  15.565%\n",
      "lif layer 2 self.abs_max_v: 6861.0\n",
      "lif layer 2 self.abs_max_v: 6882.5\n",
      "lif layer 2 self.abs_max_v: 6925.5\n",
      "lif layer 2 self.abs_max_v: 6947.0\n",
      "lif layer 2 self.abs_max_v: 6957.5\n",
      "lif layer 2 self.abs_max_v: 6998.5\n",
      "lif layer 2 self.abs_max_v: 7012.5\n",
      "lif layer 2 self.abs_max_v: 7026.5\n",
      "lif layer 2 self.abs_max_v: 7033.5\n",
      "fc layer 2 self.abs_max_out: 4250.0\n",
      "fc layer 2 self.abs_max_out: 4294.0\n",
      "lif layer 1 self.abs_max_v: 17221.5\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss: 11.841723/ 67.994720, val:  35.00%, val_best:  36.25%, tr:  98.67%, tr_best:  99.18%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4068%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.7059%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10623  15.501%\n",
      "lif layer 2 self.abs_max_v: 7148.5\n",
      "lif layer 2 self.abs_max_v: 7300.5\n",
      "lif layer 2 self.abs_max_v: 7376.5\n",
      "lif layer 2 self.abs_max_v: 7414.5\n",
      "lif layer 2 self.abs_max_v: 7433.5\n",
      "lif layer 2 self.abs_max_v: 7443.0\n",
      "fc layer 2 self.abs_max_out: 4374.0\n",
      "fc layer 2 self.abs_max_out: 4406.0\n",
      "lif layer 2 self.abs_max_v: 7443.5\n",
      "lif layer 2 self.abs_max_v: 7538.5\n",
      "lif layer 2 self.abs_max_v: 7823.5\n",
      "lif layer 2 self.abs_max_v: 7966.0\n",
      "fc layer 1 self.abs_max_out: 9817.0\n",
      "fc layer 1 self.abs_max_out: 9875.0\n",
      "lif layer 1 self.abs_max_v: 18190.5\n",
      "lif layer 1 self.abs_max_v: 18408.5\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss: 11.003366/ 71.179947, val:  28.33%, val_best:  36.25%, tr:  98.16%, tr_best:  99.18%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3322%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 12025  15.354%\n",
      "lif layer 2 self.abs_max_v: 8058.0\n",
      "lif layer 2 self.abs_max_v: 8388.0\n",
      "fc layer 2 self.abs_max_out: 4419.0\n",
      "fc layer 2 self.abs_max_out: 4853.0\n",
      "lif layer 2 self.abs_max_v: 8463.0\n",
      "lif layer 2 self.abs_max_v: 8599.5\n",
      "lif layer 2 self.abs_max_v: 8668.0\n",
      "lif layer 2 self.abs_max_v: 8702.0\n",
      "lif layer 2 self.abs_max_v: 8719.0\n",
      "lif layer 2 self.abs_max_v: 8727.5\n",
      "lif layer 2 self.abs_max_v: 8944.0\n",
      "fc layer 1 self.abs_max_out: 10133.0\n",
      "lif layer 1 self.abs_max_v: 18627.5\n",
      "lif layer 1 self.abs_max_v: 18824.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss: 11.626679/ 53.620491, val:  34.17%, val_best:  36.25%, tr:  97.75%, tr_best:  99.18%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6181%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.9499%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 13545  15.373%\n",
      "fc layer 2 self.abs_max_out: 5102.0\n",
      "fc layer 1 self.abs_max_out: 10850.0\n",
      "lif layer 2 self.abs_max_v: 9073.5\n",
      "lif layer 2 self.abs_max_v: 9287.0\n",
      "lif layer 2 self.abs_max_v: 9378.0\n",
      "lif layer 2 self.abs_max_v: 9460.0\n",
      "lif layer 2 self.abs_max_v: 9501.0\n",
      "lif layer 2 self.abs_max_v: 9514.0\n",
      "lif layer 1 self.abs_max_v: 19440.5\n",
      "lif layer 1 self.abs_max_v: 19518.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss: 11.223407/ 69.973625, val:  37.50%, val_best:  37.50%, tr:  97.75%, tr_best:  99.18%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4363%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.1463%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 15014  15.336%\n",
      "lif layer 2 self.abs_max_v: 9534.0\n",
      "lif layer 2 self.abs_max_v: 9564.0\n",
      "lif layer 2 self.abs_max_v: 9573.0\n",
      "lif layer 2 self.abs_max_v: 9610.5\n",
      "lif layer 2 self.abs_max_v: 9629.5\n",
      "lif layer 2 self.abs_max_v: 9711.0\n",
      "fc layer 2 self.abs_max_out: 5136.0\n",
      "fc layer 2 self.abs_max_out: 5156.0\n",
      "lif layer 2 self.abs_max_v: 9915.0\n",
      "lif layer 2 self.abs_max_v: 10113.5\n",
      "fc layer 1 self.abs_max_out: 11252.0\n",
      "lif layer 1 self.abs_max_v: 19741.5\n",
      "fc layer 2 self.abs_max_out: 5176.0\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss: 11.461488/ 70.472939, val:  36.67%, val_best:  37.50%, tr:  97.96%, tr_best:  99.18%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4578%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1930%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 16536  15.355%\n",
      "fc layer 1 self.abs_max_out: 11284.0\n",
      "fc layer 2 self.abs_max_out: 5477.0\n",
      "fc layer 2 self.abs_max_out: 5579.0\n",
      "lif layer 2 self.abs_max_v: 10119.5\n",
      "lif layer 2 self.abs_max_v: 10259.0\n",
      "lif layer 1 self.abs_max_v: 20287.0\n",
      "fc layer 1 self.abs_max_out: 11376.0\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss: 11.472589/ 89.134308, val:  20.00%, val_best:  37.50%, tr:  97.45%, tr_best:  99.18%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7655%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2726%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 18043  15.358%\n",
      "fc layer 2 self.abs_max_out: 6069.0\n",
      "lif layer 2 self.abs_max_v: 10276.0\n",
      "lif layer 2 self.abs_max_v: 10306.0\n",
      "lif layer 1 self.abs_max_v: 20515.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss: 11.218564/ 72.288757, val:  30.42%, val_best:  37.50%, tr:  97.85%, tr_best:  99.18%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1011%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3501%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 19534  15.348%\n",
      "fc layer 2 self.abs_max_out: 6617.0\n",
      "lif layer 2 self.abs_max_v: 10401.0\n",
      "lif layer 2 self.abs_max_v: 10483.5\n",
      "lif layer 2 self.abs_max_v: 10525.0\n",
      "lif layer 2 self.abs_max_v: 10545.5\n",
      "lif layer 2 self.abs_max_v: 10556.0\n",
      "lif layer 2 self.abs_max_v: 10559.5\n",
      "lif layer 2 self.abs_max_v: 10603.0\n",
      "lif layer 2 self.abs_max_v: 10624.5\n",
      "fc layer 2 self.abs_max_out: 6849.0\n",
      "fc layer 1 self.abs_max_out: 11668.0\n",
      "lif layer 1 self.abs_max_v: 21208.5\n",
      "fc layer 2 self.abs_max_out: 6890.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss: 11.019085/ 75.356392, val:  26.67%, val_best:  37.50%, tr:  98.47%, tr_best:  99.18%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5028%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0598%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 21019  15.336%\n",
      "lif layer 2 self.abs_max_v: 10680.0\n",
      "lif layer 2 self.abs_max_v: 10807.0\n",
      "lif layer 2 self.abs_max_v: 10934.5\n",
      "lif layer 2 self.abs_max_v: 10969.5\n",
      "lif layer 2 self.abs_max_v: 11075.5\n",
      "fc layer 1 self.abs_max_out: 11880.0\n",
      "lif layer 1 self.abs_max_v: 21562.5\n",
      "fc layer 1 self.abs_max_out: 11944.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss: 10.605947/ 93.123215, val:  21.25%, val_best:  37.50%, tr:  98.67%, tr_best:  99.18%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7164%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6637%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 22496  15.319%\n",
      "lif layer 2 self.abs_max_v: 11119.5\n",
      "fc layer 1 self.abs_max_out: 11969.0\n",
      "lif layer 1 self.abs_max_v: 21722.5\n",
      "fc layer 1 self.abs_max_out: 12055.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  9.843617/ 57.750370, val:  35.42%, val_best:  37.50%, tr:  98.47%, tr_best:  99.18%, epoch time: 75.81 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0380%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4658%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 23941  15.284%\n",
      "lif layer 2 self.abs_max_v: 11133.0\n",
      "lif layer 2 self.abs_max_v: 11229.5\n",
      "fc layer 2 self.abs_max_out: 6900.0\n",
      "fc layer 2 self.abs_max_out: 7625.0\n",
      "lif layer 2 self.abs_max_v: 12957.5\n",
      "fc layer 1 self.abs_max_out: 12470.0\n",
      "lif layer 1 self.abs_max_v: 22554.5\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  9.956356/ 70.355759, val:  34.17%, val_best:  37.50%, tr:  98.47%, tr_best:  99.18%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0704%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8276%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 25402  15.263%\n",
      "fc layer 2 self.abs_max_out: 8398.0\n",
      "fc layer 1 self.abs_max_out: 12924.0\n",
      "lif layer 1 self.abs_max_v: 23483.0\n",
      "fc layer 1 self.abs_max_out: 12945.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss: 10.241325/ 46.469486, val:  39.58%, val_best:  39.58%, tr:  98.26%, tr_best:  99.18%, epoch time: 75.41 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3989%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6836%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 26905  15.268%\n",
      "fc layer 1 self.abs_max_out: 13054.0\n",
      "fc layer 1 self.abs_max_out: 13327.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss: 10.472941/ 54.762512, val:  35.83%, val_best:  39.58%, tr:  98.06%, tr_best:  99.18%, epoch time: 75.16 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2899%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7864%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 28462  15.301%\n",
      "lif layer 2 self.abs_max_v: 13706.5\n",
      "fc layer 2 self.abs_max_out: 8571.0\n",
      "fc layer 1 self.abs_max_out: 13661.0\n",
      "lif layer 1 self.abs_max_v: 23829.0\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  9.547812/ 68.547737, val:  30.83%, val_best:  39.58%, tr:  97.96%, tr_best:  99.18%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9301%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3889%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 29922  15.282%\n",
      "fc layer 1 self.abs_max_out: 13773.0\n",
      "lif layer 1 self.abs_max_v: 23994.5\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  9.697784/ 63.292755, val:  35.83%, val_best:  39.58%, tr:  98.06%, tr_best:  99.18%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8105%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5002%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 31394  15.270%\n",
      "fc layer 1 self.abs_max_out: 14382.0\n",
      "lif layer 1 self.abs_max_v: 25135.0\n",
      "lif layer 1 self.abs_max_v: 25146.0\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  9.961028/ 60.238194, val:  23.75%, val_best:  39.58%, tr:  97.85%, tr_best:  99.18%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6525%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2224%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 32918  15.284%\n",
      "lif layer 2 self.abs_max_v: 13814.5\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  9.438993/ 60.871170, val:  37.92%, val_best:  39.58%, tr:  98.37%, tr_best:  99.18%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8360%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0098%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 34359  15.259%\n",
      "lif layer 2 self.abs_max_v: 14083.0\n",
      "fc layer 1 self.abs_max_out: 14400.0\n",
      "lif layer 1 self.abs_max_v: 25209.5\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  9.594230/ 47.163418, val:  41.25%, val_best:  41.25%, tr:  97.55%, tr_best:  99.18%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0449%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5874%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 35824  15.247%\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  9.394130/ 70.811890, val:  29.58%, val_best:  41.25%, tr:  98.67%, tr_best:  99.18%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0683%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3358%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 37351  15.261%\n",
      "fc layer 1 self.abs_max_out: 14478.0\n",
      "lif layer 1 self.abs_max_v: 25297.0\n",
      "lif layer 1 self.abs_max_v: 25343.5\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  9.778813/ 57.512470, val:  44.58%, val_best:  44.58%, tr:  98.16%, tr_best:  99.18%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2733%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2293%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 38893  15.280%\n",
      "lif layer 2 self.abs_max_v: 14633.0\n",
      "fc layer 1 self.abs_max_out: 14786.0\n",
      "lif layer 1 self.abs_max_v: 25933.0\n",
      "lif layer 1 self.abs_max_v: 25949.5\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  9.782998/ 46.131325, val:  40.42%, val_best:  44.58%, tr:  97.96%, tr_best:  99.18%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2948%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5466%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 40377  15.275%\n",
      "lif layer 2 self.abs_max_v: 15082.0\n",
      "lif layer 2 self.abs_max_v: 15480.0\n",
      "fc layer 1 self.abs_max_out: 15294.0\n",
      "lif layer 1 self.abs_max_v: 27049.5\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  9.729621/ 49.354103, val:  44.58%, val_best:  44.58%, tr:  98.16%, tr_best:  99.18%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5221%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8281%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 41882  15.279%\n",
      "fc layer 1 self.abs_max_out: 15559.0\n",
      "lif layer 1 self.abs_max_v: 27447.5\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  9.134682/ 69.633751, val:  27.50%, val_best:  44.58%, tr:  98.06%, tr_best:  99.18%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8692%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2208%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 43365  15.274%\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  9.676693/ 56.793423, val:  33.75%, val_best:  44.58%, tr:  98.67%, tr_best:  99.18%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8630%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9176%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 44913  15.292%\n",
      "fc layer 2 self.abs_max_out: 9310.0\n",
      "lif layer 2 self.abs_max_v: 15879.0\n",
      "lif layer 2 self.abs_max_v: 15969.5\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  9.611366/ 49.245037, val:  37.50%, val_best:  44.58%, tr:  99.18%, tr_best:  99.18%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4233%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5454%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 46422  15.296%\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  9.513463/ 62.514336, val:  37.50%, val_best:  44.58%, tr:  98.67%, tr_best:  99.18%, epoch time: 75.75 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5953%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4644%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 47962  15.310%\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  9.450875/ 64.633965, val:  27.92%, val_best:  44.58%, tr:  98.57%, tr_best:  99.18%, epoch time: 75.69 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5920%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3523%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 49467  15.312%\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  9.555560/ 58.991867, val:  39.58%, val_best:  44.58%, tr:  98.88%, tr_best:  99.18%, epoch time: 75.62 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7308%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4241%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 50971  15.313%\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  9.996274/ 86.884567, val:  31.67%, val_best:  44.58%, tr:  98.47%, tr_best:  99.18%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2530%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9355%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 52523  15.328%\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  9.662867/ 38.465218, val:  42.50%, val_best:  44.58%, tr:  98.47%, tr_best:  99.18%, epoch time: 75.68 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3227%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5132%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 54017  15.327%\n",
      "fc layer 2 self.abs_max_out: 9767.0\n",
      "fc layer 1 self.abs_max_out: 15638.0\n",
      "lif layer 1 self.abs_max_v: 27756.5\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  9.572696/ 54.778957, val:  35.42%, val_best:  44.58%, tr:  98.98%, tr_best:  99.18%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3589%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0387%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 55515  15.326%\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  9.564577/ 50.699730, val:  41.67%, val_best:  44.58%, tr:  98.16%, tr_best:  99.18%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5233%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3804%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 57015  15.326%\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss: 10.065297/ 49.847633, val:  38.75%, val_best:  44.58%, tr:  98.57%, tr_best:  99.18%, epoch time: 74.94 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9423%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3007%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 58607  15.350%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  9.644680/ 54.744522, val:  33.75%, val_best:  44.58%, tr:  98.57%, tr_best:  99.18%, epoch time: 75.93 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0700%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2126%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 60093  15.346%\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  9.765975/ 51.394913, val:  46.25%, val_best:  46.25%, tr:  98.77%, tr_best:  99.18%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0309%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5923%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 61596  15.346%\n",
      "fc layer 1 self.abs_max_out: 15755.0\n",
      "lif layer 1 self.abs_max_v: 28014.5\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  9.883830/ 61.069374, val:  28.33%, val_best:  46.25%, tr:  98.06%, tr_best:  99.18%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2084%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1696%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 63139  15.356%\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  9.532845/ 59.984104, val:  29.58%, val_best:  46.25%, tr:  98.98%, tr_best:  99.18%, epoch time: 75.53 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0708%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8429%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 64634  15.354%\n",
      "fc layer 1 self.abs_max_out: 15789.0\n",
      "lif layer 1 self.abs_max_v: 28035.0\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  9.697421/ 46.971138, val:  38.33%, val_best:  46.25%, tr:  98.47%, tr_best:  99.18%, epoch time: 75.38 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0754%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1849%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 66137  15.354%\n",
      "fc layer 1 self.abs_max_out: 15898.0\n",
      "lif layer 1 self.abs_max_v: 28261.5\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss: 10.091140/ 62.195797, val:  39.58%, val_best:  46.25%, tr:  98.98%, tr_best:  99.18%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2616%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8388%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 67652  15.356%\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  9.927005/ 74.167038, val:  42.50%, val_best:  46.25%, tr:  98.37%, tr_best:  99.18%, epoch time: 75.53 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4231%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0711%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 69094  15.343%\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss: 10.467764/ 48.289001, val:  35.83%, val_best:  46.25%, tr:  98.98%, tr_best:  99.18%, epoch time: 76.01 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2427%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2291%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 70584  15.340%\n",
      "fc layer 1 self.abs_max_out: 16082.0\n",
      "lif layer 1 self.abs_max_v: 28662.5\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss: 10.107075/ 90.772835, val:  31.67%, val_best:  46.25%, tr:  98.47%, tr_best:  99.18%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6572%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8272%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 72033  15.329%\n",
      "fc layer 1 self.abs_max_out: 16330.0\n",
      "lif layer 1 self.abs_max_v: 29158.5\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss: 10.466996/ 76.917152, val:  40.42%, val_best:  46.25%, tr:  98.77%, tr_best:  99.18%, epoch time: 76.01 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8006%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2705%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 73432  15.308%\n",
      "fc layer 3 self.abs_max_out: 465.0\n",
      "fc layer 3 self.abs_max_out: 482.0\n",
      "fc layer 3 self.abs_max_out: 490.0\n",
      "fc layer 3 self.abs_max_out: 518.0\n",
      "fc layer 3 self.abs_max_out: 533.0\n",
      "fc layer 3 self.abs_max_out: 543.0\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss: 10.814884/ 41.523403, val:  45.83%, val_best:  46.25%, tr:  98.77%, tr_best:  99.18%, epoch time: 75.59 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3561%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1632%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 74854  15.292%\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss: 11.104077/ 65.152451, val:  41.67%, val_best:  46.25%, tr:  98.88%, tr_best:  99.18%, epoch time: 75.12 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4070%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8263%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 76314  15.285%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss: 10.592344/ 71.971939, val:  41.25%, val_best:  46.25%, tr:  99.18%, tr_best:  99.18%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3584%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2198%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 77721  15.267%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss: 11.098868/ 93.717415, val:  32.92%, val_best:  46.25%, tr:  98.57%, tr_best:  99.18%, epoch time: 75.83 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1921%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7333%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 79194  15.263%\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss: 10.825090/ 71.867996, val:  32.92%, val_best:  46.25%, tr:  98.57%, tr_best:  99.18%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4761%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1495%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 80612  15.248%\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss: 10.605939/ 59.527290, val:  47.92%, val_best:  47.92%, tr:  98.77%, tr_best:  99.18%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0530%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6395%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 82024  15.233%\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss: 11.043968/ 55.529716, val:  38.75%, val_best:  47.92%, tr:  98.37%, tr_best:  99.18%, epoch time: 75.30 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7385%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2817%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 83473  15.226%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss: 11.169357/ 94.322464, val:  26.67%, val_best:  47.92%, tr:  99.08%, tr_best:  99.18%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4533%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1995%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 84939  15.221%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss: 10.494391/ 52.837944, val:  38.75%, val_best:  47.92%, tr:  98.98%, tr_best:  99.18%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3026%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1165%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 86376  15.212%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss: 10.394117/ 73.332268, val:  31.67%, val_best:  47.92%, tr:  98.98%, tr_best:  99.18%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0051%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5155%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 87811  15.202%\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss: 10.335558/ 94.446350, val:  35.42%, val_best:  47.92%, tr:  99.18%, tr_best:  99.18%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6287%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0023%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 89276  15.199%\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss: 10.348022/ 82.783806, val:  36.25%, val_best:  47.92%, tr:  98.57%, tr_best:  99.18%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8587%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7100%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 90765  15.199%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss: 10.332998/ 56.040199, val:  37.92%, val_best:  47.92%, tr:  98.98%, tr_best:  99.18%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9867%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5436%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 92206  15.191%\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss: 10.413928/ 68.399857, val:  41.25%, val_best:  47.92%, tr:  98.77%, tr_best:  99.18%, epoch time: 75.90 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1414%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7149%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 93620  15.179%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss: 10.298522/ 71.530319, val:  42.92%, val_best:  47.92%, tr:  99.18%, tr_best:  99.18%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9869%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2731%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 95006  15.163%\n",
      "lif layer 2 self.abs_max_v: 15984.0\n",
      "lif layer 2 self.abs_max_v: 16454.0\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss: 10.179010/ 57.220417, val:  39.17%, val_best:  47.92%, tr:  98.57%, tr_best:  99.18%, epoch time: 75.53 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0486%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1824%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 96436  15.155%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss: 10.675112/ 50.288918, val:  47.08%, val_best:  47.92%, tr:  98.47%, tr_best:  99.18%, epoch time: 75.56 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7016%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6790%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 97922  15.155%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss: 10.716382/ 54.050076, val:  40.42%, val_best:  47.92%, tr:  99.08%, tr_best:  99.18%, epoch time: 75.53 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9061%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1447%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 99427  15.158%\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss: 10.624391/ 58.548954, val:  38.33%, val_best:  47.92%, tr:  98.88%, tr_best:  99.18%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6469%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2305%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 100876  15.153%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss: 10.947515/ 59.658772, val:  48.33%, val_best:  48.33%, tr:  98.77%, tr_best:  99.18%, epoch time: 75.70 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8382%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7210%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 102368  15.154%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss: 10.749828/ 75.655983, val:  35.83%, val_best:  48.33%, tr:  98.88%, tr_best:  99.18%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2349%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4487%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 103828  15.151%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss: 11.103326/ 62.141479, val:  42.08%, val_best:  48.33%, tr:  98.47%, tr_best:  99.18%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9619%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1897%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 105356  15.157%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss: 11.040417/ 71.706268, val:  35.42%, val_best:  48.33%, tr:  98.47%, tr_best:  99.18%, epoch time: 75.74 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9812%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4569%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 106904  15.166%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss: 10.607346/ 64.084137, val:  41.67%, val_best:  48.33%, tr:  98.98%, tr_best:  99.18%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3056%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1281%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 108370  15.164%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss: 10.942538/ 86.285873, val:  28.75%, val_best:  48.33%, tr:  98.98%, tr_best:  99.18%, epoch time: 75.48 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7575%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1658%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 109912  15.172%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss: 11.276213/ 53.748409, val:  41.67%, val_best:  48.33%, tr:  99.28%, tr_best:  99.28%, epoch time: 75.90 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6144%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8297%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 111435  15.177%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss: 10.020030/ 55.519524, val:  45.83%, val_best:  48.33%, tr:  98.98%, tr_best:  99.28%, epoch time: 74.43 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9217%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4667%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 112875  15.171%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss: 10.820940/ 49.042202, val:  50.00%, val_best:  50.00%, tr:  98.67%, tr_best:  99.28%, epoch time: 73.90 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5094%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5109%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 114360  15.171%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss: 10.572105/ 87.683182, val:  25.42%, val_best:  50.00%, tr:  98.37%, tr_best:  99.28%, epoch time: 73.54 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4843%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4772%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 115816  15.167%\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss: 10.386051/ 74.214096, val:  37.92%, val_best:  50.00%, tr:  98.88%, tr_best:  99.28%, epoch time: 74.05 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5263%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0569%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 117261  15.162%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss: 10.962961/ 63.114998, val:  42.50%, val_best:  50.00%, tr:  99.49%, tr_best:  99.49%, epoch time: 75.15 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3512%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6523%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 118702  15.156%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss: 10.651210/ 54.530453, val:  42.08%, val_best:  50.00%, tr:  98.47%, tr_best:  99.49%, epoch time: 75.59 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3774%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8411%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 120161  15.153%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss: 10.718942/ 70.836411, val:  35.42%, val_best:  50.00%, tr:  98.88%, tr_best:  99.49%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4538%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4982%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 121599  15.147%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss: 10.457869/ 67.469543, val:  39.58%, val_best:  50.00%, tr:  98.88%, tr_best:  99.49%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7239%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5006%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 122996  15.137%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss: 10.932795/ 95.311508, val:  33.33%, val_best:  50.00%, tr:  99.08%, tr_best:  99.49%, epoch time: 75.60 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9919%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5710%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 124448  15.133%\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss: 11.096654/ 66.590958, val:  40.00%, val_best:  50.00%, tr:  98.67%, tr_best:  99.49%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6171%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3622%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 125885  15.128%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss: 10.696386/ 52.303951, val:  38.33%, val_best:  50.00%, tr:  99.49%, tr_best:  99.49%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1751%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7179%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 127324  15.123%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss: 10.523114/ 47.653862, val:  47.08%, val_best:  50.00%, tr:  98.67%, tr_best:  99.49%, epoch time: 75.62 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3091%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1416%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 128770  15.119%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss: 10.903096/ 61.038074, val:  39.58%, val_best:  50.00%, tr:  99.39%, tr_best:  99.49%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0081%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3633%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 130248  15.118%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss: 10.979072/ 64.880814, val:  41.67%, val_best:  50.00%, tr:  98.77%, tr_best:  99.49%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1292%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0495%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 131637  15.108%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss: 10.390261/ 70.026642, val:  43.33%, val_best:  50.00%, tr:  98.47%, tr_best:  99.49%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8507%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2158%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 133025  15.098%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss: 11.052976/ 69.486771, val:  42.50%, val_best:  50.00%, tr:  99.08%, tr_best:  99.49%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2967%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1863%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 134481  15.095%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss: 10.540840/ 53.469524, val:  45.00%, val_best:  50.00%, tr:  98.57%, tr_best:  99.49%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4280%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2938%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 135905  15.089%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss: 11.148930/ 68.210724, val:  36.25%, val_best:  50.00%, tr:  98.88%, tr_best:  99.49%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2677%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5617%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 137353  15.086%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss: 10.687952/ 87.079987, val:  28.75%, val_best:  50.00%, tr:  97.96%, tr_best:  99.49%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3221%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4140%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 138835  15.086%\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss: 10.329218/ 66.221565, val:  44.58%, val_best:  50.00%, tr:  98.77%, tr_best:  99.49%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6955%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9028%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 140233  15.078%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss: 10.969848/ 58.184315, val:  45.83%, val_best:  50.00%, tr:  98.77%, tr_best:  99.49%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1116%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6837%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 141679  15.075%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss: 10.258163/ 69.684288, val:  43.75%, val_best:  50.00%, tr:  99.39%, tr_best:  99.49%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0324%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4770%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 143076  15.066%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss: 10.325002/ 66.504120, val:  39.58%, val_best:  50.00%, tr:  98.88%, tr_best:  99.49%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5210%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6299%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 144475  15.059%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss: 10.476749/ 91.596268, val:  27.50%, val_best:  50.00%, tr:  98.67%, tr_best:  99.49%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1239%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1346%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 145844  15.048%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss: 10.906583/ 68.197166, val:  31.67%, val_best:  50.00%, tr:  99.18%, tr_best:  99.49%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6945%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8877%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 147286  15.045%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss: 10.534697/ 67.521667, val:  31.67%, val_best:  50.00%, tr:  98.37%, tr_best:  99.49%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3293%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5531%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 148692  15.038%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss: 10.993880/ 76.019432, val:  37.92%, val_best:  50.00%, tr:  99.18%, tr_best:  99.49%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4065%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3424%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 150123  15.034%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss: 10.513639/ 70.689751, val:  37.50%, val_best:  50.00%, tr:  98.57%, tr_best:  99.49%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1389%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9651%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 151493  15.024%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss: 11.101922/ 71.697853, val:  34.17%, val_best:  50.00%, tr:  98.67%, tr_best:  99.49%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2393%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3936%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 152956  15.023%\n",
      "fc layer 2 self.abs_max_out: 9865.0\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss: 10.797633/ 77.531914, val:  33.75%, val_best:  50.00%, tr:  98.06%, tr_best:  99.49%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8730%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9914%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 154390  15.019%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss: 10.940546/ 48.298477, val:  42.08%, val_best:  50.00%, tr:  98.67%, tr_best:  99.49%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7217%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2434%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 155845  15.018%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss: 10.235130/ 96.558983, val:  25.42%, val_best:  50.00%, tr:  98.67%, tr_best:  99.49%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8036%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5590%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 157268  15.013%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss: 10.343639/ 71.295380, val:  45.42%, val_best:  50.00%, tr:  98.77%, tr_best:  99.49%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3342%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7138%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 158698  15.009%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss: 10.948077/ 52.955414, val:  34.17%, val_best:  50.00%, tr:  98.47%, tr_best:  99.49%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0941%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3877%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 160194  15.012%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss: 10.761937/ 70.371567, val:  38.33%, val_best:  50.00%, tr:  99.08%, tr_best:  99.49%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0099%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9022%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 161656  15.011%\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss: 10.618617/ 44.565025, val:  51.25%, val_best:  51.25%, tr:  98.47%, tr_best:  99.49%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8602%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5894%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 163105  15.009%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss: 10.485180/ 71.784851, val:  26.67%, val_best:  51.25%, tr:  98.88%, tr_best:  99.49%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5533%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3089%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 164529  15.005%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss: 10.750607/ 41.011124, val:  48.75%, val_best:  51.25%, tr:  98.67%, tr_best:  99.49%, epoch time: 75.76 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8384%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7419%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 165948  15.001%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss: 10.762879/ 78.451836, val:  30.42%, val_best:  51.25%, tr:  99.18%, tr_best:  99.49%, epoch time: 75.90 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6785%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9942%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 167414  15.000%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss: 10.546084/ 73.306900, val:  37.50%, val_best:  51.25%, tr:  99.08%, tr_best:  99.49%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9510%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8804%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 168848  14.997%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss: 10.857561/ 82.303955, val:  33.33%, val_best:  51.25%, tr:  98.77%, tr_best:  99.49%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8543%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4249%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 170251  14.992%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss: 10.423175/ 61.950062, val:  32.92%, val_best:  51.25%, tr:  98.57%, tr_best:  99.49%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8610%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9194%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 171629  14.984%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss: 10.700582/ 56.985386, val:  45.42%, val_best:  51.25%, tr:  98.67%, tr_best:  99.49%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2453%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1272%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 173031  14.978%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss: 10.703550/ 71.158951, val:  35.00%, val_best:  51.25%, tr:  98.67%, tr_best:  99.49%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2849%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2543%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 174481  14.977%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss: 10.284226/ 65.610321, val:  45.00%, val_best:  51.25%, tr:  99.28%, tr_best:  99.49%, epoch time: 75.35 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1541%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9584%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 175900  14.973%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss: 10.247151/ 41.791687, val:  44.17%, val_best:  51.25%, tr:  99.18%, tr_best:  99.49%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6276%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0072%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 177320  14.969%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss: 10.023734/ 65.456680, val:  34.17%, val_best:  51.25%, tr:  99.08%, tr_best:  99.49%, epoch time: 75.57 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4389%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7824%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 178746  14.966%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss: 10.771323/ 57.490143, val:  40.83%, val_best:  51.25%, tr:  99.18%, tr_best:  99.49%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4328%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7002%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 180231  14.967%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss: 10.722957/ 63.640263, val:  46.25%, val_best:  51.25%, tr:  99.28%, tr_best:  99.49%, epoch time: 75.14 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1824%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6137%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 181719  14.969%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss: 10.881012/ 55.919842, val:  48.75%, val_best:  51.25%, tr:  98.37%, tr_best:  99.49%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8379%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0743%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 183201  14.970%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  9.998086/ 89.104546, val:  34.17%, val_best:  51.25%, tr:  98.77%, tr_best:  99.49%, epoch time: 75.87 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1209%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2575%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 184622  14.967%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss: 10.370722/ 66.370338, val:  40.00%, val_best:  51.25%, tr:  98.77%, tr_best:  99.49%, epoch time: 75.16 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9713%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6323%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 186044  14.963%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  9.830662/ 75.855858, val:  26.25%, val_best:  51.25%, tr:  98.67%, tr_best:  99.49%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8274%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9664%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 187401  14.955%\n",
      "lif layer 2 self.abs_max_v: 17176.5\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss: 10.477576/ 52.342659, val:  38.33%, val_best:  51.25%, tr:  97.85%, tr_best:  99.49%, epoch time: 76.22 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9491%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4472%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 188844  14.953%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss: 10.326617/ 56.765804, val:  37.92%, val_best:  51.25%, tr:  98.26%, tr_best:  99.49%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0991%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0082%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 190242  14.948%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss: 10.973872/108.018204, val:  31.25%, val_best:  51.25%, tr:  98.88%, tr_best:  99.49%, epoch time: 75.35 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0740%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.4686%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 191682  14.946%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss: 10.937346/ 68.075417, val:  38.75%, val_best:  51.25%, tr:  98.47%, tr_best:  99.49%, epoch time: 75.13 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4089%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7098%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 193074  14.941%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss: 10.386540/ 75.171005, val:  36.25%, val_best:  51.25%, tr:  99.08%, tr_best:  99.49%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3910%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8386%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 194436  14.933%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss: 10.982890/ 65.370216, val:  35.83%, val_best:  51.25%, tr:  98.67%, tr_best:  99.49%, epoch time: 75.03 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0435%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2328%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 195854  14.929%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss: 10.522055/ 73.742729, val:  40.83%, val_best:  51.25%, tr:  98.88%, tr_best:  99.49%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8260%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4311%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 197271  14.926%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss: 10.866967/ 61.514805, val:  42.08%, val_best:  51.25%, tr:  99.18%, tr_best:  99.49%, epoch time: 75.77 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7645%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3358%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 198707  14.924%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss: 10.512084/ 53.246670, val:  45.83%, val_best:  51.25%, tr:  98.77%, tr_best:  99.49%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8402%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8156%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 200056  14.916%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss: 10.567848/ 74.994507, val:  36.67%, val_best:  51.25%, tr:  98.98%, tr_best:  99.49%, epoch time: 75.07 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8484%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7987%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 201423  14.909%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss: 10.584420/ 52.890015, val:  47.08%, val_best:  51.25%, tr:  99.18%, tr_best:  99.49%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8711%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4683%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 202873  14.908%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss: 10.627362/ 97.978653, val:  29.58%, val_best:  51.25%, tr:  98.98%, tr_best:  99.49%, epoch time: 75.60 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1520%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0493%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 204291  14.905%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss: 10.624826/ 68.338600, val:  42.08%, val_best:  51.25%, tr:  98.98%, tr_best:  99.49%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0335%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5263%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 205747  14.905%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss: 10.268148/ 66.711777, val:  32.50%, val_best:  51.25%, tr:  99.18%, tr_best:  99.49%, epoch time: 75.54 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5329%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0377%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 207112  14.898%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss: 10.295712/ 87.285164, val:  36.67%, val_best:  51.25%, tr:  98.98%, tr_best:  99.49%, epoch time: 75.81 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3160%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6573%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 208496  14.893%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss: 10.094151/ 73.520264, val:  36.67%, val_best:  51.25%, tr:  99.18%, tr_best:  99.49%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2499%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1845%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 209887  14.888%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  9.934833/ 73.208191, val:  35.00%, val_best:  51.25%, tr:  98.88%, tr_best:  99.49%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8502%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5991%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 211222  14.880%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss: 10.496113/ 63.895184, val:  38.33%, val_best:  51.25%, tr:  98.77%, tr_best:  99.49%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2615%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5388%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 212599  14.874%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss: 10.359254/ 70.789276, val:  41.67%, val_best:  51.25%, tr:  98.88%, tr_best:  99.49%, epoch time: 75.55 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4117%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2427%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 213982  14.869%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  9.956496/112.795235, val:  29.17%, val_best:  51.25%, tr:  99.08%, tr_best:  99.49%, epoch time: 75.76 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1849%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5805%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 215352  14.863%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss: 10.246094/ 58.274044, val:  37.50%, val_best:  51.25%, tr:  98.77%, tr_best:  99.49%, epoch time: 75.69 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1275%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9772%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 216721  14.857%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss: 10.179550/ 93.043526, val:  40.42%, val_best:  51.25%, tr:  98.77%, tr_best:  99.49%, epoch time: 75.89 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8951%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5235%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 218088  14.851%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss: 10.449059/ 75.036110, val:  32.08%, val_best:  51.25%, tr:  98.88%, tr_best:  99.49%, epoch time: 74.87 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8237%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3262%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 219450  14.845%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss: 10.420587/ 59.667896, val:  39.17%, val_best:  51.25%, tr:  98.57%, tr_best:  99.49%, epoch time: 73.43 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2819%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0320%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 220807  14.838%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss: 10.866479/ 63.279568, val:  34.58%, val_best:  51.25%, tr:  98.67%, tr_best:  99.49%, epoch time: 74.15 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9603%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1319%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 222207  14.835%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss: 10.635648/ 64.928299, val:  40.42%, val_best:  51.25%, tr:  98.77%, tr_best:  99.49%, epoch time: 74.04 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0578%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1073%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 223590  14.830%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss: 10.470675/ 67.420677, val:  43.75%, val_best:  51.25%, tr:  98.67%, tr_best:  99.49%, epoch time: 73.78 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2352%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7583%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 224961  14.825%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss: 11.077868/ 74.123322, val:  35.42%, val_best:  51.25%, tr:  98.77%, tr_best:  99.49%, epoch time: 73.88 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2160%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2279%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 226378  14.823%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss: 10.179497/ 71.679787, val:  39.17%, val_best:  51.25%, tr:  98.57%, tr_best:  99.49%, epoch time: 73.97 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2597%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5845%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 227681  14.813%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss: 10.839034/ 49.011646, val:  43.75%, val_best:  51.25%, tr:  99.18%, tr_best:  99.49%, epoch time: 73.43 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0550%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.1242%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 229040  14.807%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss: 10.385250/ 94.033157, val:  28.75%, val_best:  51.25%, tr:  99.49%, tr_best:  99.49%, epoch time: 73.84 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1087%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2760%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 230400  14.801%\n",
      "fc layer 2 self.abs_max_out: 10103.0\n",
      "fc layer 2 self.abs_max_out: 10740.0\n",
      "lif layer 2 self.abs_max_v: 18251.5\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss: 11.158046/ 98.864586, val:  35.83%, val_best:  51.25%, tr:  99.18%, tr_best:  99.49%, epoch time: 73.55 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0914%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6703%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 231775  14.797%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss: 10.251602/ 65.680099, val:  44.17%, val_best:  51.25%, tr:  98.77%, tr_best:  99.49%, epoch time: 73.32 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5749%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.8927%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 233093  14.788%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss: 10.564199/ 60.645477, val:  42.92%, val_best:  51.25%, tr:  98.88%, tr_best:  99.49%, epoch time: 73.30 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2858%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7416%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 234458  14.783%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss: 10.690349/ 75.517685, val:  42.08%, val_best:  51.25%, tr:  98.88%, tr_best:  99.49%, epoch time: 73.62 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9422%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2724%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 235868  14.781%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss: 10.295546/ 74.922821, val:  39.58%, val_best:  51.25%, tr:  99.39%, tr_best:  99.49%, epoch time: 73.74 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3005%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9208%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 237208  14.774%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss: 10.291297/ 65.079185, val:  50.42%, val_best:  51.25%, tr:  98.57%, tr_best:  99.49%, epoch time: 73.37 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1632%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5740%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 238554  14.768%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss: 11.138734/101.867447, val:  25.42%, val_best:  51.25%, tr:  98.67%, tr_best:  99.49%, epoch time: 73.09 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2014%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.4683%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 239962  14.766%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss: 10.586134/ 88.503311, val:  39.17%, val_best:  51.25%, tr:  98.67%, tr_best:  99.49%, epoch time: 73.83 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2342%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6911%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 241344  14.762%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss: 10.453366/ 82.395592, val:  41.25%, val_best:  51.25%, tr:  98.26%, tr_best:  99.49%, epoch time: 73.42 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0374%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.4715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 242709  14.757%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss: 10.291585/ 86.584671, val:  32.08%, val_best:  51.25%, tr:  98.98%, tr_best:  99.49%, epoch time: 73.22 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0505%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.9848%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 244024  14.749%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss: 10.256281/ 74.528229, val:  40.83%, val_best:  51.25%, tr:  98.37%, tr_best:  99.49%, epoch time: 73.57 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3001%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6623%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 245336  14.741%\n",
      "fc layer 3 self.abs_max_out: 552.0\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss: 11.050989/ 75.941711, val:  32.08%, val_best:  51.25%, tr:  98.88%, tr_best:  99.49%, epoch time: 73.03 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1141%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5089%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9075%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 246770  14.741%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss: 10.648295/ 81.620369, val:  42.08%, val_best:  51.25%, tr:  99.49%, tr_best:  99.49%, epoch time: 73.75 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1294%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5967%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 248184  14.739%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss: 11.073014/ 55.819195, val:  41.25%, val_best:  51.25%, tr:  99.18%, tr_best:  99.49%, epoch time: 72.09 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3528%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6217%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 249605  14.738%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss: 10.350798/ 75.551254, val:  38.33%, val_best:  51.25%, tr:  98.67%, tr_best:  99.49%, epoch time: 72.80 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8506%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.3660%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 250925  14.730%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss: 11.438691/ 54.243484, val:  50.83%, val_best:  51.25%, tr:  99.39%, tr_best:  99.49%, epoch time: 72.46 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6307%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.0274%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 252341  14.729%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss: 11.168694/ 54.831676, val:  46.67%, val_best:  51.25%, tr:  98.77%, tr_best:  99.49%, epoch time: 72.01 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7554%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.9281%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 253755  14.727%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss: 11.037728/ 75.859901, val:  40.42%, val_best:  51.25%, tr:  99.08%, tr_best:  99.49%, epoch time: 73.25 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8322%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.9041%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 255120  14.723%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss: 10.943956/ 54.972424, val:  50.00%, val_best:  51.25%, tr:  99.08%, tr_best:  99.49%, epoch time: 73.14 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0312%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2915%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.0969%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 256443  14.716%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss: 10.758757/107.630226, val:  26.25%, val_best:  51.25%, tr:  98.57%, tr_best:  99.49%, epoch time: 73.24 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0376%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1653%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.0644%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 257785  14.710%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss: 11.224328/109.778343, val:  21.67%, val_best:  51.25%, tr:  99.18%, tr_best:  99.49%, epoch time: 73.42 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9888%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.1103%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 259156  14.706%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss: 10.708861/ 71.206566, val:  42.92%, val_best:  51.25%, tr:  99.28%, tr_best:  99.49%, epoch time: 73.30 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8783%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.7699%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 260526  14.702%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss: 10.560481/ 75.594833, val:  35.42%, val_best:  51.25%, tr:  98.47%, tr_best:  99.49%, epoch time: 73.15 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9108%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2970%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 261858  14.696%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss: 10.717035/ 55.450851, val:  38.75%, val_best:  51.25%, tr:  99.49%, tr_best:  99.49%, epoch time: 70.22 seconds, 1.17 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6556%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8587%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 263180  14.690%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss: 11.234077/124.569893, val:  29.17%, val_best:  51.25%, tr:  99.39%, tr_best:  99.49%, epoch time: 73.08 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0426%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7389%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.1098%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 264556  14.686%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss: 11.704780/ 70.321854, val:  38.75%, val_best:  51.25%, tr:  99.18%, tr_best:  99.49%, epoch time: 73.72 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1499%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.1324%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 265927  14.683%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss: 11.413425/ 65.458443, val:  42.50%, val_best:  51.25%, tr:  99.18%, tr_best:  99.49%, epoch time: 74.39 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8746%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.9295%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 267315  14.680%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss: 11.902333/ 78.794319, val:  40.00%, val_best:  51.25%, tr:  99.49%, tr_best:  99.49%, epoch time: 73.30 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1143%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.5789%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 268757  14.680%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss: 11.169565/ 95.008308, val:  36.67%, val_best:  51.25%, tr:  98.77%, tr_best:  99.49%, epoch time: 73.52 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1543%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.4301%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 270140  14.677%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss: 11.233828/ 67.951233, val:  43.33%, val_best:  51.25%, tr:  98.67%, tr_best:  99.49%, epoch time: 73.56 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9105%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.3942%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 271529  14.675%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss: 11.292188/ 74.351151, val:  39.58%, val_best:  51.25%, tr:  98.77%, tr_best:  99.49%, epoch time: 73.62 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7260%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.4889%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 272925  14.673%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss: 11.503680/ 65.892059, val:  43.75%, val_best:  51.25%, tr:  99.18%, tr_best:  99.49%, epoch time: 74.04 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6947%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.3893%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 274353  14.672%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss: 11.143437/ 64.489769, val:  44.58%, val_best:  51.25%, tr:  98.88%, tr_best:  99.49%, epoch time: 74.18 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6315%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.4612%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 275764  14.671%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss: 11.492061/ 87.088867, val:  31.25%, val_best:  51.25%, tr:  99.39%, tr_best:  99.49%, epoch time: 73.92 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0491%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.6330%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 277161  14.669%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss: 11.086543/ 92.944595, val:  28.75%, val_best:  51.25%, tr:  98.98%, tr_best:  99.49%, epoch time: 73.06 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7892%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.5107%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 278511  14.664%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss: 11.368518/114.963974, val:  31.25%, val_best:  51.25%, tr:  98.98%, tr_best:  99.49%, epoch time: 73.03 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7139%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.6585%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 279895  14.661%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss: 11.247113/ 64.536850, val:  45.00%, val_best:  51.25%, tr:  99.39%, tr_best:  99.49%, epoch time: 73.64 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9901%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5071%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 281308  14.660%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss: 11.344640/ 78.295639, val:  33.75%, val_best:  51.25%, tr:  98.88%, tr_best:  99.49%, epoch time: 73.98 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0629%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3364%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 282752  14.661%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss: 11.047164/ 80.204292, val:  33.75%, val_best:  51.25%, tr:  98.67%, tr_best:  99.49%, epoch time: 73.63 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7311%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0108%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 284168  14.660%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss: 11.595167/ 88.938545, val:  35.00%, val_best:  51.25%, tr:  98.88%, tr_best:  99.49%, epoch time: 73.32 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8217%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3295%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 285635  14.661%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss: 11.217225/ 89.528412, val:  39.17%, val_best:  51.25%, tr:  99.08%, tr_best:  99.49%, epoch time: 73.36 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6747%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1753%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced3e9313d3f4cb2bba1c7fe6ba5da73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÉ‚ñÉ‚ñÅ‚ñÖ‚ñÇ‚ñá‚ñÖ‚ñÜ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÇ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñÉ‚ñá‚ñÑ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñÉ‚ñÖ‚ñÉ‚ñÖ</td></tr><tr><td>tr_acc</td><td>‚ñá‚ñÑ‚ñÅ‚ñÖ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñá‚ñÑ‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñà‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñÑ‚ñÖ‚ñá‚ñà‚ñÜ‚ñá‚ñá</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÖ‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÉ‚ñÉ‚ñÅ‚ñÖ‚ñÇ‚ñá‚ñÖ‚ñÜ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÇ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñÉ‚ñá‚ñÑ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñÉ‚ñÖ‚ñÉ‚ñÖ</td></tr><tr><td>val_loss</td><td>‚ñÑ‚ñÇ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñá‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñà‚ñÜ‚ñÖ‚ñÖ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99081</td></tr><tr><td>tr_epoch_loss</td><td>11.21723</td></tr><tr><td>val_acc_best</td><td>0.5125</td></tr><tr><td>val_acc_now</td><td>0.39167</td></tr><tr><td>val_loss</td><td>89.52841</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">usual-sweep-1</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5q330707' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5q330707</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_010553-5q330707/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vxnn1kzx with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.046875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0234375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_051751-vxnn1kzx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vxnn1kzx' target=\"_blank\">dry-sweep-5</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vxnn1kzx' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vxnn1kzx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': '20251214_051800_134', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 32, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 32, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 1, 'lif_layer_v_threshold2': 32, 'init_scaling': [0.03125, 0.046875, 0.0234375], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 32, self.v_threshold 32\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 1, self.v_threshold 32\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=32, v_reset=10000, sg_width=32, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=32, v_reset=10000, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 46.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 6.0\n",
      "lif layer 2 self.abs_max_v: 6.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 59.5\n",
      "fc layer 2 self.abs_max_out: 36.0\n",
      "lif layer 2 self.abs_max_v: 35.5\n",
      "fc layer 3 self.abs_max_out: 2.0\n",
      "fc layer 1 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 69.0\n",
      "fc layer 2 self.abs_max_out: 48.0\n",
      "lif layer 2 self.abs_max_v: 53.0\n",
      "fc layer 3 self.abs_max_out: 10.0\n",
      "fc layer 1 self.abs_max_out: 62.0\n",
      "lif layer 1 self.abs_max_v: 90.0\n",
      "lif layer 2 self.abs_max_v: 59.5\n",
      "fc layer 1 self.abs_max_out: 101.0\n",
      "lif layer 1 self.abs_max_v: 109.0\n",
      "fc layer 2 self.abs_max_out: 50.0\n",
      "lif layer 2 self.abs_max_v: 66.5\n",
      "fc layer 3 self.abs_max_out: 16.0\n",
      "fc layer 1 self.abs_max_out: 147.0\n",
      "lif layer 1 self.abs_max_v: 147.0\n",
      "fc layer 2 self.abs_max_out: 65.0\n",
      "lif layer 2 self.abs_max_v: 97.0\n",
      "fc layer 1 self.abs_max_out: 200.0\n",
      "lif layer 1 self.abs_max_v: 200.0\n",
      "lif layer 2 self.abs_max_v: 113.5\n",
      "fc layer 3 self.abs_max_out: 18.0\n",
      "fc layer 2 self.abs_max_out: 70.0\n",
      "fc layer 2 self.abs_max_out: 75.0\n",
      "fc layer 3 self.abs_max_out: 28.0\n",
      "fc layer 2 self.abs_max_out: 89.0\n",
      "fc layer 1 self.abs_max_out: 205.0\n",
      "lif layer 1 self.abs_max_v: 205.0\n",
      "fc layer 3 self.abs_max_out: 38.0\n",
      "fc layer 1 self.abs_max_out: 271.0\n",
      "lif layer 1 self.abs_max_v: 271.0\n",
      "lif layer 2 self.abs_max_v: 121.5\n",
      "fc layer 2 self.abs_max_out: 117.0\n",
      "lif layer 2 self.abs_max_v: 124.0\n",
      "lif layer 2 self.abs_max_v: 137.0\n",
      "lif layer 1 self.abs_max_v: 281.5\n",
      "lif layer 2 self.abs_max_v: 143.5\n",
      "lif layer 2 self.abs_max_v: 159.0\n",
      "lif layer 1 self.abs_max_v: 300.5\n",
      "lif layer 2 self.abs_max_v: 159.5\n",
      "fc layer 3 self.abs_max_out: 42.0\n",
      "fc layer 2 self.abs_max_out: 149.0\n",
      "fc layer 2 self.abs_max_out: 154.0\n",
      "lif layer 2 self.abs_max_v: 184.5\n",
      "lif layer 2 self.abs_max_v: 197.5\n",
      "fc layer 3 self.abs_max_out: 44.0\n",
      "lif layer 2 self.abs_max_v: 200.5\n",
      "fc layer 3 self.abs_max_out: 49.0\n",
      "lif layer 1 self.abs_max_v: 306.5\n",
      "fc layer 2 self.abs_max_out: 159.0\n",
      "lif layer 2 self.abs_max_v: 211.5\n",
      "fc layer 3 self.abs_max_out: 53.0\n",
      "lif layer 1 self.abs_max_v: 372.5\n",
      "fc layer 2 self.abs_max_out: 202.0\n",
      "lif layer 2 self.abs_max_v: 251.0\n",
      "fc layer 1 self.abs_max_out: 274.0\n",
      "lif layer 1 self.abs_max_v: 378.5\n",
      "lif layer 2 self.abs_max_v: 292.5\n",
      "fc layer 1 self.abs_max_out: 343.0\n",
      "lif layer 1 self.abs_max_v: 416.5\n",
      "lif layer 2 self.abs_max_v: 311.5\n",
      "fc layer 3 self.abs_max_out: 65.0\n",
      "fc layer 2 self.abs_max_out: 210.0\n",
      "lif layer 2 self.abs_max_v: 331.0\n",
      "fc layer 3 self.abs_max_out: 67.0\n",
      "fc layer 3 self.abs_max_out: 78.0\n",
      "fc layer 2 self.abs_max_out: 221.0\n",
      "fc layer 3 self.abs_max_out: 89.0\n",
      "fc layer 1 self.abs_max_out: 401.0\n",
      "fc layer 3 self.abs_max_out: 94.0\n",
      "fc layer 2 self.abs_max_out: 222.0\n",
      "lif layer 1 self.abs_max_v: 499.5\n",
      "fc layer 2 self.abs_max_out: 225.0\n",
      "lif layer 2 self.abs_max_v: 331.5\n",
      "fc layer 1 self.abs_max_out: 405.0\n",
      "fc layer 2 self.abs_max_out: 252.0\n",
      "lif layer 2 self.abs_max_v: 343.0\n",
      "lif layer 2 self.abs_max_v: 368.5\n",
      "lif layer 2 self.abs_max_v: 379.5\n",
      "fc layer 3 self.abs_max_out: 107.0\n",
      "lif layer 1 self.abs_max_v: 527.0\n",
      "lif layer 2 self.abs_max_v: 381.0\n",
      "lif layer 2 self.abs_max_v: 384.5\n",
      "fc layer 3 self.abs_max_out: 108.0\n",
      "fc layer 3 self.abs_max_out: 109.0\n",
      "lif layer 1 self.abs_max_v: 558.0\n",
      "lif layer 1 self.abs_max_v: 611.0\n",
      "lif layer 1 self.abs_max_v: 695.5\n",
      "lif layer 1 self.abs_max_v: 705.0\n",
      "fc layer 2 self.abs_max_out: 329.0\n",
      "fc layer 2 self.abs_max_out: 336.0\n",
      "fc layer 3 self.abs_max_out: 125.0\n",
      "lif layer 2 self.abs_max_v: 406.5\n",
      "fc layer 3 self.abs_max_out: 127.0\n",
      "fc layer 3 self.abs_max_out: 130.0\n",
      "fc layer 3 self.abs_max_out: 147.0\n",
      "fc layer 3 self.abs_max_out: 172.0\n",
      "lif layer 2 self.abs_max_v: 420.0\n",
      "lif layer 2 self.abs_max_v: 422.0\n",
      "lif layer 2 self.abs_max_v: 430.0\n",
      "lif layer 2 self.abs_max_v: 457.0\n",
      "lif layer 2 self.abs_max_v: 473.0\n",
      "lif layer 2 self.abs_max_v: 512.0\n",
      "lif layer 2 self.abs_max_v: 521.5\n",
      "lif layer 2 self.abs_max_v: 524.0\n",
      "fc layer 3 self.abs_max_out: 185.0\n",
      "fc layer 1 self.abs_max_out: 414.0\n",
      "fc layer 1 self.abs_max_out: 420.0\n",
      "lif layer 1 self.abs_max_v: 719.5\n",
      "lif layer 2 self.abs_max_v: 529.0\n",
      "lif layer 2 self.abs_max_v: 554.0\n",
      "lif layer 2 self.abs_max_v: 563.5\n",
      "lif layer 2 self.abs_max_v: 568.5\n",
      "lif layer 1 self.abs_max_v: 764.5\n",
      "lif layer 2 self.abs_max_v: 573.5\n",
      "lif layer 2 self.abs_max_v: 576.5\n",
      "fc layer 3 self.abs_max_out: 198.0\n",
      "fc layer 1 self.abs_max_out: 504.0\n",
      "fc layer 3 self.abs_max_out: 205.0\n",
      "fc layer 3 self.abs_max_out: 240.0\n",
      "fc layer 3 self.abs_max_out: 242.0\n",
      "fc layer 3 self.abs_max_out: 248.0\n",
      "fc layer 1 self.abs_max_out: 522.0\n",
      "fc layer 2 self.abs_max_out: 347.0\n",
      "fc layer 2 self.abs_max_out: 359.0\n",
      "fc layer 2 self.abs_max_out: 366.0\n",
      "lif layer 2 self.abs_max_v: 598.0\n",
      "lif layer 2 self.abs_max_v: 622.0\n",
      "lif layer 2 self.abs_max_v: 640.0\n",
      "lif layer 2 self.abs_max_v: 652.5\n",
      "lif layer 2 self.abs_max_v: 678.5\n",
      "fc layer 2 self.abs_max_out: 370.0\n",
      "lif layer 2 self.abs_max_v: 698.5\n",
      "lif layer 2 self.abs_max_v: 704.5\n",
      "fc layer 3 self.abs_max_out: 249.0\n",
      "fc layer 3 self.abs_max_out: 254.0\n",
      "fc layer 3 self.abs_max_out: 280.0\n",
      "fc layer 3 self.abs_max_out: 292.0\n",
      "fc layer 3 self.abs_max_out: 295.0\n",
      "fc layer 2 self.abs_max_out: 371.0\n",
      "fc layer 2 self.abs_max_out: 407.0\n",
      "lif layer 2 self.abs_max_v: 725.0\n",
      "fc layer 2 self.abs_max_out: 417.0\n",
      "fc layer 2 self.abs_max_out: 438.0\n",
      "lif layer 1 self.abs_max_v: 823.5\n",
      "fc layer 2 self.abs_max_out: 453.0\n",
      "lif layer 2 self.abs_max_v: 747.0\n",
      "lif layer 1 self.abs_max_v: 913.0\n",
      "fc layer 1 self.abs_max_out: 563.0\n",
      "fc layer 2 self.abs_max_out: 460.0\n",
      "fc layer 2 self.abs_max_out: 468.0\n",
      "lif layer 2 self.abs_max_v: 770.0\n",
      "fc layer 1 self.abs_max_out: 601.0\n",
      "lif layer 1 self.abs_max_v: 973.5\n",
      "lif layer 1 self.abs_max_v: 983.0\n",
      "lif layer 2 self.abs_max_v: 806.5\n",
      "fc layer 2 self.abs_max_out: 490.0\n",
      "fc layer 3 self.abs_max_out: 313.0\n",
      "fc layer 3 self.abs_max_out: 322.0\n",
      "fc layer 3 self.abs_max_out: 324.0\n",
      "fc layer 2 self.abs_max_out: 493.0\n",
      "fc layer 2 self.abs_max_out: 495.0\n",
      "fc layer 2 self.abs_max_out: 498.0\n",
      "fc layer 2 self.abs_max_out: 503.0\n",
      "fc layer 3 self.abs_max_out: 333.0\n",
      "fc layer 1 self.abs_max_out: 681.0\n",
      "lif layer 1 self.abs_max_v: 1109.5\n",
      "lif layer 1 self.abs_max_v: 1128.0\n",
      "lif layer 2 self.abs_max_v: 830.5\n",
      "fc layer 2 self.abs_max_out: 524.0\n",
      "fc layer 3 self.abs_max_out: 370.0\n",
      "fc layer 2 self.abs_max_out: 538.0\n",
      "fc layer 2 self.abs_max_out: 539.0\n",
      "fc layer 2 self.abs_max_out: 540.0\n",
      "fc layer 2 self.abs_max_out: 570.0\n",
      "fc layer 1 self.abs_max_out: 688.0\n",
      "fc layer 2 self.abs_max_out: 588.0\n",
      "fc layer 2 self.abs_max_out: 589.0\n",
      "fc layer 2 self.abs_max_out: 607.0\n",
      "lif layer 2 self.abs_max_v: 854.0\n",
      "lif layer 2 self.abs_max_v: 901.0\n",
      "lif layer 2 self.abs_max_v: 902.0\n",
      "fc layer 2 self.abs_max_out: 608.0\n",
      "fc layer 2 self.abs_max_out: 640.0\n",
      "fc layer 2 self.abs_max_out: 650.0\n",
      "fc layer 2 self.abs_max_out: 682.0\n",
      "fc layer 2 self.abs_max_out: 687.0\n",
      "fc layer 2 self.abs_max_out: 714.0\n",
      "lif layer 2 self.abs_max_v: 904.0\n",
      "fc layer 1 self.abs_max_out: 689.0\n",
      "lif layer 1 self.abs_max_v: 1134.5\n",
      "lif layer 1 self.abs_max_v: 1144.0\n",
      "fc layer 2 self.abs_max_out: 716.0\n",
      "lif layer 1 self.abs_max_v: 1160.5\n",
      "lif layer 1 self.abs_max_v: 1165.5\n",
      "lif layer 1 self.abs_max_v: 1200.0\n",
      "lif layer 1 self.abs_max_v: 1203.0\n",
      "fc layer 2 self.abs_max_out: 728.0\n",
      "fc layer 2 self.abs_max_out: 739.0\n",
      "lif layer 2 self.abs_max_v: 983.0\n",
      "fc layer 2 self.abs_max_out: 740.0\n",
      "fc layer 2 self.abs_max_out: 815.0\n",
      "fc layer 1 self.abs_max_out: 778.0\n",
      "fc layer 1 self.abs_max_out: 795.0\n",
      "fc layer 1 self.abs_max_out: 855.0\n",
      "lif layer 2 self.abs_max_v: 1004.0\n",
      "lif layer 2 self.abs_max_v: 1050.0\n",
      "lif layer 2 self.abs_max_v: 1059.0\n",
      "lif layer 2 self.abs_max_v: 1076.5\n",
      "lif layer 2 self.abs_max_v: 1093.5\n",
      "lif layer 2 self.abs_max_v: 1098.0\n",
      "fc layer 2 self.abs_max_out: 819.0\n",
      "lif layer 2 self.abs_max_v: 1129.5\n",
      "lif layer 1 self.abs_max_v: 1257.0\n",
      "lif layer 1 self.abs_max_v: 1273.5\n",
      "lif layer 1 self.abs_max_v: 1396.0\n",
      "lif layer 1 self.abs_max_v: 1458.0\n",
      "lif layer 1 self.abs_max_v: 1467.0\n",
      "lif layer 1 self.abs_max_v: 1471.5\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss:  9.828664/ 75.036896, val:  33.33%, val_best:  33.33%, tr:  98.47%, tr_best:  98.47%, epoch time: 74.46 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6460%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.4421%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 1458  14.893%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 848.0\n",
      "fc layer 2 self.abs_max_out: 864.0\n",
      "lif layer 2 self.abs_max_v: 1220.0\n",
      "lif layer 2 self.abs_max_v: 1228.5\n",
      "fc layer 2 self.abs_max_out: 873.0\n",
      "fc layer 2 self.abs_max_out: 903.0\n",
      "fc layer 2 self.abs_max_out: 904.0\n",
      "fc layer 2 self.abs_max_out: 922.0\n",
      "fc layer 2 self.abs_max_out: 937.0\n",
      "fc layer 2 self.abs_max_out: 951.0\n",
      "fc layer 2 self.abs_max_out: 963.0\n",
      "fc layer 2 self.abs_max_out: 972.0\n",
      "fc layer 2 self.abs_max_out: 978.0\n",
      "lif layer 2 self.abs_max_v: 1256.0\n",
      "lif layer 2 self.abs_max_v: 1262.0\n",
      "lif layer 2 self.abs_max_v: 1358.0\n",
      "lif layer 2 self.abs_max_v: 1409.5\n",
      "lif layer 2 self.abs_max_v: 1455.0\n",
      "fc layer 2 self.abs_max_out: 984.0\n",
      "fc layer 2 self.abs_max_out: 997.0\n",
      "fc layer 2 self.abs_max_out: 1010.0\n",
      "fc layer 2 self.abs_max_out: 1021.0\n",
      "fc layer 2 self.abs_max_out: 1029.0\n",
      "fc layer 2 self.abs_max_out: 1051.0\n",
      "fc layer 2 self.abs_max_out: 1071.0\n",
      "fc layer 2 self.abs_max_out: 1114.0\n",
      "fc layer 2 self.abs_max_out: 1117.0\n",
      "fc layer 2 self.abs_max_out: 1123.0\n",
      "fc layer 2 self.abs_max_out: 1144.0\n",
      "fc layer 2 self.abs_max_out: 1206.0\n",
      "fc layer 3 self.abs_max_out: 379.0\n",
      "fc layer 3 self.abs_max_out: 382.0\n",
      "fc layer 3 self.abs_max_out: 400.0\n",
      "lif layer 2 self.abs_max_v: 1462.0\n",
      "lif layer 2 self.abs_max_v: 1511.5\n",
      "lif layer 2 self.abs_max_v: 1531.5\n",
      "lif layer 2 self.abs_max_v: 1532.5\n",
      "lif layer 2 self.abs_max_v: 1600.5\n",
      "lif layer 2 self.abs_max_v: 1684.5\n",
      "lif layer 2 self.abs_max_v: 1710.5\n",
      "lif layer 2 self.abs_max_v: 1773.5\n",
      "fc layer 1 self.abs_max_out: 866.0\n",
      "lif layer 1 self.abs_max_v: 1500.5\n",
      "lif layer 1 self.abs_max_v: 1544.5\n",
      "fc layer 3 self.abs_max_out: 452.0\n",
      "lif layer 2 self.abs_max_v: 1774.5\n",
      "lif layer 1 self.abs_max_v: 1545.5\n",
      "fc layer 1 self.abs_max_out: 873.0\n",
      "fc layer 1 self.abs_max_out: 879.0\n",
      "fc layer 1 self.abs_max_out: 880.0\n",
      "lif layer 1 self.abs_max_v: 1630.5\n",
      "lif layer 1 self.abs_max_v: 1678.5\n",
      "lif layer 1 self.abs_max_v: 1689.5\n",
      "lif layer 1 self.abs_max_v: 1705.0\n",
      "fc layer 2 self.abs_max_out: 1209.0\n",
      "lif layer 2 self.abs_max_v: 1890.0\n",
      "fc layer 2 self.abs_max_out: 1234.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 10.118023/ 93.332741, val:  37.50%, val_best:  37.50%, tr:  99.39%, tr_best:  99.39%, epoch time: 70.98 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7469%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.9097%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 2776  14.178%\n",
      "fc layer 2 self.abs_max_out: 1244.0\n",
      "fc layer 1 self.abs_max_out: 932.0\n",
      "fc layer 2 self.abs_max_out: 1265.0\n",
      "fc layer 2 self.abs_max_out: 1309.0\n",
      "fc layer 2 self.abs_max_out: 1313.0\n",
      "lif layer 2 self.abs_max_v: 1975.5\n",
      "fc layer 2 self.abs_max_out: 1350.0\n",
      "lif layer 2 self.abs_max_v: 2022.5\n",
      "fc layer 1 self.abs_max_out: 1015.0\n",
      "lif layer 1 self.abs_max_v: 1799.0\n",
      "fc layer 2 self.abs_max_out: 1363.0\n",
      "fc layer 2 self.abs_max_out: 1428.0\n",
      "lif layer 2 self.abs_max_v: 2174.0\n",
      "lif layer 2 self.abs_max_v: 2249.0\n",
      "fc layer 2 self.abs_max_out: 1450.0\n",
      "lif layer 2 self.abs_max_v: 2274.0\n",
      "fc layer 2 self.abs_max_out: 1475.0\n",
      "fc layer 1 self.abs_max_out: 1017.0\n",
      "fc layer 1 self.abs_max_out: 1104.0\n",
      "lif layer 1 self.abs_max_v: 1915.5\n",
      "lif layer 1 self.abs_max_v: 1974.5\n",
      "lif layer 1 self.abs_max_v: 2005.0\n",
      "lif layer 1 self.abs_max_v: 2105.5\n",
      "lif layer 1 self.abs_max_v: 2107.0\n",
      "fc layer 2 self.abs_max_out: 1483.0\n",
      "lif layer 2 self.abs_max_v: 2291.0\n",
      "fc layer 2 self.abs_max_out: 1509.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 10.795777/ 69.621468, val:  31.25%, val_best:  37.50%, tr:  98.57%, tr_best:  99.39%, epoch time: 73.04 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.4996%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.2055%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 4115  14.011%\n",
      "fc layer 2 self.abs_max_out: 1540.0\n",
      "fc layer 3 self.abs_max_out: 471.0\n",
      "lif layer 2 self.abs_max_v: 2364.0\n",
      "lif layer 2 self.abs_max_v: 2464.5\n",
      "fc layer 2 self.abs_max_out: 1572.0\n",
      "lif layer 2 self.abs_max_v: 2480.0\n",
      "lif layer 2 self.abs_max_v: 2730.0\n",
      "fc layer 2 self.abs_max_out: 1581.0\n",
      "fc layer 2 self.abs_max_out: 1608.0\n",
      "fc layer 2 self.abs_max_out: 1657.0\n",
      "fc layer 2 self.abs_max_out: 1661.0\n",
      "lif layer 2 self.abs_max_v: 3001.5\n",
      "fc layer 2 self.abs_max_out: 1699.0\n",
      "fc layer 2 self.abs_max_out: 1737.0\n",
      "fc layer 2 self.abs_max_out: 1742.0\n",
      "fc layer 2 self.abs_max_out: 1781.0\n",
      "fc layer 2 self.abs_max_out: 1786.0\n",
      "fc layer 2 self.abs_max_out: 1830.0\n",
      "fc layer 2 self.abs_max_out: 1867.0\n",
      "fc layer 2 self.abs_max_out: 1874.0\n",
      "fc layer 2 self.abs_max_out: 1878.0\n",
      "fc layer 2 self.abs_max_out: 1879.0\n",
      "fc layer 1 self.abs_max_out: 1108.0\n",
      "fc layer 1 self.abs_max_out: 1281.0\n",
      "lif layer 1 self.abs_max_v: 2197.5\n",
      "lif layer 1 self.abs_max_v: 2329.0\n",
      "lif layer 1 self.abs_max_v: 2397.5\n",
      "fc layer 2 self.abs_max_out: 1913.0\n",
      "lif layer 2 self.abs_max_v: 3162.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss: 10.974975/ 70.592957, val:  28.75%, val_best:  37.50%, tr:  98.98%, tr_best:  99.39%, epoch time: 73.66 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3605%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8734%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 5508  14.065%\n",
      "fc layer 2 self.abs_max_out: 1981.0\n",
      "fc layer 2 self.abs_max_out: 1985.0\n",
      "fc layer 2 self.abs_max_out: 2002.0\n",
      "fc layer 2 self.abs_max_out: 2011.0\n",
      "lif layer 2 self.abs_max_v: 3327.0\n",
      "lif layer 2 self.abs_max_v: 3356.5\n",
      "fc layer 2 self.abs_max_out: 2024.0\n",
      "fc layer 2 self.abs_max_out: 2033.0\n",
      "fc layer 2 self.abs_max_out: 2060.0\n",
      "fc layer 2 self.abs_max_out: 2061.0\n",
      "fc layer 2 self.abs_max_out: 2066.0\n",
      "fc layer 2 self.abs_max_out: 2105.0\n",
      "fc layer 2 self.abs_max_out: 2116.0\n",
      "fc layer 2 self.abs_max_out: 2176.0\n",
      "lif layer 2 self.abs_max_v: 3476.5\n",
      "lif layer 2 self.abs_max_v: 3636.5\n",
      "fc layer 2 self.abs_max_out: 2189.0\n",
      "fc layer 2 self.abs_max_out: 2202.0\n",
      "fc layer 2 self.abs_max_out: 2287.0\n",
      "fc layer 2 self.abs_max_out: 2291.0\n",
      "fc layer 2 self.abs_max_out: 2294.0\n",
      "fc layer 2 self.abs_max_out: 2299.0\n",
      "fc layer 2 self.abs_max_out: 2348.0\n",
      "fc layer 2 self.abs_max_out: 2398.0\n",
      "fc layer 2 self.abs_max_out: 2404.0\n",
      "fc layer 2 self.abs_max_out: 2411.0\n",
      "fc layer 2 self.abs_max_out: 2429.0\n",
      "fc layer 2 self.abs_max_out: 2438.0\n",
      "fc layer 2 self.abs_max_out: 2463.0\n",
      "lif layer 1 self.abs_max_v: 2409.0\n",
      "fc layer 2 self.abs_max_out: 2510.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss: 10.803593/ 54.234291, val:  39.58%, val_best:  39.58%, tr:  98.88%, tr_best:  99.39%, epoch time: 73.77 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5355%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.3915%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 6872  14.039%\n",
      "fc layer 2 self.abs_max_out: 2548.0\n",
      "fc layer 2 self.abs_max_out: 2562.0\n",
      "fc layer 2 self.abs_max_out: 2587.0\n",
      "fc layer 2 self.abs_max_out: 2632.0\n",
      "fc layer 2 self.abs_max_out: 2645.0\n",
      "fc layer 2 self.abs_max_out: 2679.0\n",
      "fc layer 2 self.abs_max_out: 2718.0\n",
      "lif layer 2 self.abs_max_v: 3710.0\n",
      "lif layer 2 self.abs_max_v: 3862.0\n",
      "fc layer 2 self.abs_max_out: 2776.0\n",
      "fc layer 1 self.abs_max_out: 1292.0\n",
      "fc layer 1 self.abs_max_out: 1300.0\n",
      "fc layer 1 self.abs_max_out: 1301.0\n",
      "lif layer 1 self.abs_max_v: 2488.0\n",
      "lif layer 2 self.abs_max_v: 4238.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss: 10.269685/ 61.646385, val:  40.83%, val_best:  40.83%, tr:  98.57%, tr_best:  99.39%, epoch time: 73.30 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.0392%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3377%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 8245  14.036%\n",
      "lif layer 2 self.abs_max_v: 4292.5\n",
      "lif layer 2 self.abs_max_v: 4453.5\n",
      "fc layer 1 self.abs_max_out: 1341.0\n",
      "fc layer 1 self.abs_max_out: 1401.0\n",
      "lif layer 1 self.abs_max_v: 2555.5\n",
      "fc layer 1 self.abs_max_out: 1450.0\n",
      "lif layer 1 self.abs_max_v: 2728.0\n",
      "fc layer 1 self.abs_max_out: 1457.0\n",
      "lif layer 1 self.abs_max_v: 2821.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  9.063657/ 73.082909, val:  42.08%, val_best:  42.08%, tr:  98.77%, tr_best:  99.39%, epoch time: 73.65 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.9099%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7374%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 9564  13.956%\n",
      "lif layer 2 self.abs_max_v: 4566.5\n",
      "fc layer 1 self.abs_max_out: 1480.0\n",
      "fc layer 1 self.abs_max_out: 1514.0\n",
      "lif layer 1 self.abs_max_v: 2849.0\n",
      "fc layer 1 self.abs_max_out: 1519.0\n",
      "lif layer 1 self.abs_max_v: 2943.5\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  8.726969/ 47.774040, val:  49.58%, val_best:  49.58%, tr:  98.88%, tr_best:  99.39%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.9244%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9689%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 10875  13.885%\n",
      "fc layer 1 self.abs_max_out: 1550.0\n",
      "fc layer 1 self.abs_max_out: 1552.0\n",
      "lif layer 1 self.abs_max_v: 3011.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  8.931780/ 57.996235, val:  31.67%, val_best:  49.58%, tr:  98.47%, tr_best:  99.39%, epoch time: 73.26 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.9605%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.3167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 12265  13.920%\n",
      "lif layer 2 self.abs_max_v: 4573.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  8.018876/ 53.931149, val:  37.92%, val_best:  49.58%, tr:  98.67%, tr_best:  99.39%, epoch time: 73.72 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.7776%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.3961%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 13623  13.915%\n",
      "fc layer 1 self.abs_max_out: 1624.0\n",
      "lif layer 1 self.abs_max_v: 3069.5\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  7.032130/ 44.684074, val:  31.25%, val_best:  49.58%, tr:  99.08%, tr_best:  99.39%, epoch time: 73.70 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2916%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.9534%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 14867  13.805%\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  7.015522/ 38.189201, val:  49.58%, val_best:  49.58%, tr:  98.98%, tr_best:  99.39%, epoch time: 73.65 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.8285%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.5366%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 16183  13.775%\n",
      "lif layer 2 self.abs_max_v: 4764.5\n",
      "lif layer 2 self.abs_max_v: 4773.0\n",
      "lif layer 2 self.abs_max_v: 4925.5\n",
      "fc layer 1 self.abs_max_out: 1625.0\n",
      "fc layer 1 self.abs_max_out: 1648.0\n",
      "fc layer 1 self.abs_max_out: 1659.0\n",
      "fc layer 1 self.abs_max_out: 1666.0\n",
      "lif layer 1 self.abs_max_v: 3111.5\n",
      "lif layer 1 self.abs_max_v: 3194.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  6.298522/ 42.968807, val:  44.17%, val_best:  49.58%, tr:  97.75%, tr_best:  99.39%, epoch time: 73.45 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.9917%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.1819%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 17402  13.673%\n",
      "fc layer 1 self.abs_max_out: 1676.0\n",
      "fc layer 1 self.abs_max_out: 1739.0\n",
      "fc layer 1 self.abs_max_out: 1744.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  6.311778/ 56.239197, val:  37.50%, val_best:  49.58%, tr:  97.96%, tr_best:  99.39%, epoch time: 73.25 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.1356%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.3533%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 18639  13.599%\n",
      "fc layer 2 self.abs_max_out: 2877.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  5.990707/ 28.521719, val:  46.67%, val_best:  49.58%, tr:  98.77%, tr_best:  99.39%, epoch time: 73.70 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.9493%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.2624%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 19873  13.533%\n",
      "fc layer 1 self.abs_max_out: 1778.0\n",
      "lif layer 2 self.abs_max_v: 5002.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  5.929912/ 31.249044, val:  43.75%, val_best:  49.58%, tr:  98.77%, tr_best:  99.39%, epoch time: 73.70 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.0707%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.6817%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 21124  13.486%\n",
      "lif layer 2 self.abs_max_v: 5115.0\n",
      "lif layer 2 self.abs_max_v: 5120.5\n",
      "lif layer 2 self.abs_max_v: 5274.5\n",
      "fc layer 2 self.abs_max_out: 3024.0\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  5.668717/ 31.218569, val:  53.75%, val_best:  53.75%, tr:  98.88%, tr_best:  99.39%, epoch time: 73.89 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.5283%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.3172%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 22354  13.431%\n",
      "fc layer 1 self.abs_max_out: 1848.0\n",
      "lif layer 1 self.abs_max_v: 3202.0\n",
      "lif layer 1 self.abs_max_v: 3219.5\n",
      "lif layer 1 self.abs_max_v: 3312.5\n",
      "lif layer 1 self.abs_max_v: 3436.5\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  5.735778/ 29.284363, val:  54.17%, val_best:  54.17%, tr:  98.98%, tr_best:  99.39%, epoch time: 73.61 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.2198%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.6672%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 23623  13.405%\n",
      "fc layer 1 self.abs_max_out: 1873.0\n",
      "lif layer 2 self.abs_max_v: 5361.0\n",
      "fc layer 1 self.abs_max_out: 1893.0\n",
      "lif layer 1 self.abs_max_v: 3541.0\n",
      "fc layer 1 self.abs_max_out: 2037.0\n",
      "lif layer 1 self.abs_max_v: 3807.5\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  5.837214/ 33.140236, val:  44.58%, val_best:  54.17%, tr:  99.18%, tr_best:  99.39%, epoch time: 73.72 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.2785%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.4849%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 24880  13.376%\n",
      "fc layer 2 self.abs_max_out: 3101.0\n",
      "lif layer 2 self.abs_max_v: 5436.0\n",
      "lif layer 2 self.abs_max_v: 5456.0\n",
      "lif layer 2 self.abs_max_v: 5529.5\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  5.344772/ 41.648643, val:  35.83%, val_best:  54.17%, tr:  98.98%, tr_best:  99.39%, epoch time: 73.66 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.2851%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.7384%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 26040  13.299%\n",
      "fc layer 2 self.abs_max_out: 3276.0\n",
      "fc layer 2 self.abs_max_out: 3289.0\n",
      "lif layer 2 self.abs_max_v: 5686.0\n",
      "lif layer 2 self.abs_max_v: 5831.5\n",
      "lif layer 2 self.abs_max_v: 5881.5\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  5.198798/ 38.332932, val:  36.25%, val_best:  54.17%, tr:  99.18%, tr_best:  99.39%, epoch time: 73.24 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.1863%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 80.3354%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 27208  13.234%\n",
      "lif layer 2 self.abs_max_v: 5886.5\n",
      "lif layer 2 self.abs_max_v: 5967.5\n",
      "fc layer 2 self.abs_max_out: 3299.0\n",
      "fc layer 2 self.abs_max_out: 3346.0\n",
      "lif layer 2 self.abs_max_v: 6037.0\n",
      "fc layer 2 self.abs_max_out: 3724.0\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  5.323534/ 26.397867, val:  52.08%, val_best:  54.17%, tr:  98.47%, tr_best:  99.39%, epoch time: 73.22 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.4713%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.1128%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 28450  13.209%\n",
      "lif layer 2 self.abs_max_v: 6229.0\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  5.160891/ 24.009167, val:  58.33%, val_best:  58.33%, tr:  98.16%, tr_best:  99.39%, epoch time: 73.91 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.2139%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.5602%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 29709  13.194%\n",
      "lif layer 2 self.abs_max_v: 6309.0\n",
      "lif layer 2 self.abs_max_v: 6370.5\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  5.186834/ 26.966793, val:  55.00%, val_best:  58.33%, tr:  98.77%, tr_best:  99.39%, epoch time: 73.43 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.9210%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.3348%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 30945  13.170%\n",
      "lif layer 2 self.abs_max_v: 6521.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  5.077557/ 36.511646, val:  46.25%, val_best:  58.33%, tr:  99.08%, tr_best:  99.39%, epoch time: 73.16 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.5610%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.0682%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 32161  13.140%\n",
      "fc layer 2 self.abs_max_out: 3803.0\n",
      "lif layer 2 self.abs_max_v: 6903.0\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  5.275828/ 32.793594, val:  47.50%, val_best:  58.33%, tr:  98.67%, tr_best:  99.39%, epoch time: 73.63 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.5650%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.2631%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 33407  13.124%\n",
      "fc layer 2 self.abs_max_out: 3912.0\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  5.030658/ 31.149721, val:  54.17%, val_best:  58.33%, tr:  98.98%, tr_best:  99.39%, epoch time: 73.29 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.9286%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.0426%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 34602  13.090%\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  5.046921/ 26.909283, val:  48.33%, val_best:  58.33%, tr:  98.77%, tr_best:  99.39%, epoch time: 72.95 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.8581%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.0769%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 35821  13.068%\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  4.725003/ 32.030548, val:  45.00%, val_best:  58.33%, tr:  99.49%, tr_best:  99.49%, epoch time: 72.83 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.9647%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.1602%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 36999  13.032%\n",
      "fc layer 1 self.abs_max_out: 2132.0\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  4.657900/ 44.427734, val:  44.58%, val_best:  58.33%, tr:  98.77%, tr_best:  99.49%, epoch time: 72.51 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.7310%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.3096%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 38163  12.994%\n",
      "fc layer 2 self.abs_max_out: 4002.0\n",
      "fc layer 1 self.abs_max_out: 2182.0\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  4.697694/ 26.249428, val:  50.83%, val_best:  58.33%, tr:  98.47%, tr_best:  99.49%, epoch time: 72.40 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.7568%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.4842%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 39337  12.962%\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  4.888087/ 26.248552, val:  54.58%, val_best:  58.33%, tr:  99.08%, tr_best:  99.49%, epoch time: 72.64 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.1456%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.3124%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 40566  12.949%\n",
      "lif layer 1 self.abs_max_v: 3817.0\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  4.576846/ 27.539127, val:  47.08%, val_best:  58.33%, tr:  98.77%, tr_best:  99.49%, epoch time: 73.30 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.0308%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.6166%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 41722  12.914%\n",
      "lif layer 1 self.abs_max_v: 3824.0\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  4.738190/ 30.780603, val:  49.17%, val_best:  58.33%, tr:  98.57%, tr_best:  99.49%, epoch time: 72.86 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.1423%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.6418%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 42912  12.892%\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  4.768319/ 29.246370, val:  50.42%, val_best:  58.33%, tr:  99.39%, tr_best:  99.49%, epoch time: 72.96 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.3068%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.5321%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 44076  12.863%\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  4.683329/ 40.016563, val:  46.25%, val_best:  58.33%, tr:  98.57%, tr_best:  99.49%, epoch time: 73.23 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.6164%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.5685%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 45223  12.831%\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  4.849693/ 20.486988, val:  65.00%, val_best:  65.00%, tr:  98.77%, tr_best:  99.49%, epoch time: 73.25 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.9996%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.5374%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 46340  12.793%\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  4.374577/ 25.859966, val:  55.42%, val_best:  65.00%, tr:  98.98%, tr_best:  99.49%, epoch time: 73.25 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.2363%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.2856%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 47426  12.748%\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  4.552906/ 34.566055, val:  45.83%, val_best:  65.00%, tr:  98.67%, tr_best:  99.49%, epoch time: 73.53 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3310%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.5486%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 48552  12.716%\n",
      "lif layer 1 self.abs_max_v: 3955.5\n",
      "lif layer 1 self.abs_max_v: 3958.5\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  4.660519/ 26.615993, val:  49.17%, val_best:  65.00%, tr:  97.96%, tr_best:  99.49%, epoch time: 72.97 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.6674%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.1639%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 49679  12.686%\n",
      "fc layer 2 self.abs_max_out: 4013.0\n",
      "fc layer 1 self.abs_max_out: 2228.0\n",
      "lif layer 1 self.abs_max_v: 3976.5\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  4.622424/ 24.175743, val:  48.33%, val_best:  65.00%, tr:  99.18%, tr_best:  99.49%, epoch time: 73.67 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.4143%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.4166%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 50806  12.658%\n",
      "fc layer 2 self.abs_max_out: 4275.0\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  4.449081/ 16.287148, val:  61.67%, val_best:  65.00%, tr:  98.57%, tr_best:  99.49%, epoch time: 73.98 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.6447%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.4595%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 51907  12.624%\n",
      "fc layer 1 self.abs_max_out: 2244.0\n",
      "fc layer 1 self.abs_max_out: 2262.0\n",
      "lif layer 1 self.abs_max_v: 4118.0\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  4.462502/ 26.384285, val:  46.67%, val_best:  65.00%, tr:  98.16%, tr_best:  99.49%, epoch time: 73.83 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.8664%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.4577%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 53009  12.592%\n",
      "lif layer 2 self.abs_max_v: 7188.5\n",
      "fc layer 1 self.abs_max_out: 2358.0\n",
      "fc layer 1 self.abs_max_out: 2389.0\n",
      "lif layer 1 self.abs_max_v: 4361.5\n",
      "lif layer 1 self.abs_max_v: 4444.5\n",
      "lif layer 1 self.abs_max_v: 4485.5\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  4.775592/ 24.314375, val:  59.17%, val_best:  65.00%, tr:  98.26%, tr_best:  99.49%, epoch time: 72.84 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.7747%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.3306%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 54171  12.576%\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  4.719060/ 35.556225, val:  47.50%, val_best:  65.00%, tr:  98.67%, tr_best:  99.49%, epoch time: 72.76 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.7605%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.6945%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 55360  12.566%\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  4.440100/ 21.509018, val:  53.75%, val_best:  65.00%, tr:  98.67%, tr_best:  99.49%, epoch time: 73.50 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.3301%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.2635%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 56496  12.545%\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  4.485393/ 28.857359, val:  55.83%, val_best:  65.00%, tr:  98.77%, tr_best:  99.49%, epoch time: 74.06 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.3255%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.9684%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 57643  12.528%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  4.571447/ 42.064301, val:  39.17%, val_best:  65.00%, tr:  98.67%, tr_best:  99.49%, epoch time: 73.67 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.7730%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.8206%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 58753  12.503%\n",
      "lif layer 2 self.abs_max_v: 7301.0\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  4.345165/ 29.040140, val:  50.83%, val_best:  65.00%, tr:  98.47%, tr_best:  99.49%, epoch time: 73.29 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.6602%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.4715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 59876  12.482%\n",
      "lif layer 2 self.abs_max_v: 7610.0\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  4.490271/ 17.693844, val:  51.67%, val_best:  65.00%, tr:  98.88%, tr_best:  99.49%, epoch time: 73.60 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.4598%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.3062%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 61006  12.463%\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  4.229549/ 30.577749, val:  49.17%, val_best:  65.00%, tr:  98.37%, tr_best:  99.49%, epoch time: 73.64 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.8804%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.6342%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 62120  12.442%\n",
      "lif layer 2 self.abs_max_v: 7612.0\n",
      "lif layer 2 self.abs_max_v: 7632.5\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  4.210185/ 28.045248, val:  45.42%, val_best:  65.00%, tr:  98.37%, tr_best:  99.49%, epoch time: 73.40 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.7916%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.4280%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 63211  12.417%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  4.533377/ 23.158411, val:  57.08%, val_best:  65.00%, tr:  98.37%, tr_best:  99.49%, epoch time: 73.24 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.6645%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.9853%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 64355  12.403%\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  4.403621/ 31.440067, val:  50.00%, val_best:  65.00%, tr:  98.67%, tr_best:  99.49%, epoch time: 73.99 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.0298%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.0876%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 65481  12.386%\n",
      "lif layer 2 self.abs_max_v: 7906.5\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  4.336507/ 18.970238, val:  55.83%, val_best:  65.00%, tr:  98.57%, tr_best:  99.49%, epoch time: 74.21 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.9432%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.2637%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 66575  12.364%\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  4.420798/ 14.859560, val:  53.75%, val_best:  65.00%, tr:  99.18%, tr_best:  99.49%, epoch time: 73.07 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.2962%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.4745%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 67666  12.342%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  4.317808/ 23.382805, val:  53.33%, val_best:  65.00%, tr:  98.98%, tr_best:  99.49%, epoch time: 73.64 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.8158%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.6782%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 68774  12.324%\n",
      "lif layer 2 self.abs_max_v: 7950.0\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  4.152384/ 25.131006, val:  52.50%, val_best:  65.00%, tr:  98.77%, tr_best:  99.49%, epoch time: 74.04 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5327%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.5026%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 69823  12.297%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  3.936771/ 20.637293, val:  54.17%, val_best:  65.00%, tr:  98.37%, tr_best:  99.49%, epoch time: 73.73 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4879%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1398%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 70858  12.267%\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  4.094562/ 27.030474, val:  51.67%, val_best:  65.00%, tr:  98.37%, tr_best:  99.49%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6005%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9176%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 71954  12.250%\n",
      "fc layer 2 self.abs_max_out: 4311.0\n",
      "lif layer 2 self.abs_max_v: 7957.5\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  4.288064/ 29.308498, val:  48.33%, val_best:  65.00%, tr:  98.88%, tr_best:  99.49%, epoch time: 73.09 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9143%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0514%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 73062  12.234%\n",
      "fc layer 2 self.abs_max_out: 4752.0\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  4.184686/ 17.522694, val:  64.17%, val_best:  65.00%, tr:  98.26%, tr_best:  99.49%, epoch time: 73.91 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6596%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1927%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 74199  12.224%\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  3.953550/ 18.196749, val:  61.25%, val_best:  65.00%, tr:  99.08%, tr_best:  99.49%, epoch time: 74.13 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6828%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.7913%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 75274  12.205%\n",
      "lif layer 2 self.abs_max_v: 8116.5\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  4.182784/ 23.178385, val:  50.83%, val_best:  65.00%, tr:  98.88%, tr_best:  99.49%, epoch time: 73.36 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.2840%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.7044%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 76335  12.183%\n",
      "fc layer 1 self.abs_max_out: 2461.0\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  4.241173/ 20.723774, val:  52.08%, val_best:  65.00%, tr:  98.47%, tr_best:  99.49%, epoch time: 73.01 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5453%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.7066%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 77378  12.160%\n",
      "lif layer 2 self.abs_max_v: 8178.0\n",
      "lif layer 2 self.abs_max_v: 8213.0\n",
      "fc layer 1 self.abs_max_out: 2491.0\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  4.015280/ 20.234396, val:  49.58%, val_best:  65.00%, tr:  99.08%, tr_best:  99.49%, epoch time: 73.63 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6277%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8571%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 78422  12.137%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  4.111344/ 30.406391, val:  49.17%, val_best:  65.00%, tr:  98.37%, tr_best:  99.49%, epoch time: 73.67 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3023%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.7570%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 79488  12.118%\n",
      "lif layer 1 self.abs_max_v: 4535.0\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  3.928388/ 17.399401, val:  62.50%, val_best:  65.00%, tr:  99.49%, tr_best:  99.49%, epoch time: 73.60 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5835%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0131%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 80490  12.091%\n",
      "lif layer 2 self.abs_max_v: 8291.5\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  3.893166/ 14.992785, val:  60.00%, val_best:  65.00%, tr:  98.88%, tr_best:  99.49%, epoch time: 73.18 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2445%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1804%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 81502  12.065%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  3.708686/ 28.276361, val:  42.50%, val_best:  65.00%, tr:  99.28%, tr_best:  99.49%, epoch time: 73.80 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.8487%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1716%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 82471  12.034%\n",
      "fc layer 1 self.abs_max_out: 2512.0\n",
      "lif layer 1 self.abs_max_v: 4567.0\n",
      "lif layer 1 self.abs_max_v: 4573.5\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  3.988547/ 21.301905, val:  58.75%, val_best:  65.00%, tr:  99.08%, tr_best:  99.49%, epoch time: 73.57 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4112%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2266%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 83542  12.019%\n",
      "fc layer 2 self.abs_max_out: 4846.0\n",
      "lif layer 2 self.abs_max_v: 8832.5\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  3.708438/ 22.744753, val:  56.67%, val_best:  65.00%, tr:  98.67%, tr_best:  99.49%, epoch time: 73.93 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4810%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6507%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 84549  11.995%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  3.538834/ 21.394705, val:  61.25%, val_best:  65.00%, tr:  98.98%, tr_best:  99.49%, epoch time: 73.49 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4323%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3620%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 85527  11.967%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  3.751199/ 40.770199, val:  46.67%, val_best:  65.00%, tr:  98.98%, tr_best:  99.49%, epoch time: 74.31 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3395%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1257%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 86502  11.940%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  3.983680/ 19.031313, val:  64.17%, val_best:  65.00%, tr:  98.67%, tr_best:  99.49%, epoch time: 73.91 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4092%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0344%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 87538  11.922%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  3.538597/ 14.154819, val:  62.50%, val_best:  65.00%, tr:  99.28%, tr_best:  99.49%, epoch time: 73.68 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4082%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4558%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 88470  11.890%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  3.686630/ 19.121254, val:  58.33%, val_best:  65.00%, tr:  98.98%, tr_best:  99.49%, epoch time: 74.09 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3253%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7925%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 89513  11.874%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  3.874818/ 15.919256, val:  61.25%, val_best:  65.00%, tr:  99.18%, tr_best:  99.49%, epoch time: 73.14 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.1146%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2768%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 90519  11.854%\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  3.771542/ 21.894888, val:  54.17%, val_best:  65.00%, tr:  98.98%, tr_best:  99.49%, epoch time: 73.63 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5974%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5524%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 91509  11.832%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  3.689700/ 22.021616, val:  57.08%, val_best:  65.00%, tr:  98.88%, tr_best:  99.49%, epoch time: 73.70 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.7760%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5162%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 92472  11.807%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  3.773537/ 26.716497, val:  50.00%, val_best:  65.00%, tr:  98.47%, tr_best:  99.49%, epoch time: 73.42 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4398%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5588%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 93495  11.790%\n",
      "fc layer 1 self.abs_max_out: 2610.0\n",
      "fc layer 2 self.abs_max_out: 4982.0\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  3.679991/ 28.204395, val:  48.75%, val_best:  65.00%, tr:  99.18%, tr_best:  99.49%, epoch time: 73.46 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.2913%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3995%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 94449  11.765%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  3.895174/ 21.952974, val:  59.17%, val_best:  65.00%, tr:  98.47%, tr_best:  99.49%, epoch time: 73.04 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.2412%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3284%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 95457  11.748%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  3.997589/ 24.828657, val:  50.00%, val_best:  65.00%, tr:  99.18%, tr_best:  99.49%, epoch time: 73.57 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4112%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8845%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 96504  11.735%\n",
      "fc layer 2 self.abs_max_out: 5039.0\n",
      "lif layer 2 self.abs_max_v: 9200.5\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  4.127480/ 21.025534, val:  51.67%, val_best:  65.00%, tr:  99.18%, tr_best:  99.49%, epoch time: 73.10 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6039%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9061%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 97533  11.721%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  3.856503/ 19.673111, val:  63.33%, val_best:  65.00%, tr:  98.77%, tr_best:  99.49%, epoch time: 73.20 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5391%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9357%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 98540  11.704%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  3.729736/ 23.420912, val:  58.75%, val_best:  65.00%, tr:  98.77%, tr_best:  99.49%, epoch time: 73.53 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.0129%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3274%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 99532  11.686%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  3.921504/ 29.533541, val:  48.75%, val_best:  65.00%, tr:  97.96%, tr_best:  99.49%, epoch time: 72.97 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.9088%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3911%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 100555  11.672%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  3.670866/ 20.283516, val:  62.08%, val_best:  65.00%, tr:  99.08%, tr_best:  99.49%, epoch time: 73.58 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4228%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2714%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 101518  11.651%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  3.660094/ 17.893833, val:  73.75%, val_best:  73.75%, tr:  98.67%, tr_best:  99.49%, epoch time: 73.26 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4955%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6169%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 102523  11.636%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  3.754834/ 17.039886, val:  64.17%, val_best:  73.75%, tr:  99.18%, tr_best:  99.49%, epoch time: 73.54 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3402%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4314%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 103520  11.620%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  3.616760/ 21.834587, val:  64.17%, val_best:  73.75%, tr:  99.08%, tr_best:  99.49%, epoch time: 73.63 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4868%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5178%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 104466  11.599%\n",
      "fc layer 2 self.abs_max_out: 5131.0\n",
      "lif layer 2 self.abs_max_v: 9215.0\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  3.582225/ 20.595362, val:  62.50%, val_best:  73.75%, tr:  98.67%, tr_best:  99.49%, epoch time: 73.39 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5703%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4759%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 105421  11.579%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  3.833366/ 19.654444, val:  67.08%, val_best:  73.75%, tr:  98.67%, tr_best:  99.49%, epoch time: 73.79 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.1375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3690%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 106416  11.564%\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  3.827717/ 29.387516, val:  52.50%, val_best:  73.75%, tr:  99.08%, tr_best:  99.49%, epoch time: 73.80 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5671%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1876%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 107380  11.546%\n",
      "fc layer 2 self.abs_max_out: 5245.0\n",
      "lif layer 2 self.abs_max_v: 9681.0\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  3.691624/ 18.112949, val:  64.58%, val_best:  73.75%, tr:  99.08%, tr_best:  99.49%, epoch time: 74.12 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6432%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5271%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 108356  11.529%\n",
      "fc layer 2 self.abs_max_out: 5663.0\n",
      "lif layer 2 self.abs_max_v: 9885.0\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  3.427142/ 22.117081, val:  62.50%, val_best:  73.75%, tr:  98.98%, tr_best:  99.49%, epoch time: 73.36 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3900%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4175%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 109296  11.509%\n",
      "fc layer 1 self.abs_max_out: 2654.0\n",
      "lif layer 1 self.abs_max_v: 4780.5\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  3.584176/ 17.550764, val:  62.92%, val_best:  73.75%, tr:  99.18%, tr_best:  99.49%, epoch time: 73.76 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.9534%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3798%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 110223  11.489%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  3.653519/ 24.020451, val:  57.50%, val_best:  73.75%, tr:  98.88%, tr_best:  99.49%, epoch time: 73.87 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.9786%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4849%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 111174  11.471%\n",
      "fc layer 1 self.abs_max_out: 2667.0\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  3.508741/ 16.310474, val:  64.58%, val_best:  73.75%, tr:  98.37%, tr_best:  99.49%, epoch time: 73.69 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.1336%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6203%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 112107  11.451%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  3.239833/ 19.016800, val:  59.17%, val_best:  73.75%, tr:  99.08%, tr_best:  99.49%, epoch time: 73.42 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.0719%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5946%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 112994  11.428%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  3.915043/ 19.703680, val:  65.83%, val_best:  73.75%, tr:  99.28%, tr_best:  99.49%, epoch time: 73.66 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5016%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0891%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 113970  11.413%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  3.826129/ 18.174181, val:  57.50%, val_best:  73.75%, tr:  98.67%, tr_best:  99.49%, epoch time: 73.95 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5701%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1264%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 114921  11.397%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  3.733186/ 23.185179, val:  58.75%, val_best:  73.75%, tr:  98.77%, tr_best:  99.49%, epoch time: 73.44 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5327%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5137%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 115896  11.383%\n",
      "fc layer 1 self.abs_max_out: 2692.0\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  3.660107/ 20.866564, val:  73.75%, val_best:  73.75%, tr:  98.37%, tr_best:  99.49%, epoch time: 73.33 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6238%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7183%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 116855  11.368%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  3.835007/ 26.728294, val:  55.83%, val_best:  73.75%, tr:  98.77%, tr_best:  99.49%, epoch time: 73.53 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4465%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3661%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 117825  11.354%\n",
      "fc layer 1 self.abs_max_out: 2737.0\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  3.599271/ 22.770212, val:  48.33%, val_best:  73.75%, tr:  98.88%, tr_best:  99.49%, epoch time: 73.37 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6965%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3784%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 118735  11.335%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  3.619745/ 31.847515, val:  51.67%, val_best:  73.75%, tr:  99.08%, tr_best:  99.49%, epoch time: 73.77 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.8980%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3723%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 119677  11.319%\n",
      "fc layer 1 self.abs_max_out: 2766.0\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  3.790915/ 19.925257, val:  63.33%, val_best:  73.75%, tr:  98.77%, tr_best:  99.49%, epoch time: 73.59 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5781%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5366%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 120632  11.305%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  3.830299/ 18.121607, val:  58.75%, val_best:  73.75%, tr:  98.57%, tr_best:  99.49%, epoch time: 73.64 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5739%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2841%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 121613  11.293%\n",
      "fc layer 1 self.abs_max_out: 3070.0\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  3.468260/ 15.114215, val:  68.33%, val_best:  73.75%, tr:  99.08%, tr_best:  99.49%, epoch time: 73.36 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5169%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8644%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 122550  11.277%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  3.524528/ 24.164879, val:  60.00%, val_best:  73.75%, tr:  99.08%, tr_best:  99.49%, epoch time: 73.91 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9083%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3682%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 123461  11.260%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  3.611515/ 17.068056, val:  64.58%, val_best:  73.75%, tr:  98.26%, tr_best:  99.49%, epoch time: 73.87 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.8903%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7347%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 124388  11.244%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  3.419488/ 27.745234, val:  57.92%, val_best:  73.75%, tr:  98.77%, tr_best:  99.49%, epoch time: 74.15 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.7023%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 125308  11.228%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  3.722362/ 27.594549, val:  44.58%, val_best:  73.75%, tr:  98.37%, tr_best:  99.49%, epoch time: 74.05 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.8990%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2685%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 126239  11.213%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  3.694820/ 21.490129, val:  65.42%, val_best:  73.75%, tr:  99.18%, tr_best:  99.49%, epoch time: 73.13 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.8895%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1269%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 127178  11.199%\n",
      "fc layer 1 self.abs_max_out: 3130.0\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  3.568657/ 18.055000, val:  64.17%, val_best:  73.75%, tr:  98.88%, tr_best:  99.49%, epoch time: 73.96 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6933%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6634%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 128098  11.183%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  3.528903/ 19.441776, val:  60.00%, val_best:  73.75%, tr:  98.26%, tr_best:  99.49%, epoch time: 73.71 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.7077%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4452%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 129004  11.167%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  3.764451/ 15.462394, val:  54.17%, val_best:  73.75%, tr:  99.08%, tr_best:  99.49%, epoch time: 73.46 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4036%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6595%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 129968  11.156%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  3.644283/ 17.662794, val:  56.25%, val_best:  73.75%, tr:  99.18%, tr_best:  99.49%, epoch time: 73.13 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3025%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7196%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 130926  11.145%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  3.399723/ 16.877422, val:  56.25%, val_best:  73.75%, tr:  98.26%, tr_best:  99.49%, epoch time: 73.34 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6719%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7564%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 131811  11.127%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  3.828346/ 25.311075, val:  63.33%, val_best:  73.75%, tr:  99.39%, tr_best:  99.49%, epoch time: 72.73 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.7339%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9610%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 132773  11.116%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  3.753307/ 22.081621, val:  57.50%, val_best:  73.75%, tr:  99.39%, tr_best:  99.49%, epoch time: 73.54 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9016%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0696%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 133722  11.105%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  3.950316/ 18.122158, val:  67.92%, val_best:  73.75%, tr:  99.08%, tr_best:  99.49%, epoch time: 73.51 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9723%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1455%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 134693  11.095%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  3.772815/ 15.893067, val:  68.75%, val_best:  73.75%, tr:  98.77%, tr_best:  99.49%, epoch time: 73.32 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.7844%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2639%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 135645  11.084%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  3.717780/ 18.117216, val:  60.00%, val_best:  73.75%, tr:  99.18%, tr_best:  99.49%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.0035%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1935%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 136588  11.073%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  3.440626/ 18.186419, val:  60.42%, val_best:  73.75%, tr:  98.98%, tr_best:  99.49%, epoch time: 73.52 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.7297%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4927%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 137482  11.058%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  3.463027/ 19.076923, val:  68.75%, val_best:  73.75%, tr:  98.77%, tr_best:  99.49%, epoch time: 73.41 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9509%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6137%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 138380  11.043%\n",
      "fc layer 1 self.abs_max_out: 3286.0\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  3.397710/ 19.130537, val:  53.75%, val_best:  73.75%, tr:  99.39%, tr_best:  99.49%, epoch time: 73.31 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.8756%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6728%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 139255  11.027%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  3.321162/ 17.373226, val:  65.00%, val_best:  73.75%, tr:  99.28%, tr_best:  99.49%, epoch time: 73.37 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2020%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4783%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 140090  11.007%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  3.338484/ 31.360159, val:  51.67%, val_best:  73.75%, tr:  98.67%, tr_best:  99.49%, epoch time: 73.33 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1901%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5755%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 140940  10.990%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  3.724633/ 24.566406, val:  58.75%, val_best:  73.75%, tr:  99.49%, tr_best:  99.49%, epoch time: 73.55 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1571%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6932%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 141888  10.980%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  3.381361/ 30.639862, val:  52.92%, val_best:  73.75%, tr:  98.47%, tr_best:  99.49%, epoch time: 73.80 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3483%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6915%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 142742  10.963%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  3.559501/ 21.469164, val:  62.92%, val_best:  73.75%, tr:  99.28%, tr_best:  99.49%, epoch time: 73.09 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2116%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4069%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 143613  10.947%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  3.177779/ 23.933010, val:  52.50%, val_best:  73.75%, tr:  99.08%, tr_best:  99.49%, epoch time: 73.66 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2783%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8536%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 144447  10.929%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  3.627890/ 18.130169, val:  61.25%, val_best:  73.75%, tr:  99.18%, tr_best:  99.49%, epoch time: 73.34 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2621%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0570%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 145384  10.919%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  3.084346/ 16.828403, val:  60.42%, val_best:  73.75%, tr:  98.26%, tr_best:  99.49%, epoch time: 73.32 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2523%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2501%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 146212  10.901%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  3.241913/ 22.616262, val:  60.42%, val_best:  73.75%, tr:  98.26%, tr_best:  99.49%, epoch time: 73.43 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2503%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3782%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 147078  10.886%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  3.092218/ 23.161798, val:  64.58%, val_best:  73.75%, tr:  98.67%, tr_best:  99.49%, epoch time: 73.56 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2706%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3315%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 147910  10.869%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  3.469079/ 20.278780, val:  70.83%, val_best:  73.75%, tr:  98.77%, tr_best:  99.49%, epoch time: 73.46 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5114%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0289%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 148808  10.857%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  3.222152/ 16.064932, val:  61.67%, val_best:  73.75%, tr:  99.08%, tr_best:  99.49%, epoch time: 73.72 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1174%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1687%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 149662  10.842%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  3.081644/ 20.779537, val:  57.92%, val_best:  73.75%, tr:  99.49%, tr_best:  99.49%, epoch time: 73.97 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9962%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2468%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 150505  10.826%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  3.133744/ 20.704502, val:  59.58%, val_best:  73.75%, tr:  99.28%, tr_best:  99.49%, epoch time: 73.64 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.0092%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5466%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 151377  10.813%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  3.182552/ 22.595833, val:  61.67%, val_best:  73.75%, tr:  98.16%, tr_best:  99.49%, epoch time: 73.67 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9977%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3458%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 152244  10.799%\n",
      "fc layer 1 self.abs_max_out: 3371.0\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  3.335903/ 22.100496, val:  60.00%, val_best:  73.75%, tr:  99.08%, tr_best:  99.49%, epoch time: 72.86 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1467%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3573%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 153111  10.786%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  3.166659/ 20.679893, val:  64.17%, val_best:  73.75%, tr:  98.98%, tr_best:  99.49%, epoch time: 73.57 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1535%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2860%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 153917  10.768%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  3.048875/ 15.996191, val:  61.25%, val_best:  73.75%, tr:  98.98%, tr_best:  99.49%, epoch time: 73.61 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.8919%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5461%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 154750  10.753%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  3.028274/ 23.794529, val:  58.33%, val_best:  73.75%, tr:  99.28%, tr_best:  99.49%, epoch time: 73.86 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9292%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4032%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 155581  10.738%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  3.389084/ 25.696810, val:  47.92%, val_best:  73.75%, tr:  98.26%, tr_best:  99.49%, epoch time: 73.79 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1138%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2405%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 156458  10.726%\n",
      "lif layer 1 self.abs_max_v: 4869.0\n",
      "lif layer 1 self.abs_max_v: 4928.5\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  3.327903/ 27.465231, val:  62.50%, val_best:  73.75%, tr:  98.98%, tr_best:  99.49%, epoch time: 74.51 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3224%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1751%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 157354  10.715%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  3.341102/ 19.902309, val:  64.58%, val_best:  73.75%, tr:  99.59%, tr_best:  99.59%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3366%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9189%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 158206  10.702%\n",
      "lif layer 1 self.abs_max_v: 4942.0\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  3.246462/ 26.200773, val:  55.42%, val_best:  73.75%, tr:  98.77%, tr_best:  99.59%, epoch time: 73.49 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2733%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1824%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 159079  10.690%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  3.290427/ 19.757534, val:  68.33%, val_best:  73.75%, tr:  98.67%, tr_best:  99.59%, epoch time: 73.82 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9509%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2769%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 159940  10.678%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  3.458540/ 25.223619, val:  52.92%, val_best:  73.75%, tr:  98.77%, tr_best:  99.59%, epoch time: 73.87 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.0820%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7967%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 160813  10.666%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  3.225482/ 17.555084, val:  67.08%, val_best:  73.75%, tr:  98.57%, tr_best:  99.59%, epoch time: 74.13 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1538%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1823%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 161645  10.652%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  3.056072/ 17.078007, val:  60.83%, val_best:  73.75%, tr:  98.77%, tr_best:  99.59%, epoch time: 73.56 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.0447%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3785%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 162475  10.638%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  2.870497/ 33.064968, val:  51.67%, val_best:  73.75%, tr:  98.37%, tr_best:  99.59%, epoch time: 73.22 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3513%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1473%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 163257  10.622%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  3.267243/ 16.831446, val:  70.00%, val_best:  73.75%, tr:  98.77%, tr_best:  99.59%, epoch time: 73.47 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2792%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9818%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 164095  10.609%\n",
      "lif layer 1 self.abs_max_v: 4947.0\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  2.937238/ 14.697390, val:  68.33%, val_best:  73.75%, tr:  99.08%, tr_best:  99.59%, epoch time: 74.17 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2811%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3776%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 164880  10.592%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  3.069285/ 16.212122, val:  72.92%, val_best:  73.75%, tr:  98.98%, tr_best:  99.59%, epoch time: 73.52 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2815%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3493%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 165697  10.578%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  3.190759/ 16.594706, val:  70.83%, val_best:  73.75%, tr:  98.77%, tr_best:  99.59%, epoch time: 73.73 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3262%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9624%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 166533  10.566%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  3.232803/ 18.413944, val:  66.25%, val_best:  73.75%, tr:  98.88%, tr_best:  99.59%, epoch time: 74.01 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3140%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4113%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 167398  10.555%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  3.170933/ 27.334412, val:  62.92%, val_best:  73.75%, tr:  98.98%, tr_best:  99.59%, epoch time: 73.70 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5201%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2593%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 168232  10.542%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  3.075543/ 17.197735, val:  62.92%, val_best:  73.75%, tr:  99.18%, tr_best:  99.59%, epoch time: 73.52 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3147%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3732%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 169029  10.528%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  2.982774/ 20.110840, val:  64.17%, val_best:  73.75%, tr:  99.18%, tr_best:  99.59%, epoch time: 73.32 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2630%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2344%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 169819  10.513%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  3.207710/ 23.051527, val:  67.50%, val_best:  73.75%, tr:  99.28%, tr_best:  99.59%, epoch time: 73.75 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3649%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2263%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 170674  10.502%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  3.206356/ 20.310505, val:  55.83%, val_best:  73.75%, tr:  99.18%, tr_best:  99.59%, epoch time: 73.43 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1955%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2729%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 171506  10.490%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  2.948677/ 20.840578, val:  67.50%, val_best:  73.75%, tr:  99.39%, tr_best:  99.59%, epoch time: 74.63 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1386%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4807%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 172286  10.475%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  3.063723/ 24.109926, val:  55.42%, val_best:  73.75%, tr:  99.28%, tr_best:  99.59%, epoch time: 73.79 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.0592%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1762%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 173088  10.462%\n",
      "lif layer 1 self.abs_max_v: 5007.5\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  3.217081/ 21.811571, val:  61.25%, val_best:  73.75%, tr:  98.88%, tr_best:  99.59%, epoch time: 73.06 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.0605%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3969%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 173935  10.451%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  3.069073/ 18.687679, val:  62.92%, val_best:  73.75%, tr:  98.98%, tr_best:  99.59%, epoch time: 73.77 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1141%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.0993%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6366%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 174715  10.436%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  2.792266/ 19.640831, val:  57.92%, val_best:  73.75%, tr:  98.88%, tr_best:  99.59%, epoch time: 73.34 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2910%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3513%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 175459  10.420%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  3.160770/ 17.591608, val:  67.92%, val_best:  73.75%, tr:  99.28%, tr_best:  99.59%, epoch time: 73.68 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.4756%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2430%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 176259  10.407%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  3.008068/ 25.955889, val:  58.75%, val_best:  73.75%, tr:  99.18%, tr_best:  99.59%, epoch time: 73.45 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1380%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3300%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 177027  10.392%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  3.027127/ 17.496988, val:  66.67%, val_best:  73.75%, tr:  98.88%, tr_best:  99.59%, epoch time: 73.39 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2646%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3509%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 177798  10.378%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  2.960582/ 22.801033, val:  58.33%, val_best:  73.75%, tr:  98.98%, tr_best:  99.59%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5211%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4122%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 178555  10.363%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  3.193110/ 16.741385, val:  76.25%, val_best:  76.25%, tr:  98.88%, tr_best:  99.59%, epoch time: 74.01 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2123%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6349%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 179400  10.353%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  3.008740/ 13.766413, val:  67.50%, val_best:  76.25%, tr:  98.57%, tr_best:  99.59%, epoch time: 73.39 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0312%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.4244%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5400%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 180191  10.340%\n",
      "fc layer 1 self.abs_max_out: 3411.0\n",
      "lif layer 1 self.abs_max_v: 5176.5\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  3.024015/ 23.842508, val:  62.50%, val_best:  76.25%, tr:  99.59%, tr_best:  99.59%, epoch time: 73.43 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0376%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.4513%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5737%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 180984  10.328%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  3.141585/ 15.937551, val:  62.08%, val_best:  76.25%, tr:  98.77%, tr_best:  99.59%, epoch time: 72.76 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3899%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.7946%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 181802  10.317%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  3.139966/ 13.496473, val:  72.50%, val_best:  76.25%, tr:  99.49%, tr_best:  99.59%, epoch time: 73.65 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.0105%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.7548%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 182658  10.308%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  2.932310/ 20.621355, val:  61.25%, val_best:  76.25%, tr:  98.88%, tr_best:  99.59%, epoch time: 73.46 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.0473%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4526%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 183425  10.294%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  2.977733/ 15.311540, val:  79.58%, val_best:  79.58%, tr:  98.98%, tr_best:  99.59%, epoch time: 72.80 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.0936%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4205%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 184173  10.280%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  2.938853/ 25.551857, val:  52.92%, val_best:  79.58%, tr:  98.77%, tr_best:  99.59%, epoch time: 73.68 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0426%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1089%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4620%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 184929  10.266%\n",
      "lif layer 1 self.abs_max_v: 5435.0\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  2.933616/ 21.759924, val:  62.50%, val_best:  79.58%, tr:  99.18%, tr_best:  99.59%, epoch time: 68.94 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3074%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4732%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 185690  10.253%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  3.036306/ 21.360821, val:  57.08%, val_best:  79.58%, tr:  99.28%, tr_best:  99.59%, epoch time: 73.39 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1729%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5056%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 186477  10.241%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  3.140383/ 20.056955, val:  65.42%, val_best:  79.58%, tr:  99.49%, tr_best:  99.59%, epoch time: 73.68 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1364%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2473%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 187284  10.230%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  2.981612/ 18.133884, val:  67.50%, val_best:  79.58%, tr:  99.69%, tr_best:  99.69%, epoch time: 73.92 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3956%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4164%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 188056  10.218%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  3.030439/ 15.782544, val:  71.25%, val_best:  79.58%, tr:  98.98%, tr_best:  99.69%, epoch time: 74.43 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3765%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.4715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 188811  10.204%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  3.112651/ 22.661186, val:  57.08%, val_best:  79.58%, tr:  99.18%, tr_best:  99.69%, epoch time: 73.60 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2674%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2386%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 189631  10.195%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  2.968854/ 16.262278, val:  76.25%, val_best:  79.58%, tr:  99.08%, tr_best:  99.69%, epoch time: 73.41 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1541%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.3977%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 190380  10.181%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  3.004446/ 25.897245, val:  52.08%, val_best:  79.58%, tr:  99.08%, tr_best:  99.69%, epoch time: 73.23 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.0039%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 191162  10.170%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  2.851118/ 20.479382, val:  61.25%, val_best:  79.58%, tr:  99.39%, tr_best:  99.69%, epoch time: 73.31 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.0996%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5511%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 191897  10.156%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  2.866538/ 20.311983, val:  55.42%, val_best:  79.58%, tr:  97.85%, tr_best:  99.69%, epoch time: 73.43 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2369%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6970%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 192660  10.144%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  3.043298/ 24.598484, val:  60.42%, val_best:  79.58%, tr:  98.98%, tr_best:  99.69%, epoch time: 73.57 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1407%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5768%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 193443  10.133%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  3.155959/ 29.077930, val:  52.08%, val_best:  79.58%, tr:  99.08%, tr_best:  99.69%, epoch time: 73.33 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2523%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5503%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 194251  10.123%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  3.165828/ 18.373095, val:  65.00%, val_best:  79.58%, tr:  99.49%, tr_best:  99.69%, epoch time: 73.57 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2682%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.5048%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 195057  10.114%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  3.025731/ 19.954668, val:  66.67%, val_best:  79.58%, tr:  99.39%, tr_best:  99.69%, epoch time: 73.63 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3728%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6907%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 195865  10.104%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  2.898164/ 22.488943, val:  59.17%, val_best:  79.58%, tr:  98.88%, tr_best:  99.69%, epoch time: 73.42 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3469%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.9308%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 196616  10.092%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  2.959483/ 15.841577, val:  63.75%, val_best:  79.58%, tr:  99.08%, tr_best:  99.69%, epoch time: 74.25 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.4787%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.6629%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d5ff044bf34a888c6e2e813adf506a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÉ‚ñÉ‚ñá‚ñÜ‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñà‚ñÜ‚ñá‚ñÜ‚ñá‚ñÑ‚ñá‚ñÖ‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñá‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÉ‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñá‚ñÖ‚ñÜ‚ñÉ‚ñÇ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñà‚ñÅ‚ñÜ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÉ‚ñÉ‚ñá‚ñÜ‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñà‚ñÜ‚ñá‚ñÜ‚ñá‚ñÑ‚ñá‚ñÖ‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99081</td></tr><tr><td>tr_epoch_loss</td><td>2.95948</td></tr><tr><td>val_acc_best</td><td>0.79583</td></tr><tr><td>val_acc_now</td><td>0.6375</td></tr><tr><td>val_loss</td><td>15.84158</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dry-sweep-5</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vxnn1kzx' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vxnn1kzx</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_051751-vxnn1kzx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jfaqnp6l with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.046875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0234375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_092339-jfaqnp6l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jfaqnp6l' target=\"_blank\">treasured-sweep-10</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jfaqnp6l' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jfaqnp6l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': '20251214_092347_690', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 32, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 4, 'lif_layer_v_threshold2': 128, 'init_scaling': [0.03125, 0.046875, 0.0234375], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 4, self.v_threshold 32\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 4, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=32, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 46.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 6.0\n",
      "lif layer 2 self.abs_max_v: 6.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 59.5\n",
      "fc layer 2 self.abs_max_out: 36.0\n",
      "lif layer 2 self.abs_max_v: 35.5\n",
      "fc layer 1 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 69.0\n",
      "fc layer 2 self.abs_max_out: 48.0\n",
      "lif layer 2 self.abs_max_v: 53.0\n",
      "fc layer 1 self.abs_max_out: 62.0\n",
      "lif layer 1 self.abs_max_v: 90.0\n",
      "lif layer 2 self.abs_max_v: 59.5\n",
      "fc layer 1 self.abs_max_out: 101.0\n",
      "lif layer 1 self.abs_max_v: 138.0\n",
      "fc layer 2 self.abs_max_out: 54.0\n",
      "lif layer 2 self.abs_max_v: 72.0\n",
      "fc layer 1 self.abs_max_out: 147.0\n",
      "lif layer 1 self.abs_max_v: 210.0\n",
      "fc layer 2 self.abs_max_out: 76.0\n",
      "lif layer 2 self.abs_max_v: 112.0\n",
      "fc layer 1 self.abs_max_out: 200.0\n",
      "lif layer 1 self.abs_max_v: 270.0\n",
      "fc layer 2 self.abs_max_out: 105.0\n",
      "lif layer 2 self.abs_max_v: 161.0\n",
      "fc layer 1 self.abs_max_out: 225.0\n",
      "lif layer 1 self.abs_max_v: 360.0\n",
      "fc layer 2 self.abs_max_out: 121.0\n",
      "lif layer 2 self.abs_max_v: 201.5\n",
      "fc layer 3 self.abs_max_out: 9.0\n",
      "fc layer 1 self.abs_max_out: 277.0\n",
      "lif layer 1 self.abs_max_v: 424.0\n",
      "fc layer 2 self.abs_max_out: 140.0\n",
      "lif layer 2 self.abs_max_v: 226.0\n",
      "fc layer 3 self.abs_max_out: 11.0\n",
      "fc layer 1 self.abs_max_out: 297.0\n",
      "fc layer 2 self.abs_max_out: 152.0\n",
      "fc layer 3 self.abs_max_out: 14.0\n",
      "fc layer 2 self.abs_max_out: 165.0\n",
      "lif layer 2 self.abs_max_v: 228.0\n",
      "fc layer 3 self.abs_max_out: 16.0\n",
      "fc layer 2 self.abs_max_out: 200.0\n",
      "lif layer 2 self.abs_max_v: 281.0\n",
      "fc layer 3 self.abs_max_out: 21.0\n",
      "fc layer 1 self.abs_max_out: 386.0\n",
      "lif layer 1 self.abs_max_v: 529.0\n",
      "lif layer 2 self.abs_max_v: 309.0\n",
      "fc layer 1 self.abs_max_out: 504.0\n",
      "lif layer 1 self.abs_max_v: 689.5\n",
      "fc layer 2 self.abs_max_out: 202.0\n",
      "lif layer 2 self.abs_max_v: 328.5\n",
      "lif layer 1 self.abs_max_v: 695.0\n",
      "fc layer 2 self.abs_max_out: 244.0\n",
      "lif layer 2 self.abs_max_v: 341.5\n",
      "lif layer 2 self.abs_max_v: 349.0\n",
      "lif layer 2 self.abs_max_v: 365.5\n",
      "fc layer 3 self.abs_max_out: 29.0\n",
      "fc layer 3 self.abs_max_out: 31.0\n",
      "lif layer 2 self.abs_max_v: 391.5\n",
      "fc layer 3 self.abs_max_out: 36.0\n",
      "fc layer 1 self.abs_max_out: 507.0\n",
      "fc layer 1 self.abs_max_out: 595.0\n",
      "fc layer 2 self.abs_max_out: 280.0\n",
      "fc layer 3 self.abs_max_out: 46.0\n",
      "fc layer 3 self.abs_max_out: 49.0\n",
      "fc layer 2 self.abs_max_out: 287.0\n",
      "fc layer 2 self.abs_max_out: 321.0\n",
      "lif layer 2 self.abs_max_v: 395.5\n",
      "lif layer 2 self.abs_max_v: 482.5\n",
      "lif layer 1 self.abs_max_v: 721.0\n",
      "lif layer 2 self.abs_max_v: 485.0\n",
      "lif layer 2 self.abs_max_v: 486.0\n",
      "lif layer 2 self.abs_max_v: 557.5\n",
      "fc layer 1 self.abs_max_out: 682.0\n",
      "fc layer 2 self.abs_max_out: 325.0\n",
      "lif layer 1 self.abs_max_v: 786.0\n",
      "lif layer 2 self.abs_max_v: 579.5\n",
      "fc layer 2 self.abs_max_out: 359.0\n",
      "lif layer 2 self.abs_max_v: 623.0\n",
      "fc layer 3 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 799.5\n",
      "fc layer 1 self.abs_max_out: 687.0\n",
      "lif layer 1 self.abs_max_v: 1087.0\n",
      "lif layer 1 self.abs_max_v: 1207.5\n",
      "lif layer 2 self.abs_max_v: 639.0\n",
      "fc layer 1 self.abs_max_out: 688.0\n",
      "lif layer 1 self.abs_max_v: 1287.5\n",
      "fc layer 2 self.abs_max_out: 380.0\n",
      "lif layer 2 self.abs_max_v: 697.5\n",
      "fc layer 1 self.abs_max_out: 749.0\n",
      "lif layer 1 self.abs_max_v: 1369.0\n",
      "fc layer 2 self.abs_max_out: 401.0\n",
      "lif layer 2 self.abs_max_v: 750.0\n",
      "lif layer 2 self.abs_max_v: 758.0\n",
      "fc layer 1 self.abs_max_out: 756.0\n",
      "lif layer 2 self.abs_max_v: 759.5\n",
      "fc layer 2 self.abs_max_out: 404.0\n",
      "lif layer 2 self.abs_max_v: 784.0\n",
      "fc layer 3 self.abs_max_out: 64.0\n",
      "fc layer 3 self.abs_max_out: 67.0\n",
      "fc layer 2 self.abs_max_out: 436.0\n",
      "fc layer 3 self.abs_max_out: 91.0\n",
      "fc layer 2 self.abs_max_out: 459.0\n",
      "fc layer 1 self.abs_max_out: 815.0\n",
      "fc layer 2 self.abs_max_out: 465.0\n",
      "fc layer 3 self.abs_max_out: 93.0\n",
      "fc layer 3 self.abs_max_out: 110.0\n",
      "fc layer 3 self.abs_max_out: 123.0\n",
      "fc layer 3 self.abs_max_out: 133.0\n",
      "fc layer 3 self.abs_max_out: 135.0\n",
      "fc layer 2 self.abs_max_out: 543.0\n",
      "fc layer 1 self.abs_max_out: 827.0\n",
      "fc layer 3 self.abs_max_out: 137.0\n",
      "fc layer 3 self.abs_max_out: 138.0\n",
      "fc layer 2 self.abs_max_out: 548.0\n",
      "fc layer 2 self.abs_max_out: 550.0\n",
      "fc layer 3 self.abs_max_out: 170.0\n",
      "fc layer 2 self.abs_max_out: 584.0\n",
      "fc layer 1 self.abs_max_out: 876.0\n",
      "fc layer 2 self.abs_max_out: 614.0\n",
      "fc layer 3 self.abs_max_out: 171.0\n",
      "fc layer 2 self.abs_max_out: 619.0\n",
      "fc layer 1 self.abs_max_out: 950.0\n",
      "fc layer 2 self.abs_max_out: 681.0\n",
      "lif layer 2 self.abs_max_v: 811.5\n",
      "lif layer 2 self.abs_max_v: 830.0\n",
      "lif layer 2 self.abs_max_v: 844.0\n",
      "lif layer 2 self.abs_max_v: 854.0\n",
      "lif layer 2 self.abs_max_v: 859.0\n",
      "lif layer 2 self.abs_max_v: 863.5\n",
      "lif layer 2 self.abs_max_v: 880.0\n",
      "lif layer 1 self.abs_max_v: 1431.5\n",
      "lif layer 1 self.abs_max_v: 1446.5\n",
      "lif layer 1 self.abs_max_v: 1649.5\n",
      "fc layer 1 self.abs_max_out: 973.0\n",
      "lif layer 1 self.abs_max_v: 1651.5\n",
      "lif layer 1 self.abs_max_v: 1762.5\n",
      "fc layer 2 self.abs_max_out: 698.0\n",
      "fc layer 3 self.abs_max_out: 183.0\n",
      "lif layer 2 self.abs_max_v: 888.0\n",
      "lif layer 2 self.abs_max_v: 918.5\n",
      "lif layer 2 self.abs_max_v: 970.0\n",
      "lif layer 2 self.abs_max_v: 1007.0\n",
      "lif layer 2 self.abs_max_v: 1015.5\n",
      "fc layer 2 self.abs_max_out: 701.0\n",
      "fc layer 1 self.abs_max_out: 976.0\n",
      "fc layer 2 self.abs_max_out: 711.0\n",
      "fc layer 2 self.abs_max_out: 746.0\n",
      "fc layer 2 self.abs_max_out: 793.0\n",
      "fc layer 3 self.abs_max_out: 200.0\n",
      "fc layer 1 self.abs_max_out: 1183.0\n",
      "lif layer 2 self.abs_max_v: 1110.5\n",
      "lif layer 2 self.abs_max_v: 1148.5\n",
      "fc layer 2 self.abs_max_out: 806.0\n",
      "lif layer 2 self.abs_max_v: 1174.5\n",
      "lif layer 2 self.abs_max_v: 1187.5\n",
      "lif layer 2 self.abs_max_v: 1199.0\n",
      "lif layer 2 self.abs_max_v: 1214.5\n",
      "fc layer 3 self.abs_max_out: 205.0\n",
      "fc layer 3 self.abs_max_out: 221.0\n",
      "lif layer 2 self.abs_max_v: 1282.5\n",
      "lif layer 2 self.abs_max_v: 1283.5\n",
      "fc layer 2 self.abs_max_out: 822.0\n",
      "fc layer 2 self.abs_max_out: 825.0\n",
      "fc layer 2 self.abs_max_out: 840.0\n",
      "fc layer 2 self.abs_max_out: 853.0\n",
      "fc layer 2 self.abs_max_out: 871.0\n",
      "fc layer 2 self.abs_max_out: 872.0\n",
      "fc layer 2 self.abs_max_out: 878.0\n",
      "fc layer 2 self.abs_max_out: 897.0\n",
      "fc layer 2 self.abs_max_out: 950.0\n",
      "fc layer 2 self.abs_max_out: 954.0\n",
      "fc layer 2 self.abs_max_out: 957.0\n",
      "lif layer 2 self.abs_max_v: 1290.5\n",
      "fc layer 3 self.abs_max_out: 229.0\n",
      "lif layer 2 self.abs_max_v: 1299.0\n",
      "lif layer 2 self.abs_max_v: 1306.5\n",
      "lif layer 1 self.abs_max_v: 1838.0\n",
      "lif layer 2 self.abs_max_v: 1341.5\n",
      "fc layer 1 self.abs_max_out: 1226.0\n",
      "lif layer 1 self.abs_max_v: 2042.0\n",
      "fc layer 1 self.abs_max_out: 1365.0\n",
      "lif layer 1 self.abs_max_v: 2386.0\n",
      "lif layer 1 self.abs_max_v: 2389.0\n",
      "lif layer 1 self.abs_max_v: 2454.5\n",
      "lif layer 1 self.abs_max_v: 2482.5\n",
      "lif layer 1 self.abs_max_v: 2548.5\n",
      "fc layer 1 self.abs_max_out: 1415.0\n",
      "lif layer 1 self.abs_max_v: 2689.5\n",
      "lif layer 2 self.abs_max_v: 1381.5\n",
      "fc layer 2 self.abs_max_out: 978.0\n",
      "fc layer 2 self.abs_max_out: 988.0\n",
      "fc layer 3 self.abs_max_out: 241.0\n",
      "fc layer 1 self.abs_max_out: 1480.0\n",
      "fc layer 2 self.abs_max_out: 995.0\n",
      "fc layer 2 self.abs_max_out: 1008.0\n",
      "fc layer 2 self.abs_max_out: 1013.0\n",
      "fc layer 2 self.abs_max_out: 1032.0\n",
      "lif layer 2 self.abs_max_v: 1389.0\n",
      "lif layer 2 self.abs_max_v: 1389.5\n",
      "lif layer 2 self.abs_max_v: 1398.0\n",
      "lif layer 2 self.abs_max_v: 1444.5\n",
      "lif layer 2 self.abs_max_v: 1502.5\n",
      "fc layer 1 self.abs_max_out: 1500.0\n",
      "fc layer 3 self.abs_max_out: 254.0\n",
      "fc layer 3 self.abs_max_out: 260.0\n",
      "fc layer 3 self.abs_max_out: 263.0\n",
      "fc layer 2 self.abs_max_out: 1060.0\n",
      "fc layer 2 self.abs_max_out: 1102.0\n",
      "fc layer 3 self.abs_max_out: 279.0\n",
      "lif layer 2 self.abs_max_v: 1504.5\n",
      "lif layer 2 self.abs_max_v: 1516.5\n",
      "lif layer 2 self.abs_max_v: 1522.5\n",
      "lif layer 2 self.abs_max_v: 1525.5\n",
      "lif layer 2 self.abs_max_v: 1527.0\n",
      "lif layer 2 self.abs_max_v: 1547.5\n",
      "fc layer 2 self.abs_max_out: 1105.0\n",
      "lif layer 2 self.abs_max_v: 1591.5\n",
      "lif layer 2 self.abs_max_v: 1606.0\n",
      "lif layer 2 self.abs_max_v: 1608.0\n",
      "fc layer 2 self.abs_max_out: 1108.0\n",
      "lif layer 1 self.abs_max_v: 2826.0\n",
      "lif layer 1 self.abs_max_v: 2845.0\n",
      "fc layer 1 self.abs_max_out: 1520.0\n",
      "lif layer 1 self.abs_max_v: 2931.5\n",
      "fc layer 1 self.abs_max_out: 1539.0\n",
      "fc layer 1 self.abs_max_out: 1540.0\n",
      "fc layer 1 self.abs_max_out: 1548.0\n",
      "lif layer 2 self.abs_max_v: 1612.0\n",
      "lif layer 2 self.abs_max_v: 1634.0\n",
      "lif layer 2 self.abs_max_v: 1695.0\n",
      "fc layer 1 self.abs_max_out: 1624.0\n",
      "fc layer 1 self.abs_max_out: 1657.0\n",
      "fc layer 1 self.abs_max_out: 1831.0\n",
      "lif layer 1 self.abs_max_v: 3145.5\n",
      "lif layer 1 self.abs_max_v: 3244.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss:  8.304759/ 65.260979, val:  30.83%, val_best:  30.83%, tr:  98.77%, tr_best:  98.77%, epoch time: 74.01 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4879%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.5211%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 1687  17.232%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1116.0\n",
      "lif layer 2 self.abs_max_v: 1713.0\n",
      "lif layer 2 self.abs_max_v: 1790.0\n",
      "fc layer 2 self.abs_max_out: 1139.0\n",
      "lif layer 2 self.abs_max_v: 1904.0\n",
      "lif layer 2 self.abs_max_v: 2008.0\n",
      "lif layer 2 self.abs_max_v: 2037.5\n",
      "fc layer 3 self.abs_max_out: 290.0\n",
      "fc layer 3 self.abs_max_out: 295.0\n",
      "fc layer 3 self.abs_max_out: 300.0\n",
      "fc layer 3 self.abs_max_out: 301.0\n",
      "fc layer 2 self.abs_max_out: 1150.0\n",
      "fc layer 2 self.abs_max_out: 1164.0\n",
      "fc layer 2 self.abs_max_out: 1175.0\n",
      "fc layer 3 self.abs_max_out: 322.0\n",
      "fc layer 2 self.abs_max_out: 1178.0\n",
      "fc layer 3 self.abs_max_out: 331.0\n",
      "fc layer 3 self.abs_max_out: 345.0\n",
      "lif layer 2 self.abs_max_v: 2107.5\n",
      "lif layer 2 self.abs_max_v: 2121.0\n",
      "lif layer 2 self.abs_max_v: 2135.5\n",
      "lif layer 2 self.abs_max_v: 2158.0\n",
      "lif layer 2 self.abs_max_v: 2165.5\n",
      "lif layer 2 self.abs_max_v: 2184.0\n",
      "fc layer 2 self.abs_max_out: 1201.0\n",
      "fc layer 2 self.abs_max_out: 1221.0\n",
      "fc layer 2 self.abs_max_out: 1224.0\n",
      "fc layer 2 self.abs_max_out: 1228.0\n",
      "fc layer 1 self.abs_max_out: 1913.0\n",
      "fc layer 1 self.abs_max_out: 2173.0\n",
      "lif layer 1 self.abs_max_v: 3572.0\n",
      "lif layer 1 self.abs_max_v: 3872.0\n",
      "fc layer 1 self.abs_max_out: 2250.0\n",
      "lif layer 1 self.abs_max_v: 4063.5\n",
      "lif layer 1 self.abs_max_v: 4070.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss:  8.370826/ 46.683952, val:  40.83%, val_best:  40.83%, tr:  99.28%, tr_best:  99.28%, epoch time: 73.94 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9093%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.3699%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3167  16.175%\n",
      "fc layer 2 self.abs_max_out: 1236.0\n",
      "fc layer 2 self.abs_max_out: 1249.0\n",
      "lif layer 2 self.abs_max_v: 2297.5\n",
      "fc layer 2 self.abs_max_out: 1250.0\n",
      "lif layer 2 self.abs_max_v: 2298.0\n",
      "fc layer 2 self.abs_max_out: 1258.0\n",
      "fc layer 2 self.abs_max_out: 1294.0\n",
      "fc layer 2 self.abs_max_out: 1295.0\n",
      "fc layer 2 self.abs_max_out: 1297.0\n",
      "fc layer 2 self.abs_max_out: 1298.0\n",
      "fc layer 2 self.abs_max_out: 1300.0\n",
      "fc layer 2 self.abs_max_out: 1302.0\n",
      "fc layer 2 self.abs_max_out: 1306.0\n",
      "fc layer 2 self.abs_max_out: 1322.0\n",
      "fc layer 2 self.abs_max_out: 1336.0\n",
      "fc layer 2 self.abs_max_out: 1341.0\n",
      "fc layer 2 self.abs_max_out: 1350.0\n",
      "fc layer 2 self.abs_max_out: 1406.0\n",
      "fc layer 2 self.abs_max_out: 1414.0\n",
      "fc layer 1 self.abs_max_out: 2333.0\n",
      "lif layer 1 self.abs_max_v: 4218.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss:  7.945007/ 35.097076, val:  39.17%, val_best:  40.83%, tr:  99.39%, tr_best:  99.39%, epoch time: 72.92 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5945%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.0769%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 4628  15.758%\n",
      "lif layer 2 self.abs_max_v: 2311.0\n",
      "lif layer 2 self.abs_max_v: 2322.0\n",
      "lif layer 2 self.abs_max_v: 2444.5\n",
      "lif layer 2 self.abs_max_v: 2551.5\n",
      "fc layer 2 self.abs_max_out: 1441.0\n",
      "fc layer 2 self.abs_max_out: 1460.0\n",
      "fc layer 1 self.abs_max_out: 2553.0\n",
      "fc layer 1 self.abs_max_out: 2623.0\n",
      "lif layer 1 self.abs_max_v: 4421.0\n",
      "lif layer 1 self.abs_max_v: 4790.5\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  7.482231/ 37.640148, val:  45.00%, val_best:  45.00%, tr:  99.49%, tr_best:  99.49%, epoch time: 74.25 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9589%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.9481%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6027  15.391%\n",
      "fc layer 2 self.abs_max_out: 1532.0\n",
      "fc layer 1 self.abs_max_out: 2633.0\n",
      "lif layer 1 self.abs_max_v: 4826.5\n",
      "fc layer 2 self.abs_max_out: 1535.0\n",
      "fc layer 2 self.abs_max_out: 1574.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  7.771857/ 48.011829, val:  40.83%, val_best:  45.00%, tr:  99.59%, tr_best:  99.59%, epoch time: 73.85 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6861%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.1895%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 7413  15.144%\n",
      "fc layer 2 self.abs_max_out: 1610.0\n",
      "fc layer 1 self.abs_max_out: 2684.0\n",
      "fc layer 1 self.abs_max_out: 2764.0\n",
      "lif layer 1 self.abs_max_v: 5033.5\n",
      "lif layer 2 self.abs_max_v: 2629.0\n",
      "lif layer 2 self.abs_max_v: 2650.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  7.656414/ 48.294689, val:  42.92%, val_best:  45.00%, tr:  99.28%, tr_best:  99.59%, epoch time: 73.76 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7736%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.9804%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 8805  14.990%\n",
      "lif layer 2 self.abs_max_v: 2670.0\n",
      "fc layer 1 self.abs_max_out: 2782.0\n",
      "fc layer 2 self.abs_max_out: 1613.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  7.480309/ 53.411583, val:  44.17%, val_best:  45.00%, tr:  99.49%, tr_best:  99.59%, epoch time: 72.94 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.4881%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.5924%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10200  14.884%\n",
      "fc layer 1 self.abs_max_out: 2822.0\n",
      "lif layer 1 self.abs_max_v: 5064.0\n",
      "lif layer 2 self.abs_max_v: 2691.0\n",
      "fc layer 2 self.abs_max_out: 1657.0\n",
      "lif layer 2 self.abs_max_v: 2811.0\n",
      "fc layer 2 self.abs_max_out: 1705.0\n",
      "lif layer 2 self.abs_max_v: 2884.5\n",
      "lif layer 2 self.abs_max_v: 2950.5\n",
      "lif layer 2 self.abs_max_v: 3028.0\n",
      "lif layer 2 self.abs_max_v: 3033.0\n",
      "fc layer 1 self.abs_max_out: 2854.0\n",
      "fc layer 1 self.abs_max_out: 2878.0\n",
      "lif layer 1 self.abs_max_v: 5153.5\n",
      "fc layer 1 self.abs_max_out: 2989.0\n",
      "fc layer 3 self.abs_max_out: 346.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  7.536977/ 54.853333, val:  51.25%, val_best:  51.25%, tr:  99.18%, tr_best:  99.59%, epoch time: 71.25 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5017%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.7534%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 11576  14.780%\n",
      "lif layer 2 self.abs_max_v: 3066.0\n",
      "lif layer 2 self.abs_max_v: 3153.0\n",
      "fc layer 3 self.abs_max_out: 348.0\n",
      "fc layer 1 self.abs_max_out: 3248.0\n",
      "lif layer 1 self.abs_max_v: 5325.5\n",
      "lif layer 1 self.abs_max_v: 5790.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  7.871658/ 46.052158, val:  42.92%, val_best:  51.25%, tr:  99.28%, tr_best:  99.59%, epoch time: 74.03 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8221%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.2025%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 13026  14.784%\n",
      "fc layer 2 self.abs_max_out: 1735.0\n",
      "fc layer 2 self.abs_max_out: 1958.0\n",
      "fc layer 1 self.abs_max_out: 3302.0\n",
      "lif layer 1 self.abs_max_v: 5852.5\n",
      "fc layer 1 self.abs_max_out: 3493.0\n",
      "lif layer 1 self.abs_max_v: 6028.0\n",
      "lif layer 1 self.abs_max_v: 6035.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  7.452945/ 54.824146, val:  40.00%, val_best:  51.25%, tr:  99.59%, tr_best:  99.59%, epoch time: 73.81 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5994%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.7864%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 14405  14.714%\n",
      "lif layer 1 self.abs_max_v: 6111.0\n",
      "lif layer 2 self.abs_max_v: 3199.0\n",
      "lif layer 2 self.abs_max_v: 3258.5\n",
      "fc layer 2 self.abs_max_out: 2068.0\n",
      "fc layer 1 self.abs_max_out: 3539.0\n",
      "lif layer 1 self.abs_max_v: 6140.0\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  7.475035/ 49.992947, val:  41.25%, val_best:  51.25%, tr:  99.80%, tr_best:  99.80%, epoch time: 73.20 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.4864%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.9919%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 15776  14.649%\n",
      "fc layer 3 self.abs_max_out: 366.0\n",
      "lif layer 2 self.abs_max_v: 3409.5\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  7.523673/ 35.457100, val:  54.17%, val_best:  54.17%, tr:  99.28%, tr_best:  99.80%, epoch time: 74.24 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0828%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.4756%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 17120  14.573%\n",
      "lif layer 1 self.abs_max_v: 6263.5\n",
      "fc layer 3 self.abs_max_out: 375.0\n",
      "fc layer 1 self.abs_max_out: 3565.0\n",
      "lif layer 1 self.abs_max_v: 6309.5\n",
      "fc layer 1 self.abs_max_out: 3733.0\n",
      "lif layer 1 self.abs_max_v: 6704.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  7.014896/ 38.977921, val:  50.83%, val_best:  54.17%, tr:  99.49%, tr_best:  99.80%, epoch time: 73.28 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7482%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.2885%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 18433  14.483%\n",
      "fc layer 2 self.abs_max_out: 2072.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  7.303832/ 53.149719, val:  38.33%, val_best:  54.17%, tr:  99.59%, tr_best:  99.80%, epoch time: 73.76 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3673%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.7519%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 19769  14.424%\n",
      "fc layer 2 self.abs_max_out: 2081.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  7.246059/ 49.329430, val:  49.17%, val_best:  54.17%, tr:  99.08%, tr_best:  99.80%, epoch time: 73.75 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3342%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.4639%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 21056  14.338%\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  7.251895/ 45.426487, val:  42.92%, val_best:  54.17%, tr:  99.59%, tr_best:  99.80%, epoch time: 73.60 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0901%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.4437%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 22342  14.263%\n",
      "fc layer 2 self.abs_max_out: 2092.0\n",
      "fc layer 1 self.abs_max_out: 3954.0\n",
      "lif layer 1 self.abs_max_v: 7009.5\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  7.004828/ 37.909172, val:  53.33%, val_best:  54.17%, tr:  99.18%, tr_best:  99.80%, epoch time: 73.42 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.2169%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.7028%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 23586  14.172%\n",
      "fc layer 2 self.abs_max_out: 2104.0\n",
      "fc layer 1 self.abs_max_out: 4094.0\n",
      "lif layer 1 self.abs_max_v: 7264.5\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  7.045705/ 45.646080, val:  52.08%, val_best:  54.17%, tr:  99.28%, tr_best:  99.80%, epoch time: 73.33 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6907%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.3612%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 24851  14.102%\n",
      "fc layer 2 self.abs_max_out: 2127.0\n",
      "fc layer 2 self.abs_max_out: 2130.0\n",
      "lif layer 2 self.abs_max_v: 3450.5\n",
      "fc layer 2 self.abs_max_out: 2137.0\n",
      "fc layer 2 self.abs_max_out: 2175.0\n",
      "fc layer 2 self.abs_max_out: 2181.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  7.282459/ 54.869591, val:  37.50%, val_best:  54.17%, tr:  99.59%, tr_best:  99.80%, epoch time: 73.79 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8179%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.6799%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 26160  14.064%\n",
      "lif layer 2 self.abs_max_v: 3517.0\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  6.255238/ 47.671783, val:  47.08%, val_best:  54.17%, tr:  99.69%, tr_best:  99.80%, epoch time: 73.97 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4076%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.6301%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 27329  13.958%\n",
      "fc layer 3 self.abs_max_out: 378.0\n",
      "fc layer 1 self.abs_max_out: 4175.0\n",
      "lif layer 1 self.abs_max_v: 7388.5\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  6.662835/ 56.979187, val:  44.58%, val_best:  54.17%, tr:  99.49%, tr_best:  99.80%, epoch time: 73.89 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4955%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.2189%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 28543  13.883%\n",
      "fc layer 2 self.abs_max_out: 2202.0\n",
      "lif layer 2 self.abs_max_v: 3543.5\n",
      "lif layer 2 self.abs_max_v: 3807.0\n",
      "lif layer 2 self.abs_max_v: 3887.5\n",
      "fc layer 1 self.abs_max_out: 4281.0\n",
      "lif layer 1 self.abs_max_v: 7574.0\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  6.782318/ 30.299999, val:  54.17%, val_best:  54.17%, tr:  99.69%, tr_best:  99.80%, epoch time: 73.61 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7425%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.2142%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 29840  13.855%\n",
      "fc layer 1 self.abs_max_out: 4314.0\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  6.501014/ 50.389599, val:  58.33%, val_best:  58.33%, tr:  99.80%, tr_best:  99.80%, epoch time: 73.57 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6390%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.5925%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 31064  13.796%\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  6.373716/ 49.161949, val:  52.08%, val_best:  58.33%, tr:  99.59%, tr_best:  99.80%, epoch time: 73.84 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6288%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.9336%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 32257  13.729%\n",
      "fc layer 1 self.abs_max_out: 4326.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  6.332607/ 36.346336, val:  56.67%, val_best:  58.33%, tr:  99.49%, tr_best:  99.80%, epoch time: 74.21 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2482%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.5857%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 33455  13.669%\n",
      "fc layer 1 self.abs_max_out: 4554.0\n",
      "lif layer 1 self.abs_max_v: 8011.5\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  6.527067/ 40.412872, val:  53.33%, val_best:  58.33%, tr:  99.90%, tr_best:  99.90%, epoch time: 73.77 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5534%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.8506%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 34697  13.631%\n",
      "fc layer 3 self.abs_max_out: 394.0\n",
      "fc layer 3 self.abs_max_out: 428.0\n",
      "fc layer 3 self.abs_max_out: 439.0\n",
      "fc layer 1 self.abs_max_out: 4612.0\n",
      "lif layer 1 self.abs_max_v: 8099.5\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  6.211541/ 41.889259, val:  54.17%, val_best:  58.33%, tr:  99.69%, tr_best:  99.90%, epoch time: 73.40 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1130%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.1144%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 35847  13.561%\n",
      "fc layer 1 self.abs_max_out: 4663.0\n",
      "lif layer 1 self.abs_max_v: 8135.5\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  6.590366/ 33.962177, val:  59.17%, val_best:  59.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 73.54 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5909%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.3799%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 37052  13.517%\n",
      "fc layer 1 self.abs_max_out: 4723.0\n",
      "lif layer 1 self.abs_max_v: 8175.5\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  6.441103/ 40.778614, val:  58.33%, val_best:  59.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 74.47 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1566%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.6697%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 38244  13.470%\n",
      "fc layer 2 self.abs_max_out: 2253.0\n",
      "fc layer 2 self.abs_max_out: 2256.0\n",
      "lif layer 2 self.abs_max_v: 4078.0\n",
      "fc layer 1 self.abs_max_out: 4946.0\n",
      "lif layer 1 self.abs_max_v: 8183.5\n",
      "lif layer 1 self.abs_max_v: 8714.0\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  6.170036/ 56.036739, val:  50.42%, val_best:  59.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 74.26 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0721%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.0594%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 39394  13.413%\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  6.023712/ 27.941319, val:  65.00%, val_best:  65.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 74.18 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3306%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.4600%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 40567  13.367%\n",
      "fc layer 1 self.abs_max_out: 5020.0\n",
      "lif layer 1 self.abs_max_v: 8889.0\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  5.942601/ 40.221512, val:  55.00%, val_best:  65.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 73.61 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0874%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.4499%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 41720  13.317%\n",
      "lif layer 1 self.abs_max_v: 8944.0\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  5.451996/ 29.842840, val:  63.75%, val_best:  65.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 74.73 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1247%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.5782%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 42812  13.252%\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  5.704549/ 39.152832, val:  56.67%, val_best:  65.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 73.49 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3634%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.6193%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 43942  13.201%\n",
      "fc layer 1 self.abs_max_out: 5047.0\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  5.320820/ 29.743011, val:  69.58%, val_best:  69.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 73.34 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3696%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.1174%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 45052  13.148%\n",
      "fc layer 2 self.abs_max_out: 2289.0\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  5.144770/ 29.536486, val:  66.25%, val_best:  69.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 73.91 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3024%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.1639%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 46123  13.087%\n",
      "fc layer 1 self.abs_max_out: 5060.0\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  5.021091/ 32.232651, val:  61.67%, val_best:  69.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 73.25 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7630%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.5212%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 47177  13.024%\n",
      "fc layer 1 self.abs_max_out: 5129.0\n",
      "lif layer 1 self.abs_max_v: 9062.0\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  4.693082/ 39.181412, val:  61.67%, val_best:  69.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 73.72 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5445%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.4144%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 48156  12.944%\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  5.447591/ 30.089502, val:  66.25%, val_best:  69.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 74.21 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8135%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.2427%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 49220  12.891%\n",
      "fc layer 1 self.abs_max_out: 5162.0\n",
      "lif layer 1 self.abs_max_v: 9174.5\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  4.857321/ 43.222832, val:  66.25%, val_best:  69.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 73.66 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0037%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.7755%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 50218  12.824%\n",
      "fc layer 1 self.abs_max_out: 5300.0\n",
      "lif layer 1 self.abs_max_v: 9422.5\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  5.224896/ 34.694225, val:  62.08%, val_best:  69.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 73.79 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5940%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.1546%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 51236  12.765%\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  5.159521/ 29.360628, val:  68.33%, val_best:  69.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 74.12 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1544%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.0332%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 52261  12.710%\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  4.754864/ 24.981472, val:  76.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.23 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9349%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.9530%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 53201  12.638%\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  4.815482/ 26.639965, val:  68.75%, val_best:  76.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.25 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2187%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.8978%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 54175  12.577%\n",
      "lif layer 2 self.abs_max_v: 4146.5\n",
      "fc layer 2 self.abs_max_out: 2355.0\n",
      "fc layer 2 self.abs_max_out: 2365.0\n",
      "lif layer 2 self.abs_max_v: 4254.5\n",
      "lif layer 1 self.abs_max_v: 9485.5\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  4.876364/ 33.316803, val:  71.25%, val_best:  76.67%, tr:  99.49%, tr_best: 100.00%, epoch time: 73.10 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2032%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.0854%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 55153  12.519%\n",
      "fc layer 2 self.abs_max_out: 2469.0\n",
      "fc layer 2 self.abs_max_out: 2493.0\n",
      "lif layer 2 self.abs_max_v: 4290.0\n",
      "lif layer 2 self.abs_max_v: 4293.0\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  4.363234/ 29.596422, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.91 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5934%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.3829%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 56101  12.457%\n",
      "fc layer 2 self.abs_max_out: 2641.0\n",
      "lif layer 2 self.abs_max_v: 4299.5\n",
      "lif layer 2 self.abs_max_v: 4313.5\n",
      "lif layer 2 self.abs_max_v: 4338.0\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  4.412183/ 42.279549, val:  66.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.29 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3504%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.4411%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 57033  12.395%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  4.295408/ 30.635477, val:  66.67%, val_best:  76.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.04 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1192%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.1039%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 57933  12.328%\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  4.563788/ 23.250559, val:  76.67%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.29 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8680%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.8879%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 58843  12.266%\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  4.211147/ 27.552837, val:  73.75%, val_best:  76.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.02 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8287%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.3299%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 59736  12.203%\n",
      "fc layer 1 self.abs_max_out: 5472.0\n",
      "lif layer 1 self.abs_max_v: 9871.0\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  4.217206/ 32.865929, val:  64.17%, val_best:  76.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.98 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8615%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.3916%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 60603  12.138%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  4.269880/ 27.183458, val:  72.92%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.63 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8882%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.1087%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 61478  12.076%\n",
      "fc layer 1 self.abs_max_out: 5510.0\n",
      "lif layer 1 self.abs_max_v: 9938.5\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  4.379048/ 29.101784, val:  77.92%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.69 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5092%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.8448%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 62379  12.022%\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  4.242379/ 32.055176, val:  67.08%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.04 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0552%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.5728%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 63259  11.966%\n",
      "fc layer 3 self.abs_max_out: 442.0\n",
      "fc layer 1 self.abs_max_out: 5589.0\n",
      "lif layer 1 self.abs_max_v: 10080.5\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  4.078847/ 21.838619, val:  81.25%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.53 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5808%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.0068%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 64124  11.909%\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  4.500814/ 23.874557, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.78 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3609%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.8525%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 65032  11.862%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  3.836982/ 29.974493, val:  72.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.08 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9338%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.1691%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 65878  11.805%\n",
      "fc layer 1 self.abs_max_out: 5721.0\n",
      "lif layer 1 self.abs_max_v: 10299.5\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  3.799412/ 30.624397, val:  65.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.53 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6550%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.1956%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 66647  11.737%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  3.629248/ 25.110027, val:  79.58%, val_best:  84.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.37 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4954%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.4149%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 67416  11.672%\n",
      "lif layer 1 self.abs_max_v: 10496.0\n",
      "lif layer 1 self.abs_max_v: 10681.0\n",
      "fc layer 1 self.abs_max_out: 5765.0\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  4.057507/ 34.053425, val:  73.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.63 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4705%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.4358%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 68261  11.621%\n",
      "fc layer 1 self.abs_max_out: 5842.0\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  4.036877/ 36.617043, val:  66.25%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1872%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.6697%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 69043  11.561%\n",
      "fc layer 1 self.abs_max_out: 5904.0\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  3.836088/ 22.219984, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.92 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7610%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.6122%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 69855  11.509%\n",
      "fc layer 1 self.abs_max_out: 5976.0\n",
      "lif layer 1 self.abs_max_v: 10734.0\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  3.850478/ 31.425999, val:  71.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.90 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0950%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.4720%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 70610  11.448%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  3.761597/ 26.468939, val:  79.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.38 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0612%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.4111%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 71388  11.394%\n",
      "fc layer 3 self.abs_max_out: 456.0\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  3.712560/ 26.840765, val:  79.58%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.53 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4615%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.3187%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 72140  11.337%\n",
      "lif layer 1 self.abs_max_v: 10814.0\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  3.607537/ 22.066494, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.94 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0987%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.1591%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 72872  11.278%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  3.481780/ 28.975410, val:  77.50%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.24 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7609%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.7119%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 73594  11.220%\n",
      "fc layer 1 self.abs_max_out: 5992.0\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  3.841462/ 22.231810, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.52 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4737%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.7003%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 74342  11.167%\n",
      "fc layer 2 self.abs_max_out: 2671.0\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  3.460317/ 24.617567, val:  77.50%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.82 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5508%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.4914%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 75073  11.114%\n",
      "fc layer 3 self.abs_max_out: 461.0\n",
      "fc layer 1 self.abs_max_out: 6116.0\n",
      "lif layer 1 self.abs_max_v: 11046.0\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  3.318600/ 29.385878, val:  77.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.78 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7009%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.7766%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 75790  11.059%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  3.444615/ 23.777670, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.41 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7501%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.8276%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 76493  11.005%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  3.462965/ 34.956772, val:  70.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.36 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8347%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.8067%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 77215  10.954%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  3.459906/ 22.992268, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.25 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7178%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.9956%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 77895  10.899%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  3.135481/ 39.025146, val:  68.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.86 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3231%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.7871%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 78557  10.844%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  3.528927/ 29.477234, val:  80.00%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.85 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7421%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.2009%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 79266  10.796%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  3.154484/ 30.911558, val:  77.08%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.22 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9628%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.2936%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 79914  10.741%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  3.637215/ 27.875046, val:  80.83%, val_best:  86.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.40 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2637%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.2514%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 80654  10.699%\n",
      "lif layer 1 self.abs_max_v: 11109.5\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  3.208241/ 34.178730, val:  71.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.17 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5119%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.7423%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 81333  10.651%\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  3.081087/ 27.137806, val:  79.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.24 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3132%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.2363%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 82001  10.603%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  2.976510/ 24.514688, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.46 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9228%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.0125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 82611  10.548%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  3.078763/ 22.877254, val:  84.17%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.54 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4094%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.9193%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 83264  10.500%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  2.743753/ 30.541775, val:  75.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.49 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1879%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.8792%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 83864  10.447%\n",
      "lif layer 2 self.abs_max_v: 4392.5\n",
      "lif layer 2 self.abs_max_v: 4477.5\n",
      "lif layer 2 self.abs_max_v: 4527.5\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  3.061650/ 25.231825, val:  76.25%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.74 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0304%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.7359%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 84515  10.401%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  2.927968/ 32.783356, val:  78.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.52 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9330%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.7253%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 85155  10.355%\n",
      "fc layer 3 self.abs_max_out: 479.0\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  3.045616/ 21.253876, val:  84.58%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.62 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2526%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.7231%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 85796  10.310%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  2.954081/ 27.862259, val:  81.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.36 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2844%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.1813%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 86425  10.265%\n",
      "lif layer 1 self.abs_max_v: 11150.5\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  2.968279/ 27.804287, val:  80.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.13 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9610%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.6169%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 87032  10.218%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  3.068460/ 30.051964, val:  80.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.96 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2930%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.0451%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 87658  10.175%\n",
      "fc layer 2 self.abs_max_out: 2780.0\n",
      "fc layer 3 self.abs_max_out: 484.0\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  2.703227/ 27.730862, val:  81.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.88 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6413%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.2300%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 88240  10.127%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  2.863534/ 20.681740, val:  84.58%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.65 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4696%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.2496%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 88852  10.084%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  3.065596/ 38.728951, val:  68.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.99 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5102%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.0039%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 89446  10.040%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  2.578447/ 23.963564, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2084%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.0820%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 90016   9.994%\n",
      "lif layer 1 self.abs_max_v: 11177.5\n",
      "lif layer 1 self.abs_max_v: 11236.0\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  2.813902/ 25.109715, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.63 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1442%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.2492%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 90618   9.953%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  2.519012/ 28.149439, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.20 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3794%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.1418%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 91190   9.909%\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  2.821014/ 22.138359, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.64 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5225%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.2378%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 91765   9.867%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  2.645723/ 29.743429, val:  79.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.86 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2617%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.5263%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 92341   9.825%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  2.552983/ 28.671610, val:  81.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.33 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6466%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.5779%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 92866   9.779%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  2.476498/ 28.095295, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.36 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6653%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.0409%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 93402   9.735%\n",
      "fc layer 1 self.abs_max_out: 6247.0\n",
      "lif layer 1 self.abs_max_v: 11566.5\n",
      "lif layer 1 self.abs_max_v: 11629.5\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  2.588964/ 28.307775, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.88 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8140%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.2418%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 93939   9.692%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  2.398453/ 27.717638, val:  79.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.77 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7447%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.6646%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 94447   9.647%\n",
      "lif layer 2 self.abs_max_v: 4622.5\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  2.369062/ 25.995306, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.72 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2146%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.0580%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 94945   9.602%\n",
      "fc layer 3 self.abs_max_out: 493.0\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  2.559041/ 23.674997, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.18 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3109%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.8767%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 95470   9.561%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  2.223696/ 24.334267, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.83 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0335%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.4372%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 95954   9.516%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  2.717472/ 30.387918, val:  75.00%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.54 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4391%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.9059%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 96495   9.477%\n",
      "fc layer 3 self.abs_max_out: 502.0\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  2.442026/ 32.636211, val:  77.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.57 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0179%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.0278%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 97012   9.437%\n",
      "fc layer 3 self.abs_max_out: 522.0\n",
      "fc layer 3 self.abs_max_out: 526.0\n",
      "fc layer 3 self.abs_max_out: 536.0\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  2.618026/ 27.879150, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.09 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1678%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.0813%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 97539   9.399%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  2.220610/ 32.403145, val:  78.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.16 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3622%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.6834%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 98009   9.356%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  2.574241/ 31.702730, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.96 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2823%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.3602%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 98541   9.320%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  2.521742/ 30.348351, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.47 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1853%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.9182%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 99065   9.283%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  2.407267/ 26.968464, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.86 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3445%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.3274%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 99560   9.245%\n",
      "fc layer 1 self.abs_max_out: 6251.0\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  2.189259/ 27.283396, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.78 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3202%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.0736%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 100059   9.208%\n",
      "fc layer 2 self.abs_max_out: 2858.0\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  2.643740/ 26.078693, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3431%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.8154%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 100602   9.175%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  2.275749/ 27.503052, val:  81.25%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.44 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5953%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.5340%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 101080   9.137%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  2.114381/ 30.889685, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.56 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.9816%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.3772%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 101527   9.097%\n",
      "fc layer 2 self.abs_max_out: 2866.0\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  2.366408/ 31.836008, val:  80.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.70 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3216%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.2956%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 102007   9.060%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  2.392165/ 22.736300, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.57 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3219%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.1356%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 102467   9.023%\n",
      "fc layer 1 self.abs_max_out: 6286.0\n",
      "lif layer 2 self.abs_max_v: 4627.5\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  2.306275/ 32.185574, val:  81.25%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.79 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0488%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.1139%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 102957   8.989%\n",
      "fc layer 2 self.abs_max_out: 3042.0\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  2.553905/ 25.779148, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.48 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.9682%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.1526%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 103475   8.957%\n",
      "lif layer 2 self.abs_max_v: 4644.5\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  2.222115/ 29.340986, val:  78.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.00 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3678%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.7458%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 103930   8.921%\n",
      "fc layer 1 self.abs_max_out: 6325.0\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  2.206572/ 23.922235, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.32 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2785%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.1714%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 104367   8.884%\n",
      "fc layer 1 self.abs_max_out: 6381.0\n",
      "lif layer 2 self.abs_max_v: 4766.5\n",
      "lif layer 2 self.abs_max_v: 4813.0\n",
      "lif layer 2 self.abs_max_v: 4831.0\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  2.105666/ 28.436136, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1766%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.1329%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 104811   8.848%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  2.178339/ 23.776859, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.50 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0321%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.9874%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 105279   8.815%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  1.959517/ 26.999174, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.48 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.6340%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.7863%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 105737   8.781%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  2.090940/ 26.449091, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.09 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.7832%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.7157%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 106177   8.746%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  2.203965/ 24.550880, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.05 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.6749%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.9296%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 106627   8.713%\n",
      "lif layer 1 self.abs_max_v: 11693.0\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  1.835472/ 33.388752, val:  80.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.92 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.7378%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.0443%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 107055   8.679%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  2.008593/ 31.794193, val:  78.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.52 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.8168%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.8105%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 107469   8.644%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  1.864033/ 29.245989, val:  76.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.67 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.7119%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.1001%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 107894   8.610%\n",
      "fc layer 1 self.abs_max_out: 6505.0\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  1.946698/ 25.929707, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.13 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.7794%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.1455%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 108289   8.575%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  1.998102/ 30.466194, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.23 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.5917%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.7415%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 108709   8.542%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  1.993771/ 34.789539, val:  78.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.80 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.6321%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.9465%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 109117   8.508%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  2.132535/ 25.765570, val:  85.83%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.87 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.7728%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.5208%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 109569   8.479%\n",
      "lif layer 1 self.abs_max_v: 11865.5\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  1.661934/ 29.654188, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.21 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.6752%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.7126%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 109962   8.445%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  2.066369/ 28.294844, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.90 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1719%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.6233%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 110371   8.413%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  1.802990/ 27.287933, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.72 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1479%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.4122%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 110761   8.381%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  1.850409/ 25.386217, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.84 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0088%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.4177%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 111159   8.349%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  1.817208/ 24.775223, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.18 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0914%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.6399%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 111536   8.316%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  2.010399/ 27.615711, val:  78.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.62 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.9670%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.1701%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 111949   8.286%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  1.532100/ 27.116503, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.60 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.9017%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.8321%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 112310   8.253%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  1.868588/ 23.012583, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.41 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.9061%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.6831%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 112691   8.222%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  1.856883/ 24.552538, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.21 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.7097%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.6007%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 113116   8.194%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  1.777889/ 27.371857, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.13 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.6886%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.9073%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 113495   8.164%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  1.838242/ 26.574142, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.14 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.5888%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.6922%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 113894   8.135%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  1.684379/ 30.169991, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.96 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.7117%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.0563%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 114260   8.105%\n",
      "fc layer 2 self.abs_max_out: 3063.0\n",
      "fc layer 1 self.abs_max_out: 6618.0\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  1.657023/ 29.867426, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.79 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.5645%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.0587%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 114628   8.075%\n",
      "fc layer 2 self.abs_max_out: 3114.0\n",
      "fc layer 3 self.abs_max_out: 562.0\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  1.604184/ 30.386156, val:  80.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.60 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.6610%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.0662%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 114964   8.043%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  1.645534/ 31.740545, val:  80.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.43 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.6932%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.5097%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 115341   8.015%\n",
      "fc layer 2 self.abs_max_out: 3205.0\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  1.522271/ 26.824877, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.39 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.6636%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.3334%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 115662   7.983%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  1.749306/ 28.611176, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.47 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4256%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.2188%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 116021   7.954%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  1.855574/ 32.128719, val:  82.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.59 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2798%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.3128%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 116415   7.927%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  1.588517/ 30.886267, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.68 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.9621%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.3657%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 116759   7.898%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  1.731363/ 26.399509, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.43 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2895%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.1616%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 117116   7.870%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  1.660706/ 30.838882, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.85 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2649%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.2142%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 117459   7.842%\n",
      "lif layer 2 self.abs_max_v: 4840.0\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  1.876980/ 25.308645, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.66 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.0829%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.9062%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 117827   7.815%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  1.754777/ 29.042595, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.94 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3159%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.9797%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 118177   7.788%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  1.712429/ 32.046497, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.52 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1134%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.4458%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 118533   7.761%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  1.654878/ 29.235632, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.19 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.1221%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 118892   7.735%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  1.351288/ 28.943419, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.51 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1363%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.3920%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 119188   7.705%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  1.400137/ 28.760437, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.81 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3285%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.6847%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 119491   7.676%\n",
      "lif layer 2 self.abs_max_v: 4853.0\n",
      "lif layer 2 self.abs_max_v: 4951.5\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  1.520757/ 27.135521, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.41 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4423%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.3950%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 119814   7.649%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  1.668684/ 24.764467, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.65 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4635%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.2373%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 120145   7.622%\n",
      "lif layer 1 self.abs_max_v: 11877.0\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  1.582523/ 26.784405, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.47 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1053%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.4269%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 120489   7.597%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  1.314510/ 25.999924, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.28 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3886%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.4323%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 120769   7.568%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  1.507729/ 26.680626, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.30 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4894%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.0999%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 121092   7.542%\n",
      "fc layer 2 self.abs_max_out: 3399.0\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  1.611386/ 24.152319, val:  87.08%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.59 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3927%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.3524%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 121438   7.518%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  1.792415/ 27.410223, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.82 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.5137%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.4842%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 121804   7.495%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  1.401363/ 27.564106, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.81 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.5935%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.5491%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 122107   7.469%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  1.506695/ 30.086414, val:  82.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.71 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2477%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.7207%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 122426   7.444%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  1.596975/ 29.823618, val:  82.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.44 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2526%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.0480%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 122757   7.420%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  1.433217/ 28.271626, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.64 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3393%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.1368%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 123071   7.395%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  1.402863/ 28.144735, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.09 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1141%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.5466%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.4147%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 123373   7.370%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  1.533560/ 28.637094, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.61 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.5058%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.7186%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 123708   7.347%\n",
      "lif layer 2 self.abs_max_v: 4966.5\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  1.323061/ 27.179485, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.58 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.5684%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.6696%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 124003   7.322%\n",
      "lif layer 2 self.abs_max_v: 5320.0\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  1.225421/ 29.849972, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.97 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4334%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.7326%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 124285   7.296%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  1.396032/ 29.472971, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.74 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1635%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.8919%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 124592   7.272%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  1.346113/ 26.763357, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.23 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2698%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.8466%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 124896   7.249%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  1.437254/ 27.698391, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.87 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2938%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.8998%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 125214   7.226%\n",
      "fc layer 3 self.abs_max_out: 564.0\n",
      "fc layer 1 self.abs_max_out: 6651.0\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  1.261739/ 29.546585, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.77 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0312%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3092%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.6461%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 125516   7.203%\n",
      "fc layer 1 self.abs_max_out: 6804.0\n",
      "fc layer 3 self.abs_max_out: 590.0\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  1.391236/ 26.082983, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.80 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0376%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3093%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.6427%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 125811   7.179%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  1.506802/ 27.016880, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.29 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.0777%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.7158%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 126147   7.158%\n",
      "fc layer 2 self.abs_max_out: 3400.0\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  1.466531/ 27.848413, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.57 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.0745%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.5963%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 126449   7.136%\n",
      "fc layer 1 self.abs_max_out: 6824.0\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  1.234544/ 24.262142, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.29 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4658%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.8843%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 126719   7.112%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  1.483302/ 30.771519, val:  83.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.47 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1392%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.9204%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 127008   7.089%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  1.269417/ 32.871899, val:  82.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.47 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0426%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4160%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.8778%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 127291   7.066%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  1.214756/ 32.980007, val:  82.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.72 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.0897%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.7329%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 127577   7.044%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  1.286902/ 25.818127, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.73 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1313%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.6277%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 127846   7.021%\n",
      "lif layer 1 self.abs_max_v: 11939.0\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  1.324287/ 26.807686, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.19 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1097%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.7144%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 128134   6.999%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  1.290534/ 31.938387, val:  82.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.91 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.9180%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.3348%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 128424   6.978%\n",
      "lif layer 1 self.abs_max_v: 12059.0\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  1.092530/ 33.831032, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.18 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.0036%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.2416%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 128686   6.955%\n",
      "fc layer 3 self.abs_max_out: 595.0\n",
      "fc layer 3 self.abs_max_out: 603.0\n",
      "lif layer 1 self.abs_max_v: 12096.5\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  1.349899/ 27.663887, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.91 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.1467%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 128959   6.933%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  1.101479/ 27.097597, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.03 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4381%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.9848%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 129218   6.910%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  1.195381/ 31.838177, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.73 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4032%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.2606%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 129471   6.888%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  1.141036/ 29.374838, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.44 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3904%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.7855%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 129709   6.865%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  1.186551/ 31.564486, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.42 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4512%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.0184%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 129960   6.843%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  1.227433/ 30.083191, val:  84.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.38 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4764%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.0896%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 130226   6.822%\n",
      "fc layer 3 self.abs_max_out: 615.0\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  1.168821/ 29.854544, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.66 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3014%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.0727%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 130487   6.800%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  1.374547/ 29.763195, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.40 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2207%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.9403%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 130766   6.780%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  1.435814/ 32.990555, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.96 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.0268%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.7485%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 131073   6.762%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  1.329636/ 29.457838, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.97 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.0489%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.8944%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 131337   6.741%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  1.058851/ 31.977768, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.62 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1254%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.6316%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f608ac6c7144f748c756ba2260e00a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñÖ‚ñá‚ñÉ‚ñÜ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.05885</td></tr><tr><td>val_acc_best</td><td>0.9</td></tr><tr><td>val_acc_now</td><td>0.85417</td></tr><tr><td>val_loss</td><td>31.97777</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">treasured-sweep-10</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jfaqnp6l' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jfaqnp6l</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_092339-jfaqnp6l/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5ejik4kr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.046875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0234375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_132955-5ejik4kr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5ejik4kr' target=\"_blank\">gentle-sweep-17</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5ejik4kr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5ejik4kr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': '20251214_133003_345', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 16, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 1, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 1, 'lif_layer_v_threshold2': 32, 'init_scaling': [0.03125, 0.046875, 0.0234375], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 1, self.v_threshold 16\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 1, self.v_threshold 32\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=16, v_reset=10000, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=32, v_reset=10000, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 46.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 41.0\n",
      "lif layer 2 self.abs_max_v: 41.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 8.0\n",
      "fc layer 1 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 51.0\n",
      "fc layer 2 self.abs_max_out: 80.0\n",
      "lif layer 2 self.abs_max_v: 94.5\n",
      "fc layer 3 self.abs_max_out: 12.0\n",
      "fc layer 1 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 69.0\n",
      "lif layer 2 self.abs_max_v: 107.5\n",
      "fc layer 1 self.abs_max_out: 55.0\n",
      "lif layer 1 self.abs_max_v: 70.0\n",
      "fc layer 3 self.abs_max_out: 16.0\n",
      "fc layer 1 self.abs_max_out: 72.0\n",
      "lif layer 1 self.abs_max_v: 86.0\n",
      "fc layer 1 self.abs_max_out: 122.0\n",
      "lif layer 1 self.abs_max_v: 122.0\n",
      "fc layer 2 self.abs_max_out: 84.0\n",
      "lif layer 2 self.abs_max_v: 108.5\n",
      "fc layer 3 self.abs_max_out: 22.0\n",
      "fc layer 1 self.abs_max_out: 141.0\n",
      "lif layer 1 self.abs_max_v: 141.0\n",
      "fc layer 2 self.abs_max_out: 129.0\n",
      "lif layer 2 self.abs_max_v: 183.5\n",
      "fc layer 3 self.abs_max_out: 36.0\n",
      "fc layer 1 self.abs_max_out: 150.0\n",
      "lif layer 1 self.abs_max_v: 185.0\n",
      "lif layer 2 self.abs_max_v: 199.0\n",
      "fc layer 1 self.abs_max_out: 166.0\n",
      "lif layer 1 self.abs_max_v: 189.5\n",
      "fc layer 3 self.abs_max_out: 37.0\n",
      "fc layer 1 self.abs_max_out: 181.0\n",
      "lif layer 1 self.abs_max_v: 231.0\n",
      "fc layer 1 self.abs_max_out: 195.0\n",
      "lif layer 1 self.abs_max_v: 239.5\n",
      "fc layer 2 self.abs_max_out: 149.0\n",
      "lif layer 2 self.abs_max_v: 218.5\n",
      "fc layer 2 self.abs_max_out: 151.0\n",
      "fc layer 1 self.abs_max_out: 216.0\n",
      "fc layer 1 self.abs_max_out: 245.0\n",
      "lif layer 1 self.abs_max_v: 245.0\n",
      "fc layer 3 self.abs_max_out: 47.0\n",
      "lif layer 1 self.abs_max_v: 259.5\n",
      "fc layer 1 self.abs_max_out: 264.0\n",
      "lif layer 1 self.abs_max_v: 331.5\n",
      "fc layer 2 self.abs_max_out: 154.0\n",
      "fc layer 2 self.abs_max_out: 167.0\n",
      "fc layer 3 self.abs_max_out: 49.0\n",
      "lif layer 1 self.abs_max_v: 357.5\n",
      "fc layer 3 self.abs_max_out: 57.0\n",
      "fc layer 2 self.abs_max_out: 196.0\n",
      "fc layer 2 self.abs_max_out: 204.0\n",
      "lif layer 1 self.abs_max_v: 390.0\n",
      "fc layer 1 self.abs_max_out: 328.0\n",
      "fc layer 2 self.abs_max_out: 220.0\n",
      "lif layer 2 self.abs_max_v: 220.0\n",
      "fc layer 2 self.abs_max_out: 231.0\n",
      "lif layer 2 self.abs_max_v: 231.0\n",
      "fc layer 1 self.abs_max_out: 348.0\n",
      "lif layer 1 self.abs_max_v: 395.0\n",
      "fc layer 3 self.abs_max_out: 62.0\n",
      "fc layer 3 self.abs_max_out: 65.0\n",
      "lif layer 2 self.abs_max_v: 258.0\n",
      "fc layer 3 self.abs_max_out: 81.0\n",
      "lif layer 2 self.abs_max_v: 291.0\n",
      "lif layer 2 self.abs_max_v: 297.5\n",
      "lif layer 1 self.abs_max_v: 409.5\n",
      "fc layer 1 self.abs_max_out: 380.0\n",
      "fc layer 1 self.abs_max_out: 427.0\n",
      "lif layer 1 self.abs_max_v: 562.5\n",
      "lif layer 1 self.abs_max_v: 596.0\n",
      "fc layer 3 self.abs_max_out: 83.0\n",
      "fc layer 2 self.abs_max_out: 232.0\n",
      "fc layer 1 self.abs_max_out: 443.0\n",
      "fc layer 3 self.abs_max_out: 84.0\n",
      "fc layer 1 self.abs_max_out: 492.0\n",
      "lif layer 2 self.abs_max_v: 312.0\n",
      "fc layer 2 self.abs_max_out: 251.0\n",
      "fc layer 3 self.abs_max_out: 98.0\n",
      "lif layer 2 self.abs_max_v: 316.5\n",
      "lif layer 1 self.abs_max_v: 604.0\n",
      "fc layer 2 self.abs_max_out: 292.0\n",
      "lif layer 1 self.abs_max_v: 623.0\n",
      "lif layer 1 self.abs_max_v: 654.5\n",
      "lif layer 2 self.abs_max_v: 320.5\n",
      "fc layer 1 self.abs_max_out: 535.0\n",
      "lif layer 2 self.abs_max_v: 324.0\n",
      "lif layer 1 self.abs_max_v: 687.5\n",
      "lif layer 2 self.abs_max_v: 326.0\n",
      "lif layer 2 self.abs_max_v: 336.0\n",
      "lif layer 2 self.abs_max_v: 373.0\n",
      "lif layer 2 self.abs_max_v: 394.0\n",
      "lif layer 2 self.abs_max_v: 396.5\n",
      "lif layer 2 self.abs_max_v: 431.5\n",
      "fc layer 2 self.abs_max_out: 318.0\n",
      "fc layer 2 self.abs_max_out: 343.0\n",
      "fc layer 2 self.abs_max_out: 352.0\n",
      "lif layer 2 self.abs_max_v: 434.0\n",
      "fc layer 3 self.abs_max_out: 107.0\n",
      "fc layer 3 self.abs_max_out: 111.0\n",
      "lif layer 2 self.abs_max_v: 442.0\n",
      "lif layer 2 self.abs_max_v: 452.5\n",
      "lif layer 2 self.abs_max_v: 469.5\n",
      "lif layer 2 self.abs_max_v: 483.5\n",
      "fc layer 3 self.abs_max_out: 113.0\n",
      "fc layer 2 self.abs_max_out: 353.0\n",
      "fc layer 3 self.abs_max_out: 124.0\n",
      "lif layer 2 self.abs_max_v: 505.5\n",
      "lif layer 2 self.abs_max_v: 566.0\n",
      "lif layer 2 self.abs_max_v: 619.0\n",
      "fc layer 3 self.abs_max_out: 135.0\n",
      "fc layer 3 self.abs_max_out: 138.0\n",
      "fc layer 3 self.abs_max_out: 139.0\n",
      "fc layer 2 self.abs_max_out: 399.0\n",
      "fc layer 1 self.abs_max_out: 605.0\n",
      "lif layer 1 self.abs_max_v: 712.5\n",
      "lif layer 2 self.abs_max_v: 623.0\n",
      "lif layer 2 self.abs_max_v: 631.5\n",
      "lif layer 2 self.abs_max_v: 637.5\n",
      "lif layer 2 self.abs_max_v: 641.0\n",
      "lif layer 2 self.abs_max_v: 691.0\n",
      "lif layer 2 self.abs_max_v: 712.0\n",
      "lif layer 2 self.abs_max_v: 732.0\n",
      "lif layer 1 self.abs_max_v: 721.5\n",
      "lif layer 2 self.abs_max_v: 736.0\n",
      "fc layer 1 self.abs_max_out: 609.0\n",
      "fc layer 1 self.abs_max_out: 620.0\n",
      "fc layer 1 self.abs_max_out: 813.0\n",
      "lif layer 1 self.abs_max_v: 813.0\n",
      "fc layer 2 self.abs_max_out: 402.0\n",
      "lif layer 2 self.abs_max_v: 762.5\n",
      "lif layer 2 self.abs_max_v: 778.5\n",
      "fc layer 2 self.abs_max_out: 417.0\n",
      "fc layer 2 self.abs_max_out: 420.0\n",
      "fc layer 2 self.abs_max_out: 435.0\n",
      "fc layer 3 self.abs_max_out: 141.0\n",
      "fc layer 3 self.abs_max_out: 146.0\n",
      "fc layer 3 self.abs_max_out: 157.0\n",
      "fc layer 3 self.abs_max_out: 168.0\n",
      "lif layer 2 self.abs_max_v: 781.0\n",
      "fc layer 2 self.abs_max_out: 446.0\n",
      "lif layer 2 self.abs_max_v: 836.5\n",
      "lif layer 2 self.abs_max_v: 842.0\n",
      "lif layer 2 self.abs_max_v: 842.5\n",
      "fc layer 3 self.abs_max_out: 196.0\n",
      "fc layer 3 self.abs_max_out: 200.0\n",
      "lif layer 1 self.abs_max_v: 827.5\n",
      "lif layer 1 self.abs_max_v: 844.0\n",
      "fc layer 2 self.abs_max_out: 462.0\n",
      "lif layer 2 self.abs_max_v: 844.0\n",
      "lif layer 2 self.abs_max_v: 851.0\n",
      "fc layer 3 self.abs_max_out: 211.0\n",
      "fc layer 2 self.abs_max_out: 469.0\n",
      "lif layer 2 self.abs_max_v: 861.0\n",
      "lif layer 2 self.abs_max_v: 866.5\n",
      "fc layer 2 self.abs_max_out: 472.0\n",
      "lif layer 2 self.abs_max_v: 905.5\n",
      "fc layer 2 self.abs_max_out: 513.0\n",
      "fc layer 3 self.abs_max_out: 223.0\n",
      "lif layer 2 self.abs_max_v: 906.0\n",
      "lif layer 2 self.abs_max_v: 915.0\n",
      "lif layer 2 self.abs_max_v: 919.5\n",
      "lif layer 2 self.abs_max_v: 921.0\n",
      "fc layer 1 self.abs_max_out: 831.0\n",
      "lif layer 1 self.abs_max_v: 945.5\n",
      "fc layer 1 self.abs_max_out: 846.0\n",
      "lif layer 1 self.abs_max_v: 1085.0\n",
      "fc layer 1 self.abs_max_out: 879.0\n",
      "fc layer 1 self.abs_max_out: 966.0\n",
      "fc layer 3 self.abs_max_out: 240.0\n",
      "lif layer 1 self.abs_max_v: 1128.5\n",
      "fc layer 1 self.abs_max_out: 1036.0\n",
      "fc layer 1 self.abs_max_out: 1065.0\n",
      "fc layer 1 self.abs_max_out: 1086.0\n",
      "fc layer 1 self.abs_max_out: 1102.0\n",
      "fc layer 1 self.abs_max_out: 1180.0\n",
      "lif layer 1 self.abs_max_v: 1180.0\n",
      "fc layer 1 self.abs_max_out: 1387.0\n",
      "lif layer 1 self.abs_max_v: 1387.0\n",
      "lif layer 1 self.abs_max_v: 1593.0\n",
      "fc layer 2 self.abs_max_out: 525.0\n",
      "fc layer 2 self.abs_max_out: 532.0\n",
      "lif layer 2 self.abs_max_v: 952.5\n",
      "lif layer 2 self.abs_max_v: 959.0\n",
      "lif layer 2 self.abs_max_v: 981.5\n",
      "lif layer 2 self.abs_max_v: 984.0\n",
      "lif layer 2 self.abs_max_v: 1008.5\n",
      "lif layer 2 self.abs_max_v: 1023.5\n",
      "lif layer 2 self.abs_max_v: 1030.5\n",
      "lif layer 2 self.abs_max_v: 1035.5\n",
      "lif layer 2 self.abs_max_v: 1041.0\n",
      "lif layer 2 self.abs_max_v: 1043.5\n",
      "lif layer 2 self.abs_max_v: 1045.0\n",
      "fc layer 2 self.abs_max_out: 547.0\n",
      "fc layer 2 self.abs_max_out: 593.0\n",
      "lif layer 2 self.abs_max_v: 1065.5\n",
      "lif layer 2 self.abs_max_v: 1075.0\n",
      "fc layer 3 self.abs_max_out: 241.0\n",
      "fc layer 3 self.abs_max_out: 242.0\n",
      "fc layer 3 self.abs_max_out: 248.0\n",
      "fc layer 3 self.abs_max_out: 253.0\n",
      "fc layer 3 self.abs_max_out: 259.0\n",
      "fc layer 3 self.abs_max_out: 262.0\n",
      "fc layer 3 self.abs_max_out: 280.0\n",
      "fc layer 1 self.abs_max_out: 1452.0\n",
      "fc layer 1 self.abs_max_out: 1603.0\n",
      "lif layer 1 self.abs_max_v: 1603.0\n",
      "fc layer 1 self.abs_max_out: 1627.0\n",
      "lif layer 1 self.abs_max_v: 1627.0\n",
      "fc layer 2 self.abs_max_out: 670.0\n",
      "lif layer 1 self.abs_max_v: 1636.5\n",
      "lif layer 1 self.abs_max_v: 1795.0\n",
      "lif layer 1 self.abs_max_v: 1910.5\n",
      "fc layer 1 self.abs_max_out: 1634.0\n",
      "fc layer 1 self.abs_max_out: 1710.0\n",
      "fc layer 1 self.abs_max_out: 1731.0\n",
      "fc layer 1 self.abs_max_out: 1750.0\n",
      "fc layer 1 self.abs_max_out: 1770.0\n",
      "fc layer 1 self.abs_max_out: 1929.0\n",
      "lif layer 1 self.abs_max_v: 1929.0\n",
      "fc layer 2 self.abs_max_out: 737.0\n",
      "lif layer 1 self.abs_max_v: 2028.0\n",
      "lif layer 1 self.abs_max_v: 2040.5\n",
      "lif layer 2 self.abs_max_v: 1078.0\n",
      "lif layer 2 self.abs_max_v: 1105.0\n",
      "lif layer 2 self.abs_max_v: 1117.5\n",
      "lif layer 2 self.abs_max_v: 1126.0\n",
      "lif layer 2 self.abs_max_v: 1140.0\n",
      "fc layer 3 self.abs_max_out: 281.0\n",
      "fc layer 3 self.abs_max_out: 286.0\n",
      "fc layer 1 self.abs_max_out: 1984.0\n",
      "fc layer 1 self.abs_max_out: 2175.0\n",
      "lif layer 1 self.abs_max_v: 2175.0\n",
      "fc layer 3 self.abs_max_out: 289.0\n",
      "fc layer 3 self.abs_max_out: 302.0\n",
      "fc layer 1 self.abs_max_out: 2227.0\n",
      "lif layer 1 self.abs_max_v: 2227.0\n",
      "fc layer 1 self.abs_max_out: 2292.0\n",
      "lif layer 1 self.abs_max_v: 2292.0\n",
      "lif layer 2 self.abs_max_v: 1145.0\n",
      "fc layer 1 self.abs_max_out: 2325.0\n",
      "lif layer 1 self.abs_max_v: 2325.0\n",
      "lif layer 2 self.abs_max_v: 1202.0\n",
      "lif layer 2 self.abs_max_v: 1236.0\n",
      "lif layer 2 self.abs_max_v: 1274.0\n",
      "fc layer 1 self.abs_max_out: 2341.0\n",
      "lif layer 1 self.abs_max_v: 2341.0\n",
      "fc layer 1 self.abs_max_out: 2352.0\n",
      "lif layer 1 self.abs_max_v: 2352.0\n",
      "fc layer 1 self.abs_max_out: 2359.0\n",
      "lif layer 1 self.abs_max_v: 2359.0\n",
      "fc layer 2 self.abs_max_out: 744.0\n",
      "lif layer 2 self.abs_max_v: 1297.0\n",
      "fc layer 2 self.abs_max_out: 772.0\n",
      "fc layer 1 self.abs_max_out: 2370.0\n",
      "lif layer 1 self.abs_max_v: 2370.0\n",
      "lif layer 1 self.abs_max_v: 2430.5\n",
      "lif layer 1 self.abs_max_v: 2525.0\n",
      "fc layer 1 self.abs_max_out: 2482.0\n",
      "fc layer 1 self.abs_max_out: 2517.0\n",
      "fc layer 2 self.abs_max_out: 796.0\n",
      "fc layer 2 self.abs_max_out: 797.0\n",
      "lif layer 1 self.abs_max_v: 2582.0\n",
      "lif layer 1 self.abs_max_v: 2733.0\n",
      "lif layer 1 self.abs_max_v: 3211.0\n",
      "lif layer 1 self.abs_max_v: 3356.5\n",
      "lif layer 1 self.abs_max_v: 3371.5\n",
      "fc layer 3 self.abs_max_out: 310.0\n",
      "fc layer 3 self.abs_max_out: 317.0\n",
      "lif layer 2 self.abs_max_v: 1299.5\n",
      "lif layer 2 self.abs_max_v: 1301.5\n",
      "lif layer 2 self.abs_max_v: 1337.0\n",
      "lif layer 2 self.abs_max_v: 1450.5\n",
      "fc layer 3 self.abs_max_out: 370.0\n",
      "fc layer 2 self.abs_max_out: 845.0\n",
      "lif layer 2 self.abs_max_v: 1521.0\n",
      "fc layer 2 self.abs_max_out: 856.0\n",
      "fc layer 2 self.abs_max_out: 947.0\n",
      "fc layer 2 self.abs_max_out: 948.0\n",
      "lif layer 1 self.abs_max_v: 3428.5\n",
      "fc layer 2 self.abs_max_out: 971.0\n",
      "fc layer 2 self.abs_max_out: 1020.0\n",
      "fc layer 2 self.abs_max_out: 1024.0\n",
      "fc layer 2 self.abs_max_out: 1090.0\n",
      "fc layer 2 self.abs_max_out: 1113.0\n",
      "lif layer 1 self.abs_max_v: 3610.5\n",
      "lif layer 1 self.abs_max_v: 3700.0\n",
      "fc layer 2 self.abs_max_out: 1118.0\n",
      "fc layer 1 self.abs_max_out: 2779.0\n",
      "fc layer 2 self.abs_max_out: 1120.0\n",
      "fc layer 2 self.abs_max_out: 1157.0\n",
      "fc layer 2 self.abs_max_out: 1341.0\n",
      "fc layer 2 self.abs_max_out: 1364.0\n",
      "lif layer 2 self.abs_max_v: 1527.0\n",
      "lif layer 2 self.abs_max_v: 1537.0\n",
      "lif layer 2 self.abs_max_v: 1561.5\n",
      "lif layer 2 self.abs_max_v: 1637.0\n",
      "lif layer 2 self.abs_max_v: 1674.5\n",
      "fc layer 2 self.abs_max_out: 1441.0\n",
      "fc layer 2 self.abs_max_out: 1444.0\n",
      "lif layer 2 self.abs_max_v: 1701.5\n",
      "fc layer 2 self.abs_max_out: 1472.0\n",
      "fc layer 2 self.abs_max_out: 1489.0\n",
      "fc layer 2 self.abs_max_out: 1552.0\n",
      "fc layer 1 self.abs_max_out: 2871.0\n",
      "fc layer 2 self.abs_max_out: 1563.0\n",
      "fc layer 2 self.abs_max_out: 1631.0\n",
      "fc layer 1 self.abs_max_out: 3194.0\n",
      "lif layer 1 self.abs_max_v: 3907.0\n",
      "lif layer 1 self.abs_max_v: 4406.5\n",
      "fc layer 1 self.abs_max_out: 3408.0\n",
      "lif layer 2 self.abs_max_v: 1705.0\n",
      "lif layer 2 self.abs_max_v: 1767.5\n",
      "lif layer 2 self.abs_max_v: 1820.0\n",
      "lif layer 1 self.abs_max_v: 4470.5\n",
      "lif layer 2 self.abs_max_v: 1836.0\n",
      "lif layer 2 self.abs_max_v: 1851.5\n",
      "fc layer 2 self.abs_max_out: 1717.0\n",
      "fc layer 1 self.abs_max_out: 3461.0\n",
      "lif layer 2 self.abs_max_v: 1873.0\n",
      "lif layer 2 self.abs_max_v: 1888.5\n",
      "lif layer 1 self.abs_max_v: 4599.5\n",
      "lif layer 1 self.abs_max_v: 5061.0\n",
      "lif layer 1 self.abs_max_v: 5111.5\n",
      "lif layer 1 self.abs_max_v: 5155.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 12.028113/ 95.515823, val:  31.25%, val_best:  31.25%, tr:  99.18%, tr_best:  99.18%, epoch time: 71.67 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9790%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.1243%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 1564  15.975%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 1899.5\n",
      "lif layer 2 self.abs_max_v: 1957.0\n",
      "lif layer 2 self.abs_max_v: 1959.5\n",
      "fc layer 2 self.abs_max_out: 1780.0\n",
      "fc layer 2 self.abs_max_out: 1817.0\n",
      "fc layer 2 self.abs_max_out: 1890.0\n",
      "fc layer 2 self.abs_max_out: 1915.0\n",
      "fc layer 2 self.abs_max_out: 1963.0\n",
      "lif layer 2 self.abs_max_v: 1963.0\n",
      "lif layer 2 self.abs_max_v: 1968.0\n",
      "lif layer 2 self.abs_max_v: 2013.0\n",
      "lif layer 2 self.abs_max_v: 2111.5\n",
      "lif layer 2 self.abs_max_v: 2122.5\n",
      "fc layer 2 self.abs_max_out: 1998.0\n",
      "fc layer 2 self.abs_max_out: 2000.0\n",
      "lif layer 2 self.abs_max_v: 2130.0\n",
      "lif layer 2 self.abs_max_v: 2134.5\n",
      "lif layer 2 self.abs_max_v: 2194.5\n",
      "lif layer 2 self.abs_max_v: 2212.5\n",
      "lif layer 2 self.abs_max_v: 2226.5\n",
      "lif layer 2 self.abs_max_v: 2233.5\n",
      "fc layer 3 self.abs_max_out: 382.0\n",
      "fc layer 3 self.abs_max_out: 386.0\n",
      "fc layer 3 self.abs_max_out: 393.0\n",
      "fc layer 3 self.abs_max_out: 397.0\n",
      "fc layer 3 self.abs_max_out: 401.0\n",
      "lif layer 2 self.abs_max_v: 2236.0\n",
      "lif layer 2 self.abs_max_v: 2238.0\n",
      "fc layer 2 self.abs_max_out: 2088.0\n",
      "fc layer 2 self.abs_max_out: 2128.0\n",
      "fc layer 2 self.abs_max_out: 2223.0\n",
      "fc layer 2 self.abs_max_out: 2255.0\n",
      "lif layer 2 self.abs_max_v: 2255.0\n",
      "fc layer 3 self.abs_max_out: 404.0\n",
      "lif layer 2 self.abs_max_v: 2268.0\n",
      "lif layer 2 self.abs_max_v: 2290.0\n",
      "lif layer 2 self.abs_max_v: 2301.0\n",
      "lif layer 2 self.abs_max_v: 2306.5\n",
      "lif layer 2 self.abs_max_v: 2308.0\n",
      "lif layer 2 self.abs_max_v: 2339.5\n",
      "lif layer 2 self.abs_max_v: 2376.5\n",
      "lif layer 2 self.abs_max_v: 2407.5\n",
      "lif layer 2 self.abs_max_v: 2439.0\n",
      "lif layer 2 self.abs_max_v: 2452.0\n",
      "lif layer 2 self.abs_max_v: 2473.0\n",
      "lif layer 2 self.abs_max_v: 2480.5\n",
      "lif layer 2 self.abs_max_v: 2550.5\n",
      "lif layer 2 self.abs_max_v: 2585.5\n",
      "lif layer 2 self.abs_max_v: 2603.0\n",
      "lif layer 2 self.abs_max_v: 2606.5\n",
      "lif layer 2 self.abs_max_v: 2615.5\n",
      "lif layer 2 self.abs_max_v: 2620.0\n",
      "fc layer 3 self.abs_max_out: 405.0\n",
      "lif layer 1 self.abs_max_v: 5172.5\n",
      "fc layer 3 self.abs_max_out: 407.0\n",
      "lif layer 2 self.abs_max_v: 2620.5\n",
      "fc layer 3 self.abs_max_out: 443.0\n",
      "lif layer 2 self.abs_max_v: 2632.5\n",
      "lif layer 2 self.abs_max_v: 2821.5\n",
      "lif layer 2 self.abs_max_v: 2920.0\n",
      "lif layer 2 self.abs_max_v: 2969.0\n",
      "lif layer 2 self.abs_max_v: 2993.5\n",
      "lif layer 1 self.abs_max_v: 5424.5\n",
      "lif layer 1 self.abs_max_v: 5672.0\n",
      "lif layer 1 self.abs_max_v: 5953.0\n",
      "lif layer 1 self.abs_max_v: 6039.5\n",
      "lif layer 2 self.abs_max_v: 3049.0\n",
      "fc layer 1 self.abs_max_out: 3630.0\n",
      "fc layer 1 self.abs_max_out: 3903.0\n",
      "lif layer 2 self.abs_max_v: 3049.5\n",
      "lif layer 1 self.abs_max_v: 6384.5\n",
      "lif layer 1 self.abs_max_v: 6954.5\n",
      "lif layer 1 self.abs_max_v: 7069.0\n",
      "lif layer 1 self.abs_max_v: 7077.0\n",
      "fc layer 1 self.abs_max_out: 4182.0\n",
      "lif layer 2 self.abs_max_v: 3100.0\n",
      "lif layer 2 self.abs_max_v: 3150.0\n",
      "lif layer 2 self.abs_max_v: 3175.0\n",
      "lif layer 2 self.abs_max_v: 3187.5\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 12.768730/ 64.500801, val:  37.50%, val_best:  37.50%, tr:  98.57%, tr_best:  99.18%, epoch time: 74.49 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.8563%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.6753%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3141  16.042%\n",
      "lif layer 2 self.abs_max_v: 3201.0\n",
      "lif layer 2 self.abs_max_v: 3230.5\n",
      "lif layer 2 self.abs_max_v: 3258.5\n",
      "lif layer 2 self.abs_max_v: 3296.5\n",
      "lif layer 2 self.abs_max_v: 3315.5\n",
      "lif layer 2 self.abs_max_v: 3340.0\n",
      "lif layer 2 self.abs_max_v: 3379.0\n",
      "lif layer 2 self.abs_max_v: 3398.5\n",
      "lif layer 2 self.abs_max_v: 3402.0\n",
      "lif layer 2 self.abs_max_v: 3429.0\n",
      "lif layer 2 self.abs_max_v: 3442.5\n",
      "lif layer 2 self.abs_max_v: 3449.5\n",
      "lif layer 2 self.abs_max_v: 3453.0\n",
      "lif layer 2 self.abs_max_v: 3468.5\n",
      "lif layer 2 self.abs_max_v: 3487.5\n",
      "lif layer 2 self.abs_max_v: 3497.0\n",
      "lif layer 2 self.abs_max_v: 3534.0\n",
      "lif layer 2 self.abs_max_v: 3562.0\n",
      "lif layer 2 self.abs_max_v: 3576.0\n",
      "lif layer 2 self.abs_max_v: 3583.0\n",
      "lif layer 2 self.abs_max_v: 3586.5\n",
      "lif layer 2 self.abs_max_v: 3602.0\n",
      "lif layer 2 self.abs_max_v: 3616.0\n",
      "lif layer 2 self.abs_max_v: 3623.0\n",
      "lif layer 2 self.abs_max_v: 3624.0\n",
      "lif layer 2 self.abs_max_v: 3687.0\n",
      "fc layer 1 self.abs_max_out: 4225.0\n",
      "lif layer 2 self.abs_max_v: 3696.5\n",
      "lif layer 2 self.abs_max_v: 3704.5\n",
      "lif layer 2 self.abs_max_v: 3708.5\n",
      "lif layer 2 self.abs_max_v: 3717.0\n",
      "lif layer 2 self.abs_max_v: 3727.5\n",
      "lif layer 2 self.abs_max_v: 3733.0\n",
      "lif layer 2 self.abs_max_v: 3772.5\n",
      "fc layer 2 self.abs_max_out: 2265.0\n",
      "lif layer 2 self.abs_max_v: 4077.0\n",
      "fc layer 3 self.abs_max_out: 448.0\n",
      "lif layer 2 self.abs_max_v: 4086.0\n",
      "lif layer 2 self.abs_max_v: 4094.0\n",
      "lif layer 2 self.abs_max_v: 4098.0\n",
      "fc layer 2 self.abs_max_out: 2269.0\n",
      "fc layer 2 self.abs_max_out: 2324.0\n",
      "lif layer 2 self.abs_max_v: 4154.0\n",
      "lif layer 2 self.abs_max_v: 4185.5\n",
      "lif layer 2 self.abs_max_v: 4209.0\n",
      "fc layer 1 self.abs_max_out: 4238.0\n",
      "fc layer 1 self.abs_max_out: 4370.0\n",
      "lif layer 1 self.abs_max_v: 7212.0\n",
      "lif layer 1 self.abs_max_v: 7928.0\n",
      "lif layer 1 self.abs_max_v: 7945.0\n",
      "fc layer 1 self.abs_max_out: 4645.0\n",
      "lif layer 1 self.abs_max_v: 8231.5\n",
      "lif layer 1 self.abs_max_v: 8239.5\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 11.877099/ 60.553116, val:  32.08%, val_best:  37.50%, tr:  98.47%, tr_best:  99.18%, epoch time: 74.15 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1927%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7305%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 4724  16.084%\n",
      "fc layer 2 self.abs_max_out: 2336.0\n",
      "fc layer 2 self.abs_max_out: 2367.0\n",
      "fc layer 2 self.abs_max_out: 2383.0\n",
      "lif layer 2 self.abs_max_v: 4213.5\n",
      "lif layer 2 self.abs_max_v: 4247.0\n",
      "lif layer 2 self.abs_max_v: 4263.5\n",
      "lif layer 2 self.abs_max_v: 4272.0\n",
      "lif layer 2 self.abs_max_v: 4276.0\n",
      "lif layer 2 self.abs_max_v: 4280.0\n",
      "lif layer 2 self.abs_max_v: 4284.0\n",
      "lif layer 2 self.abs_max_v: 4305.0\n",
      "lif layer 2 self.abs_max_v: 4314.5\n",
      "lif layer 2 self.abs_max_v: 4353.0\n",
      "fc layer 2 self.abs_max_out: 2516.0\n",
      "fc layer 2 self.abs_max_out: 2575.0\n",
      "lif layer 2 self.abs_max_v: 4630.0\n",
      "lif layer 2 self.abs_max_v: 4890.0\n",
      "fc layer 1 self.abs_max_out: 4893.0\n",
      "lif layer 1 self.abs_max_v: 8947.0\n",
      "fc layer 1 self.abs_max_out: 5080.0\n",
      "lif layer 1 self.abs_max_v: 9071.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss: 10.128359/ 54.070850, val:  32.50%, val_best:  37.50%, tr:  98.06%, tr_best:  99.18%, epoch time: 74.18 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3221%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4023%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6287  16.055%\n",
      "fc layer 1 self.abs_max_out: 5189.0\n",
      "fc layer 2 self.abs_max_out: 2601.0\n",
      "lif layer 1 self.abs_max_v: 9105.5\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  8.754292/ 64.778336, val:  30.83%, val_best:  37.50%, tr:  98.47%, tr_best:  99.18%, epoch time: 73.73 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2291%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.1952%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 7782  15.898%\n",
      "fc layer 2 self.abs_max_out: 2775.0\n",
      "lif layer 2 self.abs_max_v: 4980.5\n",
      "lif layer 2 self.abs_max_v: 5088.5\n",
      "lif layer 2 self.abs_max_v: 5142.5\n",
      "fc layer 2 self.abs_max_out: 2806.0\n",
      "lif layer 2 self.abs_max_v: 5377.5\n",
      "fc layer 2 self.abs_max_out: 2899.0\n",
      "fc layer 2 self.abs_max_out: 2955.0\n",
      "fc layer 1 self.abs_max_out: 6261.0\n",
      "lif layer 1 self.abs_max_v: 9803.5\n",
      "lif layer 1 self.abs_max_v: 10617.5\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  8.690224/ 52.391712, val:  41.25%, val_best:  41.25%, tr:  98.37%, tr_best:  99.18%, epoch time: 73.69 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0530%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.3902%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9258  15.761%\n",
      "fc layer 2 self.abs_max_out: 3038.0\n",
      "lif layer 2 self.abs_max_v: 5592.5\n",
      "fc layer 2 self.abs_max_out: 3073.0\n",
      "fc layer 2 self.abs_max_out: 3212.0\n",
      "lif layer 2 self.abs_max_v: 5618.5\n",
      "lif layer 2 self.abs_max_v: 5702.5\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  7.965014/ 53.355801, val:  37.08%, val_best:  41.25%, tr:  97.85%, tr_best:  99.18%, epoch time: 74.76 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4021%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.9103%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10759  15.700%\n",
      "fc layer 2 self.abs_max_out: 3254.0\n",
      "lif layer 2 self.abs_max_v: 5817.0\n",
      "lif layer 2 self.abs_max_v: 5908.5\n",
      "fc layer 2 self.abs_max_out: 3368.0\n",
      "fc layer 2 self.abs_max_out: 3388.0\n",
      "lif layer 2 self.abs_max_v: 6073.0\n",
      "fc layer 2 self.abs_max_out: 3575.0\n",
      "lif layer 2 self.abs_max_v: 6096.5\n",
      "fc layer 2 self.abs_max_out: 3657.0\n",
      "lif layer 1 self.abs_max_v: 10659.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  8.175208/ 58.178261, val:  30.42%, val_best:  41.25%, tr:  98.37%, tr_best:  99.18%, epoch time: 73.61 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8705%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.3187%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 12282  15.682%\n",
      "lif layer 2 self.abs_max_v: 6132.5\n",
      "lif layer 2 self.abs_max_v: 6272.0\n",
      "lif layer 2 self.abs_max_v: 6475.0\n",
      "fc layer 1 self.abs_max_out: 6334.0\n",
      "fc layer 1 self.abs_max_out: 6497.0\n",
      "lif layer 1 self.abs_max_v: 11070.5\n",
      "lif layer 1 self.abs_max_v: 11842.5\n",
      "lif layer 2 self.abs_max_v: 6519.5\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  7.962522/ 44.604198, val:  36.67%, val_best:  41.25%, tr:  98.47%, tr_best:  99.18%, epoch time: 73.91 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2268%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.2541%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 13840  15.708%\n",
      "lif layer 2 self.abs_max_v: 6635.0\n",
      "lif layer 2 self.abs_max_v: 6775.5\n",
      "lif layer 2 self.abs_max_v: 6847.0\n",
      "lif layer 2 self.abs_max_v: 6854.0\n",
      "lif layer 2 self.abs_max_v: 6886.0\n",
      "fc layer 1 self.abs_max_out: 6657.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  7.530320/ 52.872993, val:  35.00%, val_best:  41.25%, tr:  98.37%, tr_best:  99.18%, epoch time: 68.87 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5749%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.6367%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 15350  15.679%\n",
      "lif layer 2 self.abs_max_v: 6954.0\n",
      "fc layer 2 self.abs_max_out: 3706.0\n",
      "fc layer 2 self.abs_max_out: 3709.0\n",
      "fc layer 2 self.abs_max_out: 3720.0\n",
      "fc layer 2 self.abs_max_out: 3735.0\n",
      "fc layer 2 self.abs_max_out: 3755.0\n",
      "lif layer 2 self.abs_max_v: 6960.5\n",
      "lif layer 2 self.abs_max_v: 6986.5\n",
      "fc layer 1 self.abs_max_out: 6694.0\n",
      "fc layer 1 self.abs_max_out: 6766.0\n",
      "lif layer 1 self.abs_max_v: 12274.5\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  7.342150/ 43.901608, val:  43.75%, val_best:  43.75%, tr:  97.96%, tr_best:  99.18%, epoch time: 67.54 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7336%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.0094%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 16850  15.647%\n",
      "fc layer 1 self.abs_max_out: 6998.0\n",
      "fc layer 2 self.abs_max_out: 3894.0\n",
      "fc layer 2 self.abs_max_out: 3979.0\n",
      "fc layer 2 self.abs_max_out: 4007.0\n",
      "lif layer 2 self.abs_max_v: 7015.5\n",
      "fc layer 1 self.abs_max_out: 7013.0\n",
      "fc layer 1 self.abs_max_out: 7150.0\n",
      "lif layer 1 self.abs_max_v: 13001.5\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  7.789156/ 64.152275, val:  31.25%, val_best:  43.75%, tr:  98.67%, tr_best:  99.18%, epoch time: 72.26 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4900%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.5634%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 18364  15.632%\n",
      "fc layer 2 self.abs_max_out: 4121.0\n",
      "fc layer 2 self.abs_max_out: 4137.0\n",
      "fc layer 2 self.abs_max_out: 4288.0\n",
      "lif layer 2 self.abs_max_v: 7078.5\n",
      "lif layer 2 self.abs_max_v: 7676.5\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  7.639541/ 53.239037, val:  29.58%, val_best:  43.75%, tr:  98.88%, tr_best:  99.18%, epoch time: 69.83 seconds, 1.16 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5475%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.8863%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 19840  15.589%\n",
      "fc layer 1 self.abs_max_out: 7360.0\n",
      "fc layer 1 self.abs_max_out: 7477.0\n",
      "lif layer 1 self.abs_max_v: 13673.5\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  7.435386/ 49.786827, val:  34.17%, val_best:  43.75%, tr:  98.26%, tr_best:  99.18%, epoch time: 69.87 seconds, 1.16 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5334%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.9363%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 21288  15.532%\n",
      "lif layer 2 self.abs_max_v: 7695.5\n",
      "lif layer 2 self.abs_max_v: 7789.0\n",
      "fc layer 2 self.abs_max_out: 4500.0\n",
      "lif layer 2 self.abs_max_v: 8372.5\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  7.538089/ 57.201893, val:  36.67%, val_best:  43.75%, tr:  98.57%, tr_best:  99.18%, epoch time: 73.72 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2701%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.2210%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 22723  15.474%\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  7.858475/ 49.325066, val:  41.25%, val_best:  43.75%, tr:  98.16%, tr_best:  99.18%, epoch time: 74.32 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9528%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.7743%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 24171  15.431%\n",
      "fc layer 1 self.abs_max_out: 7559.0\n",
      "lif layer 1 self.abs_max_v: 13851.5\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  7.276539/ 39.142105, val:  45.42%, val_best:  45.42%, tr:  98.88%, tr_best:  99.18%, epoch time: 73.75 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1919%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.9205%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 25521  15.334%\n",
      "fc layer 1 self.abs_max_out: 7626.0\n",
      "fc layer 1 self.abs_max_out: 7785.0\n",
      "lif layer 1 self.abs_max_v: 14181.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  7.852645/ 54.088383, val:  45.00%, val_best:  45.42%, tr:  99.28%, tr_best:  99.28%, epoch time: 74.47 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1473%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.5648%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 26949  15.293%\n",
      "fc layer 1 self.abs_max_out: 7944.0\n",
      "lif layer 1 self.abs_max_v: 14216.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  8.314680/ 53.543739, val:  35.83%, val_best:  45.42%, tr:  98.98%, tr_best:  99.28%, epoch time: 74.91 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8022%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.7571%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 28469  15.305%\n",
      "fc layer 2 self.abs_max_out: 4885.0\n",
      "fc layer 2 self.abs_max_out: 5174.0\n",
      "lif layer 2 self.abs_max_v: 9256.5\n",
      "lif layer 2 self.abs_max_v: 9463.5\n",
      "fc layer 2 self.abs_max_out: 5951.0\n",
      "lif layer 2 self.abs_max_v: 10547.5\n",
      "fc layer 1 self.abs_max_out: 8212.0\n",
      "lif layer 1 self.abs_max_v: 14764.0\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  7.957415/ 52.722466, val:  30.83%, val_best:  45.42%, tr:  99.18%, tr_best:  99.28%, epoch time: 74.11 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0757%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.2377%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 29886  15.264%\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  7.422596/ 57.593548, val:  37.08%, val_best:  45.42%, tr:  98.26%, tr_best:  99.28%, epoch time: 74.16 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2014%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.5489%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 31272  15.211%\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  8.012136/ 40.806026, val:  34.58%, val_best:  45.42%, tr:  99.18%, tr_best:  99.28%, epoch time: 74.38 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.3752%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 32768  15.214%\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  7.822175/ 37.052937, val:  52.50%, val_best:  52.50%, tr:  99.49%, tr_best:  99.49%, epoch time: 74.10 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7721%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.9717%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 34197  15.187%\n",
      "fc layer 1 self.abs_max_out: 8270.0\n",
      "lif layer 1 self.abs_max_v: 14990.0\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  7.873823/ 36.950211, val:  50.83%, val_best:  52.50%, tr:  99.18%, tr_best:  99.49%, epoch time: 74.04 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0308%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.4426%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 35586  15.146%\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  7.566621/ 54.462914, val:  36.67%, val_best:  52.50%, tr:  98.57%, tr_best:  99.49%, epoch time: 74.36 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3085%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.9692%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 36978  15.108%\n",
      "fc layer 1 self.abs_max_out: 8369.0\n",
      "lif layer 1 self.abs_max_v: 15208.0\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  8.749117/ 45.202011, val:  42.92%, val_best:  52.50%, tr:  99.18%, tr_best:  99.49%, epoch time: 74.34 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2671%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.1487%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 38508  15.128%\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  8.135263/ 35.344097, val:  51.67%, val_best:  52.50%, tr:  99.18%, tr_best:  99.49%, epoch time: 74.19 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2162%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.0957%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 39953  15.115%\n",
      "fc layer 1 self.abs_max_out: 8379.0\n",
      "fc layer 1 self.abs_max_out: 8732.0\n",
      "lif layer 1 self.abs_max_v: 15908.5\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  8.339435/ 41.826733, val:  41.25%, val_best:  52.50%, tr:  99.18%, tr_best:  99.49%, epoch time: 74.33 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.8604%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 41377  15.094%\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  8.102205/ 60.523243, val:  43.75%, val_best:  52.50%, tr:  99.59%, tr_best:  99.59%, epoch time: 74.24 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8969%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.9162%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 42761  15.061%\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  8.172564/ 71.906952, val:  38.33%, val_best:  52.50%, tr:  99.18%, tr_best:  99.59%, epoch time: 74.17 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1654%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.9958%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 44096  15.014%\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  8.118226/ 50.621307, val:  38.33%, val_best:  52.50%, tr:  99.49%, tr_best:  99.59%, epoch time: 74.59 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5332%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.4011%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 45488  14.988%\n",
      "fc layer 1 self.abs_max_out: 8812.0\n",
      "lif layer 1 self.abs_max_v: 16203.0\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  8.367727/ 59.867531, val:  39.58%, val_best:  52.50%, tr:  99.18%, tr_best:  99.59%, epoch time: 74.18 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9612%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.7276%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 46875  14.963%\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  8.103594/ 54.146282, val:  37.92%, val_best:  52.50%, tr:  99.28%, tr_best:  99.59%, epoch time: 73.84 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.1123%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.0389%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 48263  14.939%\n",
      "fc layer 1 self.abs_max_out: 8870.0\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  8.251477/ 79.689804, val:  41.67%, val_best:  52.50%, tr:  98.77%, tr_best:  99.59%, epoch time: 74.53 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.2717%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.8174%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 49636  14.912%\n",
      "fc layer 1 self.abs_max_out: 8879.0\n",
      "lif layer 1 self.abs_max_v: 16332.5\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  8.382755/ 65.983826, val:  34.17%, val_best:  52.50%, tr:  99.08%, tr_best:  99.59%, epoch time: 74.42 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9927%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.3475%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 51069  14.904%\n",
      "fc layer 1 self.abs_max_out: 8964.0\n",
      "fc layer 1 self.abs_max_out: 8977.0\n",
      "lif layer 1 self.abs_max_v: 16568.0\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  8.290915/ 31.370747, val:  57.92%, val_best:  57.92%, tr:  99.59%, tr_best:  99.59%, epoch time: 74.35 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4363%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.2798%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 52500  14.896%\n",
      "fc layer 1 self.abs_max_out: 9012.0\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  8.404974/ 31.418243, val:  52.92%, val_best:  57.92%, tr:  99.28%, tr_best:  99.59%, epoch time: 74.21 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3471%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.2932%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 53893  14.878%\n",
      "fc layer 1 self.abs_max_out: 9035.0\n",
      "lif layer 1 self.abs_max_v: 16625.0\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  7.611154/ 51.606339, val:  48.33%, val_best:  57.92%, tr:  98.98%, tr_best:  99.59%, epoch time: 74.40 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9507%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.4233%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 55217  14.842%\n",
      "fc layer 1 self.abs_max_out: 9242.0\n",
      "lif layer 1 self.abs_max_v: 16972.5\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  8.279071/ 52.708458, val:  43.75%, val_best:  57.92%, tr:  99.28%, tr_best:  99.59%, epoch time: 74.62 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9721%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.9537%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 56605  14.825%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  8.206953/ 50.602886, val:  43.33%, val_best:  57.92%, tr:  99.18%, tr_best:  99.59%, epoch time: 74.65 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3477%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.5471%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 58012  14.814%\n",
      "fc layer 1 self.abs_max_out: 9561.0\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  7.989115/ 43.784515, val:  45.83%, val_best:  57.92%, tr:  98.88%, tr_best:  99.59%, epoch time: 74.25 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.2364%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.8730%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 59412  14.802%\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  8.577372/ 55.868027, val:  37.50%, val_best:  57.92%, tr:  98.98%, tr_best:  99.59%, epoch time: 74.48 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.2188%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.7516%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 60801  14.787%\n",
      "lif layer 1 self.abs_max_v: 17222.0\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  8.003126/ 58.977051, val:  38.75%, val_best:  57.92%, tr:  99.08%, tr_best:  99.59%, epoch time: 74.91 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8017%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6867%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 62129  14.759%\n",
      "lif layer 1 self.abs_max_v: 17283.0\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  8.387023/ 50.985405, val:  45.83%, val_best:  57.92%, tr:  99.39%, tr_best:  99.59%, epoch time: 75.70 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9178%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.2709%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 63559  14.755%\n",
      "lif layer 1 self.abs_max_v: 17292.5\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  8.605266/ 58.448582, val:  42.08%, val_best:  57.92%, tr:  99.49%, tr_best:  99.59%, epoch time: 75.14 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9663%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.2776%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 65023  14.760%\n",
      "lif layer 1 self.abs_max_v: 17455.0\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  8.512813/ 51.411743, val:  43.75%, val_best:  57.92%, tr:  98.98%, tr_best:  99.59%, epoch time: 75.42 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8249%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6553%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 66448  14.755%\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  8.514881/ 32.338459, val:  54.17%, val_best:  57.92%, tr:  99.49%, tr_best:  99.59%, epoch time: 75.09 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0968%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.3078%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 67845  14.745%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  7.887425/ 66.302025, val:  38.75%, val_best:  57.92%, tr:  99.08%, tr_best:  99.59%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3571%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.8392%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 69143  14.714%\n",
      "fc layer 1 self.abs_max_out: 9581.0\n",
      "lif layer 1 self.abs_max_v: 17621.5\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  8.575712/ 68.120323, val:  45.83%, val_best:  57.92%, tr:  99.28%, tr_best:  99.59%, epoch time: 74.68 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3026%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3993%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 70513  14.699%\n",
      "fc layer 1 self.abs_max_out: 9666.0\n",
      "lif layer 1 self.abs_max_v: 17761.5\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  8.630923/ 56.027660, val:  46.67%, val_best:  57.92%, tr:  99.49%, tr_best:  99.59%, epoch time: 74.85 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0572%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5571%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 71863  14.681%\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  8.755354/ 69.327316, val:  47.50%, val_best:  57.92%, tr:  99.39%, tr_best:  99.59%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6082%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4904%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 73220  14.665%\n",
      "fc layer 1 self.abs_max_out: 9684.0\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  8.777353/ 43.652496, val:  52.92%, val_best:  57.92%, tr:  99.69%, tr_best:  99.69%, epoch time: 75.49 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.4116%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7879%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 74546  14.643%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  9.131687/ 55.503548, val:  50.42%, val_best:  57.92%, tr:  99.59%, tr_best:  99.69%, epoch time: 74.81 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6704%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8861%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 75886  14.625%\n",
      "fc layer 1 self.abs_max_out: 9758.0\n",
      "lif layer 1 self.abs_max_v: 17838.0\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  8.737133/ 77.547829, val:  35.42%, val_best:  57.92%, tr:  99.08%, tr_best:  99.69%, epoch time: 74.61 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1998%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6161%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 77181  14.599%\n",
      "fc layer 3 self.abs_max_out: 492.0\n",
      "fc layer 1 self.abs_max_out: 9838.0\n",
      "fc layer 1 self.abs_max_out: 9998.0\n",
      "lif layer 1 self.abs_max_v: 18314.0\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  9.718200/ 63.640373, val:  42.50%, val_best:  57.92%, tr:  99.08%, tr_best:  99.69%, epoch time: 74.50 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.4489%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0438%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 78549  14.588%\n",
      "fc layer 1 self.abs_max_out: 10028.0\n",
      "lif layer 1 self.abs_max_v: 18367.0\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  9.337645/ 45.237537, val:  45.83%, val_best:  57.92%, tr:  99.49%, tr_best:  99.69%, epoch time: 74.69 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9945%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0970%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 79863  14.567%\n",
      "fc layer 1 self.abs_max_out: 10077.0\n",
      "lif layer 1 self.abs_max_v: 18460.5\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  9.313010/ 58.846092, val:  42.08%, val_best:  57.92%, tr:  99.18%, tr_best:  99.69%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9970%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2939%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 81234  14.557%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  9.128104/ 54.789421, val:  47.08%, val_best:  57.92%, tr:  99.18%, tr_best:  99.69%, epoch time: 74.63 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0180%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5311%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 82608  14.548%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  8.957239/ 58.178825, val:  49.17%, val_best:  57.92%, tr:  99.08%, tr_best:  99.69%, epoch time: 74.75 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6458%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0286%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 83945  14.533%\n",
      "fc layer 1 self.abs_max_out: 10135.0\n",
      "lif layer 1 self.abs_max_v: 18502.0\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  8.932249/ 76.937340, val:  47.08%, val_best:  57.92%, tr:  98.88%, tr_best:  99.69%, epoch time: 75.13 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0171%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5926%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 85238  14.511%\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  9.282709/ 61.065258, val:  41.67%, val_best:  57.92%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9934%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8156%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 86584  14.499%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  9.207584/ 69.067093, val:  40.83%, val_best:  57.92%, tr:  99.59%, tr_best:  99.69%, epoch time: 75.49 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.4116%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9970%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 87938  14.488%\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  9.279432/ 42.580864, val:  53.75%, val_best:  57.92%, tr:  99.28%, tr_best:  99.69%, epoch time: 74.78 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3987%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8228%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 89279  14.475%\n",
      "fc layer 1 self.abs_max_out: 10302.0\n",
      "lif layer 1 self.abs_max_v: 18508.5\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  8.695266/ 63.719917, val:  46.67%, val_best:  57.92%, tr:  99.39%, tr_best:  99.69%, epoch time: 74.92 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9406%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9788%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 90554  14.453%\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  8.531998/ 55.195419, val:  47.50%, val_best:  57.92%, tr:  99.39%, tr_best:  99.69%, epoch time: 74.69 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8954%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7026%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 91825  14.430%\n",
      "fc layer 1 self.abs_max_out: 10305.0\n",
      "lif layer 1 self.abs_max_v: 18710.5\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  8.928644/ 48.910774, val:  52.08%, val_best:  57.92%, tr:  99.80%, tr_best:  99.80%, epoch time: 74.27 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6219%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4767%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 93087  14.407%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  9.376484/ 52.283367, val:  49.58%, val_best:  57.92%, tr:  98.98%, tr_best:  99.80%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6991%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3652%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 94433  14.397%\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  8.847556/ 59.183765, val:  50.00%, val_best:  57.92%, tr:  99.59%, tr_best:  99.80%, epoch time: 74.81 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5261%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1278%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 95700  14.375%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  9.105000/ 37.975815, val:  56.25%, val_best:  57.92%, tr:  99.18%, tr_best:  99.80%, epoch time: 74.45 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9981%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2916%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 96998  14.359%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  8.866415/ 63.894352, val:  42.08%, val_best:  57.92%, tr:  99.39%, tr_best:  99.80%, epoch time: 74.99 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0176%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9705%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 98268  14.339%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  9.271432/ 44.375351, val:  52.08%, val_best:  57.92%, tr:  99.39%, tr_best:  99.80%, epoch time: 75.03 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0562%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2881%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 99594  14.328%\n",
      "fc layer 1 self.abs_max_out: 10430.0\n",
      "lif layer 1 self.abs_max_v: 18906.5\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  8.992161/ 51.762131, val:  49.17%, val_best:  57.92%, tr:  98.77%, tr_best:  99.80%, epoch time: 74.77 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9538%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5986%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 100904  14.315%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  9.040567/ 55.642120, val:  45.00%, val_best:  57.92%, tr:  99.08%, tr_best:  99.80%, epoch time: 75.16 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2523%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1907%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 102247  14.307%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  8.800193/ 64.202606, val:  45.83%, val_best:  57.92%, tr:  99.59%, tr_best:  99.80%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3922%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4721%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 103556  14.294%\n",
      "fc layer 1 self.abs_max_out: 10435.0\n",
      "lif layer 1 self.abs_max_v: 18974.5\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  8.498580/ 47.965412, val:  51.25%, val_best:  57.92%, tr:  99.69%, tr_best:  99.80%, epoch time: 75.09 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8412%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1691%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 104849  14.280%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  8.683766/ 57.025703, val:  54.58%, val_best:  57.92%, tr:  98.47%, tr_best:  99.80%, epoch time: 74.77 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7235%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2081%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 106125  14.263%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  8.829919/ 38.876461, val:  59.58%, val_best:  59.58%, tr:  99.18%, tr_best:  99.80%, epoch time: 75.17 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8395%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0457%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 107437  14.252%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  9.046377/103.218262, val:  32.92%, val_best:  59.58%, tr:  99.49%, tr_best:  99.80%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6171%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1131%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 108768  14.244%\n",
      "fc layer 1 self.abs_max_out: 10555.0\n",
      "lif layer 1 self.abs_max_v: 19065.0\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  8.941682/ 64.491318, val:  45.42%, val_best:  59.58%, tr:  99.69%, tr_best:  99.80%, epoch time: 75.19 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.2986%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1516%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 110078  14.233%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  8.803582/ 61.409515, val:  46.25%, val_best:  59.58%, tr:  99.28%, tr_best:  99.80%, epoch time: 74.81 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.2561%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1193%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 111373  14.220%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  8.420560/ 49.613693, val:  48.75%, val_best:  59.58%, tr:  99.49%, tr_best:  99.80%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9494%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5675%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 112651  14.206%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  8.337037/ 70.024742, val:  39.58%, val_best:  59.58%, tr:  99.08%, tr_best:  99.80%, epoch time: 74.54 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8185%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3993%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 113923  14.191%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  8.975904/ 45.905579, val:  51.67%, val_best:  59.58%, tr:  99.39%, tr_best:  99.80%, epoch time: 74.91 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4138%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0331%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 115248  14.183%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  8.801273/ 79.344223, val:  40.00%, val_best:  59.58%, tr:  99.49%, tr_best:  99.80%, epoch time: 74.86 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2050%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7822%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 116547  14.172%\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  9.077507/ 70.006935, val:  36.67%, val_best:  59.58%, tr:  99.49%, tr_best:  99.80%, epoch time: 74.75 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6642%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3151%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 117890  14.167%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  9.387094/ 60.574951, val:  37.50%, val_best:  59.58%, tr:  98.98%, tr_best:  99.80%, epoch time: 75.10 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2212%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9441%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 119257  14.165%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  8.941143/ 45.032921, val:  53.75%, val_best:  59.58%, tr:  99.18%, tr_best:  99.80%, epoch time: 74.91 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1719%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0967%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 120551  14.154%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  9.666145/ 68.718513, val:  49.58%, val_best:  59.58%, tr:  99.49%, tr_best:  99.80%, epoch time: 75.37 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0004%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0436%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 121931  14.153%\n",
      "fc layer 1 self.abs_max_out: 10696.0\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  9.021950/ 49.901958, val:  57.50%, val_best:  59.58%, tr:  99.39%, tr_best:  99.80%, epoch time: 74.64 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8775%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4009%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 123253  14.146%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  9.632550/ 56.034336, val:  47.08%, val_best:  59.58%, tr:  99.39%, tr_best:  99.80%, epoch time: 74.20 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1450%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3628%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 124607  14.142%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  9.635239/ 59.292355, val:  49.58%, val_best:  59.58%, tr:  99.28%, tr_best:  99.80%, epoch time: 74.76 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3238%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6461%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 125978  14.141%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  8.817713/ 45.748409, val:  57.08%, val_best:  59.58%, tr:  99.69%, tr_best:  99.80%, epoch time: 74.28 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0256%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8998%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 127260  14.129%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  8.963862/ 48.532570, val:  45.00%, val_best:  59.58%, tr:  99.49%, tr_best:  99.80%, epoch time: 74.71 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2119%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3153%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 128579  14.122%\n",
      "lif layer 1 self.abs_max_v: 19206.5\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  8.966483/ 83.562241, val:  37.50%, val_best:  59.58%, tr:  98.98%, tr_best:  99.80%, epoch time: 74.57 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1950%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4203%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 129951  14.121%\n",
      "lif layer 1 self.abs_max_v: 19400.0\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  8.840580/ 42.516235, val:  48.75%, val_best:  59.58%, tr:  99.59%, tr_best:  99.80%, epoch time: 75.15 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6510%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9819%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 131238  14.111%\n",
      "fc layer 1 self.abs_max_out: 10874.0\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  9.373373/ 55.035351, val:  46.67%, val_best:  59.58%, tr:  99.39%, tr_best:  99.80%, epoch time: 75.40 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5335%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2644%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 132600  14.109%\n",
      "lif layer 1 self.abs_max_v: 19522.5\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  8.541291/ 60.665043, val:  44.58%, val_best:  59.58%, tr:  98.77%, tr_best:  99.80%, epoch time: 75.06 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1795%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6389%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 133882  14.098%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  8.506887/ 58.420319, val:  50.83%, val_best:  59.58%, tr:  99.69%, tr_best:  99.80%, epoch time: 74.53 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2749%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2338%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 135164  14.088%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  9.375966/ 72.032692, val:  44.58%, val_best:  59.58%, tr:  99.49%, tr_best:  99.80%, epoch time: 75.90 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2516%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1672%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 136474  14.081%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  8.637880/ 47.478901, val:  44.58%, val_best:  59.58%, tr:  99.59%, tr_best:  99.80%, epoch time: 74.98 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9237%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7419%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 137746  14.070%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  8.596227/ 45.431740, val:  54.17%, val_best:  59.58%, tr:  99.69%, tr_best:  99.80%, epoch time: 74.58 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8632%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0055%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 139024  14.060%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  8.659434/ 43.112206, val:  54.58%, val_best:  59.58%, tr:  99.39%, tr_best:  99.80%, epoch time: 74.22 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9927%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5288%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 140312  14.051%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  8.415353/ 72.915825, val:  40.00%, val_best:  59.58%, tr:  99.18%, tr_best:  99.80%, epoch time: 75.22 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1814%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0391%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 141556  14.038%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  8.468686/ 58.595203, val:  47.08%, val_best:  59.58%, tr:  99.49%, tr_best:  99.80%, epoch time: 74.76 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0171%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4802%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 142827  14.028%\n",
      "lif layer 2 self.abs_max_v: 10646.5\n",
      "lif layer 2 self.abs_max_v: 10851.5\n",
      "lif layer 2 self.abs_max_v: 10982.0\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  9.387770/ 53.971634, val:  45.42%, val_best:  59.58%, tr:  99.28%, tr_best:  99.80%, epoch time: 74.95 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8539%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8094%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 144224  14.030%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  9.147923/ 63.158943, val:  42.92%, val_best:  59.58%, tr:  99.80%, tr_best:  99.80%, epoch time: 75.06 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1016%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5352%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 145582  14.029%\n",
      "fc layer 1 self.abs_max_out: 10918.0\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  8.549037/ 75.392395, val:  32.08%, val_best:  59.58%, tr:  99.18%, tr_best:  99.80%, epoch time: 74.47 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6654%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4726%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 146888  14.022%\n",
      "fc layer 2 self.abs_max_out: 6004.0\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  8.782226/ 60.520409, val:  47.92%, val_best:  59.58%, tr:  99.18%, tr_best:  99.80%, epoch time: 74.99 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3849%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2815%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 148262  14.022%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  9.118586/ 40.598145, val:  55.83%, val_best:  59.58%, tr:  99.18%, tr_best:  99.80%, epoch time: 74.94 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4724%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9797%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 149642  14.023%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  8.473904/ 58.100586, val:  40.42%, val_best:  59.58%, tr:  99.69%, tr_best:  99.80%, epoch time: 75.13 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2588%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7834%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 150934  14.016%\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  8.720882/ 48.526455, val:  50.42%, val_best:  59.58%, tr:  98.77%, tr_best:  99.80%, epoch time: 74.63 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4503%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9190%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 152254  14.011%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  8.825509/ 52.940609, val:  53.75%, val_best:  59.58%, tr:  99.69%, tr_best:  99.80%, epoch time: 74.47 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3843%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4093%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 153571  14.006%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  8.812537/ 43.196552, val:  60.83%, val_best:  60.83%, tr:  99.18%, tr_best:  99.80%, epoch time: 75.01 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3228%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0739%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 154875  14.000%\n",
      "fc layer 2 self.abs_max_out: 6036.0\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  8.555963/ 46.912674, val:  51.67%, val_best:  60.83%, tr:  99.39%, tr_best:  99.80%, epoch time: 74.86 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3685%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1867%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 156151  13.991%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  8.725032/ 78.588089, val:  41.25%, val_best:  60.83%, tr:  99.28%, tr_best:  99.80%, epoch time: 74.59 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4523%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7010%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 157444  13.984%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  9.060516/ 48.024113, val:  52.08%, val_best:  60.83%, tr:  99.28%, tr_best:  99.80%, epoch time: 74.52 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4839%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3012%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 158793  13.983%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  8.955589/ 62.065449, val:  51.67%, val_best:  60.83%, tr:  99.49%, tr_best:  99.80%, epoch time: 74.91 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4277%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3291%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 160120  13.979%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  9.099080/ 50.884178, val:  49.17%, val_best:  60.83%, tr:  98.88%, tr_best:  99.80%, epoch time: 74.47 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5139%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2767%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 161440  13.975%\n",
      "lif layer 1 self.abs_max_v: 19887.5\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  8.855742/ 54.187916, val:  46.25%, val_best:  60.83%, tr:  99.59%, tr_best:  99.80%, epoch time: 74.92 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3272%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6340%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 162773  13.972%\n",
      "fc layer 1 self.abs_max_out: 11074.0\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  8.652999/ 65.404610, val:  48.75%, val_best:  60.83%, tr:  99.18%, tr_best:  99.80%, epoch time: 74.65 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1305%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9780%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 164040  13.963%\n",
      "fc layer 2 self.abs_max_out: 6219.0\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  8.685905/ 55.062115, val:  53.75%, val_best:  60.83%, tr:  99.08%, tr_best:  99.80%, epoch time: 74.91 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3614%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2515%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 165293  13.954%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  9.171173/ 66.423874, val:  44.58%, val_best:  60.83%, tr:  99.08%, tr_best:  99.80%, epoch time: 74.70 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4185%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3987%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 166630  13.951%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  8.609550/ 60.288303, val:  46.67%, val_best:  60.83%, tr:  99.39%, tr_best:  99.80%, epoch time: 74.67 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1936%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4847%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 167956  13.948%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  9.385530/ 45.894966, val:  54.17%, val_best:  60.83%, tr:  99.49%, tr_best:  99.80%, epoch time: 74.94 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8101%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4539%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 169327  13.948%\n",
      "fc layer 3 self.abs_max_out: 502.0\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  9.213357/ 43.825809, val:  63.75%, val_best:  63.75%, tr:  99.18%, tr_best:  99.80%, epoch time: 74.75 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0953%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0510%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 170696  13.949%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  8.924623/ 71.458580, val:  42.92%, val_best:  63.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5947%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7211%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 172023  13.945%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  9.115834/ 76.390022, val:  41.25%, val_best:  63.75%, tr:  99.28%, tr_best:  99.80%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0507%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2956%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 173369  13.944%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  8.400571/ 53.375328, val:  54.17%, val_best:  63.75%, tr:  99.39%, tr_best:  99.80%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2068%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7473%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 174641  13.936%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  8.670365/ 53.505299, val:  53.75%, val_best:  63.75%, tr:  99.59%, tr_best:  99.80%, epoch time: 74.91 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7350%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2705%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 175897  13.928%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  9.236348/ 68.126541, val:  36.25%, val_best:  63.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 74.79 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3865%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3920%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 177224  13.925%\n",
      "fc layer 2 self.abs_max_out: 6320.0\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  8.847984/ 83.976349, val:  41.67%, val_best:  63.75%, tr:  99.69%, tr_best:  99.80%, epoch time: 74.66 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9285%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1651%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 178561  13.923%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  9.144312/ 75.519257, val:  42.92%, val_best:  63.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 74.84 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4216%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1287%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 179897  13.921%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  8.554553/ 72.416931, val:  44.58%, val_best:  63.75%, tr:  99.18%, tr_best:  99.80%, epoch time: 74.24 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1253%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4324%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 181161  13.913%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  9.267415/ 50.766678, val:  52.50%, val_best:  63.75%, tr:  99.39%, tr_best:  99.80%, epoch time: 74.82 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0539%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3029%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 182506  13.912%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  8.960740/ 51.662468, val:  47.92%, val_best:  63.75%, tr:  99.59%, tr_best:  99.80%, epoch time: 74.83 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1330%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4496%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 183816  13.908%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  9.480199/ 49.543903, val:  55.42%, val_best:  63.75%, tr:  99.39%, tr_best:  99.80%, epoch time: 74.70 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.1934%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4748%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 185195  13.909%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  8.660468/ 44.126232, val:  56.67%, val_best:  63.75%, tr:  99.18%, tr_best:  99.80%, epoch time: 74.83 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1182%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2050%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 186456  13.902%\n",
      "fc layer 2 self.abs_max_out: 6398.0\n",
      "fc layer 3 self.abs_max_out: 533.0\n",
      "fc layer 3 self.abs_max_out: 538.0\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  8.586993/ 57.543869, val:  47.50%, val_best:  63.75%, tr:  99.69%, tr_best:  99.80%, epoch time: 74.87 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8231%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9581%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 187702  13.893%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  8.219603/ 56.266712, val:  53.33%, val_best:  63.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 75.08 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9947%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6452%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 188960  13.886%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  9.367583/ 47.586487, val:  58.75%, val_best:  63.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9468%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8652%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 190326  13.886%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  9.065801/ 54.714027, val:  45.42%, val_best:  63.75%, tr:  98.88%, tr_best:  99.80%, epoch time: 74.95 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5553%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6164%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 191660  13.884%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  9.035006/ 60.149902, val:  41.25%, val_best:  63.75%, tr:  98.88%, tr_best:  99.80%, epoch time: 75.27 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1852%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8199%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 192997  13.883%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  9.277402/ 71.501366, val:  42.92%, val_best:  63.75%, tr:  99.69%, tr_best:  99.80%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6692%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3629%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 194382  13.885%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  8.681242/ 58.434319, val:  45.42%, val_best:  63.75%, tr:  99.69%, tr_best:  99.80%, epoch time: 75.57 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3719%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0061%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 195699  13.882%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  8.880188/ 54.127052, val:  46.25%, val_best:  63.75%, tr:  99.28%, tr_best:  99.80%, epoch time: 75.49 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9654%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3440%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 197026  13.879%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  8.898032/ 69.442078, val:  47.50%, val_best:  63.75%, tr:  98.98%, tr_best:  99.80%, epoch time: 75.07 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1781%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2204%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 198337  13.876%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  8.734390/ 51.808575, val:  51.67%, val_best:  63.75%, tr:  99.90%, tr_best:  99.90%, epoch time: 74.87 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2153%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9218%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 199648  13.873%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  8.724197/ 67.564888, val:  34.58%, val_best:  63.75%, tr:  99.28%, tr_best:  99.90%, epoch time: 74.74 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8514%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0218%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 200923  13.867%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  8.700816/ 67.815575, val:  40.00%, val_best:  63.75%, tr:  99.90%, tr_best:  99.90%, epoch time: 74.89 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9653%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4536%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 202236  13.864%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  8.921406/ 47.868237, val:  50.42%, val_best:  63.75%, tr:  99.39%, tr_best:  99.90%, epoch time: 75.32 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6133%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7738%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 203585  13.863%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  8.734448/ 56.000443, val:  46.25%, val_best:  63.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 75.12 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8800%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9541%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 204899  13.861%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  8.740958/ 45.246750, val:  58.33%, val_best:  63.75%, tr:  99.39%, tr_best:  99.90%, epoch time: 75.17 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5635%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2928%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 206219  13.858%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  8.699746/ 42.465313, val:  52.92%, val_best:  63.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 74.88 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9292%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9221%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 207535  13.855%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  8.667215/ 52.617176, val:  52.50%, val_best:  63.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 75.32 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9745%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2614%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 208862  13.853%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  8.816236/ 59.089054, val:  48.33%, val_best:  63.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 74.58 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3678%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2575%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 210162  13.850%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  8.569080/ 49.233234, val:  49.58%, val_best:  63.75%, tr:  99.80%, tr_best:  99.90%, epoch time: 74.88 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3184%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1733%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 211425  13.844%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  8.789013/ 49.065403, val:  48.33%, val_best:  63.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 75.09 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7482%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6747%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 212720  13.840%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  8.364447/ 40.710415, val:  58.75%, val_best:  63.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 74.32 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3627%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9605%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 213959  13.832%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  8.344962/ 58.357944, val:  44.58%, val_best:  63.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 74.71 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9219%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7060%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 215236  13.827%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  8.437063/ 53.686695, val:  51.25%, val_best:  63.75%, tr:  99.18%, tr_best:  99.90%, epoch time: 74.73 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9179%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6792%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 216543  13.824%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  8.291660/ 69.129822, val:  47.50%, val_best:  63.75%, tr:  99.18%, tr_best:  99.90%, epoch time: 74.80 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0208%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3602%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 217796  13.818%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  8.340148/ 49.630356, val:  54.17%, val_best:  63.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 74.29 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1631%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7598%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 219092  13.814%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  8.590837/ 59.252213, val:  45.83%, val_best:  63.75%, tr:  99.08%, tr_best:  99.90%, epoch time: 75.66 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9455%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4846%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 220397  13.811%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  8.472818/ 53.194641, val:  50.83%, val_best:  63.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 74.80 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0856%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8880%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 221658  13.806%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  8.263106/ 52.396797, val:  59.58%, val_best:  63.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 74.66 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9308%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4747%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 222881  13.798%\n",
      "fc layer 1 self.abs_max_out: 11274.0\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  8.792165/ 54.623737, val:  49.17%, val_best:  63.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 74.51 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1810%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7909%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 224183  13.795%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  8.485351/ 70.123978, val:  42.92%, val_best:  63.75%, tr:  99.39%, tr_best:  99.90%, epoch time: 74.58 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0557%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7365%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 225447  13.789%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  8.750375/ 79.177597, val:  42.50%, val_best:  63.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 75.30 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.4680%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5316%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 226744  13.786%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  8.506248/ 71.067490, val:  42.50%, val_best:  63.75%, tr:  99.18%, tr_best:  99.90%, epoch time: 74.77 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0328%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0922%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 227977  13.779%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  8.013500/ 48.269123, val:  57.92%, val_best:  63.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 74.94 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0769%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7751%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 229224  13.773%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  8.198302/ 46.203617, val:  47.50%, val_best:  63.75%, tr:  99.18%, tr_best:  99.90%, epoch time: 75.00 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1141%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1724%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0473%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 230498  13.769%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  8.623373/ 69.644188, val:  46.25%, val_best:  63.75%, tr:  99.28%, tr_best:  99.90%, epoch time: 74.78 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1002%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7372%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 231792  13.765%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  8.616322/ 35.768490, val:  59.58%, val_best:  63.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 74.84 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1055%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7064%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 233100  13.763%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  7.936209/ 62.396358, val:  52.50%, val_best:  63.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 74.80 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1033%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9433%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 234321  13.756%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  8.712421/ 45.888268, val:  54.17%, val_best:  63.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 74.95 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9658%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6733%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 235631  13.753%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  8.597543/ 50.116703, val:  52.50%, val_best:  63.75%, tr:  98.98%, tr_best:  99.90%, epoch time: 74.66 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7756%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6725%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 236963  13.753%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  8.782758/ 57.156937, val:  48.33%, val_best:  63.75%, tr:  99.80%, tr_best:  99.90%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7683%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6156%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 238290  13.751%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  8.475679/ 44.845722, val:  53.33%, val_best:  63.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 74.82 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0312%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6689%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6688%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 239575  13.748%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  8.143487/ 83.774918, val:  43.33%, val_best:  63.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0376%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6267%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1843%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 240795  13.741%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  8.922530/ 60.747871, val:  42.92%, val_best:  63.75%, tr:  99.39%, tr_best:  99.90%, epoch time: 75.45 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8448%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2050%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 242112  13.739%\n",
      "lif layer 2 self.abs_max_v: 10987.5\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  8.678875/ 62.579105, val:  52.08%, val_best:  63.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 74.31 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1832%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1871%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 243388  13.735%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  8.329081/ 44.897236, val:  54.17%, val_best:  63.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 74.94 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6456%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0539%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 244641  13.730%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  9.115489/ 43.226482, val:  61.25%, val_best:  63.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 75.63 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.2969%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9640%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 245963  13.729%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  8.638338/ 65.960991, val:  44.17%, val_best:  63.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 74.76 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0426%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6498%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5856%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 247269  13.727%\n",
      "lif layer 2 self.abs_max_v: 11102.5\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  8.513758/ 43.107471, val:  58.33%, val_best:  63.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 74.79 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7893%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5122%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 248564  13.724%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  9.282654/ 50.455288, val:  49.17%, val_best:  63.75%, tr:  99.80%, tr_best:  99.90%, epoch time: 75.01 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7706%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0881%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 249906  13.724%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  8.757848/ 47.894287, val:  56.67%, val_best:  63.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 74.17 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3695%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9055%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 251219  13.722%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  8.963778/ 87.875710, val:  41.67%, val_best:  63.75%, tr:  99.80%, tr_best:  99.90%, epoch time: 74.70 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3941%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2286%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 252571  13.723%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  8.697902/ 58.920799, val:  44.17%, val_best:  63.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 75.48 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0127%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2914%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 253840  13.719%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  8.118160/ 45.529285, val:  54.58%, val_best:  63.75%, tr:  99.90%, tr_best:  99.90%, epoch time: 74.93 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9413%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7644%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 255099  13.714%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  8.774909/ 39.953686, val:  51.67%, val_best:  63.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 75.35 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6134%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8732%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 256427  13.713%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  8.449022/ 68.600624, val:  52.50%, val_best:  63.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 74.58 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5046%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6903%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 257714  13.711%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  8.716976/ 59.909103, val:  47.92%, val_best:  63.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 74.87 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5470%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3046%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 259001  13.708%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  8.506473/ 70.527809, val:  45.42%, val_best:  63.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 75.64 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7628%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9587%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 260303  13.705%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  8.173675/ 81.225220, val:  46.25%, val_best:  63.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6247%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0321%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 261577  13.702%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  8.739790/ 65.847282, val:  46.25%, val_best:  63.75%, tr:  99.39%, tr_best:  99.90%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5743%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8705%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 262900  13.701%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  8.743343/ 61.447708, val:  46.25%, val_best:  63.75%, tr:  99.80%, tr_best:  99.90%, epoch time: 75.00 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7813%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1165%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 264240  13.701%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  8.496355/ 43.592937, val:  55.42%, val_best:  63.75%, tr:  99.18%, tr_best:  99.90%, epoch time: 75.03 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7082%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0580%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 265539  13.699%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  8.415306/ 49.625946, val:  50.42%, val_best:  63.75%, tr:  99.18%, tr_best:  99.90%, epoch time: 74.68 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.4105%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2363%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 266834  13.696%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  8.288857/ 58.045551, val:  50.42%, val_best:  63.75%, tr:  99.18%, tr_best:  99.90%, epoch time: 74.68 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.4658%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1384%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e952987f7f094cc2a2514c3ccd7c5ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÉ‚ñÑ‚ñÅ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñà‚ñÉ‚ñÑ‚ñá‚ñÖ‚ñÉ‚ñÜ‚ñÜ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÜ‚ñá‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÇ‚ñÜ‚ñà‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñÑ‚ñÖ‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñà‚ñá‚ñá‚ñá‚ñÑ‚ñá‚ñÖ‚ñá‚ñà‚ñà‚ñÜ‚ñÖ‚ñà‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñà‚ñÜ‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñÖ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÉ‚ñÑ‚ñÅ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñà‚ñÉ‚ñÑ‚ñá‚ñÖ‚ñÉ‚ñÜ‚ñÜ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÜ‚ñá‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÇ‚ñÜ‚ñà‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñÑ‚ñÖ‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÖ‚ñÑ‚ñÉ‚ñà‚ñÖ‚ñÖ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÜ‚ñÑ‚ñÇ‚ñÑ‚ñá‚ñÖ‚ñÑ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99183</td></tr><tr><td>tr_epoch_loss</td><td>8.28886</td></tr><tr><td>val_acc_best</td><td>0.6375</td></tr><tr><td>val_acc_now</td><td>0.50417</td></tr><tr><td>val_loss</td><td>58.04555</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gentle-sweep-17</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5ejik4kr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5ejik4kr</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_132955-5ejik4kr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nezllsmx with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.046875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0234375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_173934-nezllsmx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nezllsmx' target=\"_blank\">celestial-sweep-25</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/e1m59f1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nezllsmx' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nezllsmx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': '20251214_173944_303', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 16, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 32, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 0.5, 'lif_layer_v_threshold2': 16, 'init_scaling': [0.03125, 0.046875, 0.0234375], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 32, self.v_threshold 16\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 0.5, self.v_threshold 16\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=16, v_reset=10000, sg_width=32, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=16, v_reset=10000, sg_width=0.5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.046875, 0.0234375])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 46.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 41.0\n",
      "lif layer 2 self.abs_max_v: 41.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 21.0\n",
      "fc layer 1 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 51.0\n",
      "fc layer 2 self.abs_max_out: 80.0\n",
      "lif layer 2 self.abs_max_v: 94.5\n",
      "fc layer 1 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 69.0\n",
      "lif layer 2 self.abs_max_v: 107.5\n",
      "fc layer 3 self.abs_max_out: 50.0\n",
      "fc layer 1 self.abs_max_out: 62.0\n",
      "lif layer 1 self.abs_max_v: 90.0\n",
      "fc layer 1 self.abs_max_out: 101.0\n",
      "lif layer 1 self.abs_max_v: 109.0\n",
      "fc layer 1 self.abs_max_out: 109.0\n",
      "lif layer 1 self.abs_max_v: 112.5\n",
      "fc layer 2 self.abs_max_out: 83.0\n",
      "lif layer 2 self.abs_max_v: 116.0\n",
      "fc layer 1 self.abs_max_out: 122.0\n",
      "lif layer 1 self.abs_max_v: 131.5\n",
      "fc layer 2 self.abs_max_out: 100.0\n",
      "lif layer 2 self.abs_max_v: 158.0\n",
      "fc layer 2 self.abs_max_out: 101.0\n",
      "lif layer 2 self.abs_max_v: 165.0\n",
      "fc layer 3 self.abs_max_out: 59.0\n",
      "lif layer 1 self.abs_max_v: 132.5\n",
      "lif layer 1 self.abs_max_v: 139.5\n",
      "lif layer 1 self.abs_max_v: 182.0\n",
      "fc layer 1 self.abs_max_out: 138.0\n",
      "fc layer 2 self.abs_max_out: 118.0\n",
      "lif layer 2 self.abs_max_v: 174.5\n",
      "fc layer 3 self.abs_max_out: 69.0\n",
      "fc layer 2 self.abs_max_out: 119.0\n",
      "fc layer 1 self.abs_max_out: 181.0\n",
      "fc layer 1 self.abs_max_out: 187.0\n",
      "lif layer 1 self.abs_max_v: 232.5\n",
      "fc layer 2 self.abs_max_out: 126.0\n",
      "fc layer 2 self.abs_max_out: 129.0\n",
      "fc layer 2 self.abs_max_out: 138.0\n",
      "fc layer 2 self.abs_max_out: 141.0\n",
      "fc layer 3 self.abs_max_out: 94.0\n",
      "fc layer 2 self.abs_max_out: 183.0\n",
      "lif layer 2 self.abs_max_v: 229.0\n",
      "fc layer 1 self.abs_max_out: 248.0\n",
      "lif layer 1 self.abs_max_v: 248.0\n",
      "fc layer 2 self.abs_max_out: 186.0\n",
      "lif layer 2 self.abs_max_v: 251.5\n",
      "lif layer 2 self.abs_max_v: 300.0\n",
      "lif layer 2 self.abs_max_v: 310.0\n",
      "fc layer 2 self.abs_max_out: 193.0\n",
      "lif layer 2 self.abs_max_v: 313.0\n",
      "lif layer 1 self.abs_max_v: 264.0\n",
      "fc layer 2 self.abs_max_out: 198.0\n",
      "lif layer 1 self.abs_max_v: 287.0\n",
      "lif layer 1 self.abs_max_v: 298.5\n",
      "lif layer 1 self.abs_max_v: 324.5\n",
      "fc layer 2 self.abs_max_out: 215.0\n",
      "fc layer 3 self.abs_max_out: 98.0\n",
      "fc layer 2 self.abs_max_out: 220.0\n",
      "fc layer 3 self.abs_max_out: 106.0\n",
      "lif layer 1 self.abs_max_v: 342.0\n",
      "fc layer 2 self.abs_max_out: 227.0\n",
      "fc layer 2 self.abs_max_out: 255.0\n",
      "lif layer 1 self.abs_max_v: 347.5\n",
      "fc layer 1 self.abs_max_out: 286.0\n",
      "lif layer 1 self.abs_max_v: 379.0\n",
      "fc layer 3 self.abs_max_out: 109.0\n",
      "fc layer 3 self.abs_max_out: 127.0\n",
      "fc layer 3 self.abs_max_out: 131.0\n",
      "lif layer 1 self.abs_max_v: 391.0\n",
      "fc layer 1 self.abs_max_out: 293.0\n",
      "lif layer 1 self.abs_max_v: 427.0\n",
      "fc layer 2 self.abs_max_out: 293.0\n",
      "lif layer 2 self.abs_max_v: 330.5\n",
      "fc layer 1 self.abs_max_out: 303.0\n",
      "lif layer 2 self.abs_max_v: 338.0\n",
      "lif layer 2 self.abs_max_v: 375.0\n",
      "lif layer 2 self.abs_max_v: 405.5\n",
      "lif layer 2 self.abs_max_v: 421.0\n",
      "lif layer 1 self.abs_max_v: 468.5\n",
      "fc layer 1 self.abs_max_out: 332.0\n",
      "fc layer 3 self.abs_max_out: 133.0\n",
      "fc layer 3 self.abs_max_out: 138.0\n",
      "lif layer 2 self.abs_max_v: 429.5\n",
      "lif layer 2 self.abs_max_v: 465.0\n",
      "lif layer 2 self.abs_max_v: 468.0\n",
      "lif layer 2 self.abs_max_v: 497.0\n",
      "fc layer 3 self.abs_max_out: 142.0\n",
      "lif layer 1 self.abs_max_v: 480.0\n",
      "lif layer 1 self.abs_max_v: 488.0\n",
      "fc layer 3 self.abs_max_out: 154.0\n",
      "fc layer 3 self.abs_max_out: 160.0\n",
      "fc layer 2 self.abs_max_out: 300.0\n",
      "fc layer 3 self.abs_max_out: 187.0\n",
      "fc layer 3 self.abs_max_out: 188.0\n",
      "fc layer 2 self.abs_max_out: 305.0\n",
      "fc layer 2 self.abs_max_out: 318.0\n",
      "fc layer 3 self.abs_max_out: 214.0\n",
      "fc layer 3 self.abs_max_out: 222.0\n",
      "fc layer 3 self.abs_max_out: 225.0\n",
      "fc layer 2 self.abs_max_out: 339.0\n",
      "fc layer 2 self.abs_max_out: 356.0\n",
      "lif layer 2 self.abs_max_v: 502.5\n",
      "lif layer 2 self.abs_max_v: 518.5\n",
      "lif layer 2 self.abs_max_v: 529.5\n",
      "lif layer 2 self.abs_max_v: 532.0\n",
      "fc layer 2 self.abs_max_out: 359.0\n",
      "lif layer 2 self.abs_max_v: 543.0\n",
      "lif layer 2 self.abs_max_v: 557.5\n",
      "lif layer 2 self.abs_max_v: 562.0\n",
      "lif layer 2 self.abs_max_v: 565.0\n",
      "fc layer 2 self.abs_max_out: 388.0\n",
      "fc layer 2 self.abs_max_out: 391.0\n",
      "fc layer 2 self.abs_max_out: 404.0\n",
      "lif layer 2 self.abs_max_v: 575.5\n",
      "lif layer 2 self.abs_max_v: 579.0\n",
      "fc layer 1 self.abs_max_out: 351.0\n",
      "lif layer 1 self.abs_max_v: 532.5\n",
      "lif layer 1 self.abs_max_v: 537.5\n",
      "lif layer 1 self.abs_max_v: 579.0\n",
      "fc layer 1 self.abs_max_out: 370.0\n",
      "lif layer 1 self.abs_max_v: 618.5\n",
      "fc layer 1 self.abs_max_out: 419.0\n",
      "fc layer 2 self.abs_max_out: 409.0\n",
      "lif layer 2 self.abs_max_v: 598.0\n",
      "lif layer 2 self.abs_max_v: 599.5\n",
      "fc layer 3 self.abs_max_out: 243.0\n",
      "lif layer 2 self.abs_max_v: 619.0\n",
      "lif layer 2 self.abs_max_v: 634.5\n",
      "lif layer 2 self.abs_max_v: 636.5\n",
      "lif layer 2 self.abs_max_v: 637.5\n",
      "fc layer 3 self.abs_max_out: 272.0\n",
      "fc layer 2 self.abs_max_out: 430.0\n",
      "lif layer 2 self.abs_max_v: 648.5\n",
      "lif layer 2 self.abs_max_v: 657.5\n",
      "lif layer 2 self.abs_max_v: 686.5\n",
      "lif layer 2 self.abs_max_v: 742.5\n",
      "lif layer 2 self.abs_max_v: 759.5\n",
      "lif layer 2 self.abs_max_v: 764.0\n",
      "lif layer 1 self.abs_max_v: 705.0\n",
      "lif layer 1 self.abs_max_v: 712.5\n",
      "fc layer 1 self.abs_max_out: 422.0\n",
      "fc layer 3 self.abs_max_out: 279.0\n",
      "lif layer 1 self.abs_max_v: 725.5\n",
      "lif layer 1 self.abs_max_v: 732.0\n",
      "fc layer 1 self.abs_max_out: 450.0\n",
      "lif layer 1 self.abs_max_v: 774.5\n",
      "lif layer 1 self.abs_max_v: 792.5\n",
      "fc layer 2 self.abs_max_out: 439.0\n",
      "fc layer 2 self.abs_max_out: 443.0\n",
      "fc layer 2 self.abs_max_out: 446.0\n",
      "lif layer 2 self.abs_max_v: 769.0\n",
      "lif layer 2 self.abs_max_v: 770.5\n",
      "lif layer 2 self.abs_max_v: 775.5\n",
      "fc layer 2 self.abs_max_out: 456.0\n",
      "fc layer 3 self.abs_max_out: 280.0\n",
      "lif layer 2 self.abs_max_v: 776.0\n",
      "lif layer 2 self.abs_max_v: 783.0\n",
      "lif layer 2 self.abs_max_v: 789.5\n",
      "fc layer 2 self.abs_max_out: 494.0\n",
      "fc layer 2 self.abs_max_out: 496.0\n",
      "fc layer 2 self.abs_max_out: 498.0\n",
      "lif layer 2 self.abs_max_v: 792.0\n",
      "lif layer 2 self.abs_max_v: 794.5\n",
      "fc layer 3 self.abs_max_out: 297.0\n",
      "fc layer 3 self.abs_max_out: 302.0\n",
      "fc layer 3 self.abs_max_out: 326.0\n",
      "lif layer 2 self.abs_max_v: 832.0\n",
      "lif layer 2 self.abs_max_v: 891.0\n",
      "lif layer 2 self.abs_max_v: 926.5\n",
      "lif layer 2 self.abs_max_v: 939.5\n",
      "fc layer 1 self.abs_max_out: 559.0\n",
      "lif layer 1 self.abs_max_v: 858.5\n",
      "lif layer 1 self.abs_max_v: 886.5\n",
      "fc layer 2 self.abs_max_out: 506.0\n",
      "lif layer 1 self.abs_max_v: 889.0\n",
      "lif layer 1 self.abs_max_v: 928.5\n",
      "lif layer 1 self.abs_max_v: 966.5\n",
      "lif layer 2 self.abs_max_v: 941.5\n",
      "fc layer 2 self.abs_max_out: 536.0\n",
      "fc layer 2 self.abs_max_out: 545.0\n",
      "fc layer 3 self.abs_max_out: 360.0\n",
      "lif layer 2 self.abs_max_v: 942.0\n",
      "lif layer 2 self.abs_max_v: 943.0\n",
      "lif layer 2 self.abs_max_v: 973.5\n",
      "lif layer 2 self.abs_max_v: 1002.0\n",
      "fc layer 2 self.abs_max_out: 547.0\n",
      "fc layer 2 self.abs_max_out: 579.0\n",
      "fc layer 2 self.abs_max_out: 604.0\n",
      "fc layer 1 self.abs_max_out: 602.0\n",
      "lif layer 1 self.abs_max_v: 977.5\n",
      "fc layer 1 self.abs_max_out: 644.0\n",
      "lif layer 1 self.abs_max_v: 1100.5\n",
      "lif layer 1 self.abs_max_v: 1134.5\n",
      "lif layer 1 self.abs_max_v: 1140.5\n",
      "fc layer 2 self.abs_max_out: 613.0\n",
      "fc layer 2 self.abs_max_out: 614.0\n",
      "fc layer 2 self.abs_max_out: 623.0\n",
      "fc layer 3 self.abs_max_out: 418.0\n",
      "fc layer 2 self.abs_max_out: 625.0\n",
      "fc layer 2 self.abs_max_out: 627.0\n",
      "lif layer 2 self.abs_max_v: 1006.5\n",
      "fc layer 2 self.abs_max_out: 643.0\n",
      "fc layer 2 self.abs_max_out: 703.0\n",
      "fc layer 2 self.abs_max_out: 742.0\n",
      "fc layer 2 self.abs_max_out: 745.0\n",
      "fc layer 2 self.abs_max_out: 756.0\n",
      "fc layer 2 self.abs_max_out: 763.0\n",
      "fc layer 2 self.abs_max_out: 807.0\n",
      "fc layer 2 self.abs_max_out: 808.0\n",
      "fc layer 2 self.abs_max_out: 819.0\n",
      "fc layer 2 self.abs_max_out: 830.0\n",
      "lif layer 2 self.abs_max_v: 1068.5\n",
      "lif layer 2 self.abs_max_v: 1183.0\n",
      "fc layer 1 self.abs_max_out: 676.0\n",
      "fc layer 2 self.abs_max_out: 854.0\n",
      "fc layer 2 self.abs_max_out: 875.0\n",
      "fc layer 2 self.abs_max_out: 881.0\n",
      "lif layer 1 self.abs_max_v: 1145.0\n",
      "fc layer 2 self.abs_max_out: 885.0\n",
      "fc layer 2 self.abs_max_out: 922.0\n",
      "lif layer 2 self.abs_max_v: 1184.0\n",
      "lif layer 2 self.abs_max_v: 1190.0\n",
      "lif layer 2 self.abs_max_v: 1223.0\n",
      "lif layer 1 self.abs_max_v: 1172.5\n",
      "fc layer 1 self.abs_max_out: 677.0\n",
      "lif layer 1 self.abs_max_v: 1241.0\n",
      "lif layer 1 self.abs_max_v: 1258.0\n",
      "fc layer 1 self.abs_max_out: 678.0\n",
      "fc layer 2 self.abs_max_out: 930.0\n",
      "fc layer 1 self.abs_max_out: 683.0\n",
      "fc layer 1 self.abs_max_out: 704.0\n",
      "lif layer 1 self.abs_max_v: 1282.0\n",
      "fc layer 1 self.abs_max_out: 739.0\n",
      "fc layer 1 self.abs_max_out: 787.0\n",
      "fc layer 1 self.abs_max_out: 847.0\n",
      "lif layer 1 self.abs_max_v: 1469.0\n",
      "lif layer 1 self.abs_max_v: 1529.5\n",
      "lif layer 1 self.abs_max_v: 1595.0\n",
      "lif layer 2 self.abs_max_v: 1223.5\n",
      "lif layer 2 self.abs_max_v: 1246.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 10.426725/ 58.083817, val:  32.50%, val_best:  32.50%, tr:  99.18%, tr_best:  99.18%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2020%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.6020%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 1348  13.769%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 932.0\n",
      "fc layer 2 self.abs_max_out: 949.0\n",
      "fc layer 2 self.abs_max_out: 969.0\n",
      "fc layer 3 self.abs_max_out: 446.0\n",
      "fc layer 2 self.abs_max_out: 997.0\n",
      "fc layer 3 self.abs_max_out: 485.0\n",
      "fc layer 2 self.abs_max_out: 1011.0\n",
      "fc layer 2 self.abs_max_out: 1023.0\n",
      "fc layer 2 self.abs_max_out: 1040.0\n",
      "fc layer 2 self.abs_max_out: 1042.0\n",
      "fc layer 2 self.abs_max_out: 1048.0\n",
      "fc layer 2 self.abs_max_out: 1078.0\n",
      "lif layer 2 self.abs_max_v: 1309.5\n",
      "lif layer 2 self.abs_max_v: 1325.5\n",
      "lif layer 2 self.abs_max_v: 1381.0\n",
      "fc layer 2 self.abs_max_out: 1086.0\n",
      "fc layer 2 self.abs_max_out: 1122.0\n",
      "lif layer 2 self.abs_max_v: 1400.0\n",
      "fc layer 2 self.abs_max_out: 1124.0\n",
      "fc layer 2 self.abs_max_out: 1126.0\n",
      "fc layer 2 self.abs_max_out: 1141.0\n",
      "fc layer 2 self.abs_max_out: 1211.0\n",
      "lif layer 2 self.abs_max_v: 1405.5\n",
      "lif layer 2 self.abs_max_v: 1409.0\n",
      "lif layer 2 self.abs_max_v: 1422.5\n",
      "lif layer 2 self.abs_max_v: 1446.0\n",
      "lif layer 2 self.abs_max_v: 1448.5\n",
      "fc layer 2 self.abs_max_out: 1224.0\n",
      "fc layer 2 self.abs_max_out: 1239.0\n",
      "fc layer 2 self.abs_max_out: 1249.0\n",
      "fc layer 1 self.abs_max_out: 886.0\n",
      "lif layer 2 self.abs_max_v: 1453.5\n",
      "lif layer 2 self.abs_max_v: 1555.5\n",
      "lif layer 2 self.abs_max_v: 1629.0\n",
      "fc layer 2 self.abs_max_out: 1256.0\n",
      "lif layer 2 self.abs_max_v: 1703.0\n",
      "fc layer 2 self.abs_max_out: 1289.0\n",
      "fc layer 2 self.abs_max_out: 1338.0\n",
      "lif layer 2 self.abs_max_v: 1713.5\n",
      "lif layer 2 self.abs_max_v: 1715.5\n",
      "lif layer 2 self.abs_max_v: 1809.5\n",
      "fc layer 1 self.abs_max_out: 888.0\n",
      "fc layer 1 self.abs_max_out: 913.0\n",
      "fc layer 1 self.abs_max_out: 947.0\n",
      "lif layer 1 self.abs_max_v: 1725.0\n",
      "fc layer 1 self.abs_max_out: 961.0\n",
      "fc layer 1 self.abs_max_out: 1039.0\n",
      "lif layer 1 self.abs_max_v: 1833.5\n",
      "lif layer 1 self.abs_max_v: 1928.0\n",
      "fc layer 1 self.abs_max_out: 1045.0\n",
      "lif layer 1 self.abs_max_v: 2009.0\n",
      "lif layer 2 self.abs_max_v: 1820.0\n",
      "lif layer 2 self.abs_max_v: 1846.0\n",
      "lif layer 2 self.abs_max_v: 1883.0\n",
      "lif layer 2 self.abs_max_v: 1960.0\n",
      "lif layer 2 self.abs_max_v: 2071.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 11.044736/ 88.213837, val:  34.17%, val_best:  34.17%, tr:  99.28%, tr_best:  99.28%, epoch time: 75.38 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.2615%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.8508%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 2727  13.927%\n",
      "fc layer 2 self.abs_max_out: 1380.0\n",
      "fc layer 2 self.abs_max_out: 1389.0\n",
      "fc layer 2 self.abs_max_out: 1392.0\n",
      "fc layer 2 self.abs_max_out: 1398.0\n",
      "fc layer 2 self.abs_max_out: 1424.0\n",
      "fc layer 2 self.abs_max_out: 1440.0\n",
      "fc layer 2 self.abs_max_out: 1468.0\n",
      "fc layer 2 self.abs_max_out: 1487.0\n",
      "fc layer 2 self.abs_max_out: 1523.0\n",
      "fc layer 2 self.abs_max_out: 1531.0\n",
      "fc layer 2 self.abs_max_out: 1551.0\n",
      "fc layer 2 self.abs_max_out: 1582.0\n",
      "fc layer 2 self.abs_max_out: 1596.0\n",
      "lif layer 2 self.abs_max_v: 2089.0\n",
      "lif layer 2 self.abs_max_v: 2169.5\n",
      "lif layer 2 self.abs_max_v: 2200.0\n",
      "fc layer 2 self.abs_max_out: 1644.0\n",
      "fc layer 3 self.abs_max_out: 498.0\n",
      "fc layer 3 self.abs_max_out: 507.0\n",
      "fc layer 2 self.abs_max_out: 1647.0\n",
      "fc layer 2 self.abs_max_out: 1662.0\n",
      "fc layer 2 self.abs_max_out: 1680.0\n",
      "fc layer 2 self.abs_max_out: 1686.0\n",
      "lif layer 2 self.abs_max_v: 2217.5\n",
      "fc layer 2 self.abs_max_out: 1693.0\n",
      "fc layer 1 self.abs_max_out: 1059.0\n",
      "fc layer 1 self.abs_max_out: 1105.0\n",
      "lif layer 1 self.abs_max_v: 2044.5\n",
      "fc layer 1 self.abs_max_out: 1106.0\n",
      "lif layer 1 self.abs_max_v: 2128.5\n",
      "fc layer 1 self.abs_max_out: 1129.0\n",
      "lif layer 1 self.abs_max_v: 2132.5\n",
      "fc layer 1 self.abs_max_out: 1173.0\n",
      "lif layer 1 self.abs_max_v: 2226.0\n",
      "fc layer 1 self.abs_max_out: 1209.0\n",
      "lif layer 1 self.abs_max_v: 2322.0\n",
      "fc layer 2 self.abs_max_out: 1700.0\n",
      "fc layer 2 self.abs_max_out: 1753.0\n",
      "lif layer 2 self.abs_max_v: 2317.5\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 10.715560/ 51.761539, val:  41.25%, val_best:  41.25%, tr:  99.28%, tr_best:  99.28%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.4165%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.6167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 4057  13.813%\n",
      "fc layer 2 self.abs_max_out: 1787.0\n",
      "fc layer 2 self.abs_max_out: 1817.0\n",
      "fc layer 2 self.abs_max_out: 1844.0\n",
      "fc layer 2 self.abs_max_out: 1846.0\n",
      "fc layer 2 self.abs_max_out: 1858.0\n",
      "lif layer 2 self.abs_max_v: 2322.5\n",
      "lif layer 2 self.abs_max_v: 2450.0\n",
      "lif layer 2 self.abs_max_v: 2496.0\n",
      "lif layer 2 self.abs_max_v: 2556.5\n",
      "lif layer 2 self.abs_max_v: 2687.5\n",
      "fc layer 2 self.abs_max_out: 1872.0\n",
      "fc layer 2 self.abs_max_out: 1945.0\n",
      "fc layer 2 self.abs_max_out: 1975.0\n",
      "fc layer 2 self.abs_max_out: 2014.0\n",
      "fc layer 2 self.abs_max_out: 2032.0\n",
      "fc layer 2 self.abs_max_out: 2078.0\n",
      "fc layer 2 self.abs_max_out: 2081.0\n",
      "fc layer 2 self.abs_max_out: 2119.0\n",
      "fc layer 2 self.abs_max_out: 2125.0\n",
      "fc layer 2 self.abs_max_out: 2220.0\n",
      "fc layer 2 self.abs_max_out: 2257.0\n",
      "fc layer 2 self.abs_max_out: 2293.0\n",
      "fc layer 2 self.abs_max_out: 2329.0\n",
      "fc layer 2 self.abs_max_out: 2389.0\n",
      "fc layer 2 self.abs_max_out: 2398.0\n",
      "fc layer 2 self.abs_max_out: 2400.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss: 10.447141/ 60.789246, val:  40.42%, val_best:  41.25%, tr:  99.18%, tr_best:  99.28%, epoch time: 74.98 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.8318%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 5391  13.767%\n",
      "fc layer 2 self.abs_max_out: 2425.0\n",
      "fc layer 2 self.abs_max_out: 2430.0\n",
      "fc layer 2 self.abs_max_out: 2505.0\n",
      "fc layer 2 self.abs_max_out: 2506.0\n",
      "fc layer 2 self.abs_max_out: 2548.0\n",
      "fc layer 2 self.abs_max_out: 2574.0\n",
      "fc layer 2 self.abs_max_out: 2580.0\n",
      "fc layer 2 self.abs_max_out: 2583.0\n",
      "fc layer 2 self.abs_max_out: 2629.0\n",
      "fc layer 2 self.abs_max_out: 2661.0\n",
      "fc layer 2 self.abs_max_out: 2671.0\n",
      "fc layer 2 self.abs_max_out: 2721.0\n",
      "lif layer 2 self.abs_max_v: 2721.0\n",
      "fc layer 2 self.abs_max_out: 2760.0\n",
      "lif layer 2 self.abs_max_v: 2760.0\n",
      "fc layer 2 self.abs_max_out: 2773.0\n",
      "lif layer 2 self.abs_max_v: 2773.0\n",
      "fc layer 2 self.abs_max_out: 2858.0\n",
      "lif layer 2 self.abs_max_v: 2858.0\n",
      "fc layer 2 self.abs_max_out: 2928.0\n",
      "lif layer 2 self.abs_max_v: 2928.0\n",
      "fc layer 2 self.abs_max_out: 2942.0\n",
      "lif layer 2 self.abs_max_v: 2942.0\n",
      "fc layer 2 self.abs_max_out: 3006.0\n",
      "lif layer 2 self.abs_max_v: 3006.0\n",
      "fc layer 2 self.abs_max_out: 3067.0\n",
      "lif layer 2 self.abs_max_v: 3067.0\n",
      "fc layer 1 self.abs_max_out: 1212.0\n",
      "fc layer 1 self.abs_max_out: 1243.0\n",
      "fc layer 1 self.abs_max_out: 1249.0\n",
      "lif layer 1 self.abs_max_v: 2394.0\n",
      "fc layer 1 self.abs_max_out: 1347.0\n",
      "lif layer 1 self.abs_max_v: 2544.0\n",
      "lif layer 1 self.abs_max_v: 2587.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss: 10.418265/ 67.328430, val:  45.83%, val_best:  45.83%, tr:  98.98%, tr_best:  99.28%, epoch time: 75.42 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.6203%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2226%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 6730  13.749%\n",
      "fc layer 2 self.abs_max_out: 3071.0\n",
      "lif layer 2 self.abs_max_v: 3071.0\n",
      "fc layer 2 self.abs_max_out: 3087.0\n",
      "lif layer 2 self.abs_max_v: 3087.0\n",
      "fc layer 2 self.abs_max_out: 3113.0\n",
      "lif layer 2 self.abs_max_v: 3113.0\n",
      "fc layer 2 self.abs_max_out: 3193.0\n",
      "lif layer 2 self.abs_max_v: 3193.0\n",
      "fc layer 2 self.abs_max_out: 3220.0\n",
      "lif layer 2 self.abs_max_v: 3220.0\n",
      "fc layer 2 self.abs_max_out: 3231.0\n",
      "lif layer 2 self.abs_max_v: 3231.0\n",
      "fc layer 2 self.abs_max_out: 3285.0\n",
      "lif layer 2 self.abs_max_v: 3285.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss: 10.632042/ 98.268860, val:  32.92%, val_best:  45.83%, tr:  99.08%, tr_best:  99.28%, epoch time: 75.00 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.5371%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5191%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 8104  13.796%\n",
      "lif layer 2 self.abs_max_v: 3291.5\n",
      "lif layer 2 self.abs_max_v: 3440.0\n",
      "lif layer 2 self.abs_max_v: 3462.0\n",
      "fc layer 2 self.abs_max_out: 3295.0\n",
      "fc layer 2 self.abs_max_out: 3369.0\n",
      "fc layer 2 self.abs_max_out: 3375.0\n",
      "fc layer 2 self.abs_max_out: 3464.0\n",
      "lif layer 2 self.abs_max_v: 3464.0\n",
      "fc layer 2 self.abs_max_out: 3498.0\n",
      "lif layer 2 self.abs_max_v: 3498.0\n",
      "lif layer 2 self.abs_max_v: 3505.0\n",
      "lif layer 2 self.abs_max_v: 3525.5\n",
      "lif layer 2 self.abs_max_v: 3581.0\n",
      "lif layer 2 self.abs_max_v: 3696.0\n",
      "fc layer 2 self.abs_max_out: 3516.0\n",
      "lif layer 2 self.abs_max_v: 3708.0\n",
      "lif layer 2 self.abs_max_v: 3725.0\n",
      "lif layer 2 self.abs_max_v: 3880.5\n",
      "fc layer 1 self.abs_max_out: 1362.0\n",
      "fc layer 1 self.abs_max_out: 1367.0\n",
      "lif layer 2 self.abs_max_v: 4089.5\n",
      "lif layer 2 self.abs_max_v: 4123.5\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss: 10.863568/ 63.517570, val:  42.92%, val_best:  45.83%, tr:  99.08%, tr_best:  99.28%, epoch time: 75.23 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.9630%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6709%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 9529  13.905%\n",
      "fc layer 2 self.abs_max_out: 3520.0\n",
      "fc layer 2 self.abs_max_out: 3595.0\n",
      "fc layer 2 self.abs_max_out: 3616.0\n",
      "fc layer 2 self.abs_max_out: 3647.0\n",
      "lif layer 2 self.abs_max_v: 4297.5\n",
      "lif layer 2 self.abs_max_v: 4329.0\n",
      "lif layer 2 self.abs_max_v: 4478.5\n",
      "lif layer 2 self.abs_max_v: 4533.0\n",
      "lif layer 2 self.abs_max_v: 4614.0\n",
      "fc layer 2 self.abs_max_out: 3712.0\n",
      "fc layer 2 self.abs_max_out: 3719.0\n",
      "fc layer 2 self.abs_max_out: 3815.0\n",
      "fc layer 2 self.abs_max_out: 3836.0\n",
      "fc layer 2 self.abs_max_out: 3846.0\n",
      "fc layer 2 self.abs_max_out: 3898.0\n",
      "lif layer 2 self.abs_max_v: 4699.5\n",
      "lif layer 2 self.abs_max_v: 5026.0\n",
      "fc layer 1 self.abs_max_out: 1402.0\n",
      "fc layer 1 self.abs_max_out: 1438.0\n",
      "lif layer 1 self.abs_max_v: 2640.5\n",
      "lif layer 2 self.abs_max_v: 5208.5\n",
      "fc layer 2 self.abs_max_out: 3902.0\n",
      "fc layer 2 self.abs_max_out: 3908.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  9.626198/ 62.096756, val:  42.50%, val_best:  45.83%, tr:  98.77%, tr_best:  99.28%, epoch time: 75.03 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.8281%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0326%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 10814  13.807%\n",
      "fc layer 2 self.abs_max_out: 3962.0\n",
      "fc layer 2 self.abs_max_out: 4013.0\n",
      "lif layer 2 self.abs_max_v: 5232.5\n",
      "lif layer 2 self.abs_max_v: 5316.5\n",
      "lif layer 2 self.abs_max_v: 5325.0\n",
      "lif layer 2 self.abs_max_v: 5350.5\n",
      "lif layer 2 self.abs_max_v: 5379.5\n",
      "lif layer 2 self.abs_max_v: 5687.0\n",
      "lif layer 2 self.abs_max_v: 5922.5\n",
      "lif layer 2 self.abs_max_v: 6041.5\n",
      "lif layer 2 self.abs_max_v: 6114.5\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  9.762796/ 49.020012, val:  42.08%, val_best:  45.83%, tr:  98.67%, tr_best:  99.28%, epoch time: 74.36 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.9868%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4718%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 12170  13.812%\n",
      "lif layer 2 self.abs_max_v: 6151.0\n",
      "lif layer 2 self.abs_max_v: 6155.5\n",
      "fc layer 2 self.abs_max_out: 4060.0\n",
      "fc layer 1 self.abs_max_out: 1560.0\n",
      "lif layer 2 self.abs_max_v: 6190.5\n",
      "lif layer 2 self.abs_max_v: 6281.0\n",
      "lif layer 2 self.abs_max_v: 6382.0\n",
      "lif layer 2 self.abs_max_v: 6522.0\n",
      "lif layer 2 self.abs_max_v: 6661.0\n",
      "lif layer 2 self.abs_max_v: 6784.5\n",
      "lif layer 1 self.abs_max_v: 2756.5\n",
      "fc layer 2 self.abs_max_out: 4079.0\n",
      "fc layer 2 self.abs_max_out: 4093.0\n",
      "fc layer 2 self.abs_max_out: 4218.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  8.772760/ 69.496765, val:  37.08%, val_best:  45.83%, tr:  98.16%, tr_best:  99.28%, epoch time: 75.75 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.2219%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2959%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 13479  13.768%\n",
      "lif layer 2 self.abs_max_v: 7042.0\n",
      "lif layer 2 self.abs_max_v: 7177.5\n",
      "lif layer 2 self.abs_max_v: 7339.5\n",
      "lif layer 1 self.abs_max_v: 2770.0\n",
      "lif layer 1 self.abs_max_v: 2779.5\n",
      "lif layer 2 self.abs_max_v: 7513.0\n",
      "lif layer 2 self.abs_max_v: 7633.0\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  8.629992/ 49.786880, val:  47.50%, val_best:  47.50%, tr:  98.67%, tr_best:  99.28%, epoch time: 74.73 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.1179%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.3471%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 14787  13.731%\n",
      "fc layer 1 self.abs_max_out: 1691.0\n",
      "lif layer 1 self.abs_max_v: 2965.5\n",
      "lif layer 1 self.abs_max_v: 2967.0\n",
      "fc layer 2 self.abs_max_out: 4347.0\n",
      "lif layer 2 self.abs_max_v: 7925.0\n",
      "lif layer 2 self.abs_max_v: 8017.5\n",
      "lif layer 2 self.abs_max_v: 8313.0\n",
      "lif layer 2 self.abs_max_v: 8402.0\n",
      "lif layer 2 self.abs_max_v: 8417.5\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  7.863325/ 28.670502, val:  61.67%, val_best:  61.67%, tr:  99.18%, tr_best:  99.28%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.9124%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.7050%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 16059  13.670%\n",
      "fc layer 2 self.abs_max_out: 4554.0\n",
      "fc layer 2 self.abs_max_out: 4562.0\n",
      "lif layer 2 self.abs_max_v: 8494.5\n",
      "lif layer 2 self.abs_max_v: 8614.0\n",
      "fc layer 1 self.abs_max_out: 1768.0\n",
      "lif layer 1 self.abs_max_v: 3095.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  7.275049/ 40.338078, val:  38.75%, val_best:  61.67%, tr:  99.28%, tr_best:  99.28%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.8155%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.4213%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 17249  13.553%\n",
      "fc layer 2 self.abs_max_out: 4649.0\n",
      "fc layer 1 self.abs_max_out: 1831.0\n",
      "lif layer 1 self.abs_max_v: 3329.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  7.404583/ 58.994526, val:  38.33%, val_best:  61.67%, tr:  98.47%, tr_best:  99.28%, epoch time: 75.12 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.7293%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.8207%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 18490  13.490%\n",
      "lif layer 2 self.abs_max_v: 8629.0\n",
      "lif layer 2 self.abs_max_v: 8811.5\n",
      "fc layer 2 self.abs_max_out: 5135.0\n",
      "lif layer 2 self.abs_max_v: 8812.5\n",
      "lif layer 2 self.abs_max_v: 8877.5\n",
      "lif layer 2 self.abs_max_v: 8936.5\n",
      "lif layer 2 self.abs_max_v: 9112.0\n",
      "lif layer 2 self.abs_max_v: 9345.0\n",
      "lif layer 2 self.abs_max_v: 9389.5\n",
      "lif layer 2 self.abs_max_v: 9423.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  7.107269/ 43.469761, val:  38.75%, val_best:  61.67%, tr:  98.88%, tr_best:  99.28%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3006%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.0026%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 19731  13.436%\n",
      "fc layer 2 self.abs_max_out: 5233.0\n",
      "fc layer 1 self.abs_max_out: 1854.0\n",
      "lif layer 1 self.abs_max_v: 3346.0\n",
      "lif layer 1 self.abs_max_v: 3412.5\n",
      "lif layer 1 self.abs_max_v: 3522.5\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  7.061267/ 40.099159, val:  41.25%, val_best:  61.67%, tr:  98.37%, tr_best:  99.28%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3008%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.5278%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 21006  13.410%\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  6.180233/ 32.762302, val:  48.75%, val_best:  61.67%, tr:  99.18%, tr_best:  99.28%, epoch time: 72.45 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.1422%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.0714%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 22178  13.326%\n",
      "fc layer 1 self.abs_max_out: 1979.0\n",
      "fc layer 1 self.abs_max_out: 1997.0\n",
      "lif layer 1 self.abs_max_v: 3645.5\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  6.129422/ 32.886803, val:  53.33%, val_best:  61.67%, tr:  98.16%, tr_best:  99.28%, epoch time: 72.33 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.6111%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.8301%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 23406  13.282%\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  5.807133/ 46.755821, val:  41.67%, val_best:  61.67%, tr:  98.26%, tr_best:  99.28%, epoch time: 74.54 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.7874%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.7582%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 24607  13.229%\n",
      "lif layer 2 self.abs_max_v: 9442.5\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  5.607118/ 47.692097, val:  32.08%, val_best:  61.67%, tr:  98.98%, tr_best:  99.28%, epoch time: 74.49 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4599%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.5303%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 25784  13.169%\n",
      "lif layer 2 self.abs_max_v: 9787.5\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  5.507992/ 44.313671, val:  34.17%, val_best:  61.67%, tr:  99.28%, tr_best:  99.28%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.0032%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.1353%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 26996  13.131%\n",
      "fc layer 1 self.abs_max_out: 2004.0\n",
      "lif layer 1 self.abs_max_v: 3669.0\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  5.746672/ 23.241295, val:  55.83%, val_best:  61.67%, tr:  98.06%, tr_best:  99.28%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.2105%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.3221%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 28257  13.120%\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  5.287395/ 30.144808, val:  54.17%, val_best:  61.67%, tr:  98.47%, tr_best:  99.28%, epoch time: 75.06 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.1919%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.8338%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 29450  13.079%\n",
      "fc layer 1 self.abs_max_out: 2009.0\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  4.882940/ 21.945787, val:  52.08%, val_best:  61.67%, tr:  98.26%, tr_best:  99.28%, epoch time: 74.93 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.6820%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.4461%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 30647  13.043%\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  4.727570/ 24.746635, val:  45.00%, val_best:  61.67%, tr:  98.06%, tr_best:  99.28%, epoch time: 75.68 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.8737%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.1512%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 31870  13.021%\n",
      "fc layer 1 self.abs_max_out: 2124.0\n",
      "fc layer 1 self.abs_max_out: 2233.0\n",
      "lif layer 1 self.abs_max_v: 3707.5\n",
      "lif layer 1 self.abs_max_v: 3907.0\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  4.794707/ 19.877865, val:  51.25%, val_best:  61.67%, tr:  97.14%, tr_best:  99.28%, epoch time: 74.27 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.0371%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.7842%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 33137  13.018%\n",
      "fc layer 2 self.abs_max_out: 5431.0\n",
      "lif layer 2 self.abs_max_v: 9800.0\n",
      "fc layer 1 self.abs_max_out: 2267.0\n",
      "lif layer 1 self.abs_max_v: 3965.0\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  4.450436/ 31.489136, val:  47.92%, val_best:  61.67%, tr:  97.65%, tr_best:  99.28%, epoch time: 75.14 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.0194%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.0414%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 34303  12.977%\n",
      "fc layer 1 self.abs_max_out: 2269.0\n",
      "lif layer 1 self.abs_max_v: 3971.5\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  4.692772/ 19.032137, val:  59.58%, val_best:  61.67%, tr:  97.85%, tr_best:  99.28%, epoch time: 74.76 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.8714%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.6054%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 35521  12.958%\n",
      "fc layer 1 self.abs_max_out: 2321.0\n",
      "lif layer 1 self.abs_max_v: 4116.5\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  4.480078/ 36.052822, val:  37.50%, val_best:  61.67%, tr:  97.75%, tr_best:  99.28%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.4845%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.8493%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 36707  12.929%\n",
      "fc layer 1 self.abs_max_out: 2338.0\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  4.544112/ 36.533260, val:  45.83%, val_best:  61.67%, tr:  96.22%, tr_best:  99.28%, epoch time: 74.70 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.5464%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.9544%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 37896  12.903%\n",
      "lif layer 1 self.abs_max_v: 4178.0\n",
      "lif layer 1 self.abs_max_v: 4187.5\n",
      "lif layer 1 self.abs_max_v: 4211.0\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  4.693702/ 21.403511, val:  50.83%, val_best:  61.67%, tr:  97.14%, tr_best:  99.28%, epoch time: 74.65 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.3516%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.0573%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 39128  12.893%\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  4.678493/ 36.876896, val:  37.92%, val_best:  61.67%, tr:  95.81%, tr_best:  99.28%, epoch time: 74.69 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.8295%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.3062%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 40356  12.882%\n",
      "lif layer 2 self.abs_max_v: 10125.0\n",
      "fc layer 2 self.abs_max_out: 5462.0\n",
      "fc layer 1 self.abs_max_out: 2353.0\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  4.308221/ 28.931684, val:  40.83%, val_best:  61.67%, tr:  96.73%, tr_best:  99.28%, epoch time: 74.98 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.2776%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.7573%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 41516  12.850%\n",
      "fc layer 1 self.abs_max_out: 2375.0\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  4.641267/ 23.411240, val:  52.08%, val_best:  61.67%, tr:  95.71%, tr_best:  99.28%, epoch time: 75.10 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.4862%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.7618%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 42737  12.839%\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  4.546721/ 22.808741, val:  48.75%, val_best:  61.67%, tr:  95.91%, tr_best:  99.28%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.7349%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0007%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 43978  12.835%\n",
      "fc layer 2 self.abs_max_out: 5545.0\n",
      "fc layer 2 self.abs_max_out: 5570.0\n",
      "lif layer 2 self.abs_max_v: 10444.0\n",
      "fc layer 1 self.abs_max_out: 2431.0\n",
      "lif layer 1 self.abs_max_v: 4216.0\n",
      "lif layer 1 self.abs_max_v: 4219.5\n",
      "lif layer 1 self.abs_max_v: 4228.0\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  4.393289/ 26.462994, val:  44.58%, val_best:  61.67%, tr:  95.61%, tr_best:  99.28%, epoch time: 74.72 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.3105%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9343%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 45172  12.817%\n",
      "lif layer 1 self.abs_max_v: 4235.0\n",
      "lif layer 1 self.abs_max_v: 4334.0\n",
      "lif layer 1 self.abs_max_v: 4359.5\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  4.582608/ 14.954896, val:  62.50%, val_best:  62.50%, tr:  96.42%, tr_best:  99.28%, epoch time: 75.35 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.3763%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.5938%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 46388  12.806%\n",
      "lif layer 1 self.abs_max_v: 4413.5\n",
      "lif layer 1 self.abs_max_v: 4512.0\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  4.119603/ 22.651077, val:  42.92%, val_best:  62.50%, tr:  95.81%, tr_best:  99.28%, epoch time: 74.89 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.7156%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.6861%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 47506  12.770%\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  4.453588/ 24.921530, val:  38.75%, val_best:  62.50%, tr:  96.94%, tr_best:  99.28%, epoch time: 75.44 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.9250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.7750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 48696  12.754%\n",
      "fc layer 1 self.abs_max_out: 2437.0\n",
      "lif layer 1 self.abs_max_v: 4611.0\n",
      "lif layer 1 self.abs_max_v: 4639.5\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  4.503218/ 20.367563, val:  49.17%, val_best:  62.50%, tr:  97.45%, tr_best:  99.28%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.6238%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.6638%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 49902  12.743%\n",
      "fc layer 1 self.abs_max_out: 2482.0\n",
      "fc layer 1 self.abs_max_out: 2515.0\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  4.315213/ 21.215481, val:  55.83%, val_best:  62.50%, tr:  96.73%, tr_best:  99.28%, epoch time: 75.10 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.9203%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.4680%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 51052  12.719%\n",
      "fc layer 1 self.abs_max_out: 2566.0\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  4.477108/ 22.357933, val:  55.00%, val_best:  62.50%, tr:  96.42%, tr_best:  99.28%, epoch time: 74.85 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3190%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8475%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 52241  12.705%\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  4.017283/ 23.021427, val:  47.08%, val_best:  62.50%, tr:  97.45%, tr_best:  99.28%, epoch time: 74.65 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.6186%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.8280%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 53321  12.666%\n",
      "lif layer 1 self.abs_max_v: 4825.0\n",
      "lif layer 1 self.abs_max_v: 4826.5\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  4.372353/ 24.564392, val:  43.33%, val_best:  62.50%, tr:  97.34%, tr_best:  99.28%, epoch time: 75.39 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.4695%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1615%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 54476  12.646%\n",
      "fc layer 1 self.abs_max_out: 2571.0\n",
      "fc layer 1 self.abs_max_out: 2598.0\n",
      "lif layer 1 self.abs_max_v: 4865.5\n",
      "fc layer 1 self.abs_max_out: 2681.0\n",
      "lif layer 1 self.abs_max_v: 5114.0\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  4.126935/ 17.253817, val:  53.75%, val_best:  62.50%, tr:  97.04%, tr_best:  99.28%, epoch time: 74.76 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.1990%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1249%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 55604  12.621%\n",
      "fc layer 1 self.abs_max_out: 2689.0\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  4.210617/ 23.203047, val:  50.00%, val_best:  62.50%, tr:  96.63%, tr_best:  99.28%, epoch time: 75.09 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.9007%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 83.9384%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 56751  12.602%\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  4.061666/ 31.728924, val:  46.67%, val_best:  62.50%, tr:  95.81%, tr_best:  99.28%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.2830%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3296%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 57857  12.574%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  4.195704/ 32.998016, val:  40.83%, val_best:  62.50%, tr:  97.24%, tr_best:  99.28%, epoch time: 74.73 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.2558%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2708%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 58990  12.553%\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  4.082125/ 21.261665, val:  51.67%, val_best:  62.50%, tr:  96.32%, tr_best:  99.28%, epoch time: 74.99 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.0111%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2007%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 60103  12.529%\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  4.324785/ 23.421709, val:  49.17%, val_best:  62.50%, tr:  96.42%, tr_best:  99.28%, epoch time: 74.70 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.9725%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2217%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 61253  12.513%\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  4.134267/ 31.025141, val:  47.92%, val_best:  62.50%, tr:  96.53%, tr_best:  99.28%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.8411%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3628%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 62356  12.489%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  3.722646/ 30.251240, val:  37.92%, val_best:  62.50%, tr:  96.73%, tr_best:  99.28%, epoch time: 75.01 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.2914%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7699%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 63431  12.460%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  3.917865/ 17.548225, val:  67.92%, val_best:  67.92%, tr:  96.32%, tr_best:  99.28%, epoch time: 75.41 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.5799%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8187%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 64528  12.436%\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  4.081462/ 21.959211, val:  51.25%, val_best:  67.92%, tr:  96.32%, tr_best:  99.28%, epoch time: 74.91 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.6495%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8425%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 65653  12.419%\n",
      "fc layer 2 self.abs_max_out: 5654.0\n",
      "fc layer 1 self.abs_max_out: 2796.0\n",
      "fc layer 1 self.abs_max_out: 2822.0\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  4.212807/ 16.718227, val:  61.67%, val_best:  67.92%, tr:  95.30%, tr_best:  99.28%, epoch time: 75.27 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.0427%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3633%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 66825  12.411%\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  4.277412/ 23.951845, val:  43.75%, val_best:  67.92%, tr:  95.71%, tr_best:  99.28%, epoch time: 75.13 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.4742%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1081%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 67961  12.396%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  4.215213/ 19.338057, val:  65.42%, val_best:  67.92%, tr:  96.83%, tr_best:  99.28%, epoch time: 75.53 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.1791%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.0419%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 69084  12.380%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  4.042112/ 23.701071, val:  57.08%, val_best:  67.92%, tr:  96.02%, tr_best:  99.28%, epoch time: 74.65 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.8832%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.1968%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 70139  12.352%\n",
      "fc layer 1 self.abs_max_out: 2842.0\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  4.084929/ 22.617630, val:  49.17%, val_best:  67.92%, tr:  96.73%, tr_best:  99.28%, epoch time: 74.85 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.9349%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5208%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 71219  12.330%\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  4.000223/ 31.875443, val:  50.00%, val_best:  67.92%, tr:  95.61%, tr_best:  99.28%, epoch time: 75.23 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.7215%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6920%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 72302  12.309%\n",
      "fc layer 2 self.abs_max_out: 5708.0\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  3.943473/ 20.376429, val:  50.42%, val_best:  67.92%, tr:  96.32%, tr_best:  99.28%, epoch time: 74.87 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.8988%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5809%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 73371  12.286%\n",
      "lif layer 2 self.abs_max_v: 10709.0\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  4.242984/ 16.491568, val:  54.17%, val_best:  67.92%, tr:  96.63%, tr_best:  99.28%, epoch time: 75.10 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.8114%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6307%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 74493  12.273%\n",
      "fc layer 2 self.abs_max_out: 5739.0\n",
      "fc layer 1 self.abs_max_out: 2855.0\n",
      "fc layer 1 self.abs_max_out: 2869.0\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  3.864927/ 24.918688, val:  51.25%, val_best:  67.92%, tr:  96.32%, tr_best:  99.28%, epoch time: 75.32 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.6891%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1003%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 75570  12.253%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  3.886074/ 25.765375, val:  50.83%, val_best:  67.92%, tr:  95.71%, tr_best:  99.28%, epoch time: 75.15 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.7145%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8501%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 76651  12.234%\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  3.811726/ 20.244009, val:  47.50%, val_best:  67.92%, tr:  96.22%, tr_best:  99.28%, epoch time: 75.20 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.6046%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5760%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 77668  12.205%\n",
      "fc layer 2 self.abs_max_out: 5746.0\n",
      "fc layer 1 self.abs_max_out: 2874.0\n",
      "fc layer 1 self.abs_max_out: 2922.0\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  3.556937/ 21.723932, val:  42.08%, val_best:  67.92%, tr:  97.55%, tr_best:  99.28%, epoch time: 75.35 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.2457%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8034%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 78628  12.169%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  3.788102/ 29.462744, val:  44.58%, val_best:  67.92%, tr:  96.42%, tr_best:  99.28%, epoch time: 74.85 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.5131%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2861%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 79635  12.141%\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  4.024698/ 18.432423, val:  52.08%, val_best:  67.92%, tr:  96.02%, tr_best:  99.28%, epoch time: 75.22 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.7598%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3347%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 80692  12.121%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  4.030580/ 16.839579, val:  68.33%, val_best:  68.33%, tr:  96.83%, tr_best:  99.28%, epoch time: 75.34 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.8741%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6913%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 81775  12.106%\n",
      "fc layer 1 self.abs_max_out: 2984.0\n",
      "fc layer 1 self.abs_max_out: 2991.0\n",
      "lif layer 1 self.abs_max_v: 5306.5\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  3.659475/ 29.534025, val:  35.42%, val_best:  68.33%, tr:  96.53%, tr_best:  99.28%, epoch time: 74.93 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.9228%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8414%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 82757  12.076%\n",
      "fc layer 1 self.abs_max_out: 3009.0\n",
      "lif layer 1 self.abs_max_v: 5319.0\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  3.946181/ 24.838966, val:  55.83%, val_best:  68.33%, tr:  96.42%, tr_best:  99.28%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.6848%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6460%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 83841  12.062%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  3.819119/ 27.268814, val:  49.17%, val_best:  68.33%, tr:  97.24%, tr_best:  99.28%, epoch time: 74.82 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.6167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9878%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 84887  12.043%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  3.596246/ 16.938925, val:  59.58%, val_best:  68.33%, tr:  96.94%, tr_best:  99.28%, epoch time: 75.17 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.3793%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8287%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 85891  12.018%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  3.789956/ 25.219002, val:  63.75%, val_best:  68.33%, tr:  96.32%, tr_best:  99.28%, epoch time: 75.42 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.6900%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8174%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 86925  11.999%\n",
      "fc layer 2 self.abs_max_out: 6132.0\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  3.817427/ 14.249601, val:  60.00%, val_best:  68.33%, tr:  96.73%, tr_best:  99.28%, epoch time: 75.10 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.9235%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9270%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 87951  11.978%\n",
      "fc layer 1 self.abs_max_out: 3043.0\n",
      "lif layer 1 self.abs_max_v: 5404.5\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  3.468842/ 18.982660, val:  57.92%, val_best:  68.33%, tr:  96.63%, tr_best:  99.28%, epoch time: 75.13 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.8231%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1021%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 88948  11.955%\n",
      "lif layer 2 self.abs_max_v: 10810.0\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  4.061663/ 20.707436, val:  52.50%, val_best:  68.33%, tr:  96.73%, tr_best:  99.28%, epoch time: 74.69 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.9727%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 90063  11.947%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  3.920084/ 27.040407, val:  41.25%, val_best:  68.33%, tr:  96.63%, tr_best:  99.28%, epoch time: 75.40 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.0046%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.2204%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 91103  11.930%\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  3.753262/ 21.489134, val:  60.83%, val_best:  68.33%, tr:  97.24%, tr_best:  99.28%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.0722%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.3650%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 92111  11.910%\n",
      "fc layer 2 self.abs_max_out: 6153.0\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  3.762334/ 16.696083, val:  61.67%, val_best:  68.33%, tr:  97.04%, tr_best:  99.28%, epoch time: 74.93 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3748%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4317%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 93090  11.886%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  3.645513/ 21.923145, val:  53.33%, val_best:  68.33%, tr:  97.34%, tr_best:  99.28%, epoch time: 75.02 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.0121%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5515%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 94088  11.865%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  3.684037/ 34.553146, val:  40.00%, val_best:  68.33%, tr:  95.71%, tr_best:  99.28%, epoch time: 74.38 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.2460%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6088%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 95055  11.841%\n",
      "fc layer 2 self.abs_max_out: 6507.0\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  3.939937/ 16.889957, val:  58.33%, val_best:  68.33%, tr:  96.02%, tr_best:  99.28%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.2789%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4729%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 96090  11.825%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  3.965681/ 28.226912, val:  36.25%, val_best:  68.33%, tr:  95.91%, tr_best:  99.28%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.0174%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4545%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 97130  11.811%\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  3.724044/ 26.527254, val:  42.08%, val_best:  68.33%, tr:  97.04%, tr_best:  99.28%, epoch time: 74.69 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3409%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6194%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 98135  11.793%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  3.741956/ 21.820686, val:  52.92%, val_best:  68.33%, tr:  97.65%, tr_best:  99.28%, epoch time: 74.90 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4665%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7840%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 99132  11.774%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  3.560133/ 20.632357, val:  61.25%, val_best:  68.33%, tr:  96.42%, tr_best:  99.28%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4252%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0210%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 100103  11.753%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  3.592394/ 24.555708, val:  47.08%, val_best:  68.33%, tr:  95.61%, tr_best:  99.28%, epoch time: 74.95 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.2811%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0038%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 101088  11.734%\n",
      "fc layer 1 self.abs_max_out: 3077.0\n",
      "fc layer 1 self.abs_max_out: 3089.0\n",
      "lif layer 1 self.abs_max_v: 5520.0\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  3.472620/ 19.426283, val:  58.75%, val_best:  68.33%, tr:  97.24%, tr_best:  99.28%, epoch time: 75.17 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4020%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 102005  11.707%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  3.879108/ 17.620607, val:  54.17%, val_best:  68.33%, tr:  96.32%, tr_best:  99.28%, epoch time: 75.03 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.0989%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7610%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 103022  11.692%\n",
      "fc layer 1 self.abs_max_out: 3094.0\n",
      "lif layer 1 self.abs_max_v: 5550.5\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  3.644295/ 21.800348, val:  50.42%, val_best:  68.33%, tr:  96.22%, tr_best:  99.28%, epoch time: 75.47 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.1218%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8974%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 103996  11.673%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  3.571309/ 14.161002, val:  72.92%, val_best:  72.92%, tr:  96.73%, tr_best:  99.28%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6882%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7516%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 104926  11.650%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  3.383791/ 13.998820, val:  59.58%, val_best:  72.92%, tr:  97.55%, tr_best:  99.28%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3595%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2180%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 105864  11.627%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  3.492229/ 23.228949, val:  59.17%, val_best:  72.92%, tr:  97.55%, tr_best:  99.28%, epoch time: 75.05 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.0861%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9365%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 106804  11.606%\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  3.442727/ 17.010418, val:  61.25%, val_best:  72.92%, tr:  96.63%, tr_best:  99.28%, epoch time: 74.43 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.1968%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9540%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 107733  11.584%\n",
      "fc layer 1 self.abs_max_out: 3139.0\n",
      "lif layer 1 self.abs_max_v: 5582.5\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  3.636747/ 22.718119, val:  58.75%, val_best:  72.92%, tr:  97.14%, tr_best:  99.28%, epoch time: 75.07 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3299%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6702%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 108696  11.565%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  3.434030/ 30.013409, val:  48.33%, val_best:  72.92%, tr:  97.04%, tr_best:  99.28%, epoch time: 74.88 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.7836%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2511%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 109619  11.543%\n",
      "fc layer 1 self.abs_max_out: 3178.0\n",
      "lif layer 1 self.abs_max_v: 5660.0\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  3.328465/ 17.590326, val:  70.00%, val_best:  72.92%, tr:  97.55%, tr_best:  99.28%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9797%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2566%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 110517  11.519%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  3.335284/ 22.000242, val:  49.17%, val_best:  72.92%, tr:  97.04%, tr_best:  99.28%, epoch time: 75.07 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9149%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1096%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 111421  11.496%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  3.269182/ 17.185448, val:  66.67%, val_best:  72.92%, tr:  96.63%, tr_best:  99.28%, epoch time: 75.48 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5418%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2489%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 112328  11.474%\n",
      "lif layer 2 self.abs_max_v: 11049.5\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  3.345051/ 19.531136, val:  54.58%, val_best:  72.92%, tr:  97.55%, tr_best:  99.28%, epoch time: 75.85 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.1211%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2408%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 113211  11.449%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  3.495589/ 16.303982, val:  59.58%, val_best:  72.92%, tr:  96.73%, tr_best:  99.28%, epoch time: 75.05 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3239%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8809%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 114143  11.431%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  3.184202/ 19.219358, val:  61.67%, val_best:  72.92%, tr:  96.94%, tr_best:  99.28%, epoch time: 75.60 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6940%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1966%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 115000  11.405%\n",
      "fc layer 1 self.abs_max_out: 3235.0\n",
      "lif layer 1 self.abs_max_v: 5815.5\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  3.479734/ 25.668018, val:  53.33%, val_best:  72.92%, tr:  97.55%, tr_best:  99.28%, epoch time: 75.57 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6523%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9021%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 115894  11.383%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  3.444557/ 15.118284, val:  70.83%, val_best:  72.92%, tr:  96.83%, tr_best:  99.28%, epoch time: 74.81 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4927%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0676%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 116806  11.363%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  3.498420/ 21.310719, val:  55.00%, val_best:  72.92%, tr:  96.63%, tr_best:  99.28%, epoch time: 74.85 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5045%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7151%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 117704  11.342%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  3.252318/ 35.293777, val:  38.75%, val_best:  72.92%, tr:  97.04%, tr_best:  99.28%, epoch time: 74.92 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5096%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7144%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 118559  11.318%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  3.632846/ 20.294676, val:  53.75%, val_best:  72.92%, tr:  96.73%, tr_best:  99.28%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.8108%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0491%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 119497  11.302%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  3.420494/ 26.308176, val:  52.92%, val_best:  72.92%, tr:  97.14%, tr_best:  99.28%, epoch time: 74.81 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5798%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8541%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 120430  11.286%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  3.406191/ 17.753712, val:  56.25%, val_best:  72.92%, tr:  96.53%, tr_best:  99.28%, epoch time: 74.98 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5660%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9563%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 121333  11.267%\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  3.178512/ 18.694773, val:  63.33%, val_best:  72.92%, tr:  97.34%, tr_best:  99.28%, epoch time: 75.03 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.2793%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0099%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 122199  11.245%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  3.449305/ 20.755726, val:  57.50%, val_best:  72.92%, tr:  96.63%, tr_best:  99.28%, epoch time: 74.92 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3984%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7462%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 123100  11.227%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  3.515463/ 14.657851, val:  71.25%, val_best:  72.92%, tr:  96.94%, tr_best:  99.28%, epoch time: 74.78 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6986%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9433%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 124005  11.209%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  3.267327/ 19.903624, val:  53.33%, val_best:  72.92%, tr:  97.14%, tr_best:  99.28%, epoch time: 75.39 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6564%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8609%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 124869  11.188%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  3.417785/ 28.262884, val:  42.50%, val_best:  72.92%, tr:  96.94%, tr_best:  99.28%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9136%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9042%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 125761  11.170%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  3.231671/ 21.684353, val:  57.50%, val_best:  72.92%, tr:  97.04%, tr_best:  99.28%, epoch time: 75.12 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.9211%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2102%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 126619  11.150%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  3.381845/ 15.848173, val:  62.50%, val_best:  72.92%, tr:  96.42%, tr_best:  99.28%, epoch time: 75.16 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4996%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9728%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 127533  11.134%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  3.381675/ 23.207655, val:  50.83%, val_best:  72.92%, tr:  97.55%, tr_best:  99.28%, epoch time: 75.40 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3938%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0556%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 128438  11.118%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  3.386695/ 17.238281, val:  57.08%, val_best:  72.92%, tr:  96.22%, tr_best:  99.28%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9773%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 129314  11.100%\n",
      "lif layer 2 self.abs_max_v: 11152.0\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  3.410669/ 21.584591, val:  47.08%, val_best:  72.92%, tr:  97.24%, tr_best:  99.28%, epoch time: 75.38 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4583%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.2034%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 130222  11.085%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  3.320563/ 21.836111, val:  50.42%, val_best:  72.92%, tr:  97.04%, tr_best:  99.28%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5892%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9526%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 131063  11.064%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  3.346578/ 32.362343, val:  45.00%, val_best:  72.92%, tr:  96.94%, tr_best:  99.28%, epoch time: 75.09 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4973%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9756%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 131948  11.047%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  3.307005/ 19.546093, val:  54.17%, val_best:  72.92%, tr:  96.94%, tr_best:  99.28%, epoch time: 74.82 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5678%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9859%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 132848  11.032%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  3.327015/ 23.018986, val:  56.25%, val_best:  72.92%, tr:  97.04%, tr_best:  99.28%, epoch time: 75.39 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.7410%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7096%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 133742  11.017%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  3.628318/ 16.286827, val:  75.00%, val_best:  75.00%, tr:  96.02%, tr_best:  99.28%, epoch time: 75.05 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.7019%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7103%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 134677  11.005%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  3.346439/ 20.049507, val:  63.75%, val_best:  75.00%, tr:  96.73%, tr_best:  99.28%, epoch time: 75.19 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5766%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.5502%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 135548  10.989%\n",
      "fc layer 1 self.abs_max_out: 3246.0\n",
      "lif layer 1 self.abs_max_v: 5866.0\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  3.463484/ 26.965580, val:  42.08%, val_best:  75.00%, tr:  96.32%, tr_best:  99.28%, epoch time: 75.27 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3046%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6111%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 136408  10.971%\n",
      "fc layer 1 self.abs_max_out: 3347.0\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  3.330614/ 16.305853, val:  67.50%, val_best:  75.00%, tr:  97.24%, tr_best:  99.28%, epoch time: 75.10 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5055%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4815%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 137251  10.953%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  3.323196/ 24.127865, val:  55.00%, val_best:  75.00%, tr:  96.12%, tr_best:  99.28%, epoch time: 75.02 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3765%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6854%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 138126  10.937%\n",
      "fc layer 1 self.abs_max_out: 3361.0\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  3.277423/ 21.587772, val:  54.17%, val_best:  75.00%, tr:  97.14%, tr_best:  99.28%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.2251%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7045%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 138943  10.917%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  3.321770/ 25.237593, val:  48.33%, val_best:  75.00%, tr:  97.24%, tr_best:  99.28%, epoch time: 74.33 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3555%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7998%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 139819  10.902%\n",
      "fc layer 1 self.abs_max_out: 3402.0\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  3.363321/ 19.967588, val:  59.58%, val_best:  75.00%, tr:  97.45%, tr_best:  99.28%, epoch time: 75.12 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5759%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8863%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 140693  10.887%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  3.139856/ 24.987694, val:  52.08%, val_best:  75.00%, tr:  97.04%, tr_best:  99.28%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6560%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0016%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 141522  10.869%\n",
      "lif layer 2 self.abs_max_v: 11298.0\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  3.363490/ 20.527437, val:  58.75%, val_best:  75.00%, tr:  96.53%, tr_best:  99.28%, epoch time: 75.55 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5804%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8825%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 142395  10.854%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  3.294911/ 28.723515, val:  54.17%, val_best:  75.00%, tr:  96.42%, tr_best:  99.28%, epoch time: 74.90 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.0912%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8172%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 143241  10.838%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  3.510549/ 16.195274, val:  63.33%, val_best:  75.00%, tr:  96.83%, tr_best:  99.28%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.1224%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8293%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 144138  10.826%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  3.154988/ 16.606922, val:  69.58%, val_best:  75.00%, tr:  97.45%, tr_best:  99.28%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6627%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6796%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 144952  10.807%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  3.220734/ 23.175640, val:  59.17%, val_best:  75.00%, tr:  96.73%, tr_best:  99.28%, epoch time: 75.12 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6132%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9601%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 145779  10.790%\n",
      "fc layer 1 self.abs_max_out: 3433.0\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  3.120429/ 22.161121, val:  62.92%, val_best:  75.00%, tr:  96.83%, tr_best:  99.28%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5940%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9823%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 146608  10.774%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  3.126542/ 19.127794, val:  59.17%, val_best:  75.00%, tr:  96.83%, tr_best:  99.28%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.7321%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0996%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 147421  10.756%\n",
      "lif layer 1 self.abs_max_v: 5883.5\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  3.382885/ 19.467283, val:  48.33%, val_best:  75.00%, tr:  95.81%, tr_best:  99.28%, epoch time: 75.38 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4962%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1394%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 148293  10.743%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  3.309906/ 16.759819, val:  64.58%, val_best:  75.00%, tr:  96.32%, tr_best:  99.28%, epoch time: 75.64 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3627%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.1910%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 149154  10.729%\n",
      "lif layer 1 self.abs_max_v: 5910.0\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  3.491716/ 20.143332, val:  67.50%, val_best:  75.00%, tr:  97.24%, tr_best:  99.28%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.6358%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6712%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 150031  10.717%\n",
      "fc layer 1 self.abs_max_out: 3451.0\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  3.200788/ 19.122284, val:  58.75%, val_best:  75.00%, tr:  96.94%, tr_best:  99.28%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5157%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8279%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 150835  10.699%\n",
      "lif layer 1 self.abs_max_v: 5969.0\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  3.414686/ 27.627794, val:  55.42%, val_best:  75.00%, tr:  97.24%, tr_best:  99.28%, epoch time: 74.40 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4209%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7094%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 151673  10.685%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  3.158964/ 21.895082, val:  55.00%, val_best:  75.00%, tr:  97.65%, tr_best:  99.28%, epoch time: 75.22 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.2177%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8104%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 152481  10.668%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  3.298832/ 13.399571, val:  64.17%, val_best:  75.00%, tr:  96.94%, tr_best:  99.28%, epoch time: 75.19 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.1615%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.8660%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 153338  10.655%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  3.188030/ 19.270617, val:  54.17%, val_best:  75.00%, tr:  97.34%, tr_best:  99.28%, epoch time: 75.23 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.0372%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9605%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 154173  10.641%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  3.354664/ 23.492117, val:  47.92%, val_best:  75.00%, tr:  97.45%, tr_best:  99.28%, epoch time: 75.01 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.1133%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 85.0438%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 155027  10.628%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  3.402389/ 23.507320, val:  61.25%, val_best:  75.00%, tr:  97.34%, tr_best:  99.28%, epoch time: 74.51 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.2858%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.6765%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 155898  10.616%\n",
      "lif layer 1 self.abs_max_v: 6020.0\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  3.406769/ 24.810057, val:  52.50%, val_best:  75.00%, tr:  96.83%, tr_best:  99.28%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.5998%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.7233%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 156747  10.603%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  3.281196/ 26.493746, val:  53.33%, val_best:  75.00%, tr:  97.34%, tr_best:  99.28%, epoch time: 74.94 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.2761%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.9032%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 157598  10.591%\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        # \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [256.0,128.0,64.0,32.0,16.0]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "\n",
    "        \"learning_rate\": {\"values\": [1.0]}, \n",
    "        # \"lr_factor\": {\"values\": [-6, -7, -8, -9]}, \n",
    "        \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [14]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [-1]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [0]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        \"scale_exp_2w\": {\"values\": [0]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        \"scale_exp_3w\": {\"values\": [0]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "        \"lif_layer_sg_width2\": {\"values\": [0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0]},\n",
    "        \"lif_layer_v_threshold2\": {\"values\": [256.0,128.0,64.0,32.0,16.0]},\n",
    "        \"learning_rate2\": {\"values\": [1.0]},\n",
    "        \"init_scaling_0\": {\"values\": [4/128]},\n",
    "        \"init_scaling_1\": {\"values\": [6/128]},\n",
    "        \"init_scaling_2\": {\"values\": [3/128]},\n",
    "        \n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"2\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_2w,wandb.config.scale_exp_2w],[wandb.config.scale_exp_3w,wandb.config.scale_exp_3w]],\n",
    "        lif_layer_sg_width2  =  wandb.config.lif_layer_sg_width2,\n",
    "        lif_layer_v_threshold2  =  wandb.config.lif_layer_v_threshold2,\n",
    "        learning_rate2  =  wandb.config.learning_rate2,\n",
    "        init_scaling = [wandb.config.init_scaling_0,wandb.config.init_scaling_1,wandb.config.init_scaling_2],\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'e1m59f1o'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
