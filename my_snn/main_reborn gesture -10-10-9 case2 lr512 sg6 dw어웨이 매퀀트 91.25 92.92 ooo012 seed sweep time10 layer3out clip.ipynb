{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18531/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' 레퍼런스\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loader에서 train dataset을 몇개 더 쓸건지 \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    ):\n",
    "    ## 함수 내 모든 로컬 변수 저장 ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFA랑 single_step공존하게해라'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb 세팅 ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader 가져오기 ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. 전체 state_dict 로드\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. 현재 모델의 state_dict 가져오기\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'가 포함된 key만 필터링 (현재 모델에도 존재하는 key만)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. 업데이트된 키 출력\n",
    "        print(\"🔄 업데이트된 SYNAPSE 관련 레이어들:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. 모델 dict 업데이트 및 로딩\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter logging해줌\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss 구해주는 친구\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> 클래스 인덱스\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE 스타일의 gradient를 흉내냄\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # 이거 걍 1.0임\n",
    "            return input_one_hot - target_one_hot, None  # target에는 gradient 없음\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"모든 파라미터에 대해 gradient descent 수행\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "                    # gradient를 이용해 파라미터 업데이트\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer 초기화\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO 처리 ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw 위 연산이랑 다름. inmemory연산이라 좀 다른 듯\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight 프린트 ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # 통계량 계산\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # 절대값 기반 max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # 그래프 그리기\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # 제목에 통계값 포함\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight 프린트 ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdm을 통한 progress_bar 생성###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train 모드로 바꿔줘야함\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "                \n",
    "            ## batch 크기 ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # 차원 전처리\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FC에 있는 sparsity_print_and_reset() 실행\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel 차원은 그대로 두고, Height, Width 차원에 대해서만 pooling 적용\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs 데이터 시각화 코드 (확인 필요할 시 써라)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient 초기화 #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first input도 ottt trace 적용하기 위한 코드 (validation 시에는 필요X) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight 업데이트!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # ottt꺼 쓸때\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net 그림 출력해보기 #################################################################\n",
    "            # print('시각화')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch 어긋남 방지 ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval 모드로 바꿔줘야함 \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel 차원은 그대로 두고, Height, Width 차원에 대해서만 pooling 적용\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network 연산 시작 ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb 키면 state_dict아닌거는 저장 안됨\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb 과거 하이퍼파라미터 가져와서 붙여넣기 (devices unique_name은 니가 할당해라)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"5\",\n",
    "#                 single_step = True, # True # False # DFA_on이랑 같이 가라\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 20664,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "#                 BATCH = 1, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.5,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "#                 lif_layer_sg_width = 6.0, # 2.570969004857107 # sigmoid류에서는 alpha값 4.0, rectangle류에서는 width값 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "#                 synapse_trace_const2 = decay, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # True로 하길 추천\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 1/512, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 300,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #일반적으로 1 또는 2 # 100ms때는 5 # 숫자만큼 크면 spike 아니면 걍 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # 있는 데이터들 #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # 한 숫자가 1us인듯 (spikingjelly코드에서)\n",
    "#                 # 한 장에 50 timestep만 생산함. 싫으면 my_snn/trying/spikingjelly_dvsgesture의__init__.py 를 참고해봐\n",
    "#                 # nmnist 5_000us, gesture는 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_step이랑 같이 켜야 됨.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # 맨 처음 input에 trace 적용 # trace_on False면 의미없음.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = 9, \n",
    "\n",
    "#                 num_workers = 2, # local wsl에서는 2가 맞고, 서버에서는 4가 좋더라.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[-10,-10],[-10,-10],[-9,-9]], \n",
    "# # 1w -11~-9\n",
    "# # 1b -11~ -7\n",
    "# # 2w -10~-8\n",
    "# # 2b -10~-8\n",
    "# # 3w -10\n",
    "# # 3b -10\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoid와 BN이 있어야 잘된다.\n",
    "# # average pooling  \n",
    "# # 이 낫다. \n",
    "\n",
    "# # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0k09yrmb with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 8237\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.21.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250906_001528-0k09yrmb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0k09yrmb' target=\"_blank\">dazzling-sweep-69</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/jwru0k4o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/jwru0k4o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/jwru0k4o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/jwru0k4o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0k09yrmb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0k09yrmb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20250906_001535_975', 'my_seed': 8237, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0df5ce43f802d21fe74cde54437db10b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = f205136b2771111650a88c4e480cfe73\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 391e4997dc3a746988cd0e9dceb2d42e\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = bb0ac3251c9e44bfe72bcb8b2e969f0d\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = c796a451486ae8cd6d0dd9bd02a9e235\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a6e81fbc907b11cedc166a7f5b843582\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = d4ded3e2b3703cdb1192f3d689158f82\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 602987c624e8b98603f8b906841eadb1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 2d3185edb0c7b53adc6375ce1392ad59\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 9e9960951042c2f18fd3576739597330\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 4436 BATCH: 1 train_data_count: 4436\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 987.0\n",
      "lif layer 1 self.abs_max_v: 987.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 285.0\n",
      "lif layer 2 self.abs_max_v: 285.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 336.0\n",
      "lif layer 2 self.abs_max_v: 432.5\n",
      "fc layer 2 self.abs_max_out: 359.0\n",
      "lif layer 2 self.abs_max_v: 439.0\n",
      "fc layer 2 self.abs_max_out: 439.0\n",
      "lif layer 2 self.abs_max_v: 528.5\n",
      "fc layer 1 self.abs_max_out: 1127.0\n",
      "lif layer 1 self.abs_max_v: 1127.0\n",
      "lif layer 2 self.abs_max_v: 610.5\n",
      "fc layer 3 self.abs_max_out: 63.0\n",
      "fc layer 1 self.abs_max_out: 1224.0\n",
      "lif layer 1 self.abs_max_v: 1224.0\n",
      "fc layer 2 self.abs_max_out: 578.0\n",
      "lif layer 2 self.abs_max_v: 695.0\n",
      "lif layer 2 self.abs_max_v: 711.5\n",
      "lif layer 2 self.abs_max_v: 728.0\n",
      "fc layer 3 self.abs_max_out: 70.0\n",
      "fc layer 1 self.abs_max_out: 1258.0\n",
      "lif layer 1 self.abs_max_v: 1438.5\n",
      "fc layer 2 self.abs_max_out: 605.0\n",
      "lif layer 2 self.abs_max_v: 849.0\n",
      "fc layer 2 self.abs_max_out: 613.0\n",
      "lif layer 2 self.abs_max_v: 865.5\n",
      "fc layer 2 self.abs_max_out: 816.0\n",
      "lif layer 2 self.abs_max_v: 1200.5\n",
      "fc layer 3 self.abs_max_out: 85.0\n",
      "lif layer 2 self.abs_max_v: 1213.5\n",
      "fc layer 1 self.abs_max_out: 1555.0\n",
      "lif layer 1 self.abs_max_v: 1555.0\n",
      "fc layer 3 self.abs_max_out: 98.0\n",
      "fc layer 1 self.abs_max_out: 1572.0\n",
      "lif layer 1 self.abs_max_v: 1572.0\n",
      "fc layer 2 self.abs_max_out: 818.0\n",
      "fc layer 3 self.abs_max_out: 121.0\n",
      "fc layer 3 self.abs_max_out: 129.0\n",
      "fc layer 3 self.abs_max_out: 134.0\n",
      "fc layer 3 self.abs_max_out: 147.0\n",
      "fc layer 1 self.abs_max_out: 1579.0\n",
      "lif layer 1 self.abs_max_v: 1579.0\n",
      "lif layer 2 self.abs_max_v: 1244.0\n",
      "fc layer 2 self.abs_max_out: 832.0\n",
      "fc layer 3 self.abs_max_out: 212.0\n",
      "fc layer 3 self.abs_max_out: 216.0\n",
      "fc layer 2 self.abs_max_out: 956.0\n",
      "lif layer 2 self.abs_max_v: 1322.0\n",
      "fc layer 3 self.abs_max_out: 239.0\n",
      "fc layer 1 self.abs_max_out: 1818.0\n",
      "lif layer 1 self.abs_max_v: 1818.0\n",
      "lif layer 2 self.abs_max_v: 1325.5\n",
      "fc layer 1 self.abs_max_out: 1965.0\n",
      "lif layer 1 self.abs_max_v: 1965.0\n",
      "fc layer 2 self.abs_max_out: 1058.0\n",
      "lif layer 2 self.abs_max_v: 1390.5\n",
      "fc layer 1 self.abs_max_out: 1988.0\n",
      "lif layer 1 self.abs_max_v: 1988.0\n",
      "fc layer 2 self.abs_max_out: 1067.0\n",
      "lif layer 2 self.abs_max_v: 1415.0\n",
      "fc layer 2 self.abs_max_out: 1209.0\n",
      "lif layer 2 self.abs_max_v: 1467.5\n",
      "fc layer 2 self.abs_max_out: 1227.0\n",
      "lif layer 2 self.abs_max_v: 1501.0\n",
      "fc layer 2 self.abs_max_out: 1279.0\n",
      "lif layer 2 self.abs_max_v: 1530.5\n",
      "fc layer 1 self.abs_max_out: 2118.0\n",
      "lif layer 1 self.abs_max_v: 2118.0\n",
      "fc layer 2 self.abs_max_out: 1293.0\n",
      "lif layer 2 self.abs_max_v: 1630.5\n",
      "lif layer 2 self.abs_max_v: 1717.5\n",
      "fc layer 2 self.abs_max_out: 1340.0\n",
      "lif layer 2 self.abs_max_v: 1787.0\n",
      "lif layer 2 self.abs_max_v: 1931.5\n",
      "fc layer 1 self.abs_max_out: 2282.0\n",
      "lif layer 1 self.abs_max_v: 2282.0\n",
      "lif layer 2 self.abs_max_v: 2117.0\n",
      "fc layer 1 self.abs_max_out: 2322.0\n",
      "lif layer 1 self.abs_max_v: 2322.0\n",
      "fc layer 3 self.abs_max_out: 282.0\n",
      "fc layer 1 self.abs_max_out: 2584.0\n",
      "lif layer 1 self.abs_max_v: 2584.0\n",
      "fc layer 3 self.abs_max_out: 328.0\n",
      "fc layer 2 self.abs_max_out: 1380.0\n",
      "fc layer 2 self.abs_max_out: 1513.0\n",
      "fc layer 1 self.abs_max_out: 2692.0\n",
      "lif layer 1 self.abs_max_v: 2692.0\n",
      "fc layer 1 self.abs_max_out: 2867.0\n",
      "lif layer 1 self.abs_max_v: 2867.0\n",
      "fc layer 2 self.abs_max_out: 1514.0\n",
      "fc layer 2 self.abs_max_out: 1552.0\n",
      "fc layer 2 self.abs_max_out: 1736.0\n",
      "fc layer 3 self.abs_max_out: 370.0\n",
      "fc layer 2 self.abs_max_out: 1772.0\n",
      "fc layer 1 self.abs_max_out: 3325.0\n",
      "lif layer 1 self.abs_max_v: 3325.0\n",
      "fc layer 2 self.abs_max_out: 1774.0\n",
      "fc layer 2 self.abs_max_out: 1782.0\n",
      "fc layer 2 self.abs_max_out: 1844.0\n",
      "fc layer 2 self.abs_max_out: 1859.0\n",
      "fc layer 2 self.abs_max_out: 2005.0\n",
      "fc layer 2 self.abs_max_out: 2073.0\n",
      "fc layer 3 self.abs_max_out: 397.0\n",
      "fc layer 1 self.abs_max_out: 3403.0\n",
      "lif layer 1 self.abs_max_v: 3403.0\n",
      "lif layer 1 self.abs_max_v: 3475.5\n",
      "fc layer 1 self.abs_max_out: 3523.0\n",
      "lif layer 1 self.abs_max_v: 3523.0\n",
      "fc layer 1 self.abs_max_out: 3557.0\n",
      "lif layer 1 self.abs_max_v: 3557.0\n",
      "fc layer 1 self.abs_max_out: 4044.0\n",
      "lif layer 1 self.abs_max_v: 4044.0\n",
      "fc layer 2 self.abs_max_out: 2082.0\n",
      "fc layer 3 self.abs_max_out: 434.0\n",
      "fc layer 3 self.abs_max_out: 457.0\n",
      "fc layer 2 self.abs_max_out: 2098.0\n",
      "lif layer 2 self.abs_max_v: 2303.5\n",
      "fc layer 2 self.abs_max_out: 2166.0\n",
      "fc layer 2 self.abs_max_out: 2179.0\n",
      "fc layer 1 self.abs_max_out: 4141.0\n",
      "lif layer 1 self.abs_max_v: 4141.0\n",
      "fc layer 2 self.abs_max_out: 2265.0\n",
      "lif layer 2 self.abs_max_v: 2850.0\n",
      "fc layer 1 self.abs_max_out: 4266.0\n",
      "lif layer 1 self.abs_max_v: 4266.0\n",
      "fc layer 2 self.abs_max_out: 2315.0\n",
      "lif layer 1 self.abs_max_v: 4393.5\n",
      "lif layer 1 self.abs_max_v: 4548.0\n",
      "lif layer 1 self.abs_max_v: 4768.0\n",
      "lif layer 1 self.abs_max_v: 4843.0\n",
      "fc layer 1 self.abs_max_out: 4507.0\n",
      "fc layer 1 self.abs_max_out: 4570.0\n",
      "fc layer 2 self.abs_max_out: 2343.0\n",
      "fc layer 1 self.abs_max_out: 4783.0\n",
      "fc layer 2 self.abs_max_out: 2387.0\n",
      "fc layer 2 self.abs_max_out: 2445.0\n",
      "fc layer 1 self.abs_max_out: 4933.0\n",
      "lif layer 1 self.abs_max_v: 4933.0\n",
      "fc layer 1 self.abs_max_out: 5560.0\n",
      "lif layer 1 self.abs_max_v: 5560.0\n",
      "fc layer 2 self.abs_max_out: 2448.0\n",
      "fc layer 2 self.abs_max_out: 2511.0\n",
      "fc layer 2 self.abs_max_out: 2530.0\n",
      "fc layer 2 self.abs_max_out: 2553.0\n",
      "fc layer 1 self.abs_max_out: 5573.0\n",
      "lif layer 1 self.abs_max_v: 5573.0\n",
      "fc layer 1 self.abs_max_out: 5684.0\n",
      "lif layer 1 self.abs_max_v: 5684.0\n",
      "fc layer 2 self.abs_max_out: 2559.0\n",
      "fc layer 2 self.abs_max_out: 2682.0\n",
      "fc layer 1 self.abs_max_out: 6168.0\n",
      "lif layer 1 self.abs_max_v: 6168.0\n",
      "fc layer 2 self.abs_max_out: 2863.0\n",
      "lif layer 2 self.abs_max_v: 2863.0\n",
      "lif layer 2 self.abs_max_v: 2898.0\n",
      "lif layer 2 self.abs_max_v: 2995.0\n",
      "fc layer 1 self.abs_max_out: 6561.0\n",
      "lif layer 1 self.abs_max_v: 6561.0\n",
      "fc layer 1 self.abs_max_out: 6634.0\n",
      "lif layer 1 self.abs_max_v: 6634.0\n",
      "lif layer 2 self.abs_max_v: 2996.5\n",
      "lif layer 2 self.abs_max_v: 3073.5\n",
      "lif layer 2 self.abs_max_v: 3162.5\n",
      "fc layer 2 self.abs_max_out: 2954.0\n",
      "fc layer 2 self.abs_max_out: 2963.0\n",
      "lif layer 2 self.abs_max_v: 3170.0\n",
      "fc layer 1 self.abs_max_out: 6751.0\n",
      "lif layer 1 self.abs_max_v: 6751.0\n",
      "fc layer 2 self.abs_max_out: 3093.0\n",
      "fc layer 1 self.abs_max_out: 6837.0\n",
      "lif layer 1 self.abs_max_v: 6837.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  2.090414/  2.147814, val:  57.92%, val_best:  57.92%, tr:  94.07%, tr_best:  94.07%, epoch time: 279.50 seconds, 4.66 minutes\n",
      "total_backward_count 44360 real_backward_count 11421  25.746%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 3129.0\n",
      "fc layer 2 self.abs_max_out: 3134.0\n",
      "fc layer 1 self.abs_max_out: 7017.0\n",
      "lif layer 1 self.abs_max_v: 7017.0\n",
      "fc layer 1 self.abs_max_out: 7155.0\n",
      "lif layer 1 self.abs_max_v: 7155.0\n",
      "fc layer 2 self.abs_max_out: 3182.0\n",
      "lif layer 2 self.abs_max_v: 3182.0\n",
      "fc layer 2 self.abs_max_out: 3362.0\n",
      "lif layer 2 self.abs_max_v: 3362.0\n",
      "fc layer 1 self.abs_max_out: 7477.0\n",
      "lif layer 1 self.abs_max_v: 7477.0\n",
      "fc layer 1 self.abs_max_out: 7623.0\n",
      "lif layer 1 self.abs_max_v: 7623.0\n",
      "fc layer 1 self.abs_max_out: 7816.0\n",
      "lif layer 1 self.abs_max_v: 7816.0\n",
      "lif layer 1 self.abs_max_v: 8596.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  2.064295/  2.143275, val:  42.08%, val_best:  57.92%, tr:  99.28%, tr_best:  99.28%, epoch time: 279.91 seconds, 4.67 minutes\n",
      "total_backward_count 88720 real_backward_count 19048  21.470%\n",
      "fc layer 1 self.abs_max_out: 7949.0\n",
      "fc layer 2 self.abs_max_out: 3524.0\n",
      "lif layer 2 self.abs_max_v: 3524.0\n",
      "fc layer 1 self.abs_max_out: 7987.0\n",
      "lif layer 2 self.abs_max_v: 3553.0\n",
      "lif layer 2 self.abs_max_v: 3727.5\n",
      "lif layer 2 self.abs_max_v: 3915.0\n",
      "lif layer 2 self.abs_max_v: 3967.0\n",
      "lif layer 1 self.abs_max_v: 9846.5\n",
      "fc layer 1 self.abs_max_out: 8192.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  2.055757/  2.118548, val:  53.33%, val_best:  57.92%, tr:  99.37%, tr_best:  99.37%, epoch time: 279.52 seconds, 4.66 minutes\n",
      "total_backward_count 133080 real_backward_count 26039  19.566%\n",
      "fc layer 1 self.abs_max_out: 8340.0\n",
      "fc layer 1 self.abs_max_out: 8560.0\n",
      "lif layer 1 self.abs_max_v: 10358.0\n",
      "fc layer 1 self.abs_max_out: 8639.0\n",
      "lif layer 2 self.abs_max_v: 3971.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  2.035756/  2.113564, val:  44.58%, val_best:  57.92%, tr:  99.57%, tr_best:  99.57%, epoch time: 279.91 seconds, 4.67 minutes\n",
      "total_backward_count 177440 real_backward_count 32615  18.381%\n",
      "lif layer 2 self.abs_max_v: 3976.5\n",
      "lif layer 2 self.abs_max_v: 3995.5\n",
      "lif layer 2 self.abs_max_v: 4088.5\n",
      "lif layer 2 self.abs_max_v: 4138.5\n",
      "lif layer 2 self.abs_max_v: 4189.0\n",
      "lif layer 2 self.abs_max_v: 4456.0\n",
      "lif layer 2 self.abs_max_v: 4476.0\n",
      "fc layer 1 self.abs_max_out: 8671.0\n",
      "lif layer 2 self.abs_max_v: 4501.5\n",
      "lif layer 2 self.abs_max_v: 4573.0\n",
      "lif layer 2 self.abs_max_v: 4584.0\n",
      "fc layer 1 self.abs_max_out: 8930.0\n",
      "lif layer 2 self.abs_max_v: 4756.5\n",
      "lif layer 2 self.abs_max_v: 4759.0\n",
      "lif layer 1 self.abs_max_v: 11154.5\n",
      "fc layer 1 self.abs_max_out: 8979.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  2.021199/  2.096494, val:  53.33%, val_best:  57.92%, tr:  99.77%, tr_best:  99.77%, epoch time: 280.20 seconds, 4.67 minutes\n",
      "total_backward_count 221800 real_backward_count 39092  17.625%\n",
      "fc layer 2 self.abs_max_out: 3643.0\n",
      "fc layer 1 self.abs_max_out: 9078.0\n",
      "lif layer 1 self.abs_max_v: 11640.0\n",
      "fc layer 1 self.abs_max_out: 9089.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  2.016673/  2.076939, val:  47.50%, val_best:  57.92%, tr:  99.82%, tr_best:  99.82%, epoch time: 279.70 seconds, 4.66 minutes\n",
      "total_backward_count 266160 real_backward_count 45248  17.000%\n",
      "lif layer 1 self.abs_max_v: 11654.5\n",
      "fc layer 1 self.abs_max_out: 9219.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  2.008807/  2.078733, val:  55.42%, val_best:  57.92%, tr:  99.91%, tr_best:  99.91%, epoch time: 279.78 seconds, 4.66 minutes\n",
      "total_backward_count 310520 real_backward_count 51102  16.457%\n",
      "fc layer 1 self.abs_max_out: 9226.0\n",
      "lif layer 1 self.abs_max_v: 11821.0\n",
      "fc layer 1 self.abs_max_out: 9298.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  2.009320/  2.078759, val:  63.33%, val_best:  63.33%, tr:  99.82%, tr_best:  99.91%, epoch time: 279.59 seconds, 4.66 minutes\n",
      "total_backward_count 354880 real_backward_count 56842  16.017%\n",
      "lif layer 1 self.abs_max_v: 12014.0\n",
      "fc layer 1 self.abs_max_out: 9337.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.997024/  2.058997, val:  73.75%, val_best:  73.75%, tr:  99.91%, tr_best:  99.91%, epoch time: 278.65 seconds, 4.64 minutes\n",
      "total_backward_count 399240 real_backward_count 62296  15.604%\n",
      "lif layer 2 self.abs_max_v: 4973.0\n",
      "lif layer 2 self.abs_max_v: 4986.5\n",
      "lif layer 2 self.abs_max_v: 5011.0\n",
      "lif layer 1 self.abs_max_v: 12045.0\n",
      "fc layer 1 self.abs_max_out: 9458.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.982717/  2.040010, val:  72.08%, val_best:  73.75%, tr:  99.84%, tr_best:  99.91%, epoch time: 279.55 seconds, 4.66 minutes\n",
      "total_backward_count 443600 real_backward_count 67492  15.215%\n",
      "lif layer 1 self.abs_max_v: 12607.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.990819/  2.066624, val:  68.75%, val_best:  73.75%, tr:  99.84%, tr_best:  99.91%, epoch time: 271.17 seconds, 4.52 minutes\n",
      "total_backward_count 487960 real_backward_count 72451  14.848%\n",
      "fc layer 2 self.abs_max_out: 3657.0\n",
      "lif layer 2 self.abs_max_v: 5044.0\n",
      "fc layer 1 self.abs_max_out: 9471.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.995912/  2.078698, val:  55.83%, val_best:  73.75%, tr:  99.93%, tr_best:  99.93%, epoch time: 277.37 seconds, 4.62 minutes\n",
      "total_backward_count 532320 real_backward_count 77402  14.541%\n",
      "lif layer 2 self.abs_max_v: 5177.5\n",
      "lif layer 2 self.abs_max_v: 5273.0\n",
      "fc layer 1 self.abs_max_out: 9531.0\n",
      "fc layer 1 self.abs_max_out: 9563.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.981772/  2.052229, val:  80.83%, val_best:  80.83%, tr:  99.84%, tr_best:  99.93%, epoch time: 277.50 seconds, 4.62 minutes\n",
      "total_backward_count 576680 real_backward_count 82292  14.270%\n",
      "fc layer 2 self.abs_max_out: 3704.0\n",
      "fc layer 2 self.abs_max_out: 3725.0\n",
      "lif layer 2 self.abs_max_v: 5318.5\n",
      "fc layer 1 self.abs_max_out: 9635.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.986153/  2.068170, val:  76.25%, val_best:  80.83%, tr:  99.86%, tr_best:  99.93%, epoch time: 279.19 seconds, 4.65 minutes\n",
      "total_backward_count 621040 real_backward_count 86965  14.003%\n",
      "fc layer 2 self.abs_max_out: 3739.0\n",
      "lif layer 1 self.abs_max_v: 13005.0\n",
      "fc layer 1 self.abs_max_out: 9793.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.978236/  2.053833, val:  68.75%, val_best:  80.83%, tr:  99.91%, tr_best:  99.93%, epoch time: 280.08 seconds, 4.67 minutes\n",
      "total_backward_count 665400 real_backward_count 91441  13.742%\n",
      "fc layer 1 self.abs_max_out: 9798.0\n",
      "lif layer 1 self.abs_max_v: 13166.5\n",
      "fc layer 1 self.abs_max_out: 9829.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.961423/  2.028987, val:  77.50%, val_best:  80.83%, tr:  99.95%, tr_best:  99.95%, epoch time: 279.00 seconds, 4.65 minutes\n",
      "total_backward_count 709760 real_backward_count 95845  13.504%\n",
      "lif layer 2 self.abs_max_v: 5402.5\n",
      "fc layer 1 self.abs_max_out: 9860.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.966578/  2.044429, val:  66.25%, val_best:  80.83%, tr:  99.95%, tr_best:  99.95%, epoch time: 278.88 seconds, 4.65 minutes\n",
      "total_backward_count 754120 real_backward_count 100027  13.264%\n",
      "fc layer 2 self.abs_max_out: 3746.0\n",
      "fc layer 1 self.abs_max_out: 9924.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.960042/  2.055232, val:  70.00%, val_best:  80.83%, tr:  99.95%, tr_best:  99.95%, epoch time: 279.33 seconds, 4.66 minutes\n",
      "total_backward_count 798480 real_backward_count 104066  13.033%\n",
      "lif layer 2 self.abs_max_v: 5410.0\n",
      "fc layer 2 self.abs_max_out: 3759.0\n",
      "fc layer 1 self.abs_max_out: 9958.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.963983/  2.037723, val:  69.58%, val_best:  80.83%, tr:  99.93%, tr_best:  99.95%, epoch time: 278.47 seconds, 4.64 minutes\n",
      "total_backward_count 842840 real_backward_count 108047  12.819%\n",
      "fc layer 2 self.abs_max_out: 3782.0\n",
      "fc layer 2 self.abs_max_out: 3855.0\n",
      "fc layer 1 self.abs_max_out: 9995.0\n",
      "fc layer 1 self.abs_max_out: 10023.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.960708/  2.026018, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.20 seconds, 4.67 minutes\n",
      "total_backward_count 887200 real_backward_count 111868  12.609%\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.946885/  2.026666, val:  76.25%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.06 seconds, 4.67 minutes\n",
      "total_backward_count 931560 real_backward_count 115634  12.413%\n",
      "fc layer 1 self.abs_max_out: 10058.0\n",
      "fc layer 1 self.abs_max_out: 10087.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.947592/  2.025148, val:  82.92%, val_best:  82.92%, tr:  99.98%, tr_best: 100.00%, epoch time: 277.83 seconds, 4.63 minutes\n",
      "total_backward_count 975920 real_backward_count 119387  12.233%\n",
      "fc layer 1 self.abs_max_out: 10096.0\n",
      "fc layer 1 self.abs_max_out: 10131.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.953867/  2.032127, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.71 seconds, 4.55 minutes\n",
      "total_backward_count 1020280 real_backward_count 123039  12.059%\n",
      "fc layer 1 self.abs_max_out: 10182.0\n",
      "fc layer 1 self.abs_max_out: 10186.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.946915/  2.019441, val:  71.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.38 seconds, 4.64 minutes\n",
      "total_backward_count 1064640 real_backward_count 126553  11.887%\n",
      "fc layer 1 self.abs_max_out: 10260.0\n",
      "fc layer 3 self.abs_max_out: 470.0\n",
      "fc layer 2 self.abs_max_out: 4018.0\n",
      "lif layer 1 self.abs_max_v: 13174.5\n",
      "fc layer 1 self.abs_max_out: 10307.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.931882/  2.025286, val:  78.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.86 seconds, 4.65 minutes\n",
      "total_backward_count 1109000 real_backward_count 130014  11.724%\n",
      "fc layer 1 self.abs_max_out: 10359.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.935436/  2.014192, val:  72.50%, val_best:  83.33%, tr:  99.98%, tr_best: 100.00%, epoch time: 280.18 seconds, 4.67 minutes\n",
      "total_backward_count 1153360 real_backward_count 133418  11.568%\n",
      "fc layer 3 self.abs_max_out: 473.0\n",
      "lif layer 1 self.abs_max_v: 13288.5\n",
      "fc layer 1 self.abs_max_out: 10469.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.923308/  1.993965, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.48 seconds, 4.66 minutes\n",
      "total_backward_count 1197720 real_backward_count 136639  11.408%\n",
      "fc layer 3 self.abs_max_out: 503.0\n",
      "fc layer 1 self.abs_max_out: 10475.0\n",
      "fc layer 1 self.abs_max_out: 10502.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.922215/  1.997940, val:  82.08%, val_best:  84.58%, tr:  99.98%, tr_best: 100.00%, epoch time: 279.62 seconds, 4.66 minutes\n",
      "total_backward_count 1242080 real_backward_count 139847  11.259%\n",
      "lif layer 1 self.abs_max_v: 13377.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.920084/  1.990988, val:  73.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.82 seconds, 4.68 minutes\n",
      "total_backward_count 1286440 real_backward_count 143005  11.116%\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.920060/  1.986574, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.25 seconds, 4.67 minutes\n",
      "total_backward_count 1330800 real_backward_count 146102  10.979%\n",
      "fc layer 2 self.abs_max_out: 4065.0\n",
      "lif layer 1 self.abs_max_v: 13433.5\n",
      "fc layer 1 self.abs_max_out: 10516.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.913886/  1.993354, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.33 seconds, 4.67 minutes\n",
      "total_backward_count 1375160 real_backward_count 149126  10.844%\n",
      "fc layer 1 self.abs_max_out: 10543.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.908168/  1.980629, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.01 seconds, 4.63 minutes\n",
      "total_backward_count 1419520 real_backward_count 152122  10.716%\n",
      "fc layer 1 self.abs_max_out: 10560.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.898636/  1.989200, val:  88.75%, val_best:  88.75%, tr:  99.98%, tr_best: 100.00%, epoch time: 281.14 seconds, 4.69 minutes\n",
      "total_backward_count 1463880 real_backward_count 155011  10.589%\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.905354/  1.985169, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.22 seconds, 4.59 minutes\n",
      "total_backward_count 1508240 real_backward_count 157723  10.457%\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.897749/  1.970202, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.19 seconds, 4.57 minutes\n",
      "total_backward_count 1552600 real_backward_count 160483  10.336%\n",
      "fc layer 3 self.abs_max_out: 505.0\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.893371/  1.969738, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.67 seconds, 4.66 minutes\n",
      "total_backward_count 1596960 real_backward_count 163161  10.217%\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.899446/  1.980357, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.11 seconds, 4.67 minutes\n",
      "total_backward_count 1641320 real_backward_count 165799  10.102%\n",
      "fc layer 1 self.abs_max_out: 10581.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.897949/  1.979762, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.10 seconds, 4.67 minutes\n",
      "total_backward_count 1685680 real_backward_count 168400   9.990%\n",
      "fc layer 1 self.abs_max_out: 10598.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.898689/  1.979864, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.09 seconds, 4.67 minutes\n",
      "total_backward_count 1730040 real_backward_count 170938   9.881%\n",
      "fc layer 1 self.abs_max_out: 10633.0\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.890280/  1.975406, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.72 seconds, 4.68 minutes\n",
      "total_backward_count 1774400 real_backward_count 173422   9.774%\n",
      "fc layer 1 self.abs_max_out: 10650.0\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.883735/  1.971617, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.47 seconds, 4.64 minutes\n",
      "total_backward_count 1818760 real_backward_count 175927   9.673%\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.883215/  1.977333, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.34 seconds, 4.67 minutes\n",
      "total_backward_count 1863120 real_backward_count 178300   9.570%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.887252/  1.966667, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.50 seconds, 4.66 minutes\n",
      "total_backward_count 1907480 real_backward_count 180677   9.472%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.887084/  1.967427, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.27 seconds, 4.67 minutes\n",
      "total_backward_count 1951840 real_backward_count 183072   9.379%\n",
      "fc layer 1 self.abs_max_out: 10677.0\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.880692/  1.970146, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.37 seconds, 4.64 minutes\n",
      "total_backward_count 1996200 real_backward_count 185385   9.287%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.876448/  1.954965, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.03 seconds, 4.55 minutes\n",
      "total_backward_count 2040560 real_backward_count 187655   9.196%\n",
      "fc layer 3 self.abs_max_out: 508.0\n",
      "fc layer 1 self.abs_max_out: 10750.0\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.865791/  1.959347, val:  86.67%, val_best:  88.75%, tr:  99.98%, tr_best: 100.00%, epoch time: 277.89 seconds, 4.63 minutes\n",
      "total_backward_count 2084920 real_backward_count 189973   9.112%\n",
      "fc layer 1 self.abs_max_out: 10777.0\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.865074/  1.946066, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.42 seconds, 4.69 minutes\n",
      "total_backward_count 2129280 real_backward_count 192191   9.026%\n",
      "fc layer 1 self.abs_max_out: 10799.0\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.868158/  1.951779, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.88 seconds, 4.68 minutes\n",
      "total_backward_count 2173640 real_backward_count 194401   8.944%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.866134/  1.950649, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.84 seconds, 4.66 minutes\n",
      "total_backward_count 2218000 real_backward_count 196578   8.863%\n",
      "fc layer 2 self.abs_max_out: 4077.0\n",
      "fc layer 3 self.abs_max_out: 519.0\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.866749/  1.944618, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.60 seconds, 4.69 minutes\n",
      "total_backward_count 2262360 real_backward_count 198625   8.780%\n",
      "fc layer 2 self.abs_max_out: 4099.0\n",
      "fc layer 3 self.abs_max_out: 534.0\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.863120/  1.958416, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.61 seconds, 4.66 minutes\n",
      "total_backward_count 2306720 real_backward_count 200696   8.700%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.872232/  1.947733, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.83 seconds, 4.65 minutes\n",
      "total_backward_count 2351080 real_backward_count 202744   8.623%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.850196/  1.955112, val:  80.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.20 seconds, 4.69 minutes\n",
      "total_backward_count 2395440 real_backward_count 204800   8.550%\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.853830/  1.945562, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.49 seconds, 4.67 minutes\n",
      "total_backward_count 2439800 real_backward_count 206865   8.479%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.847374/  1.947782, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.69 seconds, 4.68 minutes\n",
      "total_backward_count 2484160 real_backward_count 208722   8.402%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.852955/  1.947239, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.19 seconds, 4.62 minutes\n",
      "total_backward_count 2528520 real_backward_count 210637   8.330%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.855106/  1.935976, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.26 seconds, 4.57 minutes\n",
      "total_backward_count 2572880 real_backward_count 212470   8.258%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.851528/  1.952253, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.39 seconds, 4.66 minutes\n",
      "total_backward_count 2617240 real_backward_count 214397   8.192%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.855679/  1.950463, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.74 seconds, 4.66 minutes\n",
      "total_backward_count 2661600 real_backward_count 216230   8.124%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.845575/  1.940543, val:  84.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.73 seconds, 4.66 minutes\n",
      "total_backward_count 2705960 real_backward_count 218050   8.058%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.842737/  1.936612, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.35 seconds, 4.69 minutes\n",
      "total_backward_count 2750320 real_backward_count 219807   7.992%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.852103/  1.947732, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.86 seconds, 4.65 minutes\n",
      "total_backward_count 2794680 real_backward_count 221585   7.929%\n",
      "fc layer 3 self.abs_max_out: 547.0\n",
      "fc layer 1 self.abs_max_out: 10801.0\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.847975/  1.949230, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.26 seconds, 4.65 minutes\n",
      "total_backward_count 2839040 real_backward_count 223265   7.864%\n",
      "fc layer 1 self.abs_max_out: 10843.0\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.843605/  1.928599, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.96 seconds, 4.67 minutes\n",
      "total_backward_count 2883400 real_backward_count 225017   7.804%\n",
      "fc layer 1 self.abs_max_out: 10888.0\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.832478/  1.922187, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.32 seconds, 4.64 minutes\n",
      "total_backward_count 2927760 real_backward_count 226659   7.742%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.825695/  1.928132, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.60 seconds, 4.68 minutes\n",
      "total_backward_count 2972120 real_backward_count 228247   7.680%\n",
      "fc layer 3 self.abs_max_out: 555.0\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.832504/  1.919446, val:  87.08%, val_best:  90.00%, tr:  99.98%, tr_best: 100.00%, epoch time: 280.15 seconds, 4.67 minutes\n",
      "total_backward_count 3016480 real_backward_count 229796   7.618%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.817342/  1.926130, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.21 seconds, 4.55 minutes\n",
      "total_backward_count 3060840 real_backward_count 231393   7.560%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.816184/  1.900960, val:  86.67%, val_best:  90.00%, tr:  99.98%, tr_best: 100.00%, epoch time: 275.02 seconds, 4.58 minutes\n",
      "total_backward_count 3105200 real_backward_count 232947   7.502%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.803581/  1.903067, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.70 seconds, 4.66 minutes\n",
      "total_backward_count 3149560 real_backward_count 234496   7.445%\n",
      "fc layer 1 self.abs_max_out: 10922.0\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.803367/  1.901050, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.66 seconds, 4.66 minutes\n",
      "total_backward_count 3193920 real_backward_count 236039   7.390%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.806137/  1.906443, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.33 seconds, 4.67 minutes\n",
      "total_backward_count 3238280 real_backward_count 237545   7.336%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.804257/  1.912289, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.54 seconds, 4.68 minutes\n",
      "total_backward_count 3282640 real_backward_count 239034   7.282%\n",
      "fc layer 2 self.abs_max_out: 4129.0\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.802155/  1.898238, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.25 seconds, 4.67 minutes\n",
      "total_backward_count 3327000 real_backward_count 240539   7.230%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.800621/  1.909684, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.41 seconds, 4.66 minutes\n",
      "total_backward_count 3371360 real_backward_count 241950   7.177%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.803278/  1.899838, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.01 seconds, 4.67 minutes\n",
      "total_backward_count 3415720 real_backward_count 243376   7.125%\n",
      "fc layer 1 self.abs_max_out: 10926.0\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.795176/  1.892868, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.17 seconds, 4.67 minutes\n",
      "total_backward_count 3460080 real_backward_count 244764   7.074%\n",
      "fc layer 1 self.abs_max_out: 10929.0\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.782739/  1.885408, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.26 seconds, 4.67 minutes\n",
      "total_backward_count 3504440 real_backward_count 246099   7.022%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.776294/  1.902281, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.70 seconds, 4.63 minutes\n",
      "total_backward_count 3548800 real_backward_count 247484   6.974%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.790095/  1.900789, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.93 seconds, 4.57 minutes\n",
      "total_backward_count 3593160 real_backward_count 248845   6.926%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.792314/  1.900769, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.56 seconds, 4.59 minutes\n",
      "total_backward_count 3637520 real_backward_count 250193   6.878%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.783546/  1.884361, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.89 seconds, 4.66 minutes\n",
      "total_backward_count 3681880 real_backward_count 251482   6.830%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.768650/  1.874090, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.80 seconds, 4.66 minutes\n",
      "total_backward_count 3726240 real_backward_count 252810   6.785%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.773872/  1.892375, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.60 seconds, 4.66 minutes\n",
      "total_backward_count 3770600 real_backward_count 254150   6.740%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.772913/  1.874513, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.79 seconds, 4.68 minutes\n",
      "total_backward_count 3814960 real_backward_count 255386   6.694%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.766344/  1.868456, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.09 seconds, 4.65 minutes\n",
      "total_backward_count 3859320 real_backward_count 256685   6.651%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.761157/  1.867776, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.05 seconds, 4.67 minutes\n",
      "total_backward_count 3903680 real_backward_count 257915   6.607%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.761664/  1.882283, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.89 seconds, 4.65 minutes\n",
      "total_backward_count 3948040 real_backward_count 259148   6.564%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.759538/  1.870957, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.09 seconds, 4.65 minutes\n",
      "total_backward_count 3992400 real_backward_count 260271   6.519%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.756627/  1.870706, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.38 seconds, 4.69 minutes\n",
      "total_backward_count 4036760 real_backward_count 261500   6.478%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.762042/  1.876216, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.92 seconds, 4.58 minutes\n",
      "total_backward_count 4081120 real_backward_count 262693   6.437%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.749145/  1.858616, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.70 seconds, 4.59 minutes\n",
      "total_backward_count 4125480 real_backward_count 263885   6.396%\n",
      "fc layer 3 self.abs_max_out: 557.0\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.745271/  1.863195, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.84 seconds, 4.60 minutes\n",
      "total_backward_count 4169840 real_backward_count 265055   6.356%\n",
      "fc layer 3 self.abs_max_out: 559.0\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.747902/  1.864753, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.66 seconds, 4.66 minutes\n",
      "total_backward_count 4214200 real_backward_count 266204   6.317%\n",
      "fc layer 3 self.abs_max_out: 563.0\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.753843/  1.874485, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.36 seconds, 4.66 minutes\n",
      "total_backward_count 4258560 real_backward_count 267289   6.277%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.755658/  1.856669, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.06 seconds, 4.67 minutes\n",
      "total_backward_count 4302920 real_backward_count 268545   6.241%\n",
      "fc layer 3 self.abs_max_out: 571.0\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.750077/  1.867493, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.93 seconds, 4.67 minutes\n",
      "total_backward_count 4347280 real_backward_count 269649   6.203%\n",
      "fc layer 3 self.abs_max_out: 572.0\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.742518/  1.858463, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.41 seconds, 4.67 minutes\n",
      "total_backward_count 4391640 real_backward_count 270770   6.166%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.737955/  1.852267, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.13 seconds, 4.67 minutes\n",
      "total_backward_count 4436000 real_backward_count 271870   6.129%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.729832/  1.864027, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.98 seconds, 4.65 minutes\n",
      "total_backward_count 4480360 real_backward_count 272993   6.093%\n",
      "fc layer 3 self.abs_max_out: 585.0\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.738024/  1.853879, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.37 seconds, 4.64 minutes\n",
      "total_backward_count 4524720 real_backward_count 274018   6.056%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.743816/  1.858975, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.39 seconds, 4.61 minutes\n",
      "total_backward_count 4569080 real_backward_count 275081   6.020%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.741978/  1.860873, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.34 seconds, 4.56 minutes\n",
      "total_backward_count 4613440 real_backward_count 276116   5.985%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.744704/  1.851183, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.21 seconds, 4.59 minutes\n",
      "total_backward_count 4657800 real_backward_count 277131   5.950%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.729721/  1.845304, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.53 seconds, 4.61 minutes\n",
      "total_backward_count 4702160 real_backward_count 278090   5.914%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.726705/  1.841073, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.01 seconds, 4.65 minutes\n",
      "total_backward_count 4746520 real_backward_count 279062   5.879%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.718880/  1.840189, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.32 seconds, 4.66 minutes\n",
      "total_backward_count 4790880 real_backward_count 280030   5.845%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.716749/  1.840070, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.32 seconds, 4.64 minutes\n",
      "total_backward_count 4835240 real_backward_count 281065   5.813%\n",
      "fc layer 3 self.abs_max_out: 601.0\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.714235/  1.833457, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.26 seconds, 4.65 minutes\n",
      "total_backward_count 4879600 real_backward_count 282081   5.781%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.717082/  1.846084, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.94 seconds, 4.63 minutes\n",
      "total_backward_count 4923960 real_backward_count 283085   5.749%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.728413/  1.849485, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.81 seconds, 4.65 minutes\n",
      "total_backward_count 4968320 real_backward_count 284109   5.718%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.729512/  1.847957, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.79 seconds, 4.63 minutes\n",
      "total_backward_count 5012680 real_backward_count 285162   5.689%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.720938/  1.843690, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.14 seconds, 4.67 minutes\n",
      "total_backward_count 5057040 real_backward_count 286171   5.659%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.718712/  1.848053, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.95 seconds, 4.58 minutes\n",
      "total_backward_count 5101400 real_backward_count 287111   5.628%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.722912/  1.845473, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.52 seconds, 4.56 minutes\n",
      "total_backward_count 5145760 real_backward_count 288021   5.597%\n",
      "fc layer 3 self.abs_max_out: 611.0\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.724087/  1.850227, val:  85.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.30 seconds, 4.62 minutes\n",
      "total_backward_count 5190120 real_backward_count 289012   5.569%\n",
      "fc layer 3 self.abs_max_out: 613.0\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.720530/  1.836483, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.04 seconds, 4.63 minutes\n",
      "total_backward_count 5234480 real_backward_count 289914   5.539%\n",
      "fc layer 3 self.abs_max_out: 614.0\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.715957/  1.844504, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.32 seconds, 4.62 minutes\n",
      "total_backward_count 5278840 real_backward_count 290871   5.510%\n",
      "fc layer 3 self.abs_max_out: 623.0\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.716073/  1.841363, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.05 seconds, 4.67 minutes\n",
      "total_backward_count 5323200 real_backward_count 291804   5.482%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.716006/  1.837783, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.14 seconds, 4.67 minutes\n",
      "total_backward_count 5367560 real_backward_count 292732   5.454%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.712036/  1.841264, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.99 seconds, 4.65 minutes\n",
      "total_backward_count 5411920 real_backward_count 293649   5.426%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.720165/  1.838761, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.41 seconds, 4.66 minutes\n",
      "total_backward_count 5456280 real_backward_count 294591   5.399%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.711835/  1.844964, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.34 seconds, 4.66 minutes\n",
      "total_backward_count 5500640 real_backward_count 295512   5.372%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.710347/  1.826357, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.08 seconds, 4.63 minutes\n",
      "total_backward_count 5545000 real_backward_count 296384   5.345%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.704879/  1.825970, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.53 seconds, 4.63 minutes\n",
      "total_backward_count 5589360 real_backward_count 297297   5.319%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.703384/  1.829672, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.75 seconds, 4.58 minutes\n",
      "total_backward_count 5633720 real_backward_count 298142   5.292%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.703827/  1.830639, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.69 seconds, 4.61 minutes\n",
      "total_backward_count 5678080 real_backward_count 299038   5.267%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.702768/  1.832320, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.37 seconds, 4.61 minutes\n",
      "total_backward_count 5722440 real_backward_count 299871   5.240%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.701854/  1.825194, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.46 seconds, 4.64 minutes\n",
      "total_backward_count 5766800 real_backward_count 300726   5.215%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.706385/  1.829035, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.16 seconds, 4.65 minutes\n",
      "total_backward_count 5811160 real_backward_count 301564   5.189%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.703793/  1.825796, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.65 seconds, 4.66 minutes\n",
      "total_backward_count 5855520 real_backward_count 302384   5.164%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.695964/  1.828561, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.13 seconds, 4.65 minutes\n",
      "total_backward_count 5899880 real_backward_count 303222   5.139%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.696536/  1.827830, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.39 seconds, 4.66 minutes\n",
      "total_backward_count 5944240 real_backward_count 304048   5.115%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.690322/  1.822860, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.87 seconds, 4.66 minutes\n",
      "total_backward_count 5988600 real_backward_count 304822   5.090%\n",
      "fc layer 3 self.abs_max_out: 639.0\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.689990/  1.820617, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.43 seconds, 4.67 minutes\n",
      "total_backward_count 6032960 real_backward_count 305628   5.066%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.690066/  1.814215, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.45 seconds, 4.66 minutes\n",
      "total_backward_count 6077320 real_backward_count 306409   5.042%\n",
      "fc layer 3 self.abs_max_out: 665.0\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.685881/  1.816812, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.29 seconds, 4.60 minutes\n",
      "total_backward_count 6121680 real_backward_count 307192   5.018%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.689260/  1.819192, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.21 seconds, 4.57 minutes\n",
      "total_backward_count 6166040 real_backward_count 307953   4.994%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.683116/  1.808906, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.26 seconds, 4.60 minutes\n",
      "total_backward_count 6210400 real_backward_count 308721   4.971%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.681820/  1.816894, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.64 seconds, 4.58 minutes\n",
      "total_backward_count 6254760 real_backward_count 309441   4.947%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.679024/  1.806211, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.22 seconds, 4.65 minutes\n",
      "total_backward_count 6299120 real_backward_count 310231   4.925%\n",
      "fc layer 3 self.abs_max_out: 667.0\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.669512/  1.807976, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.49 seconds, 4.62 minutes\n",
      "total_backward_count 6343480 real_backward_count 311063   4.904%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.670155/  1.798681, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.03 seconds, 4.63 minutes\n",
      "total_backward_count 6387840 real_backward_count 311800   4.881%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.675158/  1.807789, val:  87.08%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.29 seconds, 4.65 minutes\n",
      "total_backward_count 6432200 real_backward_count 312541   4.859%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.672251/  1.803247, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.32 seconds, 4.64 minutes\n",
      "total_backward_count 6476560 real_backward_count 313286   4.837%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.671746/  1.816109, val:  87.08%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.89 seconds, 4.63 minutes\n",
      "total_backward_count 6520920 real_backward_count 314000   4.815%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.667579/  1.800683, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.81 seconds, 4.65 minutes\n",
      "total_backward_count 6565280 real_backward_count 314729   4.794%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.670024/  1.807832, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.23 seconds, 4.64 minutes\n",
      "total_backward_count 6609640 real_backward_count 315459   4.773%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.668052/  1.791987, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.75 seconds, 4.60 minutes\n",
      "total_backward_count 6654000 real_backward_count 316199   4.752%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.661421/  1.799411, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.83 seconds, 4.56 minutes\n",
      "total_backward_count 6698360 real_backward_count 316939   4.732%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.668860/  1.802270, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.13 seconds, 4.65 minutes\n",
      "total_backward_count 6742720 real_backward_count 317687   4.712%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.662259/  1.783782, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.95 seconds, 4.58 minutes\n",
      "total_backward_count 6787080 real_backward_count 318416   4.692%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.654757/  1.787761, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.33 seconds, 4.64 minutes\n",
      "total_backward_count 6831440 real_backward_count 319096   4.671%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.655162/  1.784456, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.91 seconds, 4.67 minutes\n",
      "total_backward_count 6875800 real_backward_count 319771   4.651%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.652345/  1.787018, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.71 seconds, 4.65 minutes\n",
      "total_backward_count 6920160 real_backward_count 320454   4.631%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.660755/  1.794878, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.07 seconds, 4.65 minutes\n",
      "total_backward_count 6964520 real_backward_count 321124   4.611%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.655623/  1.791051, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.79 seconds, 4.63 minutes\n",
      "total_backward_count 7008880 real_backward_count 321812   4.591%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.647828/  1.783363, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.67 seconds, 4.66 minutes\n",
      "total_backward_count 7053240 real_backward_count 322505   4.572%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.655180/  1.796742, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.43 seconds, 4.64 minutes\n",
      "total_backward_count 7097600 real_backward_count 323230   4.554%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.655081/  1.781917, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.35 seconds, 4.59 minutes\n",
      "total_backward_count 7141960 real_backward_count 323948   4.536%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.643084/  1.781723, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.37 seconds, 4.59 minutes\n",
      "total_backward_count 7186320 real_backward_count 324553   4.516%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.640418/  1.783111, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.91 seconds, 4.58 minutes\n",
      "total_backward_count 7230680 real_backward_count 325197   4.497%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.646721/  1.783190, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.12 seconds, 4.59 minutes\n",
      "total_backward_count 7275040 real_backward_count 325813   4.479%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.650127/  1.784624, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.88 seconds, 4.58 minutes\n",
      "total_backward_count 7319400 real_backward_count 326489   4.461%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.645973/  1.783573, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.50 seconds, 4.63 minutes\n",
      "total_backward_count 7363760 real_backward_count 327115   4.442%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.648988/  1.783073, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.71 seconds, 4.65 minutes\n",
      "total_backward_count 7408120 real_backward_count 327734   4.424%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.644207/  1.782963, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.45 seconds, 4.64 minutes\n",
      "total_backward_count 7452480 real_backward_count 328384   4.406%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.643091/  1.778575, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.44 seconds, 4.62 minutes\n",
      "total_backward_count 7496840 real_backward_count 329007   4.389%\n",
      "fc layer 3 self.abs_max_out: 668.0\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.641194/  1.787682, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.05 seconds, 4.65 minutes\n",
      "total_backward_count 7541200 real_backward_count 329614   4.371%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.655321/  1.788209, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.73 seconds, 4.65 minutes\n",
      "total_backward_count 7585560 real_backward_count 330268   4.354%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.647300/  1.793075, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.33 seconds, 4.64 minutes\n",
      "total_backward_count 7629920 real_backward_count 330906   4.337%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.645744/  1.788610, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.45 seconds, 4.59 minutes\n",
      "total_backward_count 7674280 real_backward_count 331521   4.320%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.644453/  1.794942, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.20 seconds, 4.55 minutes\n",
      "total_backward_count 7718640 real_backward_count 332119   4.303%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.647236/  1.784760, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.80 seconds, 4.61 minutes\n",
      "total_backward_count 7763000 real_backward_count 332703   4.286%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.644015/  1.784108, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.28 seconds, 4.59 minutes\n",
      "total_backward_count 7807360 real_backward_count 333343   4.270%\n",
      "fc layer 3 self.abs_max_out: 672.0\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.646953/  1.789878, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.51 seconds, 4.61 minutes\n",
      "total_backward_count 7851720 real_backward_count 333971   4.253%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.647155/  1.793634, val:  88.33%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.07 seconds, 4.62 minutes\n",
      "total_backward_count 7896080 real_backward_count 334577   4.237%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.642382/  1.787412, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.36 seconds, 4.62 minutes\n",
      "total_backward_count 7940440 real_backward_count 335207   4.222%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.638309/  1.781659, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.52 seconds, 4.61 minutes\n",
      "total_backward_count 7984800 real_backward_count 335798   4.205%\n",
      "lif layer 1 self.abs_max_v: 13556.5\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.640396/  1.778217, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.94 seconds, 4.62 minutes\n",
      "total_backward_count 8029160 real_backward_count 336409   4.190%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.640077/  1.782433, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.58 seconds, 4.64 minutes\n",
      "total_backward_count 8073520 real_backward_count 337017   4.174%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.638718/  1.784937, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.09 seconds, 4.65 minutes\n",
      "total_backward_count 8117880 real_backward_count 337593   4.159%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.638951/  1.774808, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.33 seconds, 4.62 minutes\n",
      "total_backward_count 8162240 real_backward_count 338142   4.143%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.637422/  1.775108, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.85 seconds, 4.60 minutes\n",
      "total_backward_count 8206600 real_backward_count 338717   4.127%\n",
      "fc layer 3 self.abs_max_out: 675.0\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.634435/  1.775809, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.64 seconds, 4.56 minutes\n",
      "total_backward_count 8250960 real_backward_count 339293   4.112%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.632209/  1.776296, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.69 seconds, 4.66 minutes\n",
      "total_backward_count 8295320 real_backward_count 339912   4.098%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.626215/  1.777318, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.58 seconds, 4.61 minutes\n",
      "total_backward_count 8339680 real_backward_count 340486   4.083%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.633694/  1.779385, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.70 seconds, 4.61 minutes\n",
      "total_backward_count 8384040 real_backward_count 341041   4.068%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.634936/  1.779757, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.48 seconds, 4.66 minutes\n",
      "total_backward_count 8428400 real_backward_count 341657   4.054%\n",
      "fc layer 3 self.abs_max_out: 696.0\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.640103/  1.775626, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.37 seconds, 4.64 minutes\n",
      "total_backward_count 8472760 real_backward_count 342232   4.039%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.637051/  1.768508, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.85 seconds, 4.60 minutes\n",
      "total_backward_count 8517120 real_backward_count 342764   4.024%\n",
      "fc layer 3 self.abs_max_out: 700.0\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.626197/  1.771198, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.07 seconds, 4.63 minutes\n",
      "total_backward_count 8561480 real_backward_count 343364   4.011%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.623153/  1.769413, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.03 seconds, 4.65 minutes\n",
      "total_backward_count 8605840 real_backward_count 343934   3.997%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.617009/  1.764312, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.79 seconds, 4.65 minutes\n",
      "total_backward_count 8650200 real_backward_count 344520   3.983%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.618330/  1.765678, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.68 seconds, 4.59 minutes\n",
      "total_backward_count 8694560 real_backward_count 345092   3.969%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.623435/  1.762132, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.79 seconds, 4.60 minutes\n",
      "total_backward_count 8738920 real_backward_count 345671   3.956%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.626542/  1.766490, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.16 seconds, 4.60 minutes\n",
      "total_backward_count 8783280 real_backward_count 346168   3.941%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.628787/  1.770766, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.99 seconds, 4.63 minutes\n",
      "total_backward_count 8827640 real_backward_count 346708   3.928%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.632175/  1.775172, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.98 seconds, 4.60 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036375b93bd94a608613a971269ec5b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>summary_val_acc</td><td>▁▂▃▆▇▅█▇▇███▇▇▇█▇█████▇███▇█████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▄▆▆▆▇▇▇▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▃▆▇▅█▇▇███▇▇▇█▇█████▇███▇█████████████</td></tr><tr><td>val_loss</td><td>█▇▇▆▆▆▅▅▅▄▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.63217</td></tr><tr><td>val_acc_best</td><td>0.925</td></tr><tr><td>val_acc_now</td><td>0.90833</td></tr><tr><td>val_loss</td><td>1.77517</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dazzling-sweep-69</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0k09yrmb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0k09yrmb</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250906_001528-0k09yrmb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: howkn0u5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 27963\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.21.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250906_154430-howkn0u5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/howkn0u5' target=\"_blank\">daily-sweep-66</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/jwru0k4o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/jwru0k4o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/jwru0k4o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/jwru0k4o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/howkn0u5' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/howkn0u5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20250906_154439_005', 'my_seed': 27963, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0df5ce43f802d21fe74cde54437db10b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = f205136b2771111650a88c4e480cfe73\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 391e4997dc3a746988cd0e9dceb2d42e\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = bb0ac3251c9e44bfe72bcb8b2e969f0d\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = c796a451486ae8cd6d0dd9bd02a9e235\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a6e81fbc907b11cedc166a7f5b843582\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = d4ded3e2b3703cdb1192f3d689158f82\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 602987c624e8b98603f8b906841eadb1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 2d3185edb0c7b53adc6375ce1392ad59\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 9e9960951042c2f18fd3576739597330\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 4436 BATCH: 1 train_data_count: 4436\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 414.0\n",
      "lif layer 1 self.abs_max_v: 414.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "lif layer 1 self.abs_max_v: 450.0\n",
      "fc layer 1 self.abs_max_out: 507.0\n",
      "lif layer 1 self.abs_max_v: 562.5\n",
      "fc layer 2 self.abs_max_out: 72.0\n",
      "lif layer 2 self.abs_max_v: 72.0\n",
      "fc layer 1 self.abs_max_out: 521.0\n",
      "lif layer 1 self.abs_max_v: 802.5\n",
      "fc layer 2 self.abs_max_out: 129.0\n",
      "lif layer 2 self.abs_max_v: 157.0\n",
      "fc layer 1 self.abs_max_out: 545.0\n",
      "lif layer 1 self.abs_max_v: 832.0\n",
      "fc layer 1 self.abs_max_out: 546.0\n",
      "fc layer 2 self.abs_max_out: 290.0\n",
      "lif layer 2 self.abs_max_v: 304.5\n",
      "fc layer 1 self.abs_max_out: 590.0\n",
      "fc layer 2 self.abs_max_out: 319.0\n",
      "lif layer 2 self.abs_max_v: 351.5\n",
      "fc layer 1 self.abs_max_out: 898.0\n",
      "lif layer 1 self.abs_max_v: 898.0\n",
      "lif layer 1 self.abs_max_v: 963.5\n",
      "lif layer 2 self.abs_max_v: 370.0\n",
      "lif layer 1 self.abs_max_v: 1113.0\n",
      "fc layer 2 self.abs_max_out: 382.0\n",
      "lif layer 2 self.abs_max_v: 475.5\n",
      "lif layer 1 self.abs_max_v: 1298.5\n",
      "fc layer 2 self.abs_max_out: 477.0\n",
      "lif layer 2 self.abs_max_v: 585.0\n",
      "fc layer 3 self.abs_max_out: 29.0\n",
      "fc layer 1 self.abs_max_out: 976.0\n",
      "fc layer 2 self.abs_max_out: 572.0\n",
      "lif layer 2 self.abs_max_v: 783.5\n",
      "fc layer 3 self.abs_max_out: 58.0\n",
      "fc layer 1 self.abs_max_out: 1107.0\n",
      "lif layer 1 self.abs_max_v: 1301.0\n",
      "fc layer 1 self.abs_max_out: 1401.0\n",
      "lif layer 1 self.abs_max_v: 1401.0\n",
      "fc layer 3 self.abs_max_out: 88.0\n",
      "fc layer 1 self.abs_max_out: 1504.0\n",
      "lif layer 1 self.abs_max_v: 1504.0\n",
      "fc layer 2 self.abs_max_out: 722.0\n",
      "lif layer 2 self.abs_max_v: 973.0\n",
      "fc layer 3 self.abs_max_out: 108.0\n",
      "fc layer 1 self.abs_max_out: 1566.0\n",
      "lif layer 1 self.abs_max_v: 1566.0\n",
      "lif layer 2 self.abs_max_v: 987.5\n",
      "fc layer 3 self.abs_max_out: 110.0\n",
      "fc layer 3 self.abs_max_out: 143.0\n",
      "fc layer 2 self.abs_max_out: 727.0\n",
      "fc layer 1 self.abs_max_out: 1585.0\n",
      "lif layer 1 self.abs_max_v: 1585.0\n",
      "fc layer 2 self.abs_max_out: 1016.0\n",
      "lif layer 2 self.abs_max_v: 1261.0\n",
      "fc layer 1 self.abs_max_out: 1594.0\n",
      "lif layer 1 self.abs_max_v: 1594.0\n",
      "fc layer 1 self.abs_max_out: 1685.0\n",
      "lif layer 1 self.abs_max_v: 1685.0\n",
      "lif layer 2 self.abs_max_v: 1341.0\n",
      "lif layer 2 self.abs_max_v: 1432.0\n",
      "fc layer 3 self.abs_max_out: 149.0\n",
      "fc layer 3 self.abs_max_out: 168.0\n",
      "fc layer 3 self.abs_max_out: 181.0\n",
      "fc layer 1 self.abs_max_out: 1704.0\n",
      "lif layer 1 self.abs_max_v: 1704.0\n",
      "fc layer 1 self.abs_max_out: 1935.0\n",
      "lif layer 1 self.abs_max_v: 1935.0\n",
      "fc layer 3 self.abs_max_out: 182.0\n",
      "fc layer 3 self.abs_max_out: 196.0\n",
      "fc layer 2 self.abs_max_out: 1052.0\n",
      "lif layer 2 self.abs_max_v: 1570.0\n",
      "lif layer 2 self.abs_max_v: 1639.0\n",
      "fc layer 1 self.abs_max_out: 2132.0\n",
      "lif layer 1 self.abs_max_v: 2132.0\n",
      "fc layer 2 self.abs_max_out: 1077.0\n",
      "fc layer 2 self.abs_max_out: 1196.0\n",
      "fc layer 2 self.abs_max_out: 1294.0\n",
      "fc layer 3 self.abs_max_out: 239.0\n",
      "lif layer 2 self.abs_max_v: 1866.0\n",
      "fc layer 3 self.abs_max_out: 242.0\n",
      "fc layer 3 self.abs_max_out: 247.0\n",
      "lif layer 2 self.abs_max_v: 1910.0\n",
      "fc layer 3 self.abs_max_out: 252.0\n",
      "fc layer 1 self.abs_max_out: 2180.0\n",
      "lif layer 1 self.abs_max_v: 2180.0\n",
      "fc layer 1 self.abs_max_out: 2201.0\n",
      "lif layer 1 self.abs_max_v: 2201.0\n",
      "fc layer 2 self.abs_max_out: 1335.0\n",
      "fc layer 3 self.abs_max_out: 284.0\n",
      "fc layer 1 self.abs_max_out: 2313.0\n",
      "lif layer 1 self.abs_max_v: 2313.0\n",
      "fc layer 2 self.abs_max_out: 1445.0\n",
      "fc layer 3 self.abs_max_out: 293.0\n",
      "fc layer 3 self.abs_max_out: 301.0\n",
      "fc layer 3 self.abs_max_out: 307.0\n",
      "lif layer 2 self.abs_max_v: 1951.5\n",
      "fc layer 3 self.abs_max_out: 317.0\n",
      "fc layer 1 self.abs_max_out: 2498.0\n",
      "lif layer 1 self.abs_max_v: 2498.0\n",
      "lif layer 2 self.abs_max_v: 2005.5\n",
      "lif layer 2 self.abs_max_v: 2258.0\n",
      "fc layer 3 self.abs_max_out: 335.0\n",
      "fc layer 1 self.abs_max_out: 2560.0\n",
      "lif layer 1 self.abs_max_v: 2560.0\n",
      "lif layer 2 self.abs_max_v: 2314.0\n",
      "fc layer 3 self.abs_max_out: 354.0\n",
      "fc layer 2 self.abs_max_out: 1493.0\n",
      "fc layer 2 self.abs_max_out: 1546.0\n",
      "fc layer 2 self.abs_max_out: 1693.0\n",
      "fc layer 1 self.abs_max_out: 2634.0\n",
      "lif layer 1 self.abs_max_v: 2634.0\n",
      "fc layer 1 self.abs_max_out: 2648.0\n",
      "lif layer 1 self.abs_max_v: 2648.0\n",
      "fc layer 1 self.abs_max_out: 2713.0\n",
      "lif layer 1 self.abs_max_v: 2713.0\n",
      "fc layer 2 self.abs_max_out: 1736.0\n",
      "fc layer 3 self.abs_max_out: 374.0\n",
      "fc layer 3 self.abs_max_out: 425.0\n",
      "fc layer 1 self.abs_max_out: 2754.0\n",
      "lif layer 1 self.abs_max_v: 2754.0\n",
      "fc layer 2 self.abs_max_out: 1786.0\n",
      "fc layer 1 self.abs_max_out: 2851.0\n",
      "lif layer 1 self.abs_max_v: 2851.0\n",
      "fc layer 2 self.abs_max_out: 1906.0\n",
      "fc layer 1 self.abs_max_out: 3098.0\n",
      "lif layer 1 self.abs_max_v: 3098.0\n",
      "fc layer 2 self.abs_max_out: 1950.0\n",
      "fc layer 2 self.abs_max_out: 1997.0\n",
      "fc layer 2 self.abs_max_out: 2147.0\n",
      "fc layer 2 self.abs_max_out: 2154.0\n",
      "fc layer 2 self.abs_max_out: 2157.0\n",
      "fc layer 2 self.abs_max_out: 2162.0\n",
      "fc layer 1 self.abs_max_out: 3124.0\n",
      "lif layer 1 self.abs_max_v: 3124.0\n",
      "fc layer 1 self.abs_max_out: 3185.0\n",
      "lif layer 1 self.abs_max_v: 3185.0\n",
      "fc layer 2 self.abs_max_out: 2198.0\n",
      "lif layer 2 self.abs_max_v: 2390.0\n",
      "fc layer 1 self.abs_max_out: 3282.0\n",
      "lif layer 1 self.abs_max_v: 3282.0\n",
      "fc layer 1 self.abs_max_out: 3328.0\n",
      "lif layer 1 self.abs_max_v: 3328.0\n",
      "fc layer 1 self.abs_max_out: 3357.0\n",
      "lif layer 1 self.abs_max_v: 3357.0\n",
      "fc layer 1 self.abs_max_out: 3383.0\n",
      "lif layer 1 self.abs_max_v: 3383.0\n",
      "lif layer 1 self.abs_max_v: 3392.0\n",
      "lif layer 1 self.abs_max_v: 3392.5\n",
      "lif layer 1 self.abs_max_v: 3497.5\n",
      "fc layer 3 self.abs_max_out: 437.0\n",
      "fc layer 3 self.abs_max_out: 445.0\n",
      "lif layer 2 self.abs_max_v: 2453.5\n",
      "fc layer 1 self.abs_max_out: 3552.0\n",
      "lif layer 1 self.abs_max_v: 3552.0\n",
      "fc layer 2 self.abs_max_out: 2437.0\n",
      "fc layer 1 self.abs_max_out: 3562.0\n",
      "lif layer 1 self.abs_max_v: 3562.0\n",
      "fc layer 3 self.abs_max_out: 449.0\n",
      "fc layer 2 self.abs_max_out: 2438.0\n",
      "lif layer 1 self.abs_max_v: 3636.0\n",
      "fc layer 2 self.abs_max_out: 2444.0\n",
      "fc layer 1 self.abs_max_out: 3690.0\n",
      "lif layer 1 self.abs_max_v: 3690.0\n",
      "fc layer 1 self.abs_max_out: 3888.0\n",
      "lif layer 1 self.abs_max_v: 3888.0\n",
      "fc layer 2 self.abs_max_out: 2459.0\n",
      "lif layer 2 self.abs_max_v: 2459.0\n",
      "fc layer 2 self.abs_max_out: 2505.0\n",
      "lif layer 2 self.abs_max_v: 2505.0\n",
      "fc layer 1 self.abs_max_out: 3998.0\n",
      "lif layer 1 self.abs_max_v: 3998.0\n",
      "lif layer 2 self.abs_max_v: 2634.5\n",
      "fc layer 2 self.abs_max_out: 2534.0\n",
      "lif layer 1 self.abs_max_v: 4191.0\n",
      "lif layer 2 self.abs_max_v: 2661.0\n",
      "fc layer 2 self.abs_max_out: 2551.0\n",
      "fc layer 2 self.abs_max_out: 2604.0\n",
      "lif layer 1 self.abs_max_v: 4370.0\n",
      "lif layer 1 self.abs_max_v: 4430.0\n",
      "lif layer 1 self.abs_max_v: 4944.0\n",
      "lif layer 2 self.abs_max_v: 2879.5\n",
      "lif layer 2 self.abs_max_v: 2902.0\n",
      "lif layer 2 self.abs_max_v: 3144.5\n",
      "fc layer 1 self.abs_max_out: 4083.0\n",
      "fc layer 2 self.abs_max_out: 2637.0\n",
      "fc layer 1 self.abs_max_out: 4507.0\n",
      "fc layer 2 self.abs_max_out: 2643.0\n",
      "lif layer 1 self.abs_max_v: 5002.0\n",
      "lif layer 1 self.abs_max_v: 5342.0\n",
      "fc layer 2 self.abs_max_out: 2766.0\n",
      "fc layer 1 self.abs_max_out: 4670.0\n",
      "fc layer 1 self.abs_max_out: 4756.0\n",
      "fc layer 2 self.abs_max_out: 2777.0\n",
      "fc layer 2 self.abs_max_out: 2806.0\n",
      "fc layer 2 self.abs_max_out: 2846.0\n",
      "fc layer 2 self.abs_max_out: 2963.0\n",
      "fc layer 2 self.abs_max_out: 2971.0\n",
      "fc layer 1 self.abs_max_out: 4828.0\n",
      "fc layer 2 self.abs_max_out: 2997.0\n",
      "fc layer 1 self.abs_max_out: 5290.0\n",
      "lif layer 1 self.abs_max_v: 5440.5\n",
      "fc layer 2 self.abs_max_out: 3074.0\n",
      "fc layer 2 self.abs_max_out: 3090.0\n",
      "fc layer 2 self.abs_max_out: 3098.0\n",
      "fc layer 1 self.abs_max_out: 5455.0\n",
      "lif layer 1 self.abs_max_v: 5455.0\n",
      "fc layer 1 self.abs_max_out: 5477.0\n",
      "lif layer 1 self.abs_max_v: 5477.0\n",
      "fc layer 1 self.abs_max_out: 5569.0\n",
      "lif layer 1 self.abs_max_v: 5569.0\n",
      "fc layer 1 self.abs_max_out: 5729.0\n",
      "lif layer 1 self.abs_max_v: 5729.0\n",
      "fc layer 2 self.abs_max_out: 3126.0\n",
      "fc layer 2 self.abs_max_out: 3237.0\n",
      "lif layer 2 self.abs_max_v: 3237.0\n",
      "fc layer 1 self.abs_max_out: 5757.0\n",
      "lif layer 1 self.abs_max_v: 5757.0\n",
      "lif layer 1 self.abs_max_v: 6149.0\n",
      "fc layer 1 self.abs_max_out: 5876.0\n",
      "lif layer 1 self.abs_max_v: 6365.5\n",
      "lif layer 1 self.abs_max_v: 6773.0\n",
      "fc layer 1 self.abs_max_out: 5885.0\n",
      "fc layer 1 self.abs_max_out: 5892.0\n",
      "fc layer 1 self.abs_max_out: 5926.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  2.008946/  2.104909, val:  42.92%, val_best:  42.92%, tr:  95.96%, tr_best:  95.96%, epoch time: 281.27 seconds, 4.69 minutes\n",
      "total_backward_count 44360 real_backward_count 10634  23.972%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 6166.0\n",
      "fc layer 1 self.abs_max_out: 6206.0\n",
      "lif layer 1 self.abs_max_v: 6911.5\n",
      "fc layer 1 self.abs_max_out: 6311.0\n",
      "fc layer 1 self.abs_max_out: 6563.0\n",
      "lif layer 1 self.abs_max_v: 7876.5\n",
      "lif layer 1 self.abs_max_v: 8325.5\n",
      "fc layer 1 self.abs_max_out: 6753.0\n",
      "fc layer 1 self.abs_max_out: 6957.0\n",
      "lif layer 2 self.abs_max_v: 3308.5\n",
      "lif layer 2 self.abs_max_v: 3330.5\n",
      "lif layer 2 self.abs_max_v: 3386.0\n",
      "lif layer 1 self.abs_max_v: 8678.5\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.991059/  2.085411, val:  56.67%, val_best:  56.67%, tr:  99.39%, tr_best:  99.39%, epoch time: 279.13 seconds, 4.65 minutes\n",
      "total_backward_count 88720 real_backward_count 18043  20.337%\n",
      "fc layer 2 self.abs_max_out: 3289.0\n",
      "fc layer 1 self.abs_max_out: 7087.0\n",
      "lif layer 2 self.abs_max_v: 3514.5\n",
      "lif layer 2 self.abs_max_v: 3599.5\n",
      "lif layer 1 self.abs_max_v: 9108.5\n",
      "lif layer 1 self.abs_max_v: 9463.5\n",
      "fc layer 2 self.abs_max_out: 3318.0\n",
      "lif layer 2 self.abs_max_v: 3690.0\n",
      "lif layer 2 self.abs_max_v: 3709.0\n",
      "lif layer 2 self.abs_max_v: 3739.5\n",
      "fc layer 2 self.abs_max_out: 3320.0\n",
      "lif layer 2 self.abs_max_v: 3791.0\n",
      "fc layer 1 self.abs_max_out: 7173.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.981985/  2.079481, val:  52.08%, val_best:  56.67%, tr:  99.46%, tr_best:  99.46%, epoch time: 280.45 seconds, 4.67 minutes\n",
      "total_backward_count 133080 real_backward_count 24809  18.642%\n",
      "lif layer 1 self.abs_max_v: 9627.5\n",
      "lif layer 1 self.abs_max_v: 10050.0\n",
      "fc layer 1 self.abs_max_out: 7604.0\n",
      "lif layer 2 self.abs_max_v: 3835.0\n",
      "lif layer 2 self.abs_max_v: 3909.0\n",
      "fc layer 3 self.abs_max_out: 455.0\n",
      "fc layer 3 self.abs_max_out: 471.0\n",
      "lif layer 1 self.abs_max_v: 10085.5\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.970697/  2.057221, val:  64.58%, val_best:  64.58%, tr:  99.80%, tr_best:  99.80%, epoch time: 281.25 seconds, 4.69 minutes\n",
      "total_backward_count 177440 real_backward_count 31231  17.601%\n",
      "lif layer 1 self.abs_max_v: 10273.5\n",
      "lif layer 1 self.abs_max_v: 10763.0\n",
      "fc layer 2 self.abs_max_out: 3374.0\n",
      "lif layer 2 self.abs_max_v: 4000.5\n",
      "lif layer 2 self.abs_max_v: 4019.5\n",
      "lif layer 2 self.abs_max_v: 4077.5\n",
      "fc layer 1 self.abs_max_out: 7653.0\n",
      "fc layer 3 self.abs_max_out: 476.0\n",
      "fc layer 1 self.abs_max_out: 7969.0\n",
      "lif layer 1 self.abs_max_v: 11014.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.957645/  2.037829, val:  54.58%, val_best:  64.58%, tr:  99.75%, tr_best:  99.80%, epoch time: 280.28 seconds, 4.67 minutes\n",
      "total_backward_count 221800 real_backward_count 37397  16.861%\n",
      "lif layer 1 self.abs_max_v: 11233.5\n",
      "lif layer 1 self.abs_max_v: 11928.0\n",
      "fc layer 2 self.abs_max_out: 3380.0\n",
      "fc layer 2 self.abs_max_out: 3515.0\n",
      "fc layer 2 self.abs_max_out: 3521.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.942825/  2.043825, val:  66.25%, val_best:  66.25%, tr:  99.82%, tr_best:  99.82%, epoch time: 279.76 seconds, 4.66 minutes\n",
      "total_backward_count 266160 real_backward_count 43279  16.261%\n",
      "lif layer 2 self.abs_max_v: 4104.5\n",
      "lif layer 2 self.abs_max_v: 4172.0\n",
      "lif layer 2 self.abs_max_v: 4207.0\n",
      "lif layer 2 self.abs_max_v: 4210.5\n",
      "lif layer 2 self.abs_max_v: 4253.0\n",
      "fc layer 1 self.abs_max_out: 8186.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.935121/  2.020663, val:  70.83%, val_best:  70.83%, tr:  99.91%, tr_best:  99.91%, epoch time: 278.95 seconds, 4.65 minutes\n",
      "total_backward_count 310520 real_backward_count 49056  15.798%\n",
      "lif layer 2 self.abs_max_v: 4436.0\n",
      "lif layer 2 self.abs_max_v: 4485.0\n",
      "fc layer 1 self.abs_max_out: 8195.0\n",
      "lif layer 2 self.abs_max_v: 4571.5\n",
      "fc layer 1 self.abs_max_out: 8539.0\n",
      "fc layer 2 self.abs_max_out: 3522.0\n",
      "lif layer 2 self.abs_max_v: 4662.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.930372/  2.020248, val:  77.50%, val_best:  77.50%, tr:  99.89%, tr_best:  99.91%, epoch time: 278.61 seconds, 4.64 minutes\n",
      "total_backward_count 354880 real_backward_count 54481  15.352%\n",
      "fc layer 2 self.abs_max_out: 3527.0\n",
      "fc layer 1 self.abs_max_out: 8623.0\n",
      "fc layer 2 self.abs_max_out: 3579.0\n",
      "lif layer 1 self.abs_max_v: 11995.5\n",
      "lif layer 1 self.abs_max_v: 12459.0\n",
      "fc layer 2 self.abs_max_out: 3672.0\n",
      "lif layer 2 self.abs_max_v: 4709.5\n",
      "lif layer 2 self.abs_max_v: 4785.0\n",
      "fc layer 3 self.abs_max_out: 486.0\n",
      "lif layer 2 self.abs_max_v: 4888.0\n",
      "lif layer 2 self.abs_max_v: 4897.0\n",
      "lif layer 2 self.abs_max_v: 5072.5\n",
      "lif layer 2 self.abs_max_v: 5260.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.929493/  2.032440, val:  62.50%, val_best:  77.50%, tr:  99.91%, tr_best:  99.91%, epoch time: 274.11 seconds, 4.57 minutes\n",
      "total_backward_count 399240 real_backward_count 59762  14.969%\n",
      "lif layer 2 self.abs_max_v: 5267.5\n",
      "lif layer 2 self.abs_max_v: 5385.5\n",
      "fc layer 1 self.abs_max_out: 8781.0\n",
      "lif layer 2 self.abs_max_v: 5475.0\n",
      "fc layer 3 self.abs_max_out: 493.0\n",
      "lif layer 1 self.abs_max_v: 12844.5\n",
      "fc layer 3 self.abs_max_out: 497.0\n",
      "fc layer 3 self.abs_max_out: 518.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.922120/  1.977269, val:  79.58%, val_best:  79.58%, tr:  99.86%, tr_best:  99.91%, epoch time: 279.46 seconds, 4.66 minutes\n",
      "total_backward_count 443600 real_backward_count 64786  14.605%\n",
      "fc layer 1 self.abs_max_out: 8821.0\n",
      "fc layer 3 self.abs_max_out: 531.0\n",
      "lif layer 2 self.abs_max_v: 5480.5\n",
      "lif layer 2 self.abs_max_v: 5637.5\n",
      "lif layer 2 self.abs_max_v: 5656.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.898947/  1.989181, val:  75.00%, val_best:  79.58%, tr:  99.95%, tr_best:  99.95%, epoch time: 277.38 seconds, 4.62 minutes\n",
      "total_backward_count 487960 real_backward_count 69522  14.247%\n",
      "fc layer 1 self.abs_max_out: 9032.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.900595/  1.966982, val:  79.17%, val_best:  79.58%, tr:  99.91%, tr_best:  99.95%, epoch time: 276.64 seconds, 4.61 minutes\n",
      "total_backward_count 532320 real_backward_count 74117  13.923%\n",
      "fc layer 1 self.abs_max_out: 9073.0\n",
      "fc layer 2 self.abs_max_out: 3677.0\n",
      "lif layer 1 self.abs_max_v: 12881.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.893081/  1.966171, val:  76.25%, val_best:  79.58%, tr:  99.98%, tr_best:  99.98%, epoch time: 278.90 seconds, 4.65 minutes\n",
      "total_backward_count 576680 real_backward_count 78548  13.621%\n",
      "fc layer 3 self.abs_max_out: 533.0\n",
      "fc layer 1 self.abs_max_out: 9252.0\n",
      "fc layer 3 self.abs_max_out: 550.0\n",
      "fc layer 2 self.abs_max_out: 3820.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.884568/  1.977939, val:  78.75%, val_best:  79.58%, tr:  99.95%, tr_best:  99.98%, epoch time: 279.78 seconds, 4.66 minutes\n",
      "total_backward_count 621040 real_backward_count 82930  13.353%\n",
      "lif layer 1 self.abs_max_v: 13047.0\n",
      "fc layer 1 self.abs_max_out: 9301.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.871212/  1.970656, val:  82.50%, val_best:  82.50%, tr:  99.98%, tr_best:  99.98%, epoch time: 279.42 seconds, 4.66 minutes\n",
      "total_backward_count 665400 real_backward_count 87125  13.094%\n",
      "lif layer 1 self.abs_max_v: 13073.5\n",
      "lif layer 1 self.abs_max_v: 13423.0\n",
      "fc layer 3 self.abs_max_out: 575.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.864734/  1.961928, val:  64.17%, val_best:  82.50%, tr:  99.98%, tr_best:  99.98%, epoch time: 280.35 seconds, 4.67 minutes\n",
      "total_backward_count 709760 real_backward_count 91337  12.869%\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.855969/  1.949201, val:  73.75%, val_best:  82.50%, tr:  99.98%, tr_best:  99.98%, epoch time: 279.70 seconds, 4.66 minutes\n",
      "total_backward_count 754120 real_backward_count 95228  12.628%\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.858617/  1.937412, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.81 seconds, 4.68 minutes\n",
      "total_backward_count 798480 real_backward_count 99074  12.408%\n",
      "fc layer 1 self.abs_max_out: 9397.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.860147/  1.953954, val:  81.25%, val_best:  84.17%, tr:  99.95%, tr_best: 100.00%, epoch time: 277.38 seconds, 4.62 minutes\n",
      "total_backward_count 842840 real_backward_count 102900  12.209%\n",
      "fc layer 1 self.abs_max_out: 9420.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.848562/  1.942768, val:  84.58%, val_best:  84.58%, tr:  99.98%, tr_best: 100.00%, epoch time: 277.12 seconds, 4.62 minutes\n",
      "total_backward_count 887200 real_backward_count 106522  12.007%\n",
      "fc layer 1 self.abs_max_out: 9469.0\n",
      "lif layer 2 self.abs_max_v: 5661.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.846170/  1.936963, val:  79.58%, val_best:  84.58%, tr:  99.98%, tr_best: 100.00%, epoch time: 276.98 seconds, 4.62 minutes\n",
      "total_backward_count 931560 real_backward_count 110180  11.827%\n",
      "fc layer 1 self.abs_max_out: 9517.0\n",
      "fc layer 3 self.abs_max_out: 580.0\n",
      "fc layer 2 self.abs_max_out: 3943.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.836073/  1.926583, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.51 seconds, 4.69 minutes\n",
      "total_backward_count 975920 real_backward_count 113676  11.648%\n",
      "fc layer 3 self.abs_max_out: 603.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.838519/  1.931339, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.46 seconds, 4.64 minutes\n",
      "total_backward_count 1020280 real_backward_count 117149  11.482%\n",
      "fc layer 2 self.abs_max_out: 4097.0\n",
      "fc layer 1 self.abs_max_out: 9609.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.837137/  1.929126, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.46 seconds, 4.66 minutes\n",
      "total_backward_count 1064640 real_backward_count 120426  11.311%\n",
      "fc layer 1 self.abs_max_out: 9616.0\n",
      "fc layer 2 self.abs_max_out: 4116.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.833978/  1.922409, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.25 seconds, 4.67 minutes\n",
      "total_backward_count 1109000 real_backward_count 123598  11.145%\n",
      "fc layer 1 self.abs_max_out: 9643.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.839219/  1.925061, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.74 seconds, 4.70 minutes\n",
      "total_backward_count 1153360 real_backward_count 126781  10.992%\n",
      "fc layer 1 self.abs_max_out: 9651.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.826483/  1.910555, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.27 seconds, 4.67 minutes\n",
      "total_backward_count 1197720 real_backward_count 130007  10.855%\n",
      "fc layer 1 self.abs_max_out: 9759.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.819725/  1.916798, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.27 seconds, 4.65 minutes\n",
      "total_backward_count 1242080 real_backward_count 133148  10.720%\n",
      "lif layer 2 self.abs_max_v: 5751.0\n",
      "fc layer 1 self.abs_max_out: 9796.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.811089/  1.908370, val:  78.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.41 seconds, 4.66 minutes\n",
      "total_backward_count 1286440 real_backward_count 136204  10.588%\n",
      "lif layer 2 self.abs_max_v: 5761.5\n",
      "fc layer 1 self.abs_max_out: 9855.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.803262/  1.890962, val:  87.92%, val_best:  88.33%, tr:  99.98%, tr_best: 100.00%, epoch time: 279.22 seconds, 4.65 minutes\n",
      "total_backward_count 1330800 real_backward_count 139023  10.447%\n",
      "fc layer 1 self.abs_max_out: 9860.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.786923/  1.883530, val:  82.92%, val_best:  88.33%, tr:  99.98%, tr_best: 100.00%, epoch time: 278.28 seconds, 4.64 minutes\n",
      "total_backward_count 1375160 real_backward_count 141752  10.308%\n",
      "fc layer 3 self.abs_max_out: 639.0\n",
      "fc layer 1 self.abs_max_out: 9888.0\n",
      "fc layer 3 self.abs_max_out: 675.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.774367/  1.875406, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.38 seconds, 4.59 minutes\n",
      "total_backward_count 1419520 real_backward_count 144585  10.185%\n",
      "fc layer 1 self.abs_max_out: 9955.0\n",
      "fc layer 1 self.abs_max_out: 9971.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.770537/  1.876729, val:  83.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.00 seconds, 4.68 minutes\n",
      "total_backward_count 1463880 real_backward_count 147275  10.061%\n",
      "fc layer 1 self.abs_max_out: 10038.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.761777/  1.872357, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.80 seconds, 4.70 minutes\n",
      "total_backward_count 1508240 real_backward_count 149930   9.941%\n",
      "fc layer 1 self.abs_max_out: 10083.0\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.770223/  1.878240, val:  82.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.79 seconds, 4.61 minutes\n",
      "total_backward_count 1552600 real_backward_count 152467   9.820%\n",
      "fc layer 1 self.abs_max_out: 10113.0\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.770015/  1.869744, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.16 seconds, 4.65 minutes\n",
      "total_backward_count 1596960 real_backward_count 155039   9.708%\n",
      "fc layer 1 self.abs_max_out: 10162.0\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.765922/  1.858069, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.16 seconds, 4.67 minutes\n",
      "total_backward_count 1641320 real_backward_count 157502   9.596%\n",
      "fc layer 1 self.abs_max_out: 10226.0\n",
      "fc layer 2 self.abs_max_out: 4152.0\n",
      "lif layer 1 self.abs_max_v: 13555.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.756693/  1.853813, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.76 seconds, 4.68 minutes\n",
      "total_backward_count 1685680 real_backward_count 159931   9.488%\n",
      "fc layer 1 self.abs_max_out: 10229.0\n",
      "fc layer 1 self.abs_max_out: 10299.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.751327/  1.862997, val:  88.75%, val_best:  89.17%, tr:  99.98%, tr_best: 100.00%, epoch time: 280.76 seconds, 4.68 minutes\n",
      "total_backward_count 1730040 real_backward_count 162310   9.382%\n",
      "fc layer 1 self.abs_max_out: 10319.0\n",
      "lif layer 1 self.abs_max_v: 13580.0\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.754338/  1.864782, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.02 seconds, 4.65 minutes\n",
      "total_backward_count 1774400 real_backward_count 164719   9.283%\n",
      "fc layer 2 self.abs_max_out: 4264.0\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.741476/  1.853057, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.32 seconds, 4.69 minutes\n",
      "total_backward_count 1818760 real_backward_count 166948   9.179%\n",
      "fc layer 1 self.abs_max_out: 10320.0\n",
      "lif layer 1 self.abs_max_v: 13715.0\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.738414/  1.840009, val:  83.33%, val_best:  90.83%, tr:  99.98%, tr_best: 100.00%, epoch time: 276.93 seconds, 4.62 minutes\n",
      "total_backward_count 1863120 real_backward_count 169205   9.082%\n",
      "fc layer 1 self.abs_max_out: 10355.0\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.727567/  1.836399, val:  83.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.13 seconds, 4.64 minutes\n",
      "total_backward_count 1907480 real_backward_count 171422   8.987%\n",
      "fc layer 1 self.abs_max_out: 10357.0\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.723694/  1.836086, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.13 seconds, 4.60 minutes\n",
      "total_backward_count 1951840 real_backward_count 173497   8.889%\n",
      "fc layer 1 self.abs_max_out: 10393.0\n",
      "lif layer 1 self.abs_max_v: 13819.0\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.724517/  1.830289, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.64 seconds, 4.66 minutes\n",
      "total_backward_count 1996200 real_backward_count 175643   8.799%\n",
      "fc layer 2 self.abs_max_out: 4300.0\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.713493/  1.826541, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.09 seconds, 4.67 minutes\n",
      "total_backward_count 2040560 real_backward_count 177690   8.708%\n",
      "fc layer 1 self.abs_max_out: 10432.0\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.721423/  1.826824, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.99 seconds, 4.62 minutes\n",
      "total_backward_count 2084920 real_backward_count 179752   8.622%\n",
      "fc layer 1 self.abs_max_out: 10446.0\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.722115/  1.820701, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.61 seconds, 4.66 minutes\n",
      "total_backward_count 2129280 real_backward_count 181757   8.536%\n",
      "fc layer 1 self.abs_max_out: 10477.0\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.712781/  1.839152, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.09 seconds, 4.67 minutes\n",
      "total_backward_count 2173640 real_backward_count 183671   8.450%\n",
      "fc layer 1 self.abs_max_out: 10520.0\n",
      "fc layer 2 self.abs_max_out: 4319.0\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.713199/  1.822334, val:  88.75%, val_best:  90.83%, tr:  99.98%, tr_best: 100.00%, epoch time: 280.99 seconds, 4.68 minutes\n",
      "total_backward_count 2218000 real_backward_count 185582   8.367%\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.710120/  1.810880, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.74 seconds, 4.66 minutes\n",
      "total_backward_count 2262360 real_backward_count 187491   8.287%\n",
      "fc layer 1 self.abs_max_out: 10555.0\n",
      "lif layer 1 self.abs_max_v: 13989.0\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.703140/  1.809799, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.11 seconds, 4.69 minutes\n",
      "total_backward_count 2306720 real_backward_count 189428   8.212%\n",
      "fc layer 1 self.abs_max_out: 10589.0\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.698336/  1.807946, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.82 seconds, 4.65 minutes\n",
      "total_backward_count 2351080 real_backward_count 191350   8.139%\n",
      "fc layer 1 self.abs_max_out: 10617.0\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.694426/  1.814312, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.80 seconds, 4.65 minutes\n",
      "total_backward_count 2395440 real_backward_count 193157   8.064%\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.694005/  1.827146, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.04 seconds, 4.63 minutes\n",
      "total_backward_count 2439800 real_backward_count 194879   7.987%\n",
      "fc layer 1 self.abs_max_out: 10622.0\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.698138/  1.822806, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.19 seconds, 4.64 minutes\n",
      "total_backward_count 2484160 real_backward_count 196629   7.915%\n",
      "fc layer 1 self.abs_max_out: 10643.0\n",
      "lif layer 1 self.abs_max_v: 14255.0\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.693745/  1.815242, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.84 seconds, 4.66 minutes\n",
      "total_backward_count 2528520 real_backward_count 198250   7.841%\n",
      "fc layer 1 self.abs_max_out: 10652.0\n",
      "lif layer 1 self.abs_max_v: 14376.0\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.698873/  1.812699, val:  84.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.33 seconds, 4.66 minutes\n",
      "total_backward_count 2572880 real_backward_count 199963   7.772%\n",
      "lif layer 1 self.abs_max_v: 14569.5\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.691929/  1.804475, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.40 seconds, 4.64 minutes\n",
      "total_backward_count 2617240 real_backward_count 201696   7.706%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.690280/  1.812995, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.59 seconds, 4.66 minutes\n",
      "total_backward_count 2661600 real_backward_count 203351   7.640%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.690126/  1.815321, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.31 seconds, 4.69 minutes\n",
      "total_backward_count 2705960 real_backward_count 204958   7.574%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.677608/  1.789612, val:  85.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.47 seconds, 4.66 minutes\n",
      "total_backward_count 2750320 real_backward_count 206512   7.509%\n",
      "fc layer 3 self.abs_max_out: 692.0\n",
      "fc layer 2 self.abs_max_out: 4350.0\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.667647/  1.785081, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.56 seconds, 4.66 minutes\n",
      "total_backward_count 2794680 real_backward_count 208119   7.447%\n",
      "fc layer 1 self.abs_max_out: 10673.0\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.665774/  1.789263, val:  83.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.13 seconds, 4.67 minutes\n",
      "total_backward_count 2839040 real_backward_count 209704   7.386%\n",
      "fc layer 1 self.abs_max_out: 10674.0\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.658696/  1.780479, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.45 seconds, 4.62 minutes\n",
      "total_backward_count 2883400 real_backward_count 211184   7.324%\n",
      "fc layer 1 self.abs_max_out: 10682.0\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.667779/  1.797255, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.23 seconds, 4.67 minutes\n",
      "total_backward_count 2927760 real_backward_count 212682   7.264%\n",
      "fc layer 2 self.abs_max_out: 4388.0\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.663972/  1.785701, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.31 seconds, 4.57 minutes\n",
      "total_backward_count 2972120 real_backward_count 214180   7.206%\n",
      "fc layer 1 self.abs_max_out: 10734.0\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.656937/  1.784133, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.81 seconds, 4.66 minutes\n",
      "total_backward_count 3016480 real_backward_count 215634   7.149%\n",
      "fc layer 1 self.abs_max_out: 10766.0\n",
      "fc layer 2 self.abs_max_out: 4439.0\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.669242/  1.776847, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.19 seconds, 4.67 minutes\n",
      "total_backward_count 3060840 real_backward_count 217101   7.093%\n",
      "fc layer 2 self.abs_max_out: 4443.0\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.658696/  1.777308, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.61 seconds, 4.64 minutes\n",
      "total_backward_count 3105200 real_backward_count 218519   7.037%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.659217/  1.784779, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.06 seconds, 4.62 minutes\n",
      "total_backward_count 3149560 real_backward_count 219901   6.982%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.654794/  1.787677, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.60 seconds, 4.68 minutes\n",
      "total_backward_count 3193920 real_backward_count 221310   6.929%\n",
      "fc layer 2 self.abs_max_out: 4468.0\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.661281/  1.783715, val:  85.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.68 seconds, 4.64 minutes\n",
      "total_backward_count 3238280 real_backward_count 222741   6.878%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.649297/  1.773162, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.51 seconds, 4.68 minutes\n",
      "total_backward_count 3282640 real_backward_count 224074   6.826%\n",
      "lif layer 1 self.abs_max_v: 14643.5\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.637966/  1.776746, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.10 seconds, 4.68 minutes\n",
      "total_backward_count 3327000 real_backward_count 225409   6.775%\n",
      "lif layer 1 self.abs_max_v: 14710.5\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.644960/  1.773565, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.80 seconds, 4.66 minutes\n",
      "total_backward_count 3371360 real_backward_count 226729   6.725%\n",
      "fc layer 3 self.abs_max_out: 700.0\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.633654/  1.768036, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.96 seconds, 4.65 minutes\n",
      "total_backward_count 3415720 real_backward_count 227938   6.673%\n",
      "fc layer 3 self.abs_max_out: 711.0\n",
      "fc layer 1 self.abs_max_out: 10791.0\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.628082/  1.760674, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.60 seconds, 4.66 minutes\n",
      "total_backward_count 3460080 real_backward_count 229216   6.625%\n",
      "fc layer 1 self.abs_max_out: 10793.0\n",
      "fc layer 2 self.abs_max_out: 4487.0\n",
      "fc layer 2 self.abs_max_out: 4501.0\n",
      "fc layer 3 self.abs_max_out: 714.0\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.622396/  1.756843, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.33 seconds, 4.61 minutes\n",
      "total_backward_count 3504440 real_backward_count 230422   6.575%\n",
      "fc layer 2 self.abs_max_out: 4578.0\n",
      "fc layer 1 self.abs_max_out: 10824.0\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.622196/  1.741012, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.23 seconds, 4.67 minutes\n",
      "total_backward_count 3548800 real_backward_count 231637   6.527%\n",
      "fc layer 1 self.abs_max_out: 10832.0\n",
      "fc layer 1 self.abs_max_out: 10862.0\n",
      "fc layer 2 self.abs_max_out: 4623.0\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.624074/  1.760007, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.41 seconds, 4.66 minutes\n",
      "total_backward_count 3593160 real_backward_count 232906   6.482%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.623601/  1.760687, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.16 seconds, 4.62 minutes\n",
      "total_backward_count 3637520 real_backward_count 234057   6.435%\n",
      "fc layer 1 self.abs_max_out: 10880.0\n",
      "fc layer 2 self.abs_max_out: 4634.0\n",
      "fc layer 1 self.abs_max_out: 10900.0\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.618582/  1.751116, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.97 seconds, 4.65 minutes\n",
      "total_backward_count 3681880 real_backward_count 235243   6.389%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.626388/  1.763442, val:  85.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.71 seconds, 4.65 minutes\n",
      "total_backward_count 3726240 real_backward_count 236459   6.346%\n",
      "fc layer 2 self.abs_max_out: 4721.0\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.632039/  1.763116, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.01 seconds, 4.67 minutes\n",
      "total_backward_count 3770600 real_backward_count 237616   6.302%\n",
      "fc layer 1 self.abs_max_out: 10908.0\n",
      "lif layer 1 self.abs_max_v: 14770.5\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.622557/  1.759126, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.85 seconds, 4.66 minutes\n",
      "total_backward_count 3814960 real_backward_count 238788   6.259%\n",
      "fc layer 3 self.abs_max_out: 720.0\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.613925/  1.748555, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.92 seconds, 4.70 minutes\n",
      "total_backward_count 3859320 real_backward_count 239916   6.217%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.603914/  1.737981, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.89 seconds, 4.63 minutes\n",
      "total_backward_count 3903680 real_backward_count 241042   6.175%\n",
      "fc layer 3 self.abs_max_out: 728.0\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.598990/  1.743056, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.26 seconds, 4.67 minutes\n",
      "total_backward_count 3948040 real_backward_count 242130   6.133%\n",
      "fc layer 3 self.abs_max_out: 731.0\n",
      "fc layer 3 self.abs_max_out: 769.0\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.600291/  1.756830, val:  86.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.57 seconds, 4.59 minutes\n",
      "total_backward_count 3992400 real_backward_count 243274   6.093%\n",
      "fc layer 1 self.abs_max_out: 10920.0\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.598625/  1.738180, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.96 seconds, 4.65 minutes\n",
      "total_backward_count 4036760 real_backward_count 244424   6.055%\n",
      "fc layer 1 self.abs_max_out: 10931.0\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.594395/  1.742866, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 283.28 seconds, 4.72 minutes\n",
      "total_backward_count 4081120 real_backward_count 245505   6.016%\n",
      "fc layer 2 self.abs_max_out: 4780.0\n",
      "fc layer 1 self.abs_max_out: 10985.0\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.594953/  1.733918, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.96 seconds, 4.68 minutes\n",
      "total_backward_count 4125480 real_backward_count 246635   5.978%\n",
      "fc layer 2 self.abs_max_out: 4784.0\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.590140/  1.747262, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.76 seconds, 4.61 minutes\n",
      "total_backward_count 4169840 real_backward_count 247699   5.940%\n",
      "fc layer 3 self.abs_max_out: 771.0\n",
      "lif layer 1 self.abs_max_v: 14771.0\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.593802/  1.733683, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.74 seconds, 4.70 minutes\n",
      "total_backward_count 4214200 real_backward_count 248745   5.903%\n",
      "fc layer 2 self.abs_max_out: 4819.0\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.587148/  1.723009, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 282.82 seconds, 4.71 minutes\n",
      "total_backward_count 4258560 real_backward_count 249747   5.865%\n",
      "lif layer 1 self.abs_max_v: 14897.0\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.582192/  1.739214, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.10 seconds, 4.69 minutes\n",
      "total_backward_count 4302920 real_backward_count 250787   5.828%\n",
      "fc layer 1 self.abs_max_out: 10994.0\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.585891/  1.730345, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.57 seconds, 4.69 minutes\n",
      "total_backward_count 4347280 real_backward_count 251818   5.793%\n",
      "fc layer 1 self.abs_max_out: 10995.0\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.578318/  1.727595, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.63 seconds, 4.68 minutes\n",
      "total_backward_count 4391640 real_backward_count 252815   5.757%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.586748/  1.730101, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.80 seconds, 4.63 minutes\n",
      "total_backward_count 4436000 real_backward_count 253815   5.722%\n",
      "fc layer 2 self.abs_max_out: 4828.0\n",
      "lif layer 1 self.abs_max_v: 15094.5\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.590377/  1.732792, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.43 seconds, 4.67 minutes\n",
      "total_backward_count 4480360 real_backward_count 254776   5.687%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.589819/  1.727440, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.71 seconds, 4.56 minutes\n",
      "total_backward_count 4524720 real_backward_count 255741   5.652%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.588251/  1.729341, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.78 seconds, 4.65 minutes\n",
      "total_backward_count 4569080 real_backward_count 256682   5.618%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.586560/  1.734708, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.03 seconds, 4.67 minutes\n",
      "total_backward_count 4613440 real_backward_count 257644   5.585%\n",
      "fc layer 2 self.abs_max_out: 4833.0\n",
      "lif layer 1 self.abs_max_v: 15156.0\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.591105/  1.729484, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.66 seconds, 4.64 minutes\n",
      "total_backward_count 4657800 real_backward_count 258665   5.553%\n",
      "lif layer 1 self.abs_max_v: 15189.0\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.583241/  1.726246, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.23 seconds, 4.62 minutes\n",
      "total_backward_count 4702160 real_backward_count 259584   5.521%\n",
      "lif layer 1 self.abs_max_v: 15216.5\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.570402/  1.709267, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.81 seconds, 4.65 minutes\n",
      "total_backward_count 4746520 real_backward_count 260525   5.489%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.566958/  1.712676, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.87 seconds, 4.65 minutes\n",
      "total_backward_count 4790880 real_backward_count 261445   5.457%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.564321/  1.712970, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.36 seconds, 4.67 minutes\n",
      "total_backward_count 4835240 real_backward_count 262348   5.426%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.564210/  1.706873, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.07 seconds, 4.67 minutes\n",
      "total_backward_count 4879600 real_backward_count 263211   5.394%\n",
      "fc layer 2 self.abs_max_out: 4859.0\n",
      "fc layer 3 self.abs_max_out: 790.0\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.564132/  1.707407, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.94 seconds, 4.63 minutes\n",
      "total_backward_count 4923960 real_backward_count 264162   5.365%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.562110/  1.701959, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.67 seconds, 4.64 minutes\n",
      "total_backward_count 4968320 real_backward_count 265016   5.334%\n",
      "fc layer 2 self.abs_max_out: 4860.0\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.567129/  1.721879, val:  87.08%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.26 seconds, 4.64 minutes\n",
      "total_backward_count 5012680 real_backward_count 265891   5.304%\n",
      "fc layer 3 self.abs_max_out: 801.0\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.565602/  1.716444, val:  87.08%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.92 seconds, 4.62 minutes\n",
      "total_backward_count 5057040 real_backward_count 266788   5.276%\n",
      "fc layer 2 self.abs_max_out: 4902.0\n",
      "fc layer 2 self.abs_max_out: 5024.0\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.566657/  1.710480, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.07 seconds, 4.67 minutes\n",
      "total_backward_count 5101400 real_backward_count 267700   5.248%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.562552/  1.706438, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.65 seconds, 4.68 minutes\n",
      "total_backward_count 5145760 real_backward_count 268588   5.220%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.560581/  1.696250, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.20 seconds, 4.64 minutes\n",
      "total_backward_count 5190120 real_backward_count 269453   5.192%\n",
      "fc layer 3 self.abs_max_out: 850.0\n",
      "lif layer 1 self.abs_max_v: 15246.0\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.556036/  1.704159, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.21 seconds, 4.64 minutes\n",
      "total_backward_count 5234480 real_backward_count 270303   5.164%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.547990/  1.693787, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.02 seconds, 4.67 minutes\n",
      "total_backward_count 5278840 real_backward_count 271215   5.138%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.540730/  1.690580, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.99 seconds, 4.65 minutes\n",
      "total_backward_count 5323200 real_backward_count 272086   5.111%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.533447/  1.681929, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.76 seconds, 4.70 minutes\n",
      "total_backward_count 5367560 real_backward_count 272922   5.085%\n",
      "lif layer 1 self.abs_max_v: 15308.0\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.529140/  1.680465, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.94 seconds, 4.65 minutes\n",
      "total_backward_count 5411920 real_backward_count 273672   5.057%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.529802/  1.686850, val:  87.08%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.24 seconds, 4.62 minutes\n",
      "total_backward_count 5456280 real_backward_count 274460   5.030%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.533187/  1.694971, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 282.20 seconds, 4.70 minutes\n",
      "total_backward_count 5500640 real_backward_count 275274   5.004%\n",
      "fc layer 2 self.abs_max_out: 5061.0\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.532252/  1.692042, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.32 seconds, 4.57 minutes\n",
      "total_backward_count 5545000 real_backward_count 276094   4.979%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.535591/  1.685203, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.48 seconds, 4.67 minutes\n",
      "total_backward_count 5589360 real_backward_count 276918   4.954%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.530412/  1.682765, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.70 seconds, 4.64 minutes\n",
      "total_backward_count 5633720 real_backward_count 277692   4.929%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.519489/  1.663878, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.12 seconds, 4.67 minutes\n",
      "total_backward_count 5678080 real_backward_count 278429   4.904%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.504091/  1.666491, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.09 seconds, 4.60 minutes\n",
      "total_backward_count 5722440 real_backward_count 279199   4.879%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.507836/  1.666903, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.68 seconds, 4.63 minutes\n",
      "total_backward_count 5766800 real_backward_count 279923   4.854%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.507356/  1.664858, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.99 seconds, 4.68 minutes\n",
      "total_backward_count 5811160 real_backward_count 280706   4.830%\n",
      "lif layer 1 self.abs_max_v: 15390.5\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.503406/  1.667946, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.93 seconds, 4.68 minutes\n",
      "total_backward_count 5855520 real_backward_count 281394   4.806%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.512557/  1.661948, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.79 seconds, 4.66 minutes\n",
      "total_backward_count 5899880 real_backward_count 282202   4.783%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.506194/  1.666817, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.84 seconds, 4.63 minutes\n",
      "total_backward_count 5944240 real_backward_count 282920   4.760%\n",
      "fc layer 3 self.abs_max_out: 853.0\n",
      "fc layer 3 self.abs_max_out: 879.0\n",
      "lif layer 1 self.abs_max_v: 15454.5\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.507990/  1.664757, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.70 seconds, 4.66 minutes\n",
      "total_backward_count 5988600 real_backward_count 283619   4.736%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.513886/  1.670274, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.28 seconds, 4.65 minutes\n",
      "total_backward_count 6032960 real_backward_count 284315   4.713%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.516740/  1.666844, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.72 seconds, 4.58 minutes\n",
      "total_backward_count 6077320 real_backward_count 284998   4.690%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.512292/  1.673996, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.74 seconds, 4.65 minutes\n",
      "total_backward_count 6121680 real_backward_count 285747   4.668%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.510907/  1.669370, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.37 seconds, 4.69 minutes\n",
      "total_backward_count 6166040 real_backward_count 286499   4.646%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.503343/  1.666958, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.04 seconds, 4.68 minutes\n",
      "total_backward_count 6210400 real_backward_count 287241   4.625%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.497179/  1.658417, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.79 seconds, 4.60 minutes\n",
      "total_backward_count 6254760 real_backward_count 287980   4.604%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.506952/  1.663483, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 282.34 seconds, 4.71 minutes\n",
      "total_backward_count 6299120 real_backward_count 288663   4.583%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.510698/  1.675825, val:  87.92%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.24 seconds, 4.69 minutes\n",
      "total_backward_count 6343480 real_backward_count 289381   4.562%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.507030/  1.662387, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.77 seconds, 4.70 minutes\n",
      "total_backward_count 6387840 real_backward_count 290074   4.541%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.502061/  1.660995, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.70 seconds, 4.66 minutes\n",
      "total_backward_count 6432200 real_backward_count 290811   4.521%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.497750/  1.665110, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.02 seconds, 4.62 minutes\n",
      "total_backward_count 6476560 real_backward_count 291438   4.500%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.502586/  1.664636, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.08 seconds, 4.68 minutes\n",
      "total_backward_count 6520920 real_backward_count 292076   4.479%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.502776/  1.659868, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.43 seconds, 4.64 minutes\n",
      "total_backward_count 6565280 real_backward_count 292766   4.459%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.495441/  1.658103, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.50 seconds, 4.61 minutes\n",
      "total_backward_count 6609640 real_backward_count 293436   4.440%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.491908/  1.646174, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.21 seconds, 4.69 minutes\n",
      "total_backward_count 6654000 real_backward_count 294108   4.420%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.489753/  1.652773, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.42 seconds, 4.67 minutes\n",
      "total_backward_count 6698360 real_backward_count 294818   4.401%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.485686/  1.650502, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.41 seconds, 4.66 minutes\n",
      "total_backward_count 6742720 real_backward_count 295455   4.382%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.484078/  1.662097, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.24 seconds, 4.60 minutes\n",
      "total_backward_count 6787080 real_backward_count 296101   4.363%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.496859/  1.652693, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 282.05 seconds, 4.70 minutes\n",
      "total_backward_count 6831440 real_backward_count 296728   4.344%\n",
      "fc layer 3 self.abs_max_out: 891.0\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.486170/  1.655961, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.69 seconds, 4.66 minutes\n",
      "total_backward_count 6875800 real_backward_count 297410   4.325%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.485309/  1.656219, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.88 seconds, 4.66 minutes\n",
      "total_backward_count 6920160 real_backward_count 298048   4.307%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.484536/  1.647846, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.59 seconds, 4.63 minutes\n",
      "total_backward_count 6964520 real_backward_count 298661   4.288%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.477348/  1.639410, val:  85.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.17 seconds, 4.62 minutes\n",
      "total_backward_count 7008880 real_backward_count 299329   4.271%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.476552/  1.642819, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.61 seconds, 4.69 minutes\n",
      "total_backward_count 7053240 real_backward_count 299925   4.252%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.485919/  1.650052, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.67 seconds, 4.56 minutes\n",
      "total_backward_count 7097600 real_backward_count 300575   4.235%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.483352/  1.651836, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.09 seconds, 4.65 minutes\n",
      "total_backward_count 7141960 real_backward_count 301245   4.218%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.477574/  1.646955, val:  88.33%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.46 seconds, 4.66 minutes\n",
      "total_backward_count 7186320 real_backward_count 301856   4.200%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.479011/  1.653079, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.73 seconds, 4.68 minutes\n",
      "total_backward_count 7230680 real_backward_count 302399   4.182%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.480907/  1.639857, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.31 seconds, 4.66 minutes\n",
      "total_backward_count 7275040 real_backward_count 303045   4.166%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.473535/  1.631569, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.67 seconds, 4.64 minutes\n",
      "total_backward_count 7319400 real_backward_count 303663   4.149%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.470161/  1.645444, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.27 seconds, 4.67 minutes\n",
      "total_backward_count 7363760 real_backward_count 304291   4.132%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.470129/  1.638449, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.91 seconds, 4.65 minutes\n",
      "total_backward_count 7408120 real_backward_count 304897   4.116%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.463805/  1.635000, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.23 seconds, 4.69 minutes\n",
      "total_backward_count 7452480 real_backward_count 305555   4.100%\n",
      "fc layer 3 self.abs_max_out: 895.0\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.460927/  1.634837, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.39 seconds, 4.61 minutes\n",
      "total_backward_count 7496840 real_backward_count 306170   4.084%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.463502/  1.629191, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.94 seconds, 4.67 minutes\n",
      "total_backward_count 7541200 real_backward_count 306796   4.068%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.467240/  1.634842, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.14 seconds, 4.64 minutes\n",
      "total_backward_count 7585560 real_backward_count 307347   4.052%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.454542/  1.637574, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.45 seconds, 4.61 minutes\n",
      "total_backward_count 7629920 real_backward_count 307938   4.036%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.459865/  1.634656, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.88 seconds, 4.68 minutes\n",
      "total_backward_count 7674280 real_backward_count 308474   4.020%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.452933/  1.632419, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.09 seconds, 4.65 minutes\n",
      "total_backward_count 7718640 real_backward_count 309045   4.004%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.451825/  1.630611, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.22 seconds, 4.67 minutes\n",
      "total_backward_count 7763000 real_backward_count 309636   3.989%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.453237/  1.639522, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.61 seconds, 4.61 minutes\n",
      "total_backward_count 7807360 real_backward_count 310245   3.974%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.452323/  1.623666, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.67 seconds, 4.66 minutes\n",
      "total_backward_count 7851720 real_backward_count 310787   3.958%\n",
      "fc layer 3 self.abs_max_out: 901.0\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.448795/  1.639662, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.29 seconds, 4.65 minutes\n",
      "total_backward_count 7896080 real_backward_count 311330   3.943%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.446085/  1.615233, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.28 seconds, 4.67 minutes\n",
      "total_backward_count 7940440 real_backward_count 311867   3.928%\n",
      "lif layer 1 self.abs_max_v: 15604.5\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.440110/  1.623662, val:  87.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.65 seconds, 4.66 minutes\n",
      "total_backward_count 7984800 real_backward_count 312392   3.912%\n",
      "lif layer 1 self.abs_max_v: 15713.5\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.443619/  1.621427, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.37 seconds, 4.64 minutes\n",
      "total_backward_count 8029160 real_backward_count 312890   3.897%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.444782/  1.623489, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.60 seconds, 4.66 minutes\n",
      "total_backward_count 8073520 real_backward_count 313379   3.882%\n",
      "fc layer 2 self.abs_max_out: 5126.0\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.442344/  1.620981, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.34 seconds, 4.61 minutes\n",
      "total_backward_count 8117880 real_backward_count 313867   3.866%\n",
      "fc layer 3 self.abs_max_out: 919.0\n",
      "lif layer 1 self.abs_max_v: 15749.5\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.445673/  1.624779, val:  87.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.07 seconds, 4.65 minutes\n",
      "total_backward_count 8162240 real_backward_count 314405   3.852%\n",
      "lif layer 1 self.abs_max_v: 15753.5\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.448224/  1.626848, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.04 seconds, 4.65 minutes\n",
      "total_backward_count 8206600 real_backward_count 314915   3.837%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.444075/  1.620831, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.76 seconds, 4.68 minutes\n",
      "total_backward_count 8250960 real_backward_count 315460   3.823%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.438527/  1.628308, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.51 seconds, 4.66 minutes\n",
      "total_backward_count 8295320 real_backward_count 316029   3.810%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.444297/  1.617091, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.58 seconds, 4.59 minutes\n",
      "total_backward_count 8339680 real_backward_count 316582   3.796%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.439454/  1.618715, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.94 seconds, 4.67 minutes\n",
      "total_backward_count 8384040 real_backward_count 317173   3.783%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.432305/  1.619818, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.91 seconds, 4.68 minutes\n",
      "total_backward_count 8428400 real_backward_count 317699   3.769%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.433363/  1.611361, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.91 seconds, 4.68 minutes\n",
      "total_backward_count 8472760 real_backward_count 318246   3.756%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.438686/  1.607201, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.58 seconds, 4.59 minutes\n",
      "total_backward_count 8517120 real_backward_count 318785   3.743%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.431957/  1.615048, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.46 seconds, 4.66 minutes\n",
      "total_backward_count 8561480 real_backward_count 319314   3.730%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.432801/  1.609048, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.42 seconds, 4.67 minutes\n",
      "total_backward_count 8605840 real_backward_count 319818   3.716%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.421480/  1.602328, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.70 seconds, 4.60 minutes\n",
      "total_backward_count 8650200 real_backward_count 320343   3.703%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.423143/  1.601589, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.98 seconds, 4.70 minutes\n",
      "total_backward_count 8694560 real_backward_count 320820   3.690%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.424194/  1.604661, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.72 seconds, 4.68 minutes\n",
      "total_backward_count 8738920 real_backward_count 321314   3.677%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.430833/  1.620025, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.25 seconds, 4.69 minutes\n",
      "total_backward_count 8783280 real_backward_count 321861   3.664%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.434956/  1.614692, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.14 seconds, 4.65 minutes\n",
      "total_backward_count 8827640 real_backward_count 322387   3.652%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.425861/  1.604056, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.94 seconds, 4.65 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d202ec09db6147699c59506b585b1fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>summary_val_acc</td><td>▁▃▆▂▇▆▇▇▆█▇▇▇█▇███▇▇▇██████████▇████▇▇█▇</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▇▇▆▆▆▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▅▆▇▇▇▇████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▆▂▇▆▇▇▆█▇▇▇█▇███▇▇▇██████████▇████▇▇█▇</td></tr><tr><td>val_loss</td><td>█▇▆▆▆▆▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.42586</td></tr><tr><td>val_acc_best</td><td>0.925</td></tr><tr><td>val_acc_now</td><td>0.89167</td></tr><tr><td>val_loss</td><td>1.60406</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">daily-sweep-66</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/howkn0u5' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/howkn0u5</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250906_154430-howkn0u5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bhx8ors0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 37078\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.21.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250907_071641-bhx8ors0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bhx8ors0' target=\"_blank\">splendid-sweep-68</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/jwru0k4o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/jwru0k4o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/jwru0k4o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/jwru0k4o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bhx8ors0' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bhx8ors0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20250907_071650_202', 'my_seed': 37078, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0df5ce43f802d21fe74cde54437db10b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = f205136b2771111650a88c4e480cfe73\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 391e4997dc3a746988cd0e9dceb2d42e\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = bb0ac3251c9e44bfe72bcb8b2e969f0d\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = c796a451486ae8cd6d0dd9bd02a9e235\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a6e81fbc907b11cedc166a7f5b843582\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = d4ded3e2b3703cdb1192f3d689158f82\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 602987c624e8b98603f8b906841eadb1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 2d3185edb0c7b53adc6375ce1392ad59\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 9e9960951042c2f18fd3576739597330\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 4436 BATCH: 1 train_data_count: 4436\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 888.0\n",
      "lif layer 1 self.abs_max_v: 888.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 188.0\n",
      "lif layer 2 self.abs_max_v: 188.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "lif layer 1 self.abs_max_v: 995.0\n",
      "fc layer 2 self.abs_max_out: 394.0\n",
      "lif layer 2 self.abs_max_v: 411.0\n",
      "lif layer 1 self.abs_max_v: 1322.5\n",
      "lif layer 2 self.abs_max_v: 511.5\n",
      "fc layer 2 self.abs_max_out: 442.0\n",
      "lif layer 2 self.abs_max_v: 583.5\n",
      "fc layer 2 self.abs_max_out: 590.0\n",
      "lif layer 2 self.abs_max_v: 802.5\n",
      "fc layer 3 self.abs_max_out: 39.0\n",
      "fc layer 1 self.abs_max_out: 1147.0\n",
      "fc layer 2 self.abs_max_out: 602.0\n",
      "lif layer 2 self.abs_max_v: 851.5\n",
      "fc layer 3 self.abs_max_out: 58.0\n",
      "fc layer 1 self.abs_max_out: 1534.0\n",
      "lif layer 1 self.abs_max_v: 1534.0\n",
      "fc layer 3 self.abs_max_out: 60.0\n",
      "fc layer 1 self.abs_max_out: 1605.0\n",
      "lif layer 1 self.abs_max_v: 1605.0\n",
      "lif layer 2 self.abs_max_v: 878.5\n",
      "fc layer 3 self.abs_max_out: 68.0\n",
      "fc layer 3 self.abs_max_out: 108.0\n",
      "fc layer 3 self.abs_max_out: 114.0\n",
      "lif layer 2 self.abs_max_v: 890.0\n",
      "fc layer 3 self.abs_max_out: 148.0\n",
      "lif layer 2 self.abs_max_v: 900.0\n",
      "fc layer 2 self.abs_max_out: 615.0\n",
      "fc layer 2 self.abs_max_out: 692.0\n",
      "lif layer 2 self.abs_max_v: 1003.0\n",
      "fc layer 1 self.abs_max_out: 1678.0\n",
      "lif layer 1 self.abs_max_v: 1678.0\n",
      "fc layer 2 self.abs_max_out: 785.0\n",
      "lif layer 2 self.abs_max_v: 1153.5\n",
      "fc layer 3 self.abs_max_out: 183.0\n",
      "lif layer 2 self.abs_max_v: 1215.0\n",
      "fc layer 1 self.abs_max_out: 1998.0\n",
      "lif layer 1 self.abs_max_v: 1998.0\n",
      "fc layer 2 self.abs_max_out: 1135.0\n",
      "fc layer 3 self.abs_max_out: 185.0\n",
      "lif layer 2 self.abs_max_v: 1237.5\n",
      "lif layer 2 self.abs_max_v: 1298.0\n",
      "lif layer 2 self.abs_max_v: 1442.0\n",
      "fc layer 1 self.abs_max_out: 2055.0\n",
      "lif layer 1 self.abs_max_v: 2055.0\n",
      "fc layer 1 self.abs_max_out: 2183.0\n",
      "lif layer 1 self.abs_max_v: 2183.0\n",
      "fc layer 3 self.abs_max_out: 208.0\n",
      "fc layer 1 self.abs_max_out: 2329.0\n",
      "lif layer 1 self.abs_max_v: 2329.0\n",
      "fc layer 2 self.abs_max_out: 1137.0\n",
      "fc layer 2 self.abs_max_out: 1238.0\n",
      "fc layer 1 self.abs_max_out: 2551.0\n",
      "lif layer 1 self.abs_max_v: 2551.0\n",
      "fc layer 3 self.abs_max_out: 213.0\n",
      "lif layer 2 self.abs_max_v: 1597.5\n",
      "fc layer 3 self.abs_max_out: 268.0\n",
      "fc layer 2 self.abs_max_out: 1241.0\n",
      "fc layer 2 self.abs_max_out: 1353.0\n",
      "fc layer 2 self.abs_max_out: 1403.0\n",
      "fc layer 2 self.abs_max_out: 1414.0\n",
      "fc layer 2 self.abs_max_out: 1470.0\n",
      "fc layer 2 self.abs_max_out: 1660.0\n",
      "lif layer 2 self.abs_max_v: 1660.0\n",
      "fc layer 3 self.abs_max_out: 281.0\n",
      "fc layer 3 self.abs_max_out: 283.0\n",
      "fc layer 1 self.abs_max_out: 2637.0\n",
      "lif layer 1 self.abs_max_v: 2637.0\n",
      "fc layer 1 self.abs_max_out: 2768.0\n",
      "lif layer 1 self.abs_max_v: 2768.0\n",
      "fc layer 3 self.abs_max_out: 345.0\n",
      "fc layer 1 self.abs_max_out: 2815.0\n",
      "lif layer 1 self.abs_max_v: 2815.0\n",
      "lif layer 2 self.abs_max_v: 1793.5\n",
      "lif layer 2 self.abs_max_v: 1814.0\n",
      "lif layer 2 self.abs_max_v: 1862.0\n",
      "fc layer 1 self.abs_max_out: 2870.0\n",
      "lif layer 1 self.abs_max_v: 2870.0\n",
      "fc layer 1 self.abs_max_out: 2973.0\n",
      "lif layer 1 self.abs_max_v: 2973.0\n",
      "fc layer 1 self.abs_max_out: 3074.0\n",
      "lif layer 1 self.abs_max_v: 3074.0\n",
      "fc layer 1 self.abs_max_out: 3114.0\n",
      "lif layer 1 self.abs_max_v: 3114.0\n",
      "fc layer 2 self.abs_max_out: 1676.0\n",
      "fc layer 2 self.abs_max_out: 1843.0\n",
      "lif layer 2 self.abs_max_v: 1927.0\n",
      "fc layer 3 self.abs_max_out: 362.0\n",
      "fc layer 2 self.abs_max_out: 1878.0\n",
      "fc layer 2 self.abs_max_out: 1889.0\n",
      "fc layer 1 self.abs_max_out: 3232.0\n",
      "lif layer 1 self.abs_max_v: 3232.0\n",
      "fc layer 1 self.abs_max_out: 3240.0\n",
      "lif layer 1 self.abs_max_v: 3240.0\n",
      "fc layer 2 self.abs_max_out: 1937.0\n",
      "lif layer 2 self.abs_max_v: 1937.0\n",
      "fc layer 3 self.abs_max_out: 364.0\n",
      "fc layer 3 self.abs_max_out: 393.0\n",
      "fc layer 1 self.abs_max_out: 3265.0\n",
      "lif layer 1 self.abs_max_v: 3265.0\n",
      "fc layer 3 self.abs_max_out: 404.0\n",
      "fc layer 3 self.abs_max_out: 447.0\n",
      "lif layer 2 self.abs_max_v: 1976.0\n",
      "fc layer 1 self.abs_max_out: 3492.0\n",
      "lif layer 1 self.abs_max_v: 3492.0\n",
      "fc layer 2 self.abs_max_out: 2122.0\n",
      "lif layer 2 self.abs_max_v: 2122.0\n",
      "fc layer 1 self.abs_max_out: 3509.0\n",
      "lif layer 1 self.abs_max_v: 3509.0\n",
      "lif layer 2 self.abs_max_v: 2257.5\n",
      "fc layer 1 self.abs_max_out: 3907.0\n",
      "lif layer 1 self.abs_max_v: 3907.0\n",
      "fc layer 2 self.abs_max_out: 2130.0\n",
      "fc layer 1 self.abs_max_out: 4048.0\n",
      "lif layer 1 self.abs_max_v: 4048.0\n",
      "fc layer 1 self.abs_max_out: 4494.0\n",
      "lif layer 1 self.abs_max_v: 4494.0\n",
      "fc layer 2 self.abs_max_out: 2144.0\n",
      "fc layer 3 self.abs_max_out: 470.0\n",
      "lif layer 2 self.abs_max_v: 2304.5\n",
      "lif layer 2 self.abs_max_v: 2478.5\n",
      "fc layer 2 self.abs_max_out: 2169.0\n",
      "fc layer 2 self.abs_max_out: 2254.0\n",
      "fc layer 2 self.abs_max_out: 2297.0\n",
      "fc layer 2 self.abs_max_out: 2340.0\n",
      "fc layer 2 self.abs_max_out: 2399.0\n",
      "fc layer 1 self.abs_max_out: 4502.0\n",
      "lif layer 1 self.abs_max_v: 4502.0\n",
      "fc layer 2 self.abs_max_out: 2402.0\n",
      "fc layer 2 self.abs_max_out: 2460.0\n",
      "fc layer 1 self.abs_max_out: 4595.0\n",
      "lif layer 1 self.abs_max_v: 4595.0\n",
      "fc layer 1 self.abs_max_out: 4690.0\n",
      "lif layer 1 self.abs_max_v: 4690.0\n",
      "fc layer 1 self.abs_max_out: 5207.0\n",
      "lif layer 1 self.abs_max_v: 5207.0\n",
      "lif layer 2 self.abs_max_v: 2519.5\n",
      "fc layer 2 self.abs_max_out: 2464.0\n",
      "fc layer 2 self.abs_max_out: 2489.0\n",
      "lif layer 2 self.abs_max_v: 2609.5\n",
      "lif layer 2 self.abs_max_v: 2642.0\n",
      "lif layer 2 self.abs_max_v: 2706.0\n",
      "fc layer 2 self.abs_max_out: 2628.0\n",
      "fc layer 1 self.abs_max_out: 5293.0\n",
      "lif layer 1 self.abs_max_v: 5293.0\n",
      "fc layer 2 self.abs_max_out: 2634.0\n",
      "lif layer 2 self.abs_max_v: 2805.0\n",
      "lif layer 2 self.abs_max_v: 2984.0\n",
      "fc layer 1 self.abs_max_out: 5385.0\n",
      "lif layer 1 self.abs_max_v: 5385.0\n",
      "fc layer 2 self.abs_max_out: 2817.0\n",
      "lif layer 2 self.abs_max_v: 3068.5\n",
      "lif layer 2 self.abs_max_v: 3143.0\n",
      "lif layer 2 self.abs_max_v: 3180.5\n",
      "lif layer 2 self.abs_max_v: 3294.0\n",
      "fc layer 1 self.abs_max_out: 5489.0\n",
      "lif layer 1 self.abs_max_v: 5489.0\n",
      "fc layer 1 self.abs_max_out: 5546.0\n",
      "lif layer 1 self.abs_max_v: 5546.0\n",
      "fc layer 1 self.abs_max_out: 5701.0\n",
      "lif layer 1 self.abs_max_v: 5701.0\n",
      "fc layer 1 self.abs_max_out: 5748.0\n",
      "lif layer 1 self.abs_max_v: 5748.0\n",
      "lif layer 2 self.abs_max_v: 3414.0\n",
      "fc layer 2 self.abs_max_out: 2822.0\n",
      "fc layer 1 self.abs_max_out: 5759.0\n",
      "lif layer 1 self.abs_max_v: 5759.0\n",
      "fc layer 1 self.abs_max_out: 5844.0\n",
      "lif layer 1 self.abs_max_v: 5844.0\n",
      "fc layer 1 self.abs_max_out: 5967.0\n",
      "lif layer 1 self.abs_max_v: 5967.0\n",
      "lif layer 1 self.abs_max_v: 6710.0\n",
      "lif layer 1 self.abs_max_v: 7049.5\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  2.030446/  2.128858, val:  50.00%, val_best:  50.00%, tr:  95.11%, tr_best:  95.11%, epoch time: 281.95 seconds, 4.70 minutes\n",
      "total_backward_count 44360 real_backward_count 10732  24.193%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 6081.0\n",
      "fc layer 2 self.abs_max_out: 2889.0\n",
      "fc layer 1 self.abs_max_out: 6459.0\n",
      "fc layer 2 self.abs_max_out: 2944.0\n",
      "lif layer 1 self.abs_max_v: 7092.0\n",
      "fc layer 1 self.abs_max_out: 6652.0\n",
      "fc layer 1 self.abs_max_out: 7009.0\n",
      "lif layer 2 self.abs_max_v: 3430.0\n",
      "lif layer 1 self.abs_max_v: 7572.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  2.026276/  2.122728, val:  48.33%, val_best:  50.00%, tr:  99.41%, tr_best:  99.41%, epoch time: 282.14 seconds, 4.70 minutes\n",
      "total_backward_count 88720 real_backward_count 18156  20.464%\n",
      "fc layer 2 self.abs_max_out: 2961.0\n",
      "fc layer 2 self.abs_max_out: 3014.0\n",
      "lif layer 2 self.abs_max_v: 3437.5\n",
      "fc layer 1 self.abs_max_out: 7270.0\n",
      "fc layer 2 self.abs_max_out: 3060.0\n",
      "lif layer 2 self.abs_max_v: 3557.0\n",
      "lif layer 2 self.abs_max_v: 3581.0\n",
      "fc layer 2 self.abs_max_out: 3134.0\n",
      "fc layer 2 self.abs_max_out: 3139.0\n",
      "fc layer 2 self.abs_max_out: 3196.0\n",
      "fc layer 2 self.abs_max_out: 3201.0\n",
      "fc layer 2 self.abs_max_out: 3242.0\n",
      "fc layer 1 self.abs_max_out: 7536.0\n",
      "fc layer 2 self.abs_max_out: 3255.0\n",
      "fc layer 2 self.abs_max_out: 3257.0\n",
      "fc layer 2 self.abs_max_out: 3539.0\n",
      "lif layer 1 self.abs_max_v: 8843.5\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  2.014283/  2.106475, val:  42.50%, val_best:  50.00%, tr:  99.44%, tr_best:  99.44%, epoch time: 277.70 seconds, 4.63 minutes\n",
      "total_backward_count 133080 real_backward_count 24973  18.765%\n",
      "lif layer 2 self.abs_max_v: 3583.0\n",
      "lif layer 2 self.abs_max_v: 3762.5\n",
      "lif layer 2 self.abs_max_v: 3946.5\n",
      "lif layer 2 self.abs_max_v: 3982.5\n",
      "lif layer 2 self.abs_max_v: 4033.0\n",
      "lif layer 2 self.abs_max_v: 4042.0\n",
      "fc layer 1 self.abs_max_out: 7541.0\n",
      "lif layer 2 self.abs_max_v: 4168.0\n",
      "lif layer 2 self.abs_max_v: 4316.0\n",
      "lif layer 2 self.abs_max_v: 4340.0\n",
      "fc layer 1 self.abs_max_out: 7795.0\n",
      "fc layer 1 self.abs_max_out: 8049.0\n",
      "fc layer 1 self.abs_max_out: 8136.0\n",
      "fc layer 2 self.abs_max_out: 3624.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.996129/  2.083421, val:  46.67%, val_best:  50.00%, tr:  99.71%, tr_best:  99.71%, epoch time: 279.09 seconds, 4.65 minutes\n",
      "total_backward_count 177440 real_backward_count 31286  17.632%\n",
      "lif layer 2 self.abs_max_v: 4346.0\n",
      "lif layer 2 self.abs_max_v: 4365.5\n",
      "fc layer 1 self.abs_max_out: 8248.0\n",
      "lif layer 2 self.abs_max_v: 4579.5\n",
      "lif layer 2 self.abs_max_v: 4707.0\n",
      "fc layer 2 self.abs_max_out: 3729.0\n",
      "fc layer 2 self.abs_max_out: 3742.0\n",
      "fc layer 2 self.abs_max_out: 3757.0\n",
      "lif layer 1 self.abs_max_v: 9370.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.984478/  2.063275, val:  58.33%, val_best:  58.33%, tr:  99.77%, tr_best:  99.77%, epoch time: 279.71 seconds, 4.66 minutes\n",
      "total_backward_count 221800 real_backward_count 37463  16.890%\n",
      "fc layer 2 self.abs_max_out: 3830.0\n",
      "fc layer 2 self.abs_max_out: 3859.0\n",
      "fc layer 2 self.abs_max_out: 3924.0\n",
      "fc layer 1 self.abs_max_out: 8455.0\n",
      "fc layer 1 self.abs_max_out: 8553.0\n",
      "fc layer 2 self.abs_max_out: 4232.0\n",
      "lif layer 1 self.abs_max_v: 9554.5\n",
      "lif layer 1 self.abs_max_v: 10440.5\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.969385/  2.045572, val:  71.25%, val_best:  71.25%, tr:  99.84%, tr_best:  99.84%, epoch time: 274.47 seconds, 4.57 minutes\n",
      "total_backward_count 266160 real_backward_count 43395  16.304%\n",
      "fc layer 1 self.abs_max_out: 8673.0\n",
      "fc layer 1 self.abs_max_out: 8709.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.961374/  2.049422, val:  58.75%, val_best:  71.25%, tr:  99.86%, tr_best:  99.86%, epoch time: 277.32 seconds, 4.62 minutes\n",
      "total_backward_count 310520 real_backward_count 49160  15.832%\n",
      "fc layer 1 self.abs_max_out: 8768.0\n",
      "fc layer 1 self.abs_max_out: 8970.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.963430/  2.059689, val:  55.42%, val_best:  71.25%, tr:  99.80%, tr_best:  99.86%, epoch time: 277.65 seconds, 4.63 minutes\n",
      "total_backward_count 354880 real_backward_count 54772  15.434%\n",
      "lif layer 2 self.abs_max_v: 4761.5\n",
      "fc layer 1 self.abs_max_out: 8984.0\n",
      "lif layer 1 self.abs_max_v: 10973.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.958185/  2.039386, val:  63.33%, val_best:  71.25%, tr:  99.86%, tr_best:  99.86%, epoch time: 277.71 seconds, 4.63 minutes\n",
      "total_backward_count 399240 real_backward_count 60012  15.032%\n",
      "lif layer 2 self.abs_max_v: 5211.5\n",
      "lif layer 2 self.abs_max_v: 5377.0\n",
      "fc layer 2 self.abs_max_out: 4292.0\n",
      "fc layer 1 self.abs_max_out: 9013.0\n",
      "lif layer 1 self.abs_max_v: 11330.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.939663/  2.019972, val:  62.50%, val_best:  71.25%, tr:  99.89%, tr_best:  99.89%, epoch time: 278.47 seconds, 4.64 minutes\n",
      "total_backward_count 443600 real_backward_count 65147  14.686%\n",
      "fc layer 1 self.abs_max_out: 9103.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.936870/  2.030324, val:  59.58%, val_best:  71.25%, tr:  99.91%, tr_best:  99.91%, epoch time: 274.59 seconds, 4.58 minutes\n",
      "total_backward_count 487960 real_backward_count 70117  14.369%\n",
      "fc layer 2 self.abs_max_out: 4316.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.934487/  2.018858, val:  70.83%, val_best:  71.25%, tr:  99.95%, tr_best:  99.95%, epoch time: 277.61 seconds, 4.63 minutes\n",
      "total_backward_count 532320 real_backward_count 74838  14.059%\n",
      "fc layer 1 self.abs_max_out: 9108.0\n",
      "lif layer 1 self.abs_max_v: 11528.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.930435/  2.025042, val:  75.00%, val_best:  75.00%, tr:  99.93%, tr_best:  99.95%, epoch time: 278.54 seconds, 4.64 minutes\n",
      "total_backward_count 576680 real_backward_count 79412  13.771%\n",
      "fc layer 1 self.abs_max_out: 9216.0\n",
      "fc layer 1 self.abs_max_out: 9226.0\n",
      "lif layer 1 self.abs_max_v: 11806.5\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.917254/  2.004535, val:  75.83%, val_best:  75.83%, tr:  99.91%, tr_best:  99.95%, epoch time: 277.58 seconds, 4.63 minutes\n",
      "total_backward_count 621040 real_backward_count 84033  13.531%\n",
      "fc layer 3 self.abs_max_out: 499.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.916363/  1.993599, val:  63.75%, val_best:  75.83%, tr:  99.93%, tr_best:  99.95%, epoch time: 276.53 seconds, 4.61 minutes\n",
      "total_backward_count 665400 real_backward_count 88311  13.272%\n",
      "fc layer 1 self.abs_max_out: 9324.0\n",
      "fc layer 1 self.abs_max_out: 9370.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.894829/  1.980735, val:  77.50%, val_best:  77.50%, tr:  99.98%, tr_best:  99.98%, epoch time: 278.87 seconds, 4.65 minutes\n",
      "total_backward_count 709760 real_backward_count 92573  13.043%\n",
      "fc layer 1 self.abs_max_out: 9398.0\n",
      "fc layer 1 self.abs_max_out: 9400.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.890803/  1.972351, val:  79.58%, val_best:  79.58%, tr:  99.91%, tr_best:  99.98%, epoch time: 279.57 seconds, 4.66 minutes\n",
      "total_backward_count 754120 real_backward_count 96750  12.830%\n",
      "fc layer 1 self.abs_max_out: 9430.0\n",
      "lif layer 1 self.abs_max_v: 12013.5\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.883716/  1.976509, val:  82.50%, val_best:  82.50%, tr:  99.98%, tr_best:  99.98%, epoch time: 273.21 seconds, 4.55 minutes\n",
      "total_backward_count 798480 real_backward_count 100828  12.627%\n",
      "fc layer 1 self.abs_max_out: 9448.0\n",
      "lif layer 1 self.abs_max_v: 12817.5\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.869468/  1.987963, val:  59.58%, val_best:  82.50%, tr:  99.95%, tr_best:  99.98%, epoch time: 277.83 seconds, 4.63 minutes\n",
      "total_backward_count 842840 real_backward_count 104654  12.417%\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.870067/  1.973281, val:  82.08%, val_best:  82.50%, tr:  99.95%, tr_best:  99.98%, epoch time: 278.26 seconds, 4.64 minutes\n",
      "total_backward_count 887200 real_backward_count 108418  12.220%\n",
      "fc layer 1 self.abs_max_out: 9458.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.872661/  1.954387, val:  77.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.64 seconds, 4.64 minutes\n",
      "total_backward_count 931560 real_backward_count 112049  12.028%\n",
      "fc layer 1 self.abs_max_out: 9490.0\n",
      "fc layer 2 self.abs_max_out: 4361.0\n",
      "lif layer 1 self.abs_max_v: 12829.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.864586/  1.946075, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.01 seconds, 4.63 minutes\n",
      "total_backward_count 975920 real_backward_count 115619  11.847%\n",
      "fc layer 1 self.abs_max_out: 9580.0\n",
      "fc layer 2 self.abs_max_out: 4394.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.858989/  1.945681, val:  80.42%, val_best:  83.75%, tr:  99.98%, tr_best: 100.00%, epoch time: 274.86 seconds, 4.58 minutes\n",
      "total_backward_count 1020280 real_backward_count 119192  11.682%\n",
      "fc layer 3 self.abs_max_out: 525.0\n",
      "lif layer 1 self.abs_max_v: 13194.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.845338/  1.940202, val:  83.75%, val_best:  83.75%, tr:  99.98%, tr_best: 100.00%, epoch time: 279.41 seconds, 4.66 minutes\n",
      "total_backward_count 1064640 real_backward_count 122655  11.521%\n",
      "fc layer 3 self.abs_max_out: 533.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.840635/  1.931684, val:  86.67%, val_best:  86.67%, tr:  99.98%, tr_best: 100.00%, epoch time: 279.19 seconds, 4.65 minutes\n",
      "total_backward_count 1109000 real_backward_count 126025  11.364%\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.844926/  1.924287, val:  85.00%, val_best:  86.67%, tr:  99.98%, tr_best: 100.00%, epoch time: 275.86 seconds, 4.60 minutes\n",
      "total_backward_count 1153360 real_backward_count 129388  11.218%\n",
      "fc layer 1 self.abs_max_out: 9648.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.832300/  1.934485, val:  80.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.74 seconds, 4.63 minutes\n",
      "total_backward_count 1197720 real_backward_count 132588  11.070%\n",
      "fc layer 1 self.abs_max_out: 9654.0\n",
      "lif layer 1 self.abs_max_v: 13309.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.839807/  1.943866, val:  75.00%, val_best:  86.67%, tr:  99.95%, tr_best: 100.00%, epoch time: 277.90 seconds, 4.63 minutes\n",
      "total_backward_count 1242080 real_backward_count 135681  10.924%\n",
      "fc layer 3 self.abs_max_out: 538.0\n",
      "fc layer 1 self.abs_max_out: 9707.0\n",
      "lif layer 2 self.abs_max_v: 5492.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.833643/  1.923777, val:  82.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.29 seconds, 4.59 minutes\n",
      "total_backward_count 1286440 real_backward_count 138792  10.789%\n",
      "fc layer 1 self.abs_max_out: 9714.0\n",
      "lif layer 1 self.abs_max_v: 13311.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.820734/  1.930835, val:  81.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.15 seconds, 4.60 minutes\n",
      "total_backward_count 1330800 real_backward_count 141771  10.653%\n",
      "fc layer 1 self.abs_max_out: 9766.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.828467/  1.929444, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.29 seconds, 4.64 minutes\n",
      "total_backward_count 1375160 real_backward_count 144769  10.527%\n",
      "fc layer 1 self.abs_max_out: 9802.0\n",
      "fc layer 3 self.abs_max_out: 552.0\n",
      "lif layer 2 self.abs_max_v: 5554.5\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.824034/  1.907594, val:  82.92%, val_best:  86.67%, tr:  99.98%, tr_best: 100.00%, epoch time: 279.73 seconds, 4.66 minutes\n",
      "total_backward_count 1419520 real_backward_count 147728  10.407%\n",
      "fc layer 1 self.abs_max_out: 9863.0\n",
      "lif layer 2 self.abs_max_v: 5925.5\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.808847/  1.900852, val:  85.00%, val_best:  86.67%, tr:  99.98%, tr_best: 100.00%, epoch time: 280.11 seconds, 4.67 minutes\n",
      "total_backward_count 1463880 real_backward_count 150626  10.290%\n",
      "fc layer 1 self.abs_max_out: 9882.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.804482/  1.905785, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.33 seconds, 4.62 minutes\n",
      "total_backward_count 1508240 real_backward_count 153404  10.171%\n",
      "fc layer 2 self.abs_max_out: 4477.0\n",
      "fc layer 1 self.abs_max_out: 9913.0\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.805869/  1.903710, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.57 seconds, 4.58 minutes\n",
      "total_backward_count 1552600 real_backward_count 156096  10.054%\n",
      "fc layer 1 self.abs_max_out: 9957.0\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.800072/  1.907629, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.59 seconds, 4.66 minutes\n",
      "total_backward_count 1596960 real_backward_count 158767   9.942%\n",
      "fc layer 1 self.abs_max_out: 9979.0\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.791402/  1.883500, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.15 seconds, 4.67 minutes\n",
      "total_backward_count 1641320 real_backward_count 161345   9.830%\n",
      "fc layer 1 self.abs_max_out: 10009.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.797036/  1.903600, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.29 seconds, 4.60 minutes\n",
      "total_backward_count 1685680 real_backward_count 164061   9.733%\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.793732/  1.901466, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.79 seconds, 4.63 minutes\n",
      "total_backward_count 1730040 real_backward_count 166703   9.636%\n",
      "fc layer 1 self.abs_max_out: 10055.0\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.793256/  1.888073, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.70 seconds, 4.63 minutes\n",
      "total_backward_count 1774400 real_backward_count 169263   9.539%\n",
      "fc layer 1 self.abs_max_out: 10088.0\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.790463/  1.895525, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.58 seconds, 4.58 minutes\n",
      "total_backward_count 1818760 real_backward_count 171769   9.444%\n",
      "fc layer 1 self.abs_max_out: 10112.0\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.786629/  1.889041, val:  77.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.91 seconds, 4.67 minutes\n",
      "total_backward_count 1863120 real_backward_count 174199   9.350%\n",
      "fc layer 1 self.abs_max_out: 10153.0\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.773048/  1.870124, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.21 seconds, 4.67 minutes\n",
      "total_backward_count 1907480 real_backward_count 176628   9.260%\n",
      "fc layer 3 self.abs_max_out: 558.0\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.760145/  1.864486, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.06 seconds, 4.63 minutes\n",
      "total_backward_count 1951840 real_backward_count 179010   9.171%\n",
      "fc layer 2 self.abs_max_out: 4495.0\n",
      "fc layer 1 self.abs_max_out: 10176.0\n",
      "lif layer 1 self.abs_max_v: 13535.0\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.760466/  1.858575, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.21 seconds, 4.67 minutes\n",
      "total_backward_count 1996200 real_backward_count 181388   9.087%\n",
      "fc layer 3 self.abs_max_out: 565.0\n",
      "fc layer 1 self.abs_max_out: 10191.0\n",
      "fc layer 3 self.abs_max_out: 567.0\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.752883/  1.867819, val:  85.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.75 seconds, 4.65 minutes\n",
      "total_backward_count 2040560 real_backward_count 183699   9.002%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.759778/  1.865377, val:  84.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.42 seconds, 4.62 minutes\n",
      "total_backward_count 2084920 real_backward_count 185906   8.917%\n",
      "fc layer 1 self.abs_max_out: 10241.0\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.755220/  1.868766, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.75 seconds, 4.68 minutes\n",
      "total_backward_count 2129280 real_backward_count 188190   8.838%\n",
      "fc layer 3 self.abs_max_out: 569.0\n",
      "fc layer 3 self.abs_max_out: 604.0\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.751019/  1.863354, val:  77.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.79 seconds, 4.65 minutes\n",
      "total_backward_count 2173640 real_backward_count 190411   8.760%\n",
      "fc layer 1 self.abs_max_out: 10281.0\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.736678/  1.839069, val:  90.00%, val_best:  90.83%, tr:  99.98%, tr_best: 100.00%, epoch time: 276.34 seconds, 4.61 minutes\n",
      "total_backward_count 2218000 real_backward_count 192504   8.679%\n",
      "fc layer 1 self.abs_max_out: 10294.0\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.739521/  1.861605, val:  84.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.62 seconds, 4.66 minutes\n",
      "total_backward_count 2262360 real_backward_count 194619   8.602%\n",
      "fc layer 1 self.abs_max_out: 10330.0\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.727183/  1.853424, val:  85.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.30 seconds, 4.66 minutes\n",
      "total_backward_count 2306720 real_backward_count 196675   8.526%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.727591/  1.833736, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.79 seconds, 4.56 minutes\n",
      "total_backward_count 2351080 real_backward_count 198761   8.454%\n",
      "fc layer 1 self.abs_max_out: 10361.0\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.736430/  1.854976, val:  86.25%, val_best:  90.83%, tr:  99.98%, tr_best: 100.00%, epoch time: 279.88 seconds, 4.66 minutes\n",
      "total_backward_count 2395440 real_backward_count 200842   8.384%\n",
      "fc layer 1 self.abs_max_out: 10390.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.740985/  1.850273, val:  84.58%, val_best:  90.83%, tr:  99.98%, tr_best: 100.00%, epoch time: 280.01 seconds, 4.67 minutes\n",
      "total_backward_count 2439800 real_backward_count 202818   8.313%\n",
      "fc layer 1 self.abs_max_out: 10414.0\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.735166/  1.831500, val:  89.17%, val_best:  90.83%, tr:  99.98%, tr_best: 100.00%, epoch time: 278.99 seconds, 4.65 minutes\n",
      "total_backward_count 2484160 real_backward_count 204869   8.247%\n",
      "fc layer 1 self.abs_max_out: 10423.0\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.726344/  1.832367, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.95 seconds, 4.68 minutes\n",
      "total_backward_count 2528520 real_backward_count 206737   8.176%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.735794/  1.833349, val:  89.58%, val_best:  90.83%, tr:  99.98%, tr_best: 100.00%, epoch time: 277.01 seconds, 4.62 minutes\n",
      "total_backward_count 2572880 real_backward_count 208611   8.108%\n",
      "fc layer 1 self.abs_max_out: 10436.0\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.729542/  1.833344, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.86 seconds, 4.61 minutes\n",
      "total_backward_count 2617240 real_backward_count 210449   8.041%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.720603/  1.830707, val:  83.75%, val_best:  90.83%, tr:  99.98%, tr_best: 100.00%, epoch time: 279.77 seconds, 4.66 minutes\n",
      "total_backward_count 2661600 real_backward_count 212291   7.976%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.725134/  1.832230, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.74 seconds, 4.60 minutes\n",
      "total_backward_count 2705960 real_backward_count 214132   7.913%\n",
      "fc layer 3 self.abs_max_out: 631.0\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.716789/  1.834330, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.64 seconds, 4.64 minutes\n",
      "total_backward_count 2750320 real_backward_count 215955   7.852%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.705912/  1.816880, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.55 seconds, 4.66 minutes\n",
      "total_backward_count 2794680 real_backward_count 217769   7.792%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.703511/  1.815774, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.23 seconds, 4.64 minutes\n",
      "total_backward_count 2839040 real_backward_count 219541   7.733%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.699449/  1.828460, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.99 seconds, 4.60 minutes\n",
      "total_backward_count 2883400 real_backward_count 221224   7.672%\n",
      "fc layer 1 self.abs_max_out: 10460.0\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.711682/  1.826776, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.69 seconds, 4.63 minutes\n",
      "total_backward_count 2927760 real_backward_count 222932   7.614%\n",
      "fc layer 1 self.abs_max_out: 10483.0\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.704646/  1.813781, val:  85.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.71 seconds, 4.63 minutes\n",
      "total_backward_count 2972120 real_backward_count 224643   7.558%\n",
      "fc layer 1 self.abs_max_out: 10510.0\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.703992/  1.824887, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.78 seconds, 4.65 minutes\n",
      "total_backward_count 3016480 real_backward_count 226278   7.501%\n",
      "fc layer 1 self.abs_max_out: 10528.0\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.703399/  1.819497, val:  82.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.13 seconds, 4.65 minutes\n",
      "total_backward_count 3060840 real_backward_count 227860   7.444%\n",
      "fc layer 1 self.abs_max_out: 10555.0\n",
      "fc layer 3 self.abs_max_out: 652.0\n",
      "fc layer 2 self.abs_max_out: 4554.0\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.697073/  1.819558, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.87 seconds, 4.61 minutes\n",
      "total_backward_count 3105200 real_backward_count 229581   7.393%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.707574/  1.827718, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.63 seconds, 4.66 minutes\n",
      "total_backward_count 3149560 real_backward_count 231126   7.338%\n",
      "fc layer 3 self.abs_max_out: 654.0\n",
      "fc layer 1 self.abs_max_out: 10588.0\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.700395/  1.812857, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.50 seconds, 4.64 minutes\n",
      "total_backward_count 3193920 real_backward_count 232611   7.283%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.698821/  1.834690, val:  80.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.63 seconds, 4.63 minutes\n",
      "total_backward_count 3238280 real_backward_count 234207   7.232%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.693062/  1.814511, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.74 seconds, 4.68 minutes\n",
      "total_backward_count 3282640 real_backward_count 235766   7.182%\n",
      "fc layer 3 self.abs_max_out: 657.0\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.686703/  1.819672, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.82 seconds, 4.63 minutes\n",
      "total_backward_count 3327000 real_backward_count 237285   7.132%\n",
      "fc layer 3 self.abs_max_out: 671.0\n",
      "fc layer 2 self.abs_max_out: 4585.0\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.693700/  1.819293, val:  84.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.16 seconds, 4.57 minutes\n",
      "total_backward_count 3371360 real_backward_count 238848   7.085%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.694968/  1.819083, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.37 seconds, 4.64 minutes\n",
      "total_backward_count 3415720 real_backward_count 240310   7.035%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.688056/  1.809325, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.20 seconds, 4.65 minutes\n",
      "total_backward_count 3460080 real_backward_count 241834   6.989%\n",
      "lif layer 1 self.abs_max_v: 13625.0\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.684621/  1.808644, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.79 seconds, 4.68 minutes\n",
      "total_backward_count 3504440 real_backward_count 243252   6.941%\n",
      "fc layer 1 self.abs_max_out: 10615.0\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.683495/  1.808787, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.62 seconds, 4.61 minutes\n",
      "total_backward_count 3548800 real_backward_count 244641   6.894%\n",
      "fc layer 2 self.abs_max_out: 4626.0\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.679060/  1.800781, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.64 seconds, 4.64 minutes\n",
      "total_backward_count 3593160 real_backward_count 246002   6.846%\n",
      "fc layer 2 self.abs_max_out: 4788.0\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.666934/  1.799335, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.03 seconds, 4.58 minutes\n",
      "total_backward_count 3637520 real_backward_count 247344   6.800%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.668418/  1.793268, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.11 seconds, 4.65 minutes\n",
      "total_backward_count 3681880 real_backward_count 248729   6.755%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.668491/  1.804348, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.57 seconds, 4.63 minutes\n",
      "total_backward_count 3726240 real_backward_count 250102   6.712%\n",
      "fc layer 1 self.abs_max_out: 10630.0\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.672275/  1.798333, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.48 seconds, 4.64 minutes\n",
      "total_backward_count 3770600 real_backward_count 251459   6.669%\n",
      "fc layer 1 self.abs_max_out: 10652.0\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.672146/  1.786027, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.46 seconds, 4.67 minutes\n",
      "total_backward_count 3814960 real_backward_count 252746   6.625%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.661944/  1.782199, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.14 seconds, 4.65 minutes\n",
      "total_backward_count 3859320 real_backward_count 254110   6.584%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.663421/  1.793780, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.72 seconds, 4.56 minutes\n",
      "total_backward_count 3903680 real_backward_count 255377   6.542%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.662583/  1.783473, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.99 seconds, 4.65 minutes\n",
      "total_backward_count 3948040 real_backward_count 256725   6.503%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.652490/  1.772897, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.91 seconds, 4.65 minutes\n",
      "total_backward_count 3992400 real_backward_count 257986   6.462%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.655710/  1.786664, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.34 seconds, 4.64 minutes\n",
      "total_backward_count 4036760 real_backward_count 259252   6.422%\n",
      "fc layer 1 self.abs_max_out: 10668.0\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.645727/  1.772370, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.15 seconds, 4.65 minutes\n",
      "total_backward_count 4081120 real_backward_count 260427   6.381%\n",
      "fc layer 1 self.abs_max_out: 10695.0\n",
      "fc layer 3 self.abs_max_out: 673.0\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.647183/  1.783412, val:  83.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.01 seconds, 4.65 minutes\n",
      "total_backward_count 4125480 real_backward_count 261715   6.344%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.647831/  1.778912, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.40 seconds, 4.59 minutes\n",
      "total_backward_count 4169840 real_backward_count 262946   6.306%\n",
      "fc layer 1 self.abs_max_out: 10702.0\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.650720/  1.783439, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.24 seconds, 4.64 minutes\n",
      "total_backward_count 4214200 real_backward_count 264138   6.268%\n",
      "fc layer 1 self.abs_max_out: 10732.0\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.654350/  1.790099, val:  81.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.97 seconds, 4.63 minutes\n",
      "total_backward_count 4258560 real_backward_count 265300   6.230%\n",
      "fc layer 1 self.abs_max_out: 10788.0\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.645629/  1.777260, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.76 seconds, 4.70 minutes\n",
      "total_backward_count 4302920 real_backward_count 266449   6.192%\n",
      "fc layer 1 self.abs_max_out: 10800.0\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.643622/  1.777722, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.11 seconds, 4.67 minutes\n",
      "total_backward_count 4347280 real_backward_count 267655   6.157%\n",
      "fc layer 1 self.abs_max_out: 10828.0\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.648295/  1.780589, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.50 seconds, 4.66 minutes\n",
      "total_backward_count 4391640 real_backward_count 268835   6.122%\n",
      "fc layer 3 self.abs_max_out: 676.0\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.646717/  1.791154, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.24 seconds, 4.62 minutes\n",
      "total_backward_count 4436000 real_backward_count 269995   6.086%\n",
      "fc layer 1 self.abs_max_out: 10829.0\n",
      "fc layer 3 self.abs_max_out: 711.0\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.641953/  1.767032, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.23 seconds, 4.64 minutes\n",
      "total_backward_count 4480360 real_backward_count 271143   6.052%\n",
      "fc layer 1 self.abs_max_out: 10843.0\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.632025/  1.766737, val:  90.42%, val_best:  91.25%, tr:  99.98%, tr_best: 100.00%, epoch time: 280.32 seconds, 4.67 minutes\n",
      "total_backward_count 4524720 real_backward_count 272268   6.017%\n",
      "fc layer 3 self.abs_max_out: 729.0\n",
      "fc layer 1 self.abs_max_out: 10865.0\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.637437/  1.775954, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.77 seconds, 4.66 minutes\n",
      "total_backward_count 4569080 real_backward_count 273420   5.984%\n",
      "fc layer 1 self.abs_max_out: 10870.0\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.637971/  1.778288, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.89 seconds, 4.66 minutes\n",
      "total_backward_count 4613440 real_backward_count 274537   5.951%\n",
      "fc layer 1 self.abs_max_out: 10872.0\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.637002/  1.768902, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.78 seconds, 4.65 minutes\n",
      "total_backward_count 4657800 real_backward_count 275616   5.917%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.632469/  1.778350, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.66 seconds, 4.63 minutes\n",
      "total_backward_count 4702160 real_backward_count 276745   5.885%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.638541/  1.764087, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 278.69 seconds, 4.64 minutes\n",
      "total_backward_count 4746520 real_backward_count 277817   5.853%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.630698/  1.760681, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.09 seconds, 4.67 minutes\n",
      "total_backward_count 4790880 real_backward_count 278955   5.823%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.624534/  1.762094, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.05 seconds, 4.67 minutes\n",
      "total_backward_count 4835240 real_backward_count 280041   5.792%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.629726/  1.769843, val:  85.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 279.52 seconds, 4.66 minutes\n",
      "total_backward_count 4879600 real_backward_count 281150   5.762%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.628603/  1.774495, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.03 seconds, 4.60 minutes\n",
      "total_backward_count 4923960 real_backward_count 282179   5.731%\n"
     ]
    }
   ],
   "source": [
    "# sweep 하는 코드, 위 셀 주석처리 해야 됨.\n",
    "\n",
    "# 이런 워닝 뜨는 거는 걍 너가 main 안에서  wandb.config.update(hyperparameters)할 때 물려서임. 어차피 근데 sweep에서 지정한 걸로 덮어짐 \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'random', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        # \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [6.0]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "        \"learning_rate\": {\"values\": [1/512]}, \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [14]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [9]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        \"scale_exp_2w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        \"scale_exp_3w\": {\"values\": [-9]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"4\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_2w,wandb.config.scale_exp_2w],[wandb.config.scale_exp_3w,wandb.config.scale_exp_3w]],\n",
    "                        ) \n",
    "    # sigmoid와 BN이 있어야 잘된다.\n",
    "    # average pooling\n",
    "    # 이 낫다. \n",
    "    \n",
    "    # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'jwru0k4o'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
